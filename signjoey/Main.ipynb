{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **SIGN-IDD SLT**"
      ],
      "metadata": {
        "id": "A6-eqEa_7mGX"
      },
      "id": "A6-eqEa_7mGX"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initial imports and configuration"
      ],
      "metadata": {
        "id": "JZqEhda67zn0"
      },
      "id": "JZqEhda67zn0"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yUCMD7oNvDQA",
        "outputId": "a7e88f46-9439-4f1d-d78a-b95ec04a3b53"
      },
      "id": "yUCMD7oNvDQA",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/Sign-IDD SLT/signjoey')"
      ],
      "metadata": {
        "id": "Mv2X_Zyqwu7R"
      },
      "id": "Mv2X_Zyqwu7R",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7abbf6bf",
      "metadata": {
        "id": "7abbf6bf"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import os\n",
        "import shutil\n",
        "import time\n",
        "import queue"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install ipdb\n",
        "%pip install portalocker\n",
        "%pip install tensorboardX"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "agU-v-OoxAVU",
        "outputId": "7beb2844-8df7-48ba-eac7-4e49cc5a6a07"
      },
      "id": "agU-v-OoxAVU",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ipdb in /usr/local/lib/python3.11/dist-packages (0.13.13)\n",
            "Requirement already satisfied: ipython>=7.31.1 in /usr/local/lib/python3.11/dist-packages (from ipdb) (7.34.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ipdb) (4.4.2)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.11/dist-packages (from ipython>=7.31.1->ipdb) (75.2.0)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.11/dist-packages (from ipython>=7.31.1->ipdb) (0.19.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from ipython>=7.31.1->ipdb) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.11/dist-packages (from ipython>=7.31.1->ipdb) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ipython>=7.31.1->ipdb) (3.0.51)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.11/dist-packages (from ipython>=7.31.1->ipdb) (2.19.2)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from ipython>=7.31.1->ipdb) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.11/dist-packages (from ipython>=7.31.1->ipdb) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython>=7.31.1->ipdb) (4.9.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython>=7.31.1->ipdb) (0.8.4)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->ipython>=7.31.1->ipdb) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=7.31.1->ipdb) (0.2.13)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.11/dist-packages (3.2.0)\n",
            "Requirement already satisfied: tensorboardX in /usr/local/lib/python3.11/dist-packages (2.6.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from tensorboardX) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorboardX) (24.2)\n",
            "Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.11/dist-packages (from tensorboardX) (5.29.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e716a202",
      "metadata": {
        "id": "e716a202"
      },
      "outputs": [],
      "source": [
        "from model import build_model,SignModel\n",
        "from batch import Batch\n",
        "from helpers import (\n",
        "    log_data_info,\n",
        "    load_config,\n",
        "    log_cfg,\n",
        "    load_checkpoint,\n",
        "    make_model_dir,\n",
        "    make_logger,\n",
        "    set_seed,\n",
        "    symlink_update,\n",
        ")\n",
        "from prediction import validate_on_data\n",
        "from loss import XentLoss\n",
        "from data import load_data, make_data_iter\n",
        "from builders import build_optimizer, build_scheduler, build_gradient_clipper\n",
        "from prediction import test\n",
        "from metrics import wer_single\n",
        "from vocabulary import SIL_TOKEN\n",
        "from torch import Tensor\n",
        "from tensorboardX import SummaryWriter\n",
        "from torchtext_compat import Dataset, BucketIterator\n",
        "from typing import List, Dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac94a384",
      "metadata": {
        "id": "ac94a384"
      },
      "outputs": [],
      "source": [
        "device= torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "354db4e4",
      "metadata": {
        "id": "354db4e4"
      },
      "outputs": [],
      "source": [
        "config_path = \"/content/drive/MyDrive/Sign-IDD SLT/configs/sign.yaml\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "93cd6e13",
      "metadata": {
        "id": "93cd6e13"
      },
      "source": [
        "# Dataset creation\n",
        "- Input data is a list dictionaries containing\n",
        "  - Name\n",
        "  - Signer\n",
        "  - Sign\n",
        "  - Gloss\n",
        "  - Text\n",
        "- The 4 fields except sign are taken from the csv file in phoenix annotations\n",
        "- Sign is taken from the .skels file\n",
        "- These are combined for each sample and stored as a dictionary\n",
        "- We have 5 dictionaries (5 samples) each in train, dev and test input files"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7636c7c0",
      "metadata": {
        "id": "7636c7c0"
      },
      "source": [
        "### Train set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "faaaff92",
      "metadata": {
        "id": "faaaff92",
        "outputId": "ed7d7923-372d-492e-cd48-00105a0dec48"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved 5 samples to ../data/PHOENIX2014T/phoenix14t.skels.train\n"
          ]
        }
      ],
      "source": [
        "import gzip\n",
        "import joblib\n",
        "import pandas as pd\n",
        "\n",
        "# Config\n",
        "feature_dim = 150\n",
        "input_csv = \"../data/PHOENIX2014T/Train/train.csv\"\n",
        "input_skels = \"../data/PHOENIX2014T/Train/train.skels\"\n",
        "output_dir = \"../data/PHOENIX2014T\"\n",
        "output_file = os.path.join(output_dir, \"phoenix14t.skels.train\")\n",
        "\n",
        "# Ensure output directory exists\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Load metadata CSV\n",
        "df = pd.read_csv(input_csv, sep=\"|\")\n",
        "assert len(df) == 5, \"Expected 5 samples in train.csv\"\n",
        "\n",
        "# Read skeleton lines\n",
        "with open(input_skels, \"r\") as f:\n",
        "    skel_lines = [line.strip() for line in f.readlines()]\n",
        "\n",
        "assert len(skel_lines) == len(df), \"Mismatch between CSV and skels line count\"\n",
        "\n",
        "samples = []\n",
        "\n",
        "for idx, (line, meta) in enumerate(zip(skel_lines, df.itertuples())):\n",
        "    raw = list(map(float, line.split()))\n",
        "    assert len(raw) % (feature_dim + 1) == 0, f\"Line {idx} isn't multiple of {feature_dim+1}\"\n",
        "\n",
        "    num_frames = len(raw) // (feature_dim + 1)\n",
        "    frames = []\n",
        "    for i in range(num_frames):\n",
        "        frame_vec = raw[i * (feature_dim + 1) : i * (feature_dim + 1) + feature_dim]\n",
        "        frames.append(torch.tensor(frame_vec, dtype=torch.float32))\n",
        "\n",
        "    sign_tensor = torch.stack(frames, dim=0)  # [T, 150]\n",
        "\n",
        "    sample = {\n",
        "        \"name\": meta.name,\n",
        "        \"signer\": meta.speaker,\n",
        "        \"sign\": sign_tensor + 1e-8,  # small value added for numerical stability\n",
        "        \"gloss\": meta.orth.strip(),\n",
        "        \"text\": meta.translation.strip(),\n",
        "    }\n",
        "    samples.append(sample)\n",
        "\n",
        "# Save as joblib file in gzip\n",
        "with gzip.open(output_file, \"wb\") as f:\n",
        "    joblib.dump(samples, f)\n",
        "\n",
        "print(f\"Saved {len(samples)} samples to {output_file}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ad400e8",
      "metadata": {
        "id": "9ad400e8",
        "outputId": "b8264535-0397-4108-ed2b-7296405aa854",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: 11August_2010_Wednesday_tagesschau-1\n",
            "Signer: Signer08\n",
            "Gloss: JETZT WETTER MORGEN DONNERSTAG ZWOELF FEBRUAR\n",
            "Text: und nun die wettervorhersage für morgen donnerstag den zwölften august\n",
            "Sign shape: torch.Size([86, 150])\n",
            "Sign (first frame): tensor([-5.6500e-03, -2.0610e-01,  5.5001e-04,  1.0000e-08,  1.0000e-08,\n",
            "         1.0000e-08,  1.6743e-01,  2.9120e-02, -1.5999e-04,  2.5389e-01,\n",
            "         2.6071e-01,  1.6992e-01,  2.7647e-01,  3.3896e-01,  3.6781e-01,\n",
            "        -1.6580e-01,  3.7330e-02,  6.0010e-05, -2.7147e-01,  3.1816e-01,\n",
            "        -1.7999e-04, -1.8947e-01,  2.6242e-01,  1.8945e-01, -1.8716e-01,\n",
            "         2.2865e-01,  1.8961e-01, -1.8512e-01,  2.0045e-01,  1.8967e-01,\n",
            "        -1.6512e-01,  1.6793e-01,  1.8987e-01, -1.4415e-01,  1.6851e-01,\n",
            "         1.8995e-01, -1.2356e-01,  1.4980e-01,  1.9004e-01, -1.7322e-01,\n",
            "         1.4471e-01,  1.8974e-01, -1.5877e-01,  1.0704e-01,  1.8984e-01,\n",
            "        -1.5540e-01,  8.4610e-02,  1.8993e-01, -1.5251e-01,  6.1880e-02,\n",
            "         1.9013e-01, -1.6310e-01,  1.5363e-01,  1.8981e-01, -1.4478e-01,\n",
            "         1.1906e-01,  1.9002e-01, -1.3370e-01,  1.0548e-01,  1.9008e-01,\n",
            "        -1.2615e-01,  8.8370e-02,  1.9016e-01, -1.4999e-01,  1.7275e-01,\n",
            "         1.8981e-01, -1.2830e-01,  1.4292e-01,  1.8990e-01, -1.2013e-01,\n",
            "         1.2778e-01,  1.8994e-01, -1.1726e-01,  1.1131e-01,  1.8998e-01,\n",
            "        -1.4249e-01,  1.8865e-01,  1.8971e-01, -1.2106e-01,  1.7177e-01,\n",
            "         1.8991e-01, -1.0558e-01,  1.6194e-01,  1.9011e-01, -9.2050e-02,\n",
            "         1.5233e-01,  1.9031e-01,  2.7457e-01,  3.0516e-01,  3.6784e-01,\n",
            "         2.7443e-01,  2.7689e-01,  3.6788e-01,  2.6863e-01,  2.3916e-01,\n",
            "         3.6793e-01,  2.6428e-01,  2.1863e-01,  3.6796e-01,  2.5933e-01,\n",
            "         1.9126e-01,  3.6800e-01,  2.5629e-01,  2.2206e-01,  3.6794e-01,\n",
            "         2.4470e-01,  1.8341e-01,  3.6814e-01,  2.4689e-01,  1.6083e-01,\n",
            "         3.6817e-01,  2.4982e-01,  1.3811e-01,  3.6837e-01,  2.5048e-01,\n",
            "         2.3015e-01,  3.6793e-01,  2.3619e-01,  1.9372e-01,  3.6814e-01,\n",
            "         2.2837e-01,  1.7804e-01,  3.6818e-01,  2.2093e-01,  1.6089e-01,\n",
            "         3.6824e-01,  2.4715e-01,  2.4388e-01,  3.6792e-01,  2.3013e-01,\n",
            "         2.1117e-01,  3.6799e-01,  2.2067e-01,  1.9679e-01,  3.6805e-01,\n",
            "         2.2091e-01,  1.8008e-01,  3.6825e-01,  2.4311e-01,  2.5411e-01,\n",
            "         3.6793e-01,  2.2711e-01,  2.3202e-01,  3.6813e-01,  2.1426e-01,\n",
            "         2.1893e-01,  3.6819e-01,  2.0111e-01,  2.0880e-01,  3.6826e-01])\n"
          ]
        }
      ],
      "source": [
        "import gzip\n",
        "import joblib\n",
        "\n",
        "# Load the skels file\n",
        "with gzip.open(\"/content/drive/MyDrive/Sign-IDD SLT/data/PHOENIX2014T/phoenix14t.skels.train\", \"rb\") as f:\n",
        "    data = joblib.load(f)\n",
        "\n",
        "# View one sample (e.g., the first one)\n",
        "sample = data[0]\n",
        "\n",
        "# Print contents\n",
        "print(\"Name:\", sample[\"name\"])\n",
        "print(\"Signer:\", sample[\"signer\"])\n",
        "print(\"Gloss:\", sample[\"gloss\"])\n",
        "print(\"Text:\", sample[\"text\"])\n",
        "print(\"Sign shape:\", sample[\"sign\"].shape)\n",
        "print(\"Sign (first frame):\", sample[\"sign\"][0])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f50f9f3",
      "metadata": {
        "id": "5f50f9f3"
      },
      "source": [
        "### Dev set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f737ef1",
      "metadata": {
        "id": "3f737ef1",
        "outputId": "b4ebc57e-1ab6-47dd-a06d-6d9c2557f487"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved 5 samples to ../data/PHOENIX2014T/phoenix14t.skels.dev\n"
          ]
        }
      ],
      "source": [
        "import gzip\n",
        "import joblib\n",
        "import pandas as pd\n",
        "\n",
        "# Config\n",
        "feature_dim = 150\n",
        "input_csv = \"../data/PHOENIX2014T/Dev/dev.csv\"\n",
        "input_skels = \"../data/PHOENIX2014T/Dev/dev.skels\"\n",
        "output_dir = \"../data/PHOENIX2014T\"\n",
        "output_file = os.path.join(output_dir, \"phoenix14t.skels.dev\")\n",
        "\n",
        "# Ensure output directory exists\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Load metadata CSV\n",
        "df = pd.read_csv(input_csv, sep=\"|\")\n",
        "assert len(df) == 5, \"Expected 5 samples in train.csv\"\n",
        "\n",
        "# Read skeleton lines\n",
        "with open(input_skels, \"r\") as f:\n",
        "    skel_lines = [line.strip() for line in f.readlines()]\n",
        "\n",
        "assert len(skel_lines) == len(df), \"Mismatch between CSV and skels line count\"\n",
        "\n",
        "samples = []\n",
        "\n",
        "for idx, (line, meta) in enumerate(zip(skel_lines, df.itertuples())):\n",
        "    raw = list(map(float, line.split()))\n",
        "    assert len(raw) % (feature_dim + 1) == 0, f\"Line {idx} isn't multiple of {feature_dim+1}\"\n",
        "\n",
        "    num_frames = len(raw) // (feature_dim + 1)\n",
        "    frames = []\n",
        "    for i in range(num_frames):\n",
        "        frame_vec = raw[i * (feature_dim + 1) : i * (feature_dim + 1) + feature_dim]\n",
        "        frames.append(torch.tensor(frame_vec, dtype=torch.float32))\n",
        "\n",
        "    sign_tensor = torch.stack(frames, dim=0)  # [T, 150]\n",
        "\n",
        "    sample = {\n",
        "        \"name\": meta.name,\n",
        "        \"signer\": meta.speaker,\n",
        "        \"sign\": sign_tensor + 1e-8,  # small value added for numerical stability\n",
        "        \"gloss\": meta.orth.strip(),\n",
        "        \"text\": meta.translation.strip(),\n",
        "    }\n",
        "    samples.append(sample)\n",
        "\n",
        "# Save as joblib file in gzip\n",
        "with gzip.open(output_file, \"wb\") as f:\n",
        "    joblib.dump(samples, f)\n",
        "\n",
        "print(f\"Saved {len(samples)} samples to {output_file}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "90e2870a",
      "metadata": {
        "id": "90e2870a"
      },
      "source": [
        "### Test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec659006",
      "metadata": {
        "id": "ec659006",
        "outputId": "a1a97dd5-d839-461e-cba1-7a5bf44ed82b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved 5 samples to ../data/PHOENIX2014T/phoenix14t.skels.test\n"
          ]
        }
      ],
      "source": [
        "import gzip\n",
        "import joblib\n",
        "import pandas as pd\n",
        "\n",
        "# Config\n",
        "feature_dim = 150\n",
        "input_csv = \"../data/PHOENIX2014T/Test/test.csv\"\n",
        "input_skels = \"../data/PHOENIX2014T/Test/test.skels\"\n",
        "output_dir = \"../data/PHOENIX2014T\"\n",
        "output_file = os.path.join(output_dir, \"phoenix14t.skels.test\")\n",
        "\n",
        "# Ensure output directory exists\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Load metadata CSV\n",
        "df = pd.read_csv(input_csv, sep=\"|\")\n",
        "assert len(df) == 5, \"Expected 5 samples in train.csv\"\n",
        "\n",
        "# Read skeleton lines\n",
        "with open(input_skels, \"r\") as f:\n",
        "    skel_lines = [line.strip() for line in f.readlines()]\n",
        "\n",
        "assert len(skel_lines) == len(df), \"Mismatch between CSV and skels line count\"\n",
        "\n",
        "samples = []\n",
        "\n",
        "for idx, (line, meta) in enumerate(zip(skel_lines, df.itertuples())):\n",
        "    raw = list(map(float, line.split()))\n",
        "    assert len(raw) % (feature_dim + 1) == 0, f\"Line {idx} isn't multiple of {feature_dim+1}\"\n",
        "\n",
        "    num_frames = len(raw) // (feature_dim + 1)\n",
        "    frames = []\n",
        "    for i in range(num_frames):\n",
        "        frame_vec = raw[i * (feature_dim + 1) : i * (feature_dim + 1) + feature_dim]\n",
        "        frames.append(torch.tensor(frame_vec, dtype=torch.float32))\n",
        "\n",
        "    sign_tensor = torch.stack(frames, dim=0)  # [T, 150]\n",
        "\n",
        "    sample = {\n",
        "        \"name\": meta.name,\n",
        "        \"signer\": meta.speaker,\n",
        "        \"sign\": sign_tensor + 1e-8,  # small value added for numerical stability\n",
        "        \"gloss\": meta.orth.strip(),\n",
        "        \"text\": meta.translation.strip(),\n",
        "    }\n",
        "    samples.append(sample)\n",
        "\n",
        "# Save as joblib file in gzip\n",
        "with gzip.open(output_file, \"wb\") as f:\n",
        "    joblib.dump(samples, f)\n",
        "\n",
        "print(f\"Saved {len(samples)} samples to {output_file}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "644bfebd",
      "metadata": {
        "id": "644bfebd"
      },
      "source": [
        "# Train Manager Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4907dd73",
      "metadata": {
        "id": "4907dd73"
      },
      "outputs": [],
      "source": [
        "class TrainManager:\n",
        "    \"\"\" Manages training loop, validations, learning rate scheduling\n",
        "    and early stopping.\"\"\"\n",
        "\n",
        "    def __init__(self, model: SignModel, config: dict) -> None:\n",
        "        \"\"\"\n",
        "        Creates a new TrainManager for a model, specified as in configuration.\n",
        "\n",
        "        :param model: torch module defining the model\n",
        "        :param config: dictionary containing the training configurations\n",
        "        \"\"\"\n",
        "        train_config = config[\"training\"]\n",
        "\n",
        "\n",
        "        # files for logging and storing\n",
        "        self.model_dir = make_model_dir(\n",
        "            train_config[\"model_dir\"], overwrite=train_config.get(\"overwrite\", False)\n",
        "        )\n",
        "        self.logger = make_logger(model_dir=self.model_dir)\n",
        "        self.logging_freq = train_config.get(\"logging_freq\", 100)\n",
        "        self.valid_report_file = \"{}/validations.txt\".format(self.model_dir)\n",
        "        self.tb_writer = SummaryWriter(log_dir=self.model_dir + \"/tensorboard/\")\n",
        "\n",
        "\n",
        "        # input\n",
        "        self.feature_size = (\n",
        "            sum(config[\"data\"][\"feature_size\"])\n",
        "            if isinstance(config[\"data\"][\"feature_size\"], list)\n",
        "            else config[\"data\"][\"feature_size\"]\n",
        "        )\n",
        "        self.dataset_version = config[\"data\"].get(\"version\", \"phoenix_2014_trans\")\n",
        "\n",
        "\n",
        "        # model\n",
        "        self.model = model\n",
        "        self.txt_pad_index = self.model.txt_pad_index\n",
        "        self.txt_bos_index = self.model.txt_bos_index\n",
        "        self._log_parameters_list()\n",
        "\n",
        "\n",
        "        # Check if we are doing only recognition or only translation or both\n",
        "        self.do_recognition = (\n",
        "            config[\"training\"].get(\"recognition_loss_weight\", 1.0) > 0.0\n",
        "        )\n",
        "        self.do_translation = (\n",
        "            config[\"training\"].get(\"translation_loss_weight\", 1.0) > 0.0\n",
        "        )\n",
        "\n",
        "\n",
        "        # Get Recognition and Translation specific parameters\n",
        "        if self.do_recognition:\n",
        "            self._get_recognition_params(train_config=train_config)\n",
        "        if self.do_translation:\n",
        "            self._get_translation_params(train_config=train_config)\n",
        "\n",
        "\n",
        "        # optimization\n",
        "        self.last_best_lr = train_config.get(\"learning_rate\", -1)\n",
        "        self.learning_rate_min = train_config.get(\"learning_rate_min\", 1.0e-8)\n",
        "        self.clip_grad_fun = build_gradient_clipper(config=train_config)\n",
        "        self.optimizer = build_optimizer(\n",
        "            config=train_config, parameters=model.parameters()\n",
        "        )\n",
        "        self.batch_multiplier = train_config.get(\"batch_multiplier\", 1)\n",
        "\n",
        "\n",
        "        # validation & early stopping\n",
        "        self.validation_freq = train_config.get(\"validation_freq\", 100)\n",
        "        self.num_valid_log = train_config.get(\"num_valid_log\", 5)\n",
        "        self.ckpt_queue = queue.Queue(maxsize=train_config.get(\"keep_last_ckpts\", 5))\n",
        "        self.eval_metric = train_config.get(\"eval_metric\", \"bleu\")\n",
        "        if self.eval_metric not in [\"bleu\", \"chrf\", \"wer\", \"rouge\"]:\n",
        "            raise ValueError(\n",
        "                \"Invalid setting for 'eval_metric': {}\".format(self.eval_metric)\n",
        "            )\n",
        "        self.early_stopping_metric = train_config.get(\n",
        "            \"early_stopping_metric\", \"eval_metric\"\n",
        "        )\n",
        "\n",
        "\n",
        "        if self.early_stopping_metric in [\n",
        "            \"ppl\",\n",
        "            \"translation_loss\",\n",
        "            \"recognition_loss\",\n",
        "        ]:\n",
        "            self.minimize_metric = True\n",
        "        elif self.early_stopping_metric == \"eval_metric\":\n",
        "            if self.eval_metric in [\"bleu\", \"chrf\", \"rouge\"]:\n",
        "                assert self.do_translation\n",
        "                self.minimize_metric = False\n",
        "            else:  # eval metric that has to get minimized (not yet implemented)\n",
        "                self.minimize_metric = True\n",
        "        else:\n",
        "            raise ValueError(\n",
        "                \"Invalid setting for 'early_stopping_metric': {}\".format(\n",
        "                    self.early_stopping_metric\n",
        "                )\n",
        "            )\n",
        "        # data_augmentation parameters\n",
        "        self.frame_subsampling_ratio = config[\"data\"].get(\n",
        "            \"frame_subsampling_ratio\", None\n",
        "        )\n",
        "\n",
        "        self.random_frame_subsampling = config[\"data\"].get(\n",
        "            \"random_frame_subsampling\", None\n",
        "        )\n",
        "        self.random_frame_masking_ratio = config[\"data\"].get(\n",
        "            \"random_frame_masking_ratio\", None\n",
        "        )\n",
        "\n",
        "\n",
        "        # learning rate scheduling\n",
        "        self.scheduler, self.scheduler_step_at = build_scheduler(\n",
        "            config=train_config,\n",
        "            scheduler_mode=\"min\" if self.minimize_metric else \"max\",\n",
        "            optimizer=self.optimizer,\n",
        "            hidden_size=config[\"model\"][\"encoder\"][\"hidden_size\"],\n",
        "        )\n",
        "\n",
        "\n",
        "        # data & batch handling\n",
        "        self.level = config[\"data\"][\"level\"]\n",
        "        if self.level not in [\"word\", \"bpe\", \"char\"]:\n",
        "            raise ValueError(\"Invalid segmentation level': {}\".format(self.level))\n",
        "\n",
        "        self.shuffle = train_config.get(\"shuffle\", True)\n",
        "        self.epochs = train_config[\"epochs\"]\n",
        "        self.batch_size = train_config[\"batch_size\"]\n",
        "        self.batch_type = train_config.get(\"batch_type\", \"sentence\")\n",
        "        self.eval_batch_size = train_config.get(\"eval_batch_size\", self.batch_size)\n",
        "        self.eval_batch_type = train_config.get(\"eval_batch_type\", self.batch_type)\n",
        "\n",
        "        self.use_cuda = train_config[\"use_cuda\"]\n",
        "        if self.use_cuda:\n",
        "            self.model.cuda()\n",
        "            if self.do_translation:\n",
        "                self.translation_loss_function.cuda()\n",
        "            if self.do_recognition:\n",
        "                self.recognition_loss_function.cuda()\n",
        "\n",
        "\n",
        "        # initialize training statistics\n",
        "        self.steps = 0\n",
        "        # stop training if this flag is True by reaching learning rate minimum\n",
        "        self.stop = False\n",
        "        self.total_txt_tokens = 0\n",
        "        self.total_gls_tokens = 0\n",
        "        self.best_ckpt_iteration = 0\n",
        "        # initial values for best scores\n",
        "        self.best_ckpt_score = np.inf if self.minimize_metric else -np.inf\n",
        "        self.best_all_ckpt_scores = {}\n",
        "        # comparison function for scores\n",
        "        self.is_best = (\n",
        "            lambda score: score < self.best_ckpt_score\n",
        "            if self.minimize_metric\n",
        "            else score > self.best_ckpt_score\n",
        "        )\n",
        "\n",
        "\n",
        "        # model parameters\n",
        "        if \"load_model\" in train_config.keys():\n",
        "            model_load_path = train_config[\"load_model\"]\n",
        "            self.logger.info(\"Loading model from %s\", model_load_path)\n",
        "            reset_best_ckpt = train_config.get(\"reset_best_ckpt\", False)\n",
        "            reset_scheduler = train_config.get(\"reset_scheduler\", False)\n",
        "            reset_optimizer = train_config.get(\"reset_optimizer\", False)\n",
        "            self.init_from_checkpoint(\n",
        "                model_load_path,\n",
        "                reset_best_ckpt=reset_best_ckpt,\n",
        "                reset_scheduler=reset_scheduler,\n",
        "                reset_optimizer=reset_optimizer,\n",
        "            )\n",
        "\n",
        "\n",
        "\n",
        "    def _get_recognition_params(self, train_config) -> None:\n",
        "        # NOTE (Cihan): The blank label is the silence index in the gloss vocabulary.\n",
        "        #   There is an assertion in the GlossVocabulary class's __init__.\n",
        "        #   This is necessary to do TensorFlow decoding, as it is hardcoded\n",
        "        #   Currently it is hardcoded as 0.\n",
        "        self.gls_silence_token = self.model.gls_vocab.stoi[SIL_TOKEN]\n",
        "        assert self.gls_silence_token == 0\n",
        "\n",
        "        self.recognition_loss_function = torch.nn.CTCLoss(\n",
        "            blank=self.gls_silence_token, zero_infinity=True\n",
        "        )\n",
        "        self.recognition_loss_weight = train_config.get(\"recognition_loss_weight\", 1.0)\n",
        "        self.eval_recognition_beam_size = train_config.get(\n",
        "            \"eval_recognition_beam_size\", 1\n",
        "        )\n",
        "\n",
        "\n",
        "    def _get_translation_params(self, train_config) -> None:\n",
        "        self.label_smoothing = train_config.get(\"label_smoothing\", 0.0)\n",
        "        self.translation_loss_function = XentLoss(\n",
        "            pad_index=self.txt_pad_index, smoothing=self.label_smoothing\n",
        "        )\n",
        "        self.translation_normalization_mode = train_config.get(\n",
        "            \"translation_normalization\", \"batch\"\n",
        "        )\n",
        "        if self.translation_normalization_mode not in [\"batch\", \"tokens\"]:\n",
        "            raise ValueError(\n",
        "                \"Invalid normalization {}.\".format(self.translation_normalization_mode)\n",
        "            )\n",
        "        self.translation_loss_weight = train_config.get(\"translation_loss_weight\", 1.0)\n",
        "        self.eval_translation_beam_size = train_config.get(\n",
        "            \"eval_translation_beam_size\", 1\n",
        "        )\n",
        "        self.eval_translation_beam_alpha = train_config.get(\n",
        "            \"eval_translation_beam_alpha\", -1\n",
        "        )\n",
        "        self.translation_max_output_length = train_config.get(\n",
        "            \"translation_max_output_length\", None\n",
        "        )\n",
        "\n",
        "\n",
        "    def _save_checkpoint(self) -> None:\n",
        "        \"\"\"\n",
        "        Save the model's current parameters and the training state to a\n",
        "        checkpoint.\n",
        "\n",
        "        The training state contains the total number of training steps,\n",
        "        the total number of training tokens,\n",
        "        the best checkpoint score and iteration so far,\n",
        "        and optimizer and scheduler states.\n",
        "\n",
        "        \"\"\"\n",
        "        model_path = \"{}/{}.ckpt\".format(self.model_dir, self.steps)\n",
        "        state = {\n",
        "            \"steps\": self.steps,\n",
        "            \"total_txt_tokens\": self.total_txt_tokens if self.do_translation else 0,\n",
        "            \"total_gls_tokens\": self.total_gls_tokens if self.do_recognition else 0,\n",
        "            \"best_ckpt_score\": self.best_ckpt_score,\n",
        "            \"best_all_ckpt_scores\": self.best_all_ckpt_scores,\n",
        "            \"best_ckpt_iteration\": self.best_ckpt_iteration,\n",
        "            \"model_state\": self.model.state_dict(),\n",
        "            \"optimizer_state\": self.optimizer.state_dict(),\n",
        "            \"scheduler_state\": self.scheduler.state_dict()\n",
        "            if self.scheduler is not None\n",
        "            else None,\n",
        "        }\n",
        "        torch.save(state, model_path)\n",
        "        if self.ckpt_queue.full():\n",
        "            to_delete = self.ckpt_queue.get()  # delete oldest ckpt\n",
        "            try:\n",
        "                os.remove(to_delete)\n",
        "            except FileNotFoundError:\n",
        "                self.logger.warning(\n",
        "                    \"Wanted to delete old checkpoint %s but \" \"file does not exist.\",\n",
        "                    to_delete,\n",
        "                )\n",
        "\n",
        "        self.ckpt_queue.put(model_path)\n",
        "\n",
        "        # create/modify symbolic link for best checkpoint\n",
        "        symlink_update(\n",
        "            \"{}.ckpt\".format(self.steps), \"{}/best.ckpt\".format(self.model_dir)\n",
        "        )\n",
        "\n",
        "\n",
        "    def init_from_checkpoint(\n",
        "        self,\n",
        "        path: str,\n",
        "        reset_best_ckpt: bool = False,\n",
        "        reset_scheduler: bool = False,\n",
        "        reset_optimizer: bool = False,\n",
        "    ) -> None:\n",
        "        \"\"\"\n",
        "        Initialize the trainer from a given checkpoint file.\n",
        "\n",
        "        This checkpoint file contains not only model parameters, but also\n",
        "        scheduler and optimizer states, see `self._save_checkpoint`.\n",
        "\n",
        "        :param path: path to checkpoint\n",
        "        :param reset_best_ckpt: reset tracking of the best checkpoint,\n",
        "                                use for domain adaptation with a new dev\n",
        "                                set or when using a new metric for fine-tuning.\n",
        "        :param reset_scheduler: reset the learning rate scheduler, and do not\n",
        "                                use the one stored in the checkpoint.\n",
        "        :param reset_optimizer: reset the optimizer, and do not use the one\n",
        "                                stored in the checkpoint.\n",
        "        \"\"\"\n",
        "        model_checkpoint = load_checkpoint(path=path, use_cuda=self.use_cuda)\n",
        "\n",
        "        # restore model and optimizer parameters\n",
        "        self.model.load_state_dict(model_checkpoint[\"model_state\"])\n",
        "\n",
        "        if not reset_optimizer:\n",
        "            self.optimizer.load_state_dict(model_checkpoint[\"optimizer_state\"])\n",
        "        else:\n",
        "            self.logger.info(\"Reset optimizer.\")\n",
        "\n",
        "        if not reset_scheduler:\n",
        "            if (\n",
        "                model_checkpoint[\"scheduler_state\"] is not None\n",
        "                and self.scheduler is not None\n",
        "            ):\n",
        "                self.scheduler.load_state_dict(model_checkpoint[\"scheduler_state\"])\n",
        "        else:\n",
        "            self.logger.info(\"Reset scheduler.\")\n",
        "\n",
        "        # restore counts\n",
        "        self.steps = model_checkpoint[\"steps\"]\n",
        "        self.total_txt_tokens = model_checkpoint[\"total_txt_tokens\"]\n",
        "        self.total_gls_tokens = model_checkpoint[\"total_gls_tokens\"]\n",
        "\n",
        "        if not reset_best_ckpt:\n",
        "            self.best_ckpt_score = model_checkpoint[\"best_ckpt_score\"]\n",
        "            self.best_all_ckpt_scores = model_checkpoint[\"best_all_ckpt_scores\"]\n",
        "            self.best_ckpt_iteration = model_checkpoint[\"best_ckpt_iteration\"]\n",
        "        else:\n",
        "            self.logger.info(\"Reset tracking of the best checkpoint.\")\n",
        "\n",
        "        # move parameters to cuda\n",
        "        if self.use_cuda:\n",
        "            self.model.cuda()\n",
        "\n",
        "\n",
        "    def train_and_validate(self, train_data: Dataset, valid_data: Dataset, ground_data: Dataset) -> None:\n",
        "        \"\"\"\n",
        "        Train the model and validate it from time to time on the validation set.\n",
        "\n",
        "        :param train_data: training data\n",
        "        :param valid_data: validation data\n",
        "        \"\"\"\n",
        "        train_iter = BucketIterator(dataset=train_data,\n",
        "                                    batch_size=self.batch_size,\n",
        "                                    shuffle=True,\n",
        "                                    sort=False,\n",
        "                                    sort_within_batch=True,\n",
        "                                    repeat=False,\n",
        "                    )\n",
        "        epoch_no = None\n",
        "        for epoch_no in range(self.epochs):\n",
        "            #self.logger.info(\"EPOCH %d\", epoch_no + 1)\n",
        "\n",
        "            if self.scheduler is not None and self.scheduler_step_at == \"epoch\":\n",
        "                self.scheduler.step(epoch=epoch_no)\n",
        "\n",
        "            self.model.train()\n",
        "            start = time.time()\n",
        "            total_valid_duration = 0\n",
        "            count = self.batch_multiplier - 1\n",
        "\n",
        "            if self.do_recognition:\n",
        "                processed_gls_tokens = self.total_gls_tokens\n",
        "                epoch_recognition_loss = 0\n",
        "            if self.do_translation:\n",
        "                processed_txt_tokens = self.total_txt_tokens\n",
        "                epoch_translation_loss = 0\n",
        "\n",
        "            for batch in iter(train_iter):\n",
        "                # reactivate training\n",
        "                # create a Batch object from torchtext batch\n",
        "                batch = Batch(\n",
        "                    is_train=True,\n",
        "                    torch_batch=batch,\n",
        "                    txt_pad_index=self.txt_pad_index,\n",
        "                    sgn_dim=self.feature_size,\n",
        "                    use_cuda=self.use_cuda,\n",
        "                    frame_subsampling_ratio=self.frame_subsampling_ratio,\n",
        "                    random_frame_subsampling=self.random_frame_subsampling,\n",
        "                    random_frame_masking_ratio=self.random_frame_masking_ratio,\n",
        "                )\n",
        "\n",
        "                # only update every batch_multiplier batches\n",
        "                # see https://medium.com/@davidlmorton/\n",
        "                # increasing-mini-batch-size-without-increasing-\n",
        "                # memory-6794e10db672\n",
        "                update = count == 0\n",
        "\n",
        "                recognition_loss, translation_loss = self._train_batch(\n",
        "                    batch, update=update\n",
        "                )\n",
        "\n",
        "                if self.do_recognition:\n",
        "                    self.tb_writer.add_scalar(\n",
        "                        \"train/train_recognition_loss\", recognition_loss, self.steps\n",
        "                    )\n",
        "                    epoch_recognition_loss += recognition_loss.detach().cpu().numpy()\n",
        "\n",
        "                if self.do_translation:\n",
        "                    self.tb_writer.add_scalar(\n",
        "                        \"train/train_translation_loss\", translation_loss, self.steps\n",
        "                    )\n",
        "                    epoch_translation_loss += translation_loss.detach().cpu().numpy()\n",
        "\n",
        "                count = self.batch_multiplier if update else count\n",
        "                count -= 1\n",
        "\n",
        "                if (\n",
        "                    self.scheduler is not None\n",
        "                    and self.scheduler_step_at == \"step\"\n",
        "                    and update\n",
        "                ):\n",
        "                    self.scheduler.step()\n",
        "\n",
        "                # log learning progress\n",
        "                if self.steps % self.logging_freq == 0 and update:\n",
        "                    elapsed = time.time() - start - total_valid_duration\n",
        "\n",
        "                    log_out = \"[Epoch: {:03d} Step: {:08d}] \".format(\n",
        "                        epoch_no + 1, self.steps,\n",
        "                    )\n",
        "\n",
        "                    if self.do_recognition:\n",
        "                        elapsed_gls_tokens = (\n",
        "                            self.total_gls_tokens - processed_gls_tokens\n",
        "                        )\n",
        "                        processed_gls_tokens = self.total_gls_tokens\n",
        "                        log_out += \"Batch Recognition Loss: {:10.6f} => \".format(\n",
        "                            recognition_loss\n",
        "                        )\n",
        "                        log_out += \"Gls Tokens per Sec: {:8.0f} || \".format(\n",
        "                            elapsed_gls_tokens / elapsed\n",
        "                        )\n",
        "                    if self.do_translation:\n",
        "                        elapsed_txt_tokens = (\n",
        "                            self.total_txt_tokens - processed_txt_tokens\n",
        "                        )\n",
        "                        processed_txt_tokens = self.total_txt_tokens\n",
        "                        log_out += \"Batch Translation Loss: {:10.6f} => \".format(\n",
        "                            translation_loss\n",
        "                        )\n",
        "                        log_out += \"Txt Tokens per Sec: {:8.0f} || \".format(\n",
        "                            elapsed_txt_tokens / elapsed\n",
        "                        )\n",
        "                    log_out += \"Lr: {:.6f}\".format(self.optimizer.param_groups[0][\"lr\"])\n",
        "                    self.logger.info(log_out)\n",
        "                    start = time.time()\n",
        "                    total_valid_duration = 0\n",
        "\n",
        "\n",
        "                # validate on the entire dev set\n",
        "                if self.steps % self.validation_freq == 0 and update:\n",
        "                    valid_start_time = time.time()\n",
        "                    # TODO (Cihan): There must be a better way of passing\n",
        "                    #   these recognition only and translation only parameters!\n",
        "                    #   Maybe have a NamedTuple with optional fields?\n",
        "                    #   Hmm... Future Cihan's problem.\n",
        "                    val_res = validate_on_data(\n",
        "                        model=self.model,\n",
        "                        data=valid_data,\n",
        "                        ground_data=ground_data,\n",
        "                        batch_size=self.eval_batch_size,\n",
        "                        use_cuda=self.use_cuda,\n",
        "                        batch_type=self.eval_batch_type,\n",
        "                        dataset_version=self.dataset_version,\n",
        "                        sgn_dim=self.feature_size,\n",
        "                        txt_pad_index=self.txt_pad_index,\n",
        "                        # Recognition Parameters\n",
        "                        do_recognition=self.do_recognition,\n",
        "                        recognition_loss_function=self.recognition_loss_function\n",
        "                        if self.do_recognition\n",
        "                        else None,\n",
        "                        recognition_loss_weight=self.recognition_loss_weight\n",
        "                        if self.do_recognition\n",
        "                        else None,\n",
        "                        recognition_beam_size=self.eval_recognition_beam_size\n",
        "                        if self.do_recognition\n",
        "                        else None,\n",
        "                        # Translation Parameters\n",
        "                        do_translation=self.do_translation,\n",
        "                        translation_loss_function=self.translation_loss_function\n",
        "                        if self.do_translation\n",
        "                        else None,\n",
        "                        translation_max_output_length=self.translation_max_output_length\n",
        "                        if self.do_translation\n",
        "                        else None,\n",
        "                        level=self.level if self.do_translation else None,\n",
        "                        translation_loss_weight=self.translation_loss_weight\n",
        "                        if self.do_translation\n",
        "                        else None,\n",
        "                        translation_beam_size=self.eval_translation_beam_size\n",
        "                        if self.do_translation\n",
        "                        else None,\n",
        "                        translation_beam_alpha=self.eval_translation_beam_alpha\n",
        "                        if self.do_translation\n",
        "                        else None,\n",
        "                        frame_subsampling_ratio=self.frame_subsampling_ratio,\n",
        "                    )\n",
        "                    self.model.train()\n",
        "\n",
        "                    if self.do_recognition:\n",
        "                        # Log Losses and ppl\n",
        "                        self.tb_writer.add_scalar(\n",
        "                            \"valid/valid_recognition_loss\",\n",
        "                            val_res[\"valid_recognition_loss\"],\n",
        "                            self.steps,\n",
        "                        )\n",
        "                        self.tb_writer.add_scalar(\n",
        "                            \"valid/wer\", val_res[\"valid_scores\"][\"wer\"], self.steps\n",
        "                        )\n",
        "                        self.tb_writer.add_scalars(\n",
        "                            \"valid/wer_scores\",\n",
        "                            val_res[\"valid_scores\"][\"wer_scores\"],\n",
        "                            self.steps,\n",
        "                        )\n",
        "\n",
        "                    if self.do_translation:\n",
        "                        self.tb_writer.add_scalar(\n",
        "                            \"valid/valid_translation_loss\",\n",
        "                            val_res[\"valid_translation_loss\"],\n",
        "                            self.steps,\n",
        "                        )\n",
        "                        self.tb_writer.add_scalar(\n",
        "                            \"valid/valid_ppl\", val_res[\"valid_ppl\"], self.steps\n",
        "                        )\n",
        "\n",
        "                        # Log Scores\n",
        "                        self.tb_writer.add_scalar(\n",
        "                            \"valid/chrf\", val_res[\"valid_scores\"][\"chrf\"], self.steps\n",
        "                        )\n",
        "                        self.tb_writer.add_scalar(\n",
        "                            \"valid/rouge\", val_res[\"valid_scores\"][\"rouge\"], self.steps\n",
        "                        )\n",
        "                        self.tb_writer.add_scalar(\n",
        "                            \"valid/bleu\", val_res[\"valid_scores\"][\"bleu\"], self.steps\n",
        "                        )\n",
        "                        self.tb_writer.add_scalars(\n",
        "                            \"valid/bleu_scores\",\n",
        "                            val_res[\"valid_scores\"][\"bleu_scores\"],\n",
        "                            self.steps,\n",
        "                        )\n",
        "\n",
        "\n",
        "                    if self.early_stopping_metric == \"recognition_loss\":\n",
        "                        assert self.do_recognition\n",
        "                        ckpt_score = val_res[\"valid_recognition_loss\"]\n",
        "                    elif self.early_stopping_metric == \"translation_loss\":\n",
        "                        assert self.do_translation\n",
        "                        ckpt_score = val_res[\"valid_translation_loss\"]\n",
        "                    elif self.early_stopping_metric in [\"ppl\", \"perplexity\"]:\n",
        "                        assert self.do_translation\n",
        "                        ckpt_score = val_res[\"valid_ppl\"]\n",
        "                    else:\n",
        "                        ckpt_score = val_res[\"valid_scores\"][self.eval_metric]\n",
        "\n",
        "                    new_best = False\n",
        "                    if self.is_best(ckpt_score):\n",
        "                        self.best_ckpt_score = ckpt_score\n",
        "                        self.best_all_ckpt_scores = val_res[\"valid_scores\"]\n",
        "                        self.best_ckpt_iteration = self.steps\n",
        "                        self.logger.info(\n",
        "                            \"Hooray! New best validation result [%s]!\",\n",
        "                            self.early_stopping_metric,\n",
        "                        )\n",
        "                        if self.ckpt_queue.maxsize > 0:\n",
        "                            self.logger.info(\"Saving new checkpoint.\")\n",
        "                            new_best = True\n",
        "                            self._save_checkpoint()\n",
        "\n",
        "\n",
        "                    if (\n",
        "                        self.scheduler is not None\n",
        "                        and self.scheduler_step_at == \"validation\"\n",
        "                    ):\n",
        "                        prev_lr = self.scheduler.optimizer.param_groups[0][\"lr\"]\n",
        "                        self.scheduler.step(ckpt_score)\n",
        "                        now_lr = self.scheduler.optimizer.param_groups[0][\"lr\"]\n",
        "\n",
        "                        if prev_lr != now_lr:\n",
        "                            if self.last_best_lr != prev_lr:\n",
        "                                self.stop = True\n",
        "\n",
        "\n",
        "                    # append to validation report\n",
        "                    self._add_report(\n",
        "                        valid_scores=val_res[\"valid_scores\"],\n",
        "                        valid_recognition_loss=val_res[\"valid_recognition_loss\"]\n",
        "                        if self.do_recognition\n",
        "                        else None,\n",
        "                        valid_translation_loss=val_res[\"valid_translation_loss\"]\n",
        "                        if self.do_translation\n",
        "                        else None,\n",
        "                        valid_ppl=val_res[\"valid_ppl\"] if self.do_translation else None,\n",
        "                        eval_metric=self.eval_metric,\n",
        "                        new_best=new_best,\n",
        "                    )\n",
        "                    valid_duration = time.time() - valid_start_time\n",
        "                    total_valid_duration += valid_duration\n",
        "                    self.logger.info(\n",
        "                        \"Validation result at epoch %3d, step %8d: duration: %.4fs\\n\\t\"\n",
        "                        \"Recognition Beam Size: %d\\t\"\n",
        "                        \"Translation Beam Size: %d\\t\"\n",
        "                        \"Translation Beam Alpha: %d\\n\\t\"\n",
        "                        \"Recognition Loss: %4.5f\\t\"\n",
        "                        \"Translation Loss: %4.5f\\t\"\n",
        "                        \"PPL: %4.5f\\n\\t\"\n",
        "                        \"Eval Metric: %s\\n\\t\"\n",
        "                        \"WER %3.2f\\t(DEL: %3.2f,\\tINS: %3.2f,\\tSUB: %3.2f)\\n\\t\"\n",
        "                        \"BLEU-4 %.2f\\t(BLEU-1: %.2f,\\tBLEU-2: %.2f,\\tBLEU-3: %.2f,\\tBLEU-4: %.2f)\\n\\t\"\n",
        "                        \"CHRF %.2f\\t\"\n",
        "                        \"ROUGE %.2f\\t\"\n",
        "                        \"FID %.2f\",\n",
        "                        epoch_no + 1,\n",
        "                        self.steps,\n",
        "                        valid_duration,\n",
        "                        self.eval_recognition_beam_size if self.do_recognition else -1,\n",
        "                        self.eval_translation_beam_size if self.do_translation else -1,\n",
        "                        self.eval_translation_beam_alpha if self.do_translation else -1,\n",
        "                        val_res[\"valid_recognition_loss\"]\n",
        "                        if self.do_recognition\n",
        "                        else -1,\n",
        "                        val_res[\"valid_translation_loss\"]\n",
        "                        if self.do_translation\n",
        "                        else -1,\n",
        "                        val_res[\"valid_ppl\"] if self.do_translation else -1,\n",
        "                        self.eval_metric.upper(),\n",
        "                        # WER\n",
        "                        val_res[\"valid_scores\"][\"wer\"] if self.do_recognition else -1,\n",
        "                        val_res[\"valid_scores\"][\"wer_scores\"][\"del_rate\"]\n",
        "                        if self.do_recognition\n",
        "                        else -1,\n",
        "                        val_res[\"valid_scores\"][\"wer_scores\"][\"ins_rate\"]\n",
        "                        if self.do_recognition\n",
        "                        else -1,\n",
        "                        val_res[\"valid_scores\"][\"wer_scores\"][\"sub_rate\"]\n",
        "                        if self.do_recognition\n",
        "                        else -1,\n",
        "                        # BLEU\n",
        "                        val_res[\"valid_scores\"][\"bleu\"] if self.do_translation else -1,\n",
        "                        val_res[\"valid_scores\"][\"bleu_scores\"][\"bleu1\"]\n",
        "                        if self.do_translation\n",
        "                        else -1,\n",
        "                        val_res[\"valid_scores\"][\"bleu_scores\"][\"bleu2\"]\n",
        "                        if self.do_translation\n",
        "                        else -1,\n",
        "                        val_res[\"valid_scores\"][\"bleu_scores\"][\"bleu3\"]\n",
        "                        if self.do_translation\n",
        "                        else -1,\n",
        "                        val_res[\"valid_scores\"][\"bleu_scores\"][\"bleu4\"]\n",
        "                        if self.do_translation\n",
        "                        else -1,\n",
        "                        # Other\n",
        "                        val_res[\"valid_scores\"][\"chrf\"] if self.do_translation else -1,\n",
        "                        val_res[\"valid_scores\"][\"rouge\"] if self.do_translation else -1,\n",
        "                        val_res[\"valid_scores\"][\"fid\"] if self.do_translation else -1\n",
        "                    )\n",
        "\n",
        "                    self._log_examples(\n",
        "                        sequences = [example.sequence for example in valid_data.examples],\n",
        "                        gls_references=val_res[\"gls_ref\"]\n",
        "                        if self.do_recognition\n",
        "                        else None,\n",
        "                        gls_hypotheses=val_res[\"gls_hyp\"]\n",
        "                        if self.do_recognition\n",
        "                        else None,\n",
        "                        txt_references=val_res[\"txt_ref\"]\n",
        "                        if self.do_translation\n",
        "                        else None,\n",
        "                        txt_hypotheses=val_res[\"txt_hyp\"]\n",
        "                        if self.do_translation\n",
        "                        else None,\n",
        "                    )\n",
        "                    #example.sequence for example in valid_data.examples\n",
        "                    valid_seq = [example.sequence for example in valid_data.examples]\n",
        "                    # store validation set outputs and references\n",
        "                    if self.do_recognition:\n",
        "                        self._store_outputs(\n",
        "                            \"dev.hyp.gls\", valid_seq, val_res[\"gls_hyp\"], \"gls\"\n",
        "                        )\n",
        "                        self._store_outputs(\n",
        "                            \"references.dev.gls\", valid_seq, val_res[\"gls_ref\"]\n",
        "                        )\n",
        "\n",
        "                    if self.do_translation:\n",
        "                        self._store_outputs(\n",
        "                            \"dev.hyp.txt\", valid_seq, val_res[\"txt_hyp\"], \"txt\"\n",
        "                        )\n",
        "                        self._store_outputs(\n",
        "                            \"references.dev.txt\", valid_seq, val_res[\"txt_ref\"]\n",
        "                        )\n",
        "\n",
        "                if self.stop:\n",
        "                    break\n",
        "\n",
        "            if self.stop:\n",
        "                if (\n",
        "                    self.scheduler is not None\n",
        "                    and self.scheduler_step_at == \"validation\"\n",
        "                    and self.last_best_lr != prev_lr\n",
        "                ):\n",
        "                    self.logger.info(\n",
        "                        \"Training ended since there were no improvements in\"\n",
        "                        \"the last learning rate step: %f\",\n",
        "                        prev_lr,\n",
        "                    )\n",
        "                else:\n",
        "                    self.logger.info(\n",
        "                        \"Training ended since minimum lr %f was reached.\",\n",
        "                        self.learning_rate_min,\n",
        "                    )\n",
        "                break\n",
        "\n",
        "            self.logger.info(\n",
        "                \"Epoch %d:\\n\"\n",
        "                \"\\t\\t\\tTotal Training Recognition Loss %f || \"\n",
        "                \"Total Training Translation Loss %f\",\n",
        "                epoch_no + 1,\n",
        "                epoch_recognition_loss if self.do_recognition else -1,\n",
        "                epoch_translation_loss if self.do_translation else -1,\n",
        "            )\n",
        "        else:\n",
        "            self.logger.info(\"Training ended after %d epochs.\", epoch_no + 1)\n",
        "        self.logger.info(\n",
        "            \"Best validation result at step %d: %f %s.\",\n",
        "            self.best_ckpt_iteration,\n",
        "            self.best_ckpt_score,\n",
        "            self.early_stopping_metric,\n",
        "        )\n",
        "\n",
        "        self.tb_writer.close()  # close Tensorboard writer\n",
        "\n",
        "\n",
        "\n",
        "    def _train_batch(self, batch: Batch, update: bool = True) -> (Tensor, Tensor):\n",
        "        \"\"\"\n",
        "        Train the model on one batch: Compute the loss, make a gradient step.\n",
        "\n",
        "        :param batch: training batch\n",
        "        :param update: if False, only store gradient. if True also make update\n",
        "        :return normalized_recognition_loss: Normalized recognition loss\n",
        "        :return normalized_translation_loss: Normalized translation loss\n",
        "        \"\"\"\n",
        "\n",
        "        recognition_loss, translation_loss = self.model.get_loss_for_batch(\n",
        "            batch=batch,\n",
        "            recognition_loss_function=self.recognition_loss_function\n",
        "            if self.do_recognition\n",
        "            else None,\n",
        "            translation_loss_function=self.translation_loss_function\n",
        "            if self.do_translation\n",
        "            else None,\n",
        "            recognition_loss_weight=self.recognition_loss_weight\n",
        "            if self.do_recognition\n",
        "            else None,\n",
        "            translation_loss_weight=self.translation_loss_weight\n",
        "            if self.do_translation\n",
        "            else None,\n",
        "        )\n",
        "\n",
        "        # normalize translation loss\n",
        "        if self.do_translation:\n",
        "            if self.translation_normalization_mode == \"batch\":\n",
        "                txt_normalization_factor = batch.num_seqs\n",
        "            elif self.translation_normalization_mode == \"tokens\":\n",
        "                txt_normalization_factor = batch.num_txt_tokens\n",
        "            else:\n",
        "                raise NotImplementedError(\"Only normalize by 'batch' or 'tokens'\")\n",
        "\n",
        "            # division needed since loss.backward sums the gradients until updated\n",
        "            normalized_translation_loss = translation_loss / (\n",
        "                txt_normalization_factor * self.batch_multiplier\n",
        "            )\n",
        "        else:\n",
        "            normalized_translation_loss = 0\n",
        "\n",
        "        # TODO (Cihan): Add Gloss Token normalization (?)\n",
        "        #   I think they are already being normalized by batch\n",
        "        #   I need to think about if I want to normalize them by token.\n",
        "        if self.do_recognition:\n",
        "            normalized_recognition_loss = recognition_loss / self.batch_multiplier\n",
        "        else:\n",
        "            normalized_recognition_loss = 0\n",
        "\n",
        "        total_loss = normalized_recognition_loss + normalized_translation_loss\n",
        "        # compute gradients\n",
        "        total_loss.backward()\n",
        "\n",
        "        if self.clip_grad_fun is not None:\n",
        "            # clip gradients (in-place)\n",
        "            self.clip_grad_fun(params=self.model.parameters())\n",
        "\n",
        "        if update:\n",
        "            # make gradient step\n",
        "            self.optimizer.step()\n",
        "            self.optimizer.zero_grad()\n",
        "\n",
        "            # increment step counter\n",
        "            self.steps += 1\n",
        "\n",
        "        # increment token counter\n",
        "        if self.do_recognition:\n",
        "            self.total_gls_tokens += batch.num_gls_tokens\n",
        "        if self.do_translation:\n",
        "            self.total_txt_tokens += batch.num_txt_tokens\n",
        "\n",
        "        return normalized_recognition_loss, normalized_translation_loss\n",
        "\n",
        "\n",
        "    def _add_report(\n",
        "        self,\n",
        "        valid_scores: Dict,\n",
        "        valid_recognition_loss: float,\n",
        "        valid_translation_loss: float,\n",
        "        valid_ppl: float,\n",
        "        eval_metric: str,\n",
        "        new_best: bool = False,\n",
        "    ) -> None:\n",
        "        \"\"\"\n",
        "        Append a one-line report to validation logging file.\n",
        "\n",
        "        :param valid_scores: Dictionary of validation scores\n",
        "        :param valid_recognition_loss: validation loss (sum over whole validation set)\n",
        "        :param valid_translation_loss: validation loss (sum over whole validation set)\n",
        "        :param valid_ppl: validation perplexity\n",
        "        :param eval_metric: evaluation metric, e.g. \"bleu\"\n",
        "        :param new_best: whether this is a new best model\n",
        "        \"\"\"\n",
        "        current_lr = -1\n",
        "        # ignores other param groups for now\n",
        "        for param_group in self.optimizer.param_groups:\n",
        "            current_lr = param_group[\"lr\"]\n",
        "\n",
        "        if new_best:\n",
        "            self.last_best_lr = current_lr\n",
        "\n",
        "        if current_lr < self.learning_rate_min:\n",
        "            self.stop = True\n",
        "\n",
        "        with open(self.valid_report_file, \"a\", encoding=\"utf-8\") as opened_file:\n",
        "            opened_file.write(\n",
        "                \"Steps: {}\\t\"\n",
        "                \"Recognition Loss: {:.5f}\\t\"\n",
        "                \"Translation Loss: {:.5f}\\t\"\n",
        "                \"PPL: {:.5f}\\t\"\n",
        "                \"Eval Metric: {}\\t\"\n",
        "                \"WER {:.2f}\\t(DEL: {:.2f},\\tINS: {:.2f},\\tSUB: {:.2f})\\t\"\n",
        "                \"BLEU-4 {:.2f}\\t(BLEU-1: {:.2f},\\tBLEU-2: {:.2f},\\tBLEU-3: {:.2f},\\tBLEU-4: {:.2f})\\t\"\n",
        "                \"CHRF {:.2f}\\t\"\n",
        "                \"ROUGE {:.2f}\\t\"\n",
        "                \"LR: {:.8f}\\t{}\\n\".format(\n",
        "                    self.steps,\n",
        "                    valid_recognition_loss if self.do_recognition else -1,\n",
        "                    valid_translation_loss if self.do_translation else -1,\n",
        "                    valid_ppl if self.do_translation else -1,\n",
        "                    eval_metric,\n",
        "                    # WER\n",
        "                    valid_scores[\"wer\"] if self.do_recognition else -1,\n",
        "                    valid_scores[\"wer_scores\"][\"del_rate\"]\n",
        "                    if self.do_recognition\n",
        "                    else -1,\n",
        "                    valid_scores[\"wer_scores\"][\"ins_rate\"]\n",
        "                    if self.do_recognition\n",
        "                    else -1,\n",
        "                    valid_scores[\"wer_scores\"][\"sub_rate\"]\n",
        "                    if self.do_recognition\n",
        "                    else -1,\n",
        "                    # BLEU\n",
        "                    valid_scores[\"bleu\"] if self.do_translation else -1,\n",
        "                    valid_scores[\"bleu_scores\"][\"bleu1\"] if self.do_translation else -1,\n",
        "                    valid_scores[\"bleu_scores\"][\"bleu2\"] if self.do_translation else -1,\n",
        "                    valid_scores[\"bleu_scores\"][\"bleu3\"] if self.do_translation else -1,\n",
        "                    valid_scores[\"bleu_scores\"][\"bleu4\"] if self.do_translation else -1,\n",
        "                    # Other\n",
        "                    valid_scores[\"chrf\"] if self.do_translation else -1,\n",
        "                    valid_scores[\"rouge\"] if self.do_translation else -1,\n",
        "                    current_lr,\n",
        "                    \"*\" if new_best else \"\",\n",
        "                )\n",
        "            )\n",
        "\n",
        "\n",
        "    def _log_parameters_list(self) -> None:\n",
        "        \"\"\"\n",
        "        Write all model parameters (name, shape) to the log.\n",
        "        \"\"\"\n",
        "        model_parameters = filter(lambda p: p.requires_grad, self.model.parameters())\n",
        "        n_params = sum([np.prod(p.size()) for p in model_parameters])\n",
        "        self.logger.info(\"Total params: %d\", n_params)\n",
        "        trainable_params = [\n",
        "            n for (n, p) in self.model.named_parameters() if p.requires_grad\n",
        "        ]\n",
        "        self.logger.info(\"Trainable parameters: %s\", sorted(trainable_params))\n",
        "        assert trainable_params\n",
        "\n",
        "\n",
        "    def _log_examples(\n",
        "        self,\n",
        "        sequences: List[str],\n",
        "        gls_references: List[str],\n",
        "        gls_hypotheses: List[str],\n",
        "        txt_references: List[str],\n",
        "        txt_hypotheses: List[str],\n",
        "    ) -> None:\n",
        "        \"\"\"\n",
        "        Log `self.num_valid_log` number of samples from valid.\n",
        "\n",
        "        :param sequences: sign video sequence names (list of strings)\n",
        "        :param txt_hypotheses: decoded txt hypotheses (list of strings)\n",
        "        :param txt_references: decoded txt references (list of strings)\n",
        "        :param gls_hypotheses: decoded gls hypotheses (list of strings)\n",
        "        :param gls_references: decoded gls references (list of strings)\n",
        "        \"\"\"\n",
        "\n",
        "        if self.do_recognition:\n",
        "            assert len(gls_references) == len(gls_hypotheses)\n",
        "            num_sequences = len(gls_hypotheses)\n",
        "        if self.do_translation:\n",
        "            assert len(txt_references) == len(txt_hypotheses)\n",
        "            num_sequences = len(txt_hypotheses)\n",
        "\n",
        "        rand_idx = np.sort(np.random.permutation(num_sequences)[: self.num_valid_log])\n",
        "        self.logger.info(\"Logging Recognition and Translation Outputs\")\n",
        "        self.logger.info(\"=\" * 88)\n",
        "        for ri in rand_idx:\n",
        "            self.logger.info(\"Logging Sequence: %s\", sequences[ri])\n",
        "            if self.do_recognition:\n",
        "                gls_res = wer_single(r=gls_references[ri], h=gls_hypotheses[ri])\n",
        "                self.logger.info(\n",
        "                    \"\\tGloss Reference :\\t%s\", gls_res[\"alignment_out\"][\"align_ref\"]\n",
        "                )\n",
        "                self.logger.info(\n",
        "                    \"\\tGloss Hypothesis:\\t%s\", gls_res[\"alignment_out\"][\"align_hyp\"]\n",
        "                )\n",
        "                self.logger.info(\n",
        "                    \"\\tGloss Alignment :\\t%s\", gls_res[\"alignment_out\"][\"alignment\"]\n",
        "                )\n",
        "            if self.do_recognition and self.do_translation:\n",
        "                self.logger.info(\"\\t\" + \"-\" * 116)\n",
        "            if self.do_translation:\n",
        "                txt_res = wer_single(r=txt_references[ri], h=txt_hypotheses[ri])\n",
        "                self.logger.info(\n",
        "                    \"\\tText Reference  :\\t%s\", txt_res[\"alignment_out\"][\"align_ref\"]\n",
        "                )\n",
        "                self.logger.info(\n",
        "                    \"\\tText Hypothesis :\\t%s\", txt_res[\"alignment_out\"][\"align_hyp\"]\n",
        "                )\n",
        "                self.logger.info(\n",
        "                    \"\\tText Alignment  :\\t%s\", txt_res[\"alignment_out\"][\"alignment\"]\n",
        "                )\n",
        "            self.logger.info(\"=\" * 88)\n",
        "\n",
        "\n",
        "    def _store_outputs(\n",
        "        self, tag: str, sequence_ids: List[str], hypotheses: List[str], sub_folder=None\n",
        "    ) -> None:\n",
        "        \"\"\"\n",
        "        Write current validation outputs to file in `self.model_dir.`\n",
        "\n",
        "        :param hypotheses: list of strings\n",
        "        \"\"\"\n",
        "        if sub_folder:\n",
        "            out_folder = os.path.join(self.model_dir, sub_folder)\n",
        "            if not os.path.exists(out_folder):\n",
        "                os.makedirs(out_folder)\n",
        "            current_valid_output_file = \"{}/{}.{}\".format(out_folder, self.steps, tag)\n",
        "        else:\n",
        "            out_folder = self.model_dir\n",
        "            current_valid_output_file = \"{}/{}\".format(out_folder, tag)\n",
        "\n",
        "        with open(current_valid_output_file, \"w\", encoding=\"utf-8\") as opened_file:\n",
        "            for seq, hyp in zip(sequence_ids, hypotheses):\n",
        "                opened_file.write(\"{}|{}\\n\".format(seq, hyp))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf002ca5",
      "metadata": {
        "id": "bf002ca5"
      },
      "outputs": [],
      "source": [
        "def train(cfg_file: str) -> None:\n",
        "    \"\"\"\n",
        "    Main training function. After training, also test on test data if given.\n",
        "\n",
        "    :param cfg_file: path to configuration yaml file\n",
        "    \"\"\"\n",
        "    cfg = load_config(cfg_file)\n",
        "\n",
        "    # set the random seed\n",
        "    set_seed(seed=cfg[\"training\"].get(\"random_seed\", 42))\n",
        "\n",
        "    train_data, dev_data, test_data, gls_vocab, txt_vocab, ground_dev_data, ground_test_data = load_data(\n",
        "        data_cfg=cfg[\"data\"]\n",
        "    )\n",
        "\n",
        "    #for ex in train_data.examples[:3]:\n",
        "        #print(\">>> Raw TXT before tokenize_text:\", ex.txt)\n",
        "\n",
        "    # build model and load parameters into it\n",
        "    do_recognition = cfg[\"training\"].get(\"recognition_loss_weight\", 1.0) > 0.0\n",
        "    do_translation = cfg[\"training\"].get(\"translation_loss_weight\", 1.0) > 0.0\n",
        "    model = build_model(\n",
        "        cfg=cfg[\"model\"],\n",
        "        gls_vocab=gls_vocab,\n",
        "        txt_vocab=txt_vocab,\n",
        "        sgn_dim=sum(cfg[\"data\"][\"feature_size\"])\n",
        "        if isinstance(cfg[\"data\"][\"feature_size\"], list)\n",
        "        else cfg[\"data\"][\"feature_size\"],\n",
        "        do_recognition=do_recognition,\n",
        "        do_translation=do_translation,\n",
        "    )\n",
        "\n",
        "    # for training management, e.g. early stopping and model selection\n",
        "    trainer = TrainManager(model=model, config=cfg)\n",
        "\n",
        "    # store copy of original training config in model dir\n",
        "    shutil.copy2(cfg_file, trainer.model_dir + \"/config.yaml\")\n",
        "\n",
        "    # log all entries of config\n",
        "    log_cfg(cfg, trainer.logger)\n",
        "\n",
        "    log_data_info(\n",
        "        train_data=train_data,\n",
        "        valid_data=dev_data,\n",
        "        test_data=test_data,\n",
        "        gls_vocab=gls_vocab,\n",
        "        txt_vocab=txt_vocab,\n",
        "        logging_function=trainer.logger.info,\n",
        "    )\n",
        "\n",
        "    trainer.logger.info(str(model))\n",
        "\n",
        "    # store the vocabs\n",
        "    gls_vocab_file = \"{}/gls.vocab\".format(cfg[\"training\"][\"model_dir\"])\n",
        "    gls_vocab.to_file(gls_vocab_file)\n",
        "    txt_vocab_file = \"{}/txt.vocab\".format(cfg[\"training\"][\"model_dir\"])\n",
        "    txt_vocab.to_file(txt_vocab_file)\n",
        "\n",
        "    # train the model\n",
        "    trainer.train_and_validate(train_data=train_data, valid_data=dev_data, ground_data=ground_dev_data)\n",
        "    # Delete to speed things up as we don't need training data anymore\n",
        "    del train_data, dev_data, test_data\n",
        "\n",
        "    # predict with the best model on validation and test\n",
        "    # (if test data is available)\n",
        "    ckpt = \"{}/{}.ckpt\".format(trainer.model_dir, trainer.best_ckpt_iteration)\n",
        "    output_name = \"best.IT_{:08d}\".format(trainer.best_ckpt_iteration)\n",
        "    output_path = os.path.join(trainer.model_dir, output_name)\n",
        "    logger = trainer.logger\n",
        "    del trainer\n",
        "    test(cfg_file, ckpt=ckpt, output_path=output_path, logger=logger)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "85fb845f",
      "metadata": {
        "id": "85fb845f",
        "outputId": "511db61b-1679-414d-d8da-4568df052d3b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-07-10 14:06:34,452 Hello! This is Joey-NMT.\n",
            "2025-07-10 14:06:34,454 Total params: 8762909\n",
            "2025-07-10 14:06:34,455 Trainable parameters: ['decoder.layer_norm.bias', 'decoder.layer_norm.weight', 'decoder.layers.0.dec_layer_norm.bias', 'decoder.layers.0.dec_layer_norm.weight', 'decoder.layers.0.feed_forward.layer_norm.bias', 'decoder.layers.0.feed_forward.layer_norm.weight', 'decoder.layers.0.feed_forward.pwff_layer.0.bias', 'decoder.layers.0.feed_forward.pwff_layer.0.weight', 'decoder.layers.0.feed_forward.pwff_layer.3.bias', 'decoder.layers.0.feed_forward.pwff_layer.3.weight', 'decoder.layers.0.src_trg_att.k_layer.bias', 'decoder.layers.0.src_trg_att.k_layer.weight', 'decoder.layers.0.src_trg_att.output_layer.bias', 'decoder.layers.0.src_trg_att.output_layer.weight', 'decoder.layers.0.src_trg_att.q_layer.bias', 'decoder.layers.0.src_trg_att.q_layer.weight', 'decoder.layers.0.src_trg_att.v_layer.bias', 'decoder.layers.0.src_trg_att.v_layer.weight', 'decoder.layers.0.trg_trg_att.k_layer.bias', 'decoder.layers.0.trg_trg_att.k_layer.weight', 'decoder.layers.0.trg_trg_att.output_layer.bias', 'decoder.layers.0.trg_trg_att.output_layer.weight', 'decoder.layers.0.trg_trg_att.q_layer.bias', 'decoder.layers.0.trg_trg_att.q_layer.weight', 'decoder.layers.0.trg_trg_att.v_layer.bias', 'decoder.layers.0.trg_trg_att.v_layer.weight', 'decoder.layers.0.x_layer_norm.bias', 'decoder.layers.0.x_layer_norm.weight', 'decoder.layers.1.dec_layer_norm.bias', 'decoder.layers.1.dec_layer_norm.weight', 'decoder.layers.1.feed_forward.layer_norm.bias', 'decoder.layers.1.feed_forward.layer_norm.weight', 'decoder.layers.1.feed_forward.pwff_layer.0.bias', 'decoder.layers.1.feed_forward.pwff_layer.0.weight', 'decoder.layers.1.feed_forward.pwff_layer.3.bias', 'decoder.layers.1.feed_forward.pwff_layer.3.weight', 'decoder.layers.1.src_trg_att.k_layer.bias', 'decoder.layers.1.src_trg_att.k_layer.weight', 'decoder.layers.1.src_trg_att.output_layer.bias', 'decoder.layers.1.src_trg_att.output_layer.weight', 'decoder.layers.1.src_trg_att.q_layer.bias', 'decoder.layers.1.src_trg_att.q_layer.weight', 'decoder.layers.1.src_trg_att.v_layer.bias', 'decoder.layers.1.src_trg_att.v_layer.weight', 'decoder.layers.1.trg_trg_att.k_layer.bias', 'decoder.layers.1.trg_trg_att.k_layer.weight', 'decoder.layers.1.trg_trg_att.output_layer.bias', 'decoder.layers.1.trg_trg_att.output_layer.weight', 'decoder.layers.1.trg_trg_att.q_layer.bias', 'decoder.layers.1.trg_trg_att.q_layer.weight', 'decoder.layers.1.trg_trg_att.v_layer.bias', 'decoder.layers.1.trg_trg_att.v_layer.weight', 'decoder.layers.1.x_layer_norm.bias', 'decoder.layers.1.x_layer_norm.weight', 'decoder.layers.2.dec_layer_norm.bias', 'decoder.layers.2.dec_layer_norm.weight', 'decoder.layers.2.feed_forward.layer_norm.bias', 'decoder.layers.2.feed_forward.layer_norm.weight', 'decoder.layers.2.feed_forward.pwff_layer.0.bias', 'decoder.layers.2.feed_forward.pwff_layer.0.weight', 'decoder.layers.2.feed_forward.pwff_layer.3.bias', 'decoder.layers.2.feed_forward.pwff_layer.3.weight', 'decoder.layers.2.src_trg_att.k_layer.bias', 'decoder.layers.2.src_trg_att.k_layer.weight', 'decoder.layers.2.src_trg_att.output_layer.bias', 'decoder.layers.2.src_trg_att.output_layer.weight', 'decoder.layers.2.src_trg_att.q_layer.bias', 'decoder.layers.2.src_trg_att.q_layer.weight', 'decoder.layers.2.src_trg_att.v_layer.bias', 'decoder.layers.2.src_trg_att.v_layer.weight', 'decoder.layers.2.trg_trg_att.k_layer.bias', 'decoder.layers.2.trg_trg_att.k_layer.weight', 'decoder.layers.2.trg_trg_att.output_layer.bias', 'decoder.layers.2.trg_trg_att.output_layer.weight', 'decoder.layers.2.trg_trg_att.q_layer.bias', 'decoder.layers.2.trg_trg_att.q_layer.weight', 'decoder.layers.2.trg_trg_att.v_layer.bias', 'decoder.layers.2.trg_trg_att.v_layer.weight', 'decoder.layers.2.x_layer_norm.bias', 'decoder.layers.2.x_layer_norm.weight', 'decoder.output_layer.weight', 'encoder.layer_norm.bias', 'encoder.layer_norm.weight', 'encoder.layers.0.feed_forward.layer_norm.bias', 'encoder.layers.0.feed_forward.layer_norm.weight', 'encoder.layers.0.feed_forward.pwff_layer.0.bias', 'encoder.layers.0.feed_forward.pwff_layer.0.weight', 'encoder.layers.0.feed_forward.pwff_layer.3.bias', 'encoder.layers.0.feed_forward.pwff_layer.3.weight', 'encoder.layers.0.layer_norm.bias', 'encoder.layers.0.layer_norm.weight', 'encoder.layers.0.src_src_att.k_layer.bias', 'encoder.layers.0.src_src_att.k_layer.weight', 'encoder.layers.0.src_src_att.output_layer.bias', 'encoder.layers.0.src_src_att.output_layer.weight', 'encoder.layers.0.src_src_att.q_layer.bias', 'encoder.layers.0.src_src_att.q_layer.weight', 'encoder.layers.0.src_src_att.v_layer.bias', 'encoder.layers.0.src_src_att.v_layer.weight', 'encoder.layers.1.feed_forward.layer_norm.bias', 'encoder.layers.1.feed_forward.layer_norm.weight', 'encoder.layers.1.feed_forward.pwff_layer.0.bias', 'encoder.layers.1.feed_forward.pwff_layer.0.weight', 'encoder.layers.1.feed_forward.pwff_layer.3.bias', 'encoder.layers.1.feed_forward.pwff_layer.3.weight', 'encoder.layers.1.layer_norm.bias', 'encoder.layers.1.layer_norm.weight', 'encoder.layers.1.src_src_att.k_layer.bias', 'encoder.layers.1.src_src_att.k_layer.weight', 'encoder.layers.1.src_src_att.output_layer.bias', 'encoder.layers.1.src_src_att.output_layer.weight', 'encoder.layers.1.src_src_att.q_layer.bias', 'encoder.layers.1.src_src_att.q_layer.weight', 'encoder.layers.1.src_src_att.v_layer.bias', 'encoder.layers.1.src_src_att.v_layer.weight', 'encoder.layers.2.feed_forward.layer_norm.bias', 'encoder.layers.2.feed_forward.layer_norm.weight', 'encoder.layers.2.feed_forward.pwff_layer.0.bias', 'encoder.layers.2.feed_forward.pwff_layer.0.weight', 'encoder.layers.2.feed_forward.pwff_layer.3.bias', 'encoder.layers.2.feed_forward.pwff_layer.3.weight', 'encoder.layers.2.layer_norm.bias', 'encoder.layers.2.layer_norm.weight', 'encoder.layers.2.src_src_att.k_layer.bias', 'encoder.layers.2.src_src_att.k_layer.weight', 'encoder.layers.2.src_src_att.output_layer.bias', 'encoder.layers.2.src_src_att.output_layer.weight', 'encoder.layers.2.src_src_att.q_layer.bias', 'encoder.layers.2.src_src_att.q_layer.weight', 'encoder.layers.2.src_src_att.v_layer.bias', 'encoder.layers.2.src_src_att.v_layer.weight', 'gloss_output_layer.bias', 'gloss_output_layer.weight', 'sgn_embed.ln.bias', 'sgn_embed.ln.weight', 'sgn_embed.norm.norm.bias', 'sgn_embed.norm.norm.weight', 'txt_embed.lut.weight', 'txt_embed.norm.norm.bias', 'txt_embed.norm.norm.weight']\n",
            "2025-07-10 14:06:35,095 cfg.name                           : sign_experiment\n",
            "2025-07-10 14:06:35,096 cfg.data.data_path                 : ../data/\n",
            "2025-07-10 14:06:35,097 cfg.data.version                   : phoenix_2014_trans\n",
            "2025-07-10 14:06:35,097 cfg.data.sgn                       : sign\n",
            "2025-07-10 14:06:35,097 cfg.data.txt                       : text\n",
            "2025-07-10 14:06:35,098 cfg.data.gls                       : gloss\n",
            "2025-07-10 14:06:35,098 cfg.data.train                     : PHOENIX2014T/phoenix14t.skels.train\n",
            "2025-07-10 14:06:35,098 cfg.data.dev                       : PHOENIX2014T/phoenix14t.skels.dev\n",
            "2025-07-10 14:06:35,099 cfg.data.test                      : PHOENIX2014T/phoenix14t.skels.test\n",
            "2025-07-10 14:06:35,099 cfg.data.ground_dev                : Ground Truth/phoenix14t.skels.dev\n",
            "2025-07-10 14:06:35,099 cfg.data.ground_test               : Ground Truth/phoenix14t.skels.test\n",
            "2025-07-10 14:06:35,100 cfg.data.feature_size              : 150\n",
            "2025-07-10 14:06:35,100 cfg.data.level                     : word\n",
            "2025-07-10 14:06:35,100 cfg.data.txt_lowercase             : True\n",
            "2025-07-10 14:06:35,101 cfg.data.max_sent_length           : 400\n",
            "2025-07-10 14:06:35,101 cfg.data.random_train_subset       : -1\n",
            "2025-07-10 14:06:35,101 cfg.data.random_dev_subset         : -1\n",
            "2025-07-10 14:06:35,102 cfg.testing.recognition_beam_sizes : [10]\n",
            "2025-07-10 14:06:35,102 cfg.testing.translation_beam_sizes : [10]\n",
            "2025-07-10 14:06:35,102 cfg.testing.translation_beam_alphas : [-1, 0, 1, 2, 3, 4, 5]\n",
            "2025-07-10 14:06:35,102 cfg.training.reset_best_ckpt       : False\n",
            "2025-07-10 14:06:35,102 cfg.training.reset_scheduler       : False\n",
            "2025-07-10 14:06:35,102 cfg.training.reset_optimizer       : False\n",
            "2025-07-10 14:06:35,103 cfg.training.random_seed           : 42\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-07-10 14:06:35,103 cfg.training.model_dir             : ./sign_skels_model\n",
            "2025-07-10 14:06:35,103 cfg.training.recognition_loss_weight : 500.0\n",
            "2025-07-10 14:06:35,103 cfg.training.translation_loss_weight : 1.0\n",
            "2025-07-10 14:06:35,103 cfg.training.eval_metric           : bleu\n",
            "2025-07-10 14:06:35,104 cfg.training.optimizer             : adam\n",
            "2025-07-10 14:06:35,104 cfg.training.learning_rate         : 0.001\n",
            "2025-07-10 14:06:35,104 cfg.training.batch_size            : 64\n",
            "2025-07-10 14:06:35,104 cfg.training.num_valid_log         : 5\n",
            "2025-07-10 14:06:35,104 cfg.training.epochs                : 5000\n",
            "2025-07-10 14:06:35,104 cfg.training.early_stopping_metric : eval_metric\n",
            "2025-07-10 14:06:35,104 cfg.training.batch_type            : sentence\n",
            "2025-07-10 14:06:35,105 cfg.training.translation_normalization : batch\n",
            "2025-07-10 14:06:35,105 cfg.training.eval_recognition_beam_size : 1\n",
            "2025-07-10 14:06:35,105 cfg.training.eval_translation_beam_size : 1\n",
            "2025-07-10 14:06:35,105 cfg.training.eval_translation_beam_alpha : -1\n",
            "2025-07-10 14:06:35,105 cfg.training.overwrite             : True\n",
            "2025-07-10 14:06:35,105 cfg.training.shuffle               : True\n",
            "2025-07-10 14:06:35,105 cfg.training.use_cuda              : False\n",
            "2025-07-10 14:06:35,105 cfg.training.translation_max_output_length : 30\n",
            "2025-07-10 14:06:35,106 cfg.training.keep_last_ckpts       : 1\n",
            "2025-07-10 14:06:35,106 cfg.training.batch_multiplier      : 1\n",
            "2025-07-10 14:06:35,106 cfg.training.logging_freq          : 100\n",
            "2025-07-10 14:06:35,106 cfg.training.validation_freq       : 100\n",
            "2025-07-10 14:06:35,106 cfg.training.betas                 : [0.9, 0.998]\n",
            "2025-07-10 14:06:35,107 cfg.training.scheduling            : plateau\n",
            "2025-07-10 14:06:35,107 cfg.training.learning_rate_min     : 1e-07\n",
            "2025-07-10 14:06:35,107 cfg.training.weight_decay          : 0.001\n",
            "2025-07-10 14:06:35,107 cfg.training.patience              : 8\n",
            "2025-07-10 14:06:35,107 cfg.training.decrease_factor       : 0.7\n",
            "2025-07-10 14:06:35,107 cfg.training.label_smoothing       : 0.0\n",
            "2025-07-10 14:06:35,108 cfg.model.initializer              : xavier\n",
            "2025-07-10 14:06:35,108 cfg.model.bias_initializer         : zeros\n",
            "2025-07-10 14:06:35,108 cfg.model.init_gain                : 1.0\n",
            "2025-07-10 14:06:35,108 cfg.model.embed_initializer        : xavier\n",
            "2025-07-10 14:06:35,108 cfg.model.embed_init_gain          : 1.0\n",
            "2025-07-10 14:06:35,108 cfg.model.tied_softmax             : False\n",
            "2025-07-10 14:06:35,109 cfg.model.encoder.type             : transformer\n",
            "2025-07-10 14:06:35,109 cfg.model.encoder.num_layers       : 3\n",
            "2025-07-10 14:06:35,109 cfg.model.encoder.num_heads        : 8\n",
            "2025-07-10 14:06:35,109 cfg.model.encoder.embeddings.embedding_dim : 256\n",
            "2025-07-10 14:06:35,109 cfg.model.encoder.embeddings.scale : False\n",
            "2025-07-10 14:06:35,109 cfg.model.encoder.embeddings.dropout : 0.1\n",
            "2025-07-10 14:06:35,109 cfg.model.encoder.embeddings.norm_type : batch\n",
            "2025-07-10 14:06:35,110 cfg.model.encoder.embeddings.activation_type : softsign\n",
            "2025-07-10 14:06:35,110 cfg.model.encoder.hidden_size      : 256\n",
            "2025-07-10 14:06:35,110 cfg.model.encoder.ff_size          : 2048\n",
            "2025-07-10 14:06:35,110 cfg.model.encoder.dropout          : 0.1\n",
            "2025-07-10 14:06:35,110 cfg.model.decoder.type             : transformer\n",
            "2025-07-10 14:06:35,110 cfg.model.decoder.num_layers       : 3\n",
            "2025-07-10 14:06:35,110 cfg.model.decoder.num_heads        : 8\n",
            "2025-07-10 14:06:35,110 cfg.model.decoder.embeddings.embedding_dim : 256\n",
            "2025-07-10 14:06:35,111 cfg.model.decoder.embeddings.scale : False\n",
            "2025-07-10 14:06:35,111 cfg.model.decoder.embeddings.dropout : 0.1\n",
            "2025-07-10 14:06:35,111 cfg.model.decoder.embeddings.norm_type : batch\n",
            "2025-07-10 14:06:35,111 cfg.model.decoder.embeddings.activation_type : softsign\n",
            "2025-07-10 14:06:35,111 cfg.model.decoder.hidden_size      : 256\n",
            "2025-07-10 14:06:35,112 cfg.model.decoder.ff_size          : 2048\n",
            "2025-07-10 14:06:35,112 cfg.model.decoder.dropout          : 0.1\n",
            "2025-07-10 14:06:35,112 Data set sizes: \n",
            "\ttrain 5,\n",
            "\tvalid 5,\n",
            "\ttest 5\n",
            "2025-07-10 14:06:35,112 First training example:\n",
            "\t[GLS] JETZT WETTER MORGEN DONNERSTAG ZWOELF FEBRUAR\n",
            "\t[TXT] und nun die wettervorhersage für morgen donnerstag den zwölften august\n",
            "2025-07-10 14:06:35,112 First 10 words (gls): (0) <si> (1) <unk> (2) <pad> (3) REGEN (4) KOENNEN (5) GEWITTER (6) NORDWEST (7) ORT (8) BLEIBEN (9) DAZU\n",
            "2025-07-10 14:06:35,112 First 10 words (txt): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) und (5) auch (6) es (7) im (8) mit (9) nordwesten\n",
            "2025-07-10 14:06:35,113 Number of unique glosses (types): 29\n",
            "2025-07-10 14:06:35,113 Number of unique words (types): 65\n",
            "2025-07-10 14:06:35,113 SignModel(\n",
            "\tencoder=TransformerEncoder(num_layers=3, num_heads=8),\n",
            "\tdecoder=TransformerDecoder(num_layers=3, num_heads=8),\n",
            "\tsgn_embed=SpatialEmbeddings(embedding_dim=256, input_size=150),\n",
            "\ttxt_embed=Embeddings(embedding_dim=256, vocab_size=65))\n",
            "2025-07-10 14:06:35,348 Epoch 1:\n",
            "\t\t\tTotal Training Recognition Loss 23957.292969 || Total Training Translation Loss 109.947182\n",
            "2025-07-10 14:06:35,529 Epoch 2:\n",
            "\t\t\tTotal Training Recognition Loss 7125.012207 || Total Training Translation Loss 146.248291\n",
            "2025-07-10 14:06:35,706 Epoch 3:\n",
            "\t\t\tTotal Training Recognition Loss 5284.612305 || Total Training Translation Loss 75.129150\n",
            "2025-07-10 14:06:35,883 Epoch 4:\n",
            "\t\t\tTotal Training Recognition Loss 5497.706543 || Total Training Translation Loss 64.245636\n",
            "2025-07-10 14:06:36,061 Epoch 5:\n",
            "\t\t\tTotal Training Recognition Loss 5047.177246 || Total Training Translation Loss 59.668030\n",
            "2025-07-10 14:06:36,232 Epoch 6:\n",
            "\t\t\tTotal Training Recognition Loss 4378.517090 || Total Training Translation Loss 58.097847\n",
            "2025-07-10 14:06:36,403 Epoch 7:\n",
            "\t\t\tTotal Training Recognition Loss 3700.147705 || Total Training Translation Loss 55.121288\n",
            "2025-07-10 14:06:36,608 Epoch 8:\n",
            "\t\t\tTotal Training Recognition Loss 3349.169189 || Total Training Translation Loss 55.340248\n",
            "2025-07-10 14:06:36,817 Epoch 9:\n",
            "\t\t\tTotal Training Recognition Loss 2804.798828 || Total Training Translation Loss 51.972923\n",
            "2025-07-10 14:06:37,026 Epoch 10:\n",
            "\t\t\tTotal Training Recognition Loss 2287.706787 || Total Training Translation Loss 49.160244\n",
            "2025-07-10 14:06:37,231 Epoch 11:\n",
            "\t\t\tTotal Training Recognition Loss 2151.680176 || Total Training Translation Loss 46.556313\n",
            "2025-07-10 14:06:37,437 Epoch 12:\n",
            "\t\t\tTotal Training Recognition Loss 2150.198486 || Total Training Translation Loss 45.095207\n",
            "2025-07-10 14:06:37,643 Epoch 13:\n",
            "\t\t\tTotal Training Recognition Loss 2073.747070 || Total Training Translation Loss 40.097115\n",
            "2025-07-10 14:06:37,848 Epoch 14:\n",
            "\t\t\tTotal Training Recognition Loss 1999.452637 || Total Training Translation Loss 36.193504\n",
            "2025-07-10 14:06:38,052 Epoch 15:\n",
            "\t\t\tTotal Training Recognition Loss 1962.286011 || Total Training Translation Loss 31.699423\n",
            "2025-07-10 14:06:38,261 Epoch 16:\n",
            "\t\t\tTotal Training Recognition Loss 1958.529785 || Total Training Translation Loss 23.174953\n",
            "2025-07-10 14:06:38,469 Epoch 17:\n",
            "\t\t\tTotal Training Recognition Loss 1945.617432 || Total Training Translation Loss 17.150347\n",
            "2025-07-10 14:06:38,677 Epoch 18:\n",
            "\t\t\tTotal Training Recognition Loss 1923.166626 || Total Training Translation Loss 13.041603\n",
            "2025-07-10 14:06:38,882 Epoch 19:\n",
            "\t\t\tTotal Training Recognition Loss 1893.265503 || Total Training Translation Loss 10.126883\n",
            "2025-07-10 14:06:39,087 Epoch 20:\n",
            "\t\t\tTotal Training Recognition Loss 1887.800537 || Total Training Translation Loss 8.594681\n",
            "2025-07-10 14:06:39,291 Epoch 21:\n",
            "\t\t\tTotal Training Recognition Loss 1881.480469 || Total Training Translation Loss 5.307356\n",
            "2025-07-10 14:06:39,495 Epoch 22:\n",
            "\t\t\tTotal Training Recognition Loss 1870.177856 || Total Training Translation Loss 5.874221\n",
            "2025-07-10 14:06:39,702 Epoch 23:\n",
            "\t\t\tTotal Training Recognition Loss 1850.642456 || Total Training Translation Loss 4.182282\n",
            "2025-07-10 14:06:39,905 Epoch 24:\n",
            "\t\t\tTotal Training Recognition Loss 1836.542480 || Total Training Translation Loss 3.693481\n",
            "2025-07-10 14:06:40,079 Epoch 25:\n",
            "\t\t\tTotal Training Recognition Loss 1824.576416 || Total Training Translation Loss 2.407848\n",
            "2025-07-10 14:06:40,286 Epoch 26:\n",
            "\t\t\tTotal Training Recognition Loss 1818.238647 || Total Training Translation Loss 2.229952\n",
            "2025-07-10 14:06:40,493 Epoch 27:\n",
            "\t\t\tTotal Training Recognition Loss 1813.862793 || Total Training Translation Loss 1.882402\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-07-10 14:06:40,700 Epoch 28:\n",
            "\t\t\tTotal Training Recognition Loss 1784.392578 || Total Training Translation Loss 1.570788\n",
            "2025-07-10 14:06:40,904 Epoch 29:\n",
            "\t\t\tTotal Training Recognition Loss 1768.431396 || Total Training Translation Loss 1.221152\n",
            "2025-07-10 14:06:41,112 Epoch 30:\n",
            "\t\t\tTotal Training Recognition Loss 1751.391357 || Total Training Translation Loss 0.897168\n",
            "2025-07-10 14:06:41,316 Epoch 31:\n",
            "\t\t\tTotal Training Recognition Loss 1741.663208 || Total Training Translation Loss 0.805992\n",
            "2025-07-10 14:06:41,501 Epoch 32:\n",
            "\t\t\tTotal Training Recognition Loss 1730.532349 || Total Training Translation Loss 0.742528\n",
            "2025-07-10 14:06:41,674 Epoch 33:\n",
            "\t\t\tTotal Training Recognition Loss 1716.650635 || Total Training Translation Loss 0.469265\n",
            "2025-07-10 14:06:41,882 Epoch 34:\n",
            "\t\t\tTotal Training Recognition Loss 1702.970459 || Total Training Translation Loss 0.382790\n",
            "2025-07-10 14:06:42,091 Epoch 35:\n",
            "\t\t\tTotal Training Recognition Loss 1694.815430 || Total Training Translation Loss 0.435141\n",
            "2025-07-10 14:06:42,298 Epoch 36:\n",
            "\t\t\tTotal Training Recognition Loss 1684.992432 || Total Training Translation Loss 0.334394\n",
            "2025-07-10 14:06:42,505 Epoch 37:\n",
            "\t\t\tTotal Training Recognition Loss 1681.449219 || Total Training Translation Loss 0.585337\n",
            "2025-07-10 14:06:42,711 Epoch 38:\n",
            "\t\t\tTotal Training Recognition Loss 1670.603149 || Total Training Translation Loss 0.160335\n",
            "2025-07-10 14:06:42,918 Epoch 39:\n",
            "\t\t\tTotal Training Recognition Loss 1659.862549 || Total Training Translation Loss 0.217400\n",
            "2025-07-10 14:06:43,127 Epoch 40:\n",
            "\t\t\tTotal Training Recognition Loss 1648.034790 || Total Training Translation Loss 0.193132\n",
            "2025-07-10 14:06:43,336 Epoch 41:\n",
            "\t\t\tTotal Training Recognition Loss 1634.478027 || Total Training Translation Loss 0.142674\n",
            "2025-07-10 14:06:43,540 Epoch 42:\n",
            "\t\t\tTotal Training Recognition Loss 1627.975830 || Total Training Translation Loss 0.146372\n",
            "2025-07-10 14:06:43,745 Epoch 43:\n",
            "\t\t\tTotal Training Recognition Loss 1613.963867 || Total Training Translation Loss 0.147409\n",
            "2025-07-10 14:06:43,952 Epoch 44:\n",
            "\t\t\tTotal Training Recognition Loss 1605.038940 || Total Training Translation Loss 0.213828\n",
            "2025-07-10 14:06:44,157 Epoch 45:\n",
            "\t\t\tTotal Training Recognition Loss 1589.491943 || Total Training Translation Loss 0.149061\n",
            "2025-07-10 14:06:44,361 Epoch 46:\n",
            "\t\t\tTotal Training Recognition Loss 1584.385254 || Total Training Translation Loss 0.086567\n",
            "2025-07-10 14:06:44,566 Epoch 47:\n",
            "\t\t\tTotal Training Recognition Loss 1574.392212 || Total Training Translation Loss 0.130416\n",
            "2025-07-10 14:06:44,773 Epoch 48:\n",
            "\t\t\tTotal Training Recognition Loss 1567.194580 || Total Training Translation Loss 0.111656\n",
            "2025-07-10 14:06:44,978 Epoch 49:\n",
            "\t\t\tTotal Training Recognition Loss 1555.390625 || Total Training Translation Loss 0.084941\n",
            "2025-07-10 14:06:45,182 Epoch 50:\n",
            "\t\t\tTotal Training Recognition Loss 1550.725708 || Total Training Translation Loss 0.076085\n",
            "2025-07-10 14:06:45,356 Epoch 51:\n",
            "\t\t\tTotal Training Recognition Loss 1540.394897 || Total Training Translation Loss 0.066865\n",
            "2025-07-10 14:06:45,530 Epoch 52:\n",
            "\t\t\tTotal Training Recognition Loss 1534.479736 || Total Training Translation Loss 0.073361\n",
            "2025-07-10 14:06:45,698 Epoch 53:\n",
            "\t\t\tTotal Training Recognition Loss 1520.149902 || Total Training Translation Loss 0.058139\n",
            "2025-07-10 14:06:45,871 Epoch 54:\n",
            "\t\t\tTotal Training Recognition Loss 1517.097290 || Total Training Translation Loss 0.057334\n",
            "2025-07-10 14:06:46,043 Epoch 55:\n",
            "\t\t\tTotal Training Recognition Loss 1505.910767 || Total Training Translation Loss 0.066457\n",
            "2025-07-10 14:06:46,222 Epoch 56:\n",
            "\t\t\tTotal Training Recognition Loss 1495.683960 || Total Training Translation Loss 0.057862\n",
            "2025-07-10 14:06:46,435 Epoch 57:\n",
            "\t\t\tTotal Training Recognition Loss 1475.933716 || Total Training Translation Loss 0.055474\n",
            "2025-07-10 14:06:46,613 Epoch 58:\n",
            "\t\t\tTotal Training Recognition Loss 1463.033569 || Total Training Translation Loss 0.058813\n",
            "2025-07-10 14:06:46,785 Epoch 59:\n",
            "\t\t\tTotal Training Recognition Loss 1452.535645 || Total Training Translation Loss 0.042306\n",
            "2025-07-10 14:06:46,961 Epoch 60:\n",
            "\t\t\tTotal Training Recognition Loss 1438.281860 || Total Training Translation Loss 0.048420\n",
            "2025-07-10 14:06:47,132 Epoch 61:\n",
            "\t\t\tTotal Training Recognition Loss 1419.154663 || Total Training Translation Loss 0.036335\n",
            "2025-07-10 14:06:47,311 Epoch 62:\n",
            "\t\t\tTotal Training Recognition Loss 1411.898560 || Total Training Translation Loss 0.034555\n",
            "2025-07-10 14:06:47,482 Epoch 63:\n",
            "\t\t\tTotal Training Recognition Loss 1383.860962 || Total Training Translation Loss 0.030031\n",
            "2025-07-10 14:06:47,654 Epoch 64:\n",
            "\t\t\tTotal Training Recognition Loss 1377.869995 || Total Training Translation Loss 0.034139\n",
            "2025-07-10 14:06:47,861 Epoch 65:\n",
            "\t\t\tTotal Training Recognition Loss 1358.995728 || Total Training Translation Loss 0.041308\n",
            "2025-07-10 14:06:48,072 Epoch 66:\n",
            "\t\t\tTotal Training Recognition Loss 1343.904419 || Total Training Translation Loss 0.035650\n",
            "2025-07-10 14:06:48,283 Epoch 67:\n",
            "\t\t\tTotal Training Recognition Loss 1326.279297 || Total Training Translation Loss 0.034448\n",
            "2025-07-10 14:06:48,490 Epoch 68:\n",
            "\t\t\tTotal Training Recognition Loss 1309.020630 || Total Training Translation Loss 0.024552\n",
            "2025-07-10 14:06:48,699 Epoch 69:\n",
            "\t\t\tTotal Training Recognition Loss 1288.296265 || Total Training Translation Loss 0.038720\n",
            "2025-07-10 14:06:48,875 Epoch 70:\n",
            "\t\t\tTotal Training Recognition Loss 1271.928223 || Total Training Translation Loss 0.029217\n",
            "2025-07-10 14:06:49,050 Epoch 71:\n",
            "\t\t\tTotal Training Recognition Loss 1253.282593 || Total Training Translation Loss 0.053844\n",
            "2025-07-10 14:06:49,224 Epoch 72:\n",
            "\t\t\tTotal Training Recognition Loss 1229.769897 || Total Training Translation Loss 0.031676\n",
            "2025-07-10 14:06:49,396 Epoch 73:\n",
            "\t\t\tTotal Training Recognition Loss 1211.225098 || Total Training Translation Loss 0.032025\n",
            "2025-07-10 14:06:49,567 Epoch 74:\n",
            "\t\t\tTotal Training Recognition Loss 1190.587891 || Total Training Translation Loss 0.040267\n",
            "2025-07-10 14:06:49,737 Epoch 75:\n",
            "\t\t\tTotal Training Recognition Loss 1169.317993 || Total Training Translation Loss 0.046579\n",
            "2025-07-10 14:06:49,908 Epoch 76:\n",
            "\t\t\tTotal Training Recognition Loss 1149.243408 || Total Training Translation Loss 0.041168\n",
            "2025-07-10 14:06:50,082 Epoch 77:\n",
            "\t\t\tTotal Training Recognition Loss 1132.378052 || Total Training Translation Loss 0.047698\n",
            "2025-07-10 14:06:50,257 Epoch 78:\n",
            "\t\t\tTotal Training Recognition Loss 1110.383911 || Total Training Translation Loss 0.050501\n",
            "2025-07-10 14:06:50,427 Epoch 79:\n",
            "\t\t\tTotal Training Recognition Loss 1092.074829 || Total Training Translation Loss 0.064700\n",
            "2025-07-10 14:06:50,598 Epoch 80:\n",
            "\t\t\tTotal Training Recognition Loss 1073.640747 || Total Training Translation Loss 0.066823\n",
            "2025-07-10 14:06:50,770 Epoch 81:\n",
            "\t\t\tTotal Training Recognition Loss 1068.005859 || Total Training Translation Loss 0.076745\n",
            "2025-07-10 14:06:50,940 Epoch 82:\n",
            "\t\t\tTotal Training Recognition Loss 1048.343262 || Total Training Translation Loss 0.072549\n",
            "2025-07-10 14:06:51,112 Epoch 83:\n",
            "\t\t\tTotal Training Recognition Loss 1035.096924 || Total Training Translation Loss 0.083162\n",
            "2025-07-10 14:06:51,283 Epoch 84:\n",
            "\t\t\tTotal Training Recognition Loss 1021.668579 || Total Training Translation Loss 0.062306\n",
            "2025-07-10 14:06:51,455 Epoch 85:\n",
            "\t\t\tTotal Training Recognition Loss 1010.200867 || Total Training Translation Loss 0.078628\n",
            "2025-07-10 14:06:51,628 Epoch 86:\n",
            "\t\t\tTotal Training Recognition Loss 997.740906 || Total Training Translation Loss 0.068820\n",
            "2025-07-10 14:06:51,800 Epoch 87:\n",
            "\t\t\tTotal Training Recognition Loss 978.799805 || Total Training Translation Loss 0.058916\n",
            "2025-07-10 14:06:51,973 Epoch 88:\n",
            "\t\t\tTotal Training Recognition Loss 965.178284 || Total Training Translation Loss 0.062232\n",
            "2025-07-10 14:06:52,143 Epoch 89:\n",
            "\t\t\tTotal Training Recognition Loss 943.876892 || Total Training Translation Loss 0.052613\n",
            "2025-07-10 14:06:52,315 Epoch 90:\n",
            "\t\t\tTotal Training Recognition Loss 934.538635 || Total Training Translation Loss 0.054284\n",
            "2025-07-10 14:06:52,487 Epoch 91:\n",
            "\t\t\tTotal Training Recognition Loss 922.420227 || Total Training Translation Loss 0.051263\n",
            "2025-07-10 14:06:52,658 Epoch 92:\n",
            "\t\t\tTotal Training Recognition Loss 903.499512 || Total Training Translation Loss 0.044489\n",
            "2025-07-10 14:06:52,831 Epoch 93:\n",
            "\t\t\tTotal Training Recognition Loss 893.026855 || Total Training Translation Loss 0.055116\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-07-10 14:06:53,001 Epoch 94:\n",
            "\t\t\tTotal Training Recognition Loss 896.596130 || Total Training Translation Loss 0.053127\n",
            "2025-07-10 14:06:53,198 Epoch 95:\n",
            "\t\t\tTotal Training Recognition Loss 872.552063 || Total Training Translation Loss 0.053959\n",
            "2025-07-10 14:06:53,403 Epoch 96:\n",
            "\t\t\tTotal Training Recognition Loss 856.711182 || Total Training Translation Loss 0.064859\n",
            "2025-07-10 14:06:53,610 Epoch 97:\n",
            "\t\t\tTotal Training Recognition Loss 844.601990 || Total Training Translation Loss 0.051279\n",
            "2025-07-10 14:06:53,815 Epoch 98:\n",
            "\t\t\tTotal Training Recognition Loss 824.115723 || Total Training Translation Loss 0.058202\n",
            "2025-07-10 14:06:53,995 Epoch 99:\n",
            "\t\t\tTotal Training Recognition Loss 824.437134 || Total Training Translation Loss 0.067165\n",
            "2025-07-10 14:06:54,167 [Epoch: 100 Step: 00000100] Batch Recognition Loss: 798.355774 => Gls Tokens per Sec:      217 || Batch Translation Loss:   0.057209 => Txt Tokens per Sec:      586 || Lr: 0.001000\n",
            "2025-07-10 14:06:54,463 Hooray! New best validation result [eval_metric]!\n",
            "2025-07-10 14:06:54,465 Saving new checkpoint.\n",
            "2025-07-10 14:06:54,559 Validation result at epoch 100, step      100: duration: 0.3912s\n",
            "\tRecognition Beam Size: 1\tTranslation Beam Size: 1\tTranslation Beam Alpha: -1\n",
            "\tRecognition Loss: 4490.42236\tTranslation Loss: 261.76822\tPPL: 21.75027\n",
            "\tEval Metric: BLEU\n",
            "\tWER 100.00\t(DEL: 100.00,\tINS: 0.00,\tSUB: 0.00)\n",
            "\tBLEU-4 0.00\t(BLEU-1: 2.67,\tBLEU-2: 0.00,\tBLEU-3: 0.00,\tBLEU-4: 0.00)\n",
            "\tCHRF 15.69\tROUGE 4.02\tFID 0.00\n",
            "2025-07-10 14:06:54,560 Logging Recognition and Translation Outputs\n",
            "2025-07-10 14:06:54,560 ========================================================================================\n",
            "2025-07-10 14:06:54,560 Logging Sequence: 11August_2010_Wednesday_tagesschau-2\n",
            "2025-07-10 14:06:54,561 \tGloss Reference :\tDRUCK TIEF KOMMEN\n",
            "2025-07-10 14:06:54,561 \tGloss Hypothesis:\t***** **** ******\n",
            "2025-07-10 14:06:54,562 \tGloss Alignment :\tD     D    D     \n",
            "2025-07-10 14:06:54,562 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:06:54,563 \tText Reference  :\t****** **** *** ***** ** ********** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** tiefer luftdruck bestimmt in    den   nächsten tagen unser wetter\n",
            "2025-07-10 14:06:54,564 \tText Hypothesis :\tfinden sich vor allem im nordwesten <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>  <unk>     <unk>    <unk> <unk> <unk>    <unk> <unk> <unk> \n",
            "2025-07-10 14:06:54,564 \tText Alignment  :\tI      I    I   I     I  I          I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     S      S         S        S     S     S        S     S     S     \n",
            "2025-07-10 14:06:54,564 ========================================================================================\n",
            "2025-07-10 14:06:54,564 Logging Sequence: 11August_2010_Wednesday_tagesschau-3\n",
            "2025-07-10 14:06:54,564 \tGloss Reference :\tES-BEDEUTET VIEL WOLKE UND KOENNEN REGEN GEWITTER KOENNEN\n",
            "2025-07-10 14:06:54,565 \tGloss Hypothesis:\t*********** **** ***** *** ******* ***** ******** *******\n",
            "2025-07-10 14:06:54,565 \tGloss Alignment :\tD           D    D     D   D       D     D        D      \n",
            "2025-07-10 14:06:54,565 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:06:54,566 \tText Reference  :\t************ ****** **** *** ***** ** ********** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** das   bedeutet viele wolken und   immer wieder zum   teil  kräftige schauer und   gewitter\n",
            "2025-07-10 14:06:54,567 \tText Hypothesis :\twolkenlücken finden sich vor allem im nordwesten <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>    <unk> <unk>  <unk> <unk> <unk>  <unk> <unk> <unk>    <unk>   <unk> <unk>   \n",
            "2025-07-10 14:06:54,567 \tText Alignment  :\tI            I      I    I   I     I  I          I     I     I     I     I     I     I     I     I     I     S     S        S     S      S     S     S      S     S     S        S       S     S       \n",
            "2025-07-10 14:06:54,567 ========================================================================================\n",
            "2025-07-10 14:06:54,567 Logging Sequence: 11August_2010_Wednesday_tagesschau-8\n",
            "2025-07-10 14:06:54,567 \tGloss Reference :\tWIND MAESSIG SCHWACH REGION WENN GEWITTER WIND KOENNEN\n",
            "2025-07-10 14:06:54,567 \tGloss Hypothesis:\t**** ******* ******* ****** **** ******** **** *******\n",
            "2025-07-10 14:06:54,568 \tGloss Alignment :\tD    D       D       D      D    D        D    D      \n",
            "2025-07-10 14:06:54,568 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:06:54,570 \tText Reference  :\t************ ****** **** *** ***** ** ********** ***** ***** ***** ***** ***** meist weht  nur   ein   schwacher wind  aus   unterschiedlichen richtungen der   bei   schauern und   gewittern stark böig  sein  kann \n",
            "2025-07-10 14:06:54,570 \tText Hypothesis :\twolkenlücken finden sich vor allem im nordwesten <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>     <unk> <unk> <unk>             <unk>      <unk> <unk> <unk>    <unk> <unk>     <unk> <unk> <unk> <unk>\n",
            "2025-07-10 14:06:54,570 \tText Alignment  :\tI            I      I    I   I     I  I          I     I     I     I     I     S     S     S     S     S         S     S     S                 S          S     S     S        S     S         S     S     S     S    \n",
            "2025-07-10 14:06:54,570 ========================================================================================\n",
            "2025-07-10 14:06:54,570 Logging Sequence: 25October_2010_Monday_tagesschau-22\n",
            "2025-07-10 14:06:54,570 \tGloss Reference :\tMITTWOCH REGEN KOENNEN NORDWEST WAHRSCHEINLICH NORD STARK WIND\n",
            "2025-07-10 14:06:54,571 \tGloss Hypothesis:\t******** ***** ******* ******** ************** **** ***** ****\n",
            "2025-07-10 14:06:54,571 \tGloss Alignment :\tD        D     D       D        D              D    D     D   \n",
            "2025-07-10 14:06:54,571 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:06:54,573 \tText Reference  :\tam *** **** ** ********** *** **** ******** ******* mittwoch hier     und ** ******* ******** ****** ********* *********** ***** ***** da    nieselregen in    der   nordwesthälfte an    den   küsten kräftiger wind \n",
            "2025-07-10 14:06:54,573 \tText Hypothesis :\tam tag gibt es verbreitet zum teil kräftige schauer oder     gewitter und in manchen regionen fallen ergiebige regenmengen <unk> <unk> <unk> <unk>       <unk> <unk> <unk>          <unk> <unk> <unk>  <unk>     <unk>\n",
            "2025-07-10 14:06:54,573 \tText Alignment  :\t   I   I    I  I          I   I    I        I       S        S            I  I       I        I      I         I           I     I     S     S           S     S     S              S     S     S      S         S    \n",
            "2025-07-10 14:06:54,574 ========================================================================================\n",
            "2025-07-10 14:06:54,574 Logging Sequence: 05May_2011_Thursday_tagesschau-25\n",
            "2025-07-10 14:06:54,574 \tGloss Reference :\tJETZT WETTER WIE-AUSSEHEN MORGEN FREITAG SECHSTE MAI ZEIGEN-BILDSCHIRM\n",
            "2025-07-10 14:06:54,574 \tGloss Hypothesis:\t***** ****** ************ ****** ******* ******* *** *****************\n",
            "2025-07-10 14:06:54,574 \tGloss Alignment :\tD     D      D            D      D       D       D   D                \n",
            "2025-07-10 14:06:54,575 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:06:54,576 \tText Reference  :\t** *** **** ** ********** *** **** ******** ******* **** ******** und ** ******* ******** ****** ********* *********** ***** ***** ***** nun   die   wettervorhersage für   morgen freitag den   sechsten mai  \n",
            "2025-07-10 14:06:54,576 \tText Hypothesis :\tam tag gibt es verbreitet zum teil kräftige schauer oder gewitter und in manchen regionen fallen ergiebige regenmengen <unk> <unk> <unk> <unk> <unk> <unk>            <unk> <unk>  <unk>   <unk> <unk>    <unk>\n",
            "2025-07-10 14:06:54,576 \tText Alignment  :\tI  I   I    I  I          I   I    I        I       I    I            I  I       I        I      I         I           I     I     I     S     S     S                S     S      S       S     S        S    \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-07-10 14:06:54,577 ========================================================================================\n",
            "2025-07-10 14:06:54,577 Epoch 100:\n",
            "\t\t\tTotal Training Recognition Loss 798.355774 || Total Training Translation Loss 0.057209\n",
            "2025-07-10 14:06:54,747 Epoch 101:\n",
            "\t\t\tTotal Training Recognition Loss 802.351013 || Total Training Translation Loss 0.051884\n",
            "2025-07-10 14:06:54,920 Epoch 102:\n",
            "\t\t\tTotal Training Recognition Loss 783.968262 || Total Training Translation Loss 0.044276\n",
            "2025-07-10 14:06:55,094 Epoch 103:\n",
            "\t\t\tTotal Training Recognition Loss 758.066772 || Total Training Translation Loss 0.061274\n",
            "2025-07-10 14:06:55,268 Epoch 104:\n",
            "\t\t\tTotal Training Recognition Loss 760.674744 || Total Training Translation Loss 0.046388\n",
            "2025-07-10 14:06:55,446 Epoch 105:\n",
            "\t\t\tTotal Training Recognition Loss 757.396606 || Total Training Translation Loss 0.056803\n",
            "2025-07-10 14:06:55,617 Epoch 106:\n",
            "\t\t\tTotal Training Recognition Loss 727.935913 || Total Training Translation Loss 0.043362\n",
            "2025-07-10 14:06:55,788 Epoch 107:\n",
            "\t\t\tTotal Training Recognition Loss 732.645142 || Total Training Translation Loss 0.044229\n",
            "2025-07-10 14:06:55,962 Epoch 108:\n",
            "\t\t\tTotal Training Recognition Loss 722.421875 || Total Training Translation Loss 0.040180\n",
            "2025-07-10 14:06:56,135 Epoch 109:\n",
            "\t\t\tTotal Training Recognition Loss 718.321289 || Total Training Translation Loss 0.034213\n",
            "2025-07-10 14:06:56,312 Epoch 110:\n",
            "\t\t\tTotal Training Recognition Loss 700.932678 || Total Training Translation Loss 0.039209\n",
            "2025-07-10 14:06:56,489 Epoch 111:\n",
            "\t\t\tTotal Training Recognition Loss 702.334778 || Total Training Translation Loss 0.040498\n",
            "2025-07-10 14:06:56,664 Epoch 112:\n",
            "\t\t\tTotal Training Recognition Loss 686.498291 || Total Training Translation Loss 0.043196\n",
            "2025-07-10 14:06:56,836 Epoch 113:\n",
            "\t\t\tTotal Training Recognition Loss 702.625366 || Total Training Translation Loss 0.038621\n",
            "2025-07-10 14:06:57,009 Epoch 114:\n",
            "\t\t\tTotal Training Recognition Loss 684.700134 || Total Training Translation Loss 0.035129\n",
            "2025-07-10 14:06:57,183 Epoch 115:\n",
            "\t\t\tTotal Training Recognition Loss 667.581543 || Total Training Translation Loss 0.039186\n",
            "2025-07-10 14:06:57,361 Epoch 116:\n",
            "\t\t\tTotal Training Recognition Loss 660.005066 || Total Training Translation Loss 0.037288\n",
            "2025-07-10 14:06:57,537 Epoch 117:\n",
            "\t\t\tTotal Training Recognition Loss 644.947205 || Total Training Translation Loss 0.051400\n",
            "2025-07-10 14:06:57,710 Epoch 118:\n",
            "\t\t\tTotal Training Recognition Loss 670.147766 || Total Training Translation Loss 0.069181\n",
            "2025-07-10 14:06:57,884 Epoch 119:\n",
            "\t\t\tTotal Training Recognition Loss 635.203613 || Total Training Translation Loss 0.047255\n",
            "2025-07-10 14:06:58,057 Epoch 120:\n",
            "\t\t\tTotal Training Recognition Loss 695.391602 || Total Training Translation Loss 0.032070\n",
            "2025-07-10 14:06:58,233 Epoch 121:\n",
            "\t\t\tTotal Training Recognition Loss 735.716492 || Total Training Translation Loss 0.024061\n",
            "2025-07-10 14:06:58,407 Epoch 122:\n",
            "\t\t\tTotal Training Recognition Loss 644.080505 || Total Training Translation Loss 0.042536\n",
            "2025-07-10 14:06:58,580 Epoch 123:\n",
            "\t\t\tTotal Training Recognition Loss 715.426331 || Total Training Translation Loss 0.028045\n",
            "2025-07-10 14:06:58,754 Epoch 124:\n",
            "\t\t\tTotal Training Recognition Loss 648.916748 || Total Training Translation Loss 0.031664\n",
            "2025-07-10 14:06:58,962 Epoch 125:\n",
            "\t\t\tTotal Training Recognition Loss 722.982544 || Total Training Translation Loss 0.035395\n",
            "2025-07-10 14:06:59,169 Epoch 126:\n",
            "\t\t\tTotal Training Recognition Loss 647.687805 || Total Training Translation Loss 0.029717\n",
            "2025-07-10 14:06:59,351 Epoch 127:\n",
            "\t\t\tTotal Training Recognition Loss 640.578369 || Total Training Translation Loss 0.020097\n",
            "2025-07-10 14:06:59,557 Epoch 128:\n",
            "\t\t\tTotal Training Recognition Loss 646.527649 || Total Training Translation Loss 0.022430\n",
            "2025-07-10 14:06:59,764 Epoch 129:\n",
            "\t\t\tTotal Training Recognition Loss 671.580200 || Total Training Translation Loss 0.025796\n",
            "2025-07-10 14:06:59,972 Epoch 130:\n",
            "\t\t\tTotal Training Recognition Loss 631.270264 || Total Training Translation Loss 0.019828\n",
            "2025-07-10 14:07:00,179 Epoch 131:\n",
            "\t\t\tTotal Training Recognition Loss 626.553162 || Total Training Translation Loss 0.020752\n",
            "2025-07-10 14:07:00,387 Epoch 132:\n",
            "\t\t\tTotal Training Recognition Loss 578.170898 || Total Training Translation Loss 0.019376\n",
            "2025-07-10 14:07:00,592 Epoch 133:\n",
            "\t\t\tTotal Training Recognition Loss 596.206421 || Total Training Translation Loss 0.019705\n",
            "2025-07-10 14:07:00,797 Epoch 134:\n",
            "\t\t\tTotal Training Recognition Loss 600.360718 || Total Training Translation Loss 0.022210\n",
            "2025-07-10 14:07:01,003 Epoch 135:\n",
            "\t\t\tTotal Training Recognition Loss 609.212769 || Total Training Translation Loss 0.025823\n",
            "2025-07-10 14:07:01,208 Epoch 136:\n",
            "\t\t\tTotal Training Recognition Loss 567.502625 || Total Training Translation Loss 0.014012\n",
            "2025-07-10 14:07:01,414 Epoch 137:\n",
            "\t\t\tTotal Training Recognition Loss 575.818420 || Total Training Translation Loss 0.019463\n",
            "2025-07-10 14:07:01,620 Epoch 138:\n",
            "\t\t\tTotal Training Recognition Loss 578.170288 || Total Training Translation Loss 0.020516\n",
            "2025-07-10 14:07:01,828 Epoch 139:\n",
            "\t\t\tTotal Training Recognition Loss 564.838074 || Total Training Translation Loss 0.017783\n",
            "2025-07-10 14:07:02,035 Epoch 140:\n",
            "\t\t\tTotal Training Recognition Loss 565.386475 || Total Training Translation Loss 0.015618\n",
            "2025-07-10 14:07:02,209 Epoch 141:\n",
            "\t\t\tTotal Training Recognition Loss 549.430481 || Total Training Translation Loss 0.019901\n",
            "2025-07-10 14:07:02,381 Epoch 142:\n",
            "\t\t\tTotal Training Recognition Loss 551.072815 || Total Training Translation Loss 0.017382\n",
            "2025-07-10 14:07:02,553 Epoch 143:\n",
            "\t\t\tTotal Training Recognition Loss 550.117798 || Total Training Translation Loss 0.012032\n",
            "2025-07-10 14:07:02,724 Epoch 144:\n",
            "\t\t\tTotal Training Recognition Loss 541.336914 || Total Training Translation Loss 0.017198\n",
            "2025-07-10 14:07:02,896 Epoch 145:\n",
            "\t\t\tTotal Training Recognition Loss 532.302307 || Total Training Translation Loss 0.012907\n",
            "2025-07-10 14:07:03,072 Epoch 146:\n",
            "\t\t\tTotal Training Recognition Loss 518.295349 || Total Training Translation Loss 0.014524\n",
            "2025-07-10 14:07:03,244 Epoch 147:\n",
            "\t\t\tTotal Training Recognition Loss 540.375916 || Total Training Translation Loss 0.016065\n",
            "2025-07-10 14:07:03,419 Epoch 148:\n",
            "\t\t\tTotal Training Recognition Loss 519.660339 || Total Training Translation Loss 0.017916\n",
            "2025-07-10 14:07:03,594 Epoch 149:\n",
            "\t\t\tTotal Training Recognition Loss 499.840027 || Total Training Translation Loss 0.016182\n",
            "2025-07-10 14:07:03,771 Epoch 150:\n",
            "\t\t\tTotal Training Recognition Loss 512.250854 || Total Training Translation Loss 0.014771\n",
            "2025-07-10 14:07:03,942 Epoch 151:\n",
            "\t\t\tTotal Training Recognition Loss 503.895630 || Total Training Translation Loss 0.017441\n",
            "2025-07-10 14:07:04,113 Epoch 152:\n",
            "\t\t\tTotal Training Recognition Loss 511.637573 || Total Training Translation Loss 0.013032\n",
            "2025-07-10 14:07:04,285 Epoch 153:\n",
            "\t\t\tTotal Training Recognition Loss 511.450836 || Total Training Translation Loss 0.011010\n",
            "2025-07-10 14:07:04,456 Epoch 154:\n",
            "\t\t\tTotal Training Recognition Loss 480.911835 || Total Training Translation Loss 0.010343\n",
            "2025-07-10 14:07:04,628 Epoch 155:\n",
            "\t\t\tTotal Training Recognition Loss 486.564056 || Total Training Translation Loss 0.013524\n",
            "2025-07-10 14:07:04,802 Epoch 156:\n",
            "\t\t\tTotal Training Recognition Loss 484.715302 || Total Training Translation Loss 0.014235\n",
            "2025-07-10 14:07:04,972 Epoch 157:\n",
            "\t\t\tTotal Training Recognition Loss 474.785156 || Total Training Translation Loss 0.015536\n",
            "2025-07-10 14:07:05,182 Epoch 158:\n",
            "\t\t\tTotal Training Recognition Loss 475.115936 || Total Training Translation Loss 0.018435\n",
            "2025-07-10 14:07:05,388 Epoch 159:\n",
            "\t\t\tTotal Training Recognition Loss 455.624054 || Total Training Translation Loss 0.015506\n",
            "2025-07-10 14:07:05,595 Epoch 160:\n",
            "\t\t\tTotal Training Recognition Loss 458.801331 || Total Training Translation Loss 0.013306\n",
            "2025-07-10 14:07:05,769 Epoch 161:\n",
            "\t\t\tTotal Training Recognition Loss 454.212677 || Total Training Translation Loss 0.012728\n",
            "2025-07-10 14:07:05,940 Epoch 162:\n",
            "\t\t\tTotal Training Recognition Loss 458.163574 || Total Training Translation Loss 0.015755\n",
            "2025-07-10 14:07:06,112 Epoch 163:\n",
            "\t\t\tTotal Training Recognition Loss 453.162964 || Total Training Translation Loss 0.017576\n",
            "2025-07-10 14:07:06,284 Epoch 164:\n",
            "\t\t\tTotal Training Recognition Loss 437.662659 || Total Training Translation Loss 0.009746\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-07-10 14:07:06,455 Epoch 165:\n",
            "\t\t\tTotal Training Recognition Loss 455.700012 || Total Training Translation Loss 0.011624\n",
            "2025-07-10 14:07:06,626 Epoch 166:\n",
            "\t\t\tTotal Training Recognition Loss 443.716675 || Total Training Translation Loss 0.016201\n",
            "2025-07-10 14:07:06,798 Epoch 167:\n",
            "\t\t\tTotal Training Recognition Loss 432.213623 || Total Training Translation Loss 0.014882\n",
            "2025-07-10 14:07:06,971 Epoch 168:\n",
            "\t\t\tTotal Training Recognition Loss 422.781158 || Total Training Translation Loss 0.013453\n",
            "2025-07-10 14:07:07,144 Epoch 169:\n",
            "\t\t\tTotal Training Recognition Loss 454.079224 || Total Training Translation Loss 0.011415\n",
            "2025-07-10 14:07:07,317 Epoch 170:\n",
            "\t\t\tTotal Training Recognition Loss 431.693451 || Total Training Translation Loss 0.009770\n",
            "2025-07-10 14:07:07,489 Epoch 171:\n",
            "\t\t\tTotal Training Recognition Loss 459.995178 || Total Training Translation Loss 0.010069\n",
            "2025-07-10 14:07:07,661 Epoch 172:\n",
            "\t\t\tTotal Training Recognition Loss 420.494873 || Total Training Translation Loss 0.014537\n",
            "2025-07-10 14:07:07,833 Epoch 173:\n",
            "\t\t\tTotal Training Recognition Loss 455.740875 || Total Training Translation Loss 0.011617\n",
            "2025-07-10 14:07:08,004 Epoch 174:\n",
            "\t\t\tTotal Training Recognition Loss 404.201752 || Total Training Translation Loss 0.010882\n",
            "2025-07-10 14:07:08,175 Epoch 175:\n",
            "\t\t\tTotal Training Recognition Loss 413.104309 || Total Training Translation Loss 0.012048\n",
            "2025-07-10 14:07:08,345 Epoch 176:\n",
            "\t\t\tTotal Training Recognition Loss 398.838867 || Total Training Translation Loss 0.012414\n",
            "2025-07-10 14:07:08,516 Epoch 177:\n",
            "\t\t\tTotal Training Recognition Loss 427.986206 || Total Training Translation Loss 0.010872\n",
            "2025-07-10 14:07:08,688 Epoch 178:\n",
            "\t\t\tTotal Training Recognition Loss 387.823334 || Total Training Translation Loss 0.010479\n",
            "2025-07-10 14:07:08,863 Epoch 179:\n",
            "\t\t\tTotal Training Recognition Loss 388.081879 || Total Training Translation Loss 0.012968\n",
            "2025-07-10 14:07:09,038 Epoch 180:\n",
            "\t\t\tTotal Training Recognition Loss 424.208252 || Total Training Translation Loss 0.011054\n",
            "2025-07-10 14:07:09,248 Epoch 181:\n",
            "\t\t\tTotal Training Recognition Loss 389.149261 || Total Training Translation Loss 0.010565\n",
            "2025-07-10 14:07:09,428 Epoch 182:\n",
            "\t\t\tTotal Training Recognition Loss 369.366791 || Total Training Translation Loss 0.015028\n",
            "2025-07-10 14:07:09,602 Epoch 183:\n",
            "\t\t\tTotal Training Recognition Loss 392.039825 || Total Training Translation Loss 0.010806\n",
            "2025-07-10 14:07:09,774 Epoch 184:\n",
            "\t\t\tTotal Training Recognition Loss 398.563019 || Total Training Translation Loss 0.013022\n",
            "2025-07-10 14:07:09,985 Epoch 185:\n",
            "\t\t\tTotal Training Recognition Loss 361.619904 || Total Training Translation Loss 0.009359\n",
            "2025-07-10 14:07:10,193 Epoch 186:\n",
            "\t\t\tTotal Training Recognition Loss 380.690186 || Total Training Translation Loss 0.014662\n",
            "2025-07-10 14:07:10,401 Epoch 187:\n",
            "\t\t\tTotal Training Recognition Loss 367.665497 || Total Training Translation Loss 0.010432\n",
            "2025-07-10 14:07:10,607 Epoch 188:\n",
            "\t\t\tTotal Training Recognition Loss 373.978149 || Total Training Translation Loss 0.011900\n",
            "2025-07-10 14:07:10,784 Epoch 189:\n",
            "\t\t\tTotal Training Recognition Loss 361.217041 || Total Training Translation Loss 0.011880\n",
            "2025-07-10 14:07:10,957 Epoch 190:\n",
            "\t\t\tTotal Training Recognition Loss 406.324310 || Total Training Translation Loss 0.009581\n",
            "2025-07-10 14:07:11,143 Epoch 191:\n",
            "\t\t\tTotal Training Recognition Loss 363.098785 || Total Training Translation Loss 0.010367\n",
            "2025-07-10 14:07:11,316 Epoch 192:\n",
            "\t\t\tTotal Training Recognition Loss 400.618256 || Total Training Translation Loss 0.014146\n",
            "2025-07-10 14:07:11,487 Epoch 193:\n",
            "\t\t\tTotal Training Recognition Loss 347.898468 || Total Training Translation Loss 0.012110\n",
            "2025-07-10 14:07:11,661 Epoch 194:\n",
            "\t\t\tTotal Training Recognition Loss 354.774780 || Total Training Translation Loss 0.010370\n",
            "2025-07-10 14:07:11,831 Epoch 195:\n",
            "\t\t\tTotal Training Recognition Loss 368.987610 || Total Training Translation Loss 0.011170\n",
            "2025-07-10 14:07:12,007 Epoch 196:\n",
            "\t\t\tTotal Training Recognition Loss 357.851196 || Total Training Translation Loss 0.009601\n",
            "2025-07-10 14:07:12,176 Epoch 197:\n",
            "\t\t\tTotal Training Recognition Loss 349.541382 || Total Training Translation Loss 0.010141\n",
            "2025-07-10 14:07:12,346 Epoch 198:\n",
            "\t\t\tTotal Training Recognition Loss 337.262329 || Total Training Translation Loss 0.010473\n",
            "2025-07-10 14:07:12,518 Epoch 199:\n",
            "\t\t\tTotal Training Recognition Loss 345.104584 || Total Training Translation Loss 0.009727\n",
            "2025-07-10 14:07:12,686 [Epoch: 200 Step: 00000200] Batch Recognition Loss: 348.263245 => Gls Tokens per Sec:      222 || Batch Translation Loss:   0.008467 => Txt Tokens per Sec:      599 || Lr: 0.001000\n",
            "2025-07-10 14:07:12,899 Hooray! New best validation result [eval_metric]!\n",
            "2025-07-10 14:07:12,900 Saving new checkpoint.\n",
            "2025-07-10 14:07:12,988 Validation result at epoch 200, step      200: duration: 0.3013s\n",
            "\tRecognition Beam Size: 1\tTranslation Beam Size: 1\tTranslation Beam Alpha: -1\n",
            "\tRecognition Loss: 5041.03369\tTranslation Loss: 273.15405\tPPL: 24.86788\n",
            "\tEval Metric: BLEU\n",
            "\tWER 91.43\t(DEL: 91.43,\tINS: 0.00,\tSUB: 0.00)\n",
            "\tBLEU-4 2.00\t(BLEU-1: 7.33,\tBLEU-2: 3.90,\tBLEU-3: 2.79,\tBLEU-4: 2.00)\n",
            "\tCHRF 22.16\tROUGE 10.16\tFID 0.00\n",
            "2025-07-10 14:07:12,989 Logging Recognition and Translation Outputs\n",
            "2025-07-10 14:07:12,989 ========================================================================================\n",
            "2025-07-10 14:07:12,989 Logging Sequence: 11August_2010_Wednesday_tagesschau-2\n",
            "2025-07-10 14:07:12,989 \tGloss Reference :\tDRUCK TIEF KOMMEN\n",
            "2025-07-10 14:07:12,990 \tGloss Hypothesis:\t***** **** ******\n",
            "2025-07-10 14:07:12,990 \tGloss Alignment :\tD     D    D     \n",
            "2025-07-10 14:07:12,990 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:07:12,991 \tText Reference  :\t** *** **** ** ********** *** **** ******** ******* tiefer luftdruck bestimmt in ******* ******** ****** ********* *********** ***** ***** ***** ***** ***** ***** ***** den   nächsten tagen unser wetter\n",
            "2025-07-10 14:07:12,991 \tText Hypothesis :\tam tag gibt es verbreitet zum teil kräftige schauer oder   gewitter  und      in manchen regionen fallen ergiebige regenmengen <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>    <unk> <unk> <unk> \n",
            "2025-07-10 14:07:12,991 \tText Alignment  :\tI  I   I    I  I          I   I    I        I       S      S         S           I       I        I      I         I           I     I     I     I     I     I     I     S     S        S     S     S     \n",
            "2025-07-10 14:07:12,992 ========================================================================================\n",
            "2025-07-10 14:07:12,992 Logging Sequence: 11August_2010_Wednesday_tagesschau-3\n",
            "2025-07-10 14:07:12,992 \tGloss Reference :\tES-BEDEUTET VIEL WOLKE UND KOENNEN REGEN GEWITTER KOENNEN\n",
            "2025-07-10 14:07:12,992 \tGloss Hypothesis:\t*********** **** ***** *** ******* ***** ******** KOENNEN\n",
            "2025-07-10 14:07:12,993 \tGloss Alignment :\tD           D    D     D   D       D     D               \n",
            "2025-07-10 14:07:12,993 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:07:12,994 \tText Reference  :\tdas bedeutet viele wolken und  immer wieder     zum teil kräftige schauer **** ******** und ** ******* ******** ****** ********* *********** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** gewitter\n",
            "2025-07-10 14:07:12,994 \tText Hypothesis :\t*** ******** am    tag    gibt es    verbreitet zum teil kräftige schauer oder gewitter und in manchen regionen fallen ergiebige regenmengen <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>   \n",
            "2025-07-10 14:07:12,995 \tText Alignment  :\tD   D        S     S      S    S     S                                    I    I            I  I       I        I      I         I           I     I     I     I     I     I     I     I     I     I     I     S       \n",
            "2025-07-10 14:07:12,995 ========================================================================================\n",
            "2025-07-10 14:07:12,995 Logging Sequence: 11August_2010_Wednesday_tagesschau-8\n",
            "2025-07-10 14:07:12,995 \tGloss Reference :\tWIND MAESSIG SCHWACH REGION WENN GEWITTER WIND KOENNEN\n",
            "2025-07-10 14:07:12,995 \tGloss Hypothesis:\t**** ******* ******* ****** **** ******** **** *******\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-07-10 14:07:12,995 \tGloss Alignment :\tD    D       D       D      D    D        D    D      \n",
            "2025-07-10 14:07:12,996 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:07:12,998 \tText Reference  :\t************ ****** **** *** ***** ** ********** ***** ***** ***** ***** ***** meist weht  nur   ein   schwacher wind  aus   unterschiedlichen richtungen der   bei   schauern und   gewittern stark böig  sein  kann \n",
            "2025-07-10 14:07:12,998 \tText Hypothesis :\twolkenlücken finden sich vor allem im nordwesten <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>     <unk> <unk> <unk>             <unk>      <unk> <unk> <unk>    <unk> <unk>     <unk> <unk> <unk> <unk>\n",
            "2025-07-10 14:07:12,998 \tText Alignment  :\tI            I      I    I   I     I  I          I     I     I     I     I     S     S     S     S     S         S     S     S                 S          S     S     S        S     S         S     S     S     S    \n",
            "2025-07-10 14:07:12,998 ========================================================================================\n",
            "2025-07-10 14:07:12,998 Logging Sequence: 25October_2010_Monday_tagesschau-22\n",
            "2025-07-10 14:07:12,999 \tGloss Reference :\tMITTWOCH REGEN KOENNEN NORDWEST WAHRSCHEINLICH NORD STARK WIND\n",
            "2025-07-10 14:07:12,999 \tGloss Hypothesis:\t******** ***** ******* ******** ************** **** ***** ****\n",
            "2025-07-10 14:07:12,999 \tGloss Alignment :\tD        D     D       D        D              D    D     D   \n",
            "2025-07-10 14:07:12,999 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:07:13,001 \tText Reference  :\tam *** **** ** ********** *** **** ******** ******* mittwoch hier     und ** ******* ******** ****** ********* *********** ***** ***** da    nieselregen in    der   nordwesthälfte an    den   küsten kräftiger wind \n",
            "2025-07-10 14:07:13,001 \tText Hypothesis :\tam tag gibt es verbreitet zum teil kräftige schauer oder     gewitter und in manchen regionen fallen ergiebige regenmengen <unk> <unk> <unk> <unk>       <unk> <unk> <unk>          <unk> <unk> <unk>  <unk>     <unk>\n",
            "2025-07-10 14:07:13,001 \tText Alignment  :\t   I   I    I  I          I   I    I        I       S        S            I  I       I        I      I         I           I     I     S     S           S     S     S              S     S     S      S         S    \n",
            "2025-07-10 14:07:13,001 ========================================================================================\n",
            "2025-07-10 14:07:13,001 Logging Sequence: 05May_2011_Thursday_tagesschau-25\n",
            "2025-07-10 14:07:13,002 \tGloss Reference :\tJETZT WETTER WIE-AUSSEHEN MORGEN FREITAG SECHSTE MAI ZEIGEN-BILDSCHIRM\n",
            "2025-07-10 14:07:13,002 \tGloss Hypothesis:\tJETZT WETTER ************ ****** ******* ******* *** *****************\n",
            "2025-07-10 14:07:13,002 \tGloss Alignment :\t             D            D      D       D       D   D                \n",
            "2025-07-10 14:07:13,002 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:07:13,003 \tText Reference  :\t** *** **** ** ********** *** **** ******** ******* **** ******** und ** ******* ******** ****** ********* *********** ***** ***** ***** nun   die   wettervorhersage für   morgen freitag den   sechsten mai  \n",
            "2025-07-10 14:07:13,003 \tText Hypothesis :\tam tag gibt es verbreitet zum teil kräftige schauer oder gewitter und in manchen regionen fallen ergiebige regenmengen <unk> <unk> <unk> <unk> <unk> <unk>            <unk> <unk>  <unk>   <unk> <unk>    <unk>\n",
            "2025-07-10 14:07:13,004 \tText Alignment  :\tI  I   I    I  I          I   I    I        I       I    I            I  I       I        I      I         I           I     I     I     S     S     S                S     S      S       S     S        S    \n",
            "2025-07-10 14:07:13,004 ========================================================================================\n",
            "2025-07-10 14:07:13,004 Epoch 200:\n",
            "\t\t\tTotal Training Recognition Loss 348.263245 || Total Training Translation Loss 0.008467\n",
            "2025-07-10 14:07:13,172 Epoch 201:\n",
            "\t\t\tTotal Training Recognition Loss 372.542847 || Total Training Translation Loss 0.009480\n",
            "2025-07-10 14:07:13,343 Epoch 202:\n",
            "\t\t\tTotal Training Recognition Loss 358.632996 || Total Training Translation Loss 0.010157\n",
            "2025-07-10 14:07:13,511 Epoch 203:\n",
            "\t\t\tTotal Training Recognition Loss 345.714142 || Total Training Translation Loss 0.013842\n",
            "2025-07-10 14:07:13,680 Epoch 204:\n",
            "\t\t\tTotal Training Recognition Loss 315.569366 || Total Training Translation Loss 0.011155\n",
            "2025-07-10 14:07:13,850 Epoch 205:\n",
            "\t\t\tTotal Training Recognition Loss 315.733276 || Total Training Translation Loss 0.008396\n",
            "2025-07-10 14:07:14,024 Epoch 206:\n",
            "\t\t\tTotal Training Recognition Loss 351.850739 || Total Training Translation Loss 0.008707\n",
            "2025-07-10 14:07:14,194 Epoch 207:\n",
            "\t\t\tTotal Training Recognition Loss 357.632446 || Total Training Translation Loss 0.011902\n",
            "2025-07-10 14:07:14,364 Epoch 208:\n",
            "\t\t\tTotal Training Recognition Loss 289.682343 || Total Training Translation Loss 0.008740\n",
            "2025-07-10 14:07:14,534 Epoch 209:\n",
            "\t\t\tTotal Training Recognition Loss 341.028381 || Total Training Translation Loss 0.009175\n",
            "2025-07-10 14:07:14,701 Epoch 210:\n",
            "\t\t\tTotal Training Recognition Loss 295.066650 || Total Training Translation Loss 0.007803\n",
            "2025-07-10 14:07:14,875 Epoch 211:\n",
            "\t\t\tTotal Training Recognition Loss 302.883911 || Total Training Translation Loss 0.011469\n",
            "2025-07-10 14:07:15,044 Epoch 212:\n",
            "\t\t\tTotal Training Recognition Loss 301.045776 || Total Training Translation Loss 0.008412\n",
            "2025-07-10 14:07:15,213 Epoch 213:\n",
            "\t\t\tTotal Training Recognition Loss 274.335144 || Total Training Translation Loss 0.007416\n",
            "2025-07-10 14:07:15,412 Epoch 214:\n",
            "\t\t\tTotal Training Recognition Loss 271.831879 || Total Training Translation Loss 0.009320\n",
            "2025-07-10 14:07:15,595 Epoch 215:\n",
            "\t\t\tTotal Training Recognition Loss 278.596588 || Total Training Translation Loss 0.009927\n",
            "2025-07-10 14:07:15,766 Epoch 216:\n",
            "\t\t\tTotal Training Recognition Loss 239.945099 || Total Training Translation Loss 0.008301\n",
            "2025-07-10 14:07:15,935 Epoch 217:\n",
            "\t\t\tTotal Training Recognition Loss 245.325851 || Total Training Translation Loss 0.009958\n",
            "2025-07-10 14:07:16,106 Epoch 218:\n",
            "\t\t\tTotal Training Recognition Loss 242.471222 || Total Training Translation Loss 0.008726\n",
            "2025-07-10 14:07:16,274 Epoch 219:\n",
            "\t\t\tTotal Training Recognition Loss 236.322403 || Total Training Translation Loss 0.009191\n",
            "2025-07-10 14:07:16,451 Epoch 220:\n",
            "\t\t\tTotal Training Recognition Loss 235.345871 || Total Training Translation Loss 0.008041\n",
            "2025-07-10 14:07:16,622 Epoch 221:\n",
            "\t\t\tTotal Training Recognition Loss 243.158432 || Total Training Translation Loss 0.008045\n",
            "2025-07-10 14:07:16,791 Epoch 222:\n",
            "\t\t\tTotal Training Recognition Loss 221.336914 || Total Training Translation Loss 0.010058\n",
            "2025-07-10 14:07:16,960 Epoch 223:\n",
            "\t\t\tTotal Training Recognition Loss 247.475891 || Total Training Translation Loss 0.010007\n",
            "2025-07-10 14:07:17,131 Epoch 224:\n",
            "\t\t\tTotal Training Recognition Loss 221.995544 || Total Training Translation Loss 0.008362\n",
            "2025-07-10 14:07:17,333 Epoch 225:\n",
            "\t\t\tTotal Training Recognition Loss 204.334427 || Total Training Translation Loss 0.009197\n",
            "2025-07-10 14:07:17,516 Epoch 226:\n",
            "\t\t\tTotal Training Recognition Loss 215.070755 || Total Training Translation Loss 0.007839\n",
            "2025-07-10 14:07:17,686 Epoch 227:\n",
            "\t\t\tTotal Training Recognition Loss 175.618912 || Total Training Translation Loss 0.010742\n",
            "2025-07-10 14:07:17,855 Epoch 228:\n",
            "\t\t\tTotal Training Recognition Loss 182.559814 || Total Training Translation Loss 0.008401\n",
            "2025-07-10 14:07:18,062 Epoch 229:\n",
            "\t\t\tTotal Training Recognition Loss 191.832260 || Total Training Translation Loss 0.011481\n",
            "2025-07-10 14:07:18,268 Epoch 230:\n",
            "\t\t\tTotal Training Recognition Loss 181.862183 || Total Training Translation Loss 0.036223\n",
            "2025-07-10 14:07:18,474 Epoch 231:\n",
            "\t\t\tTotal Training Recognition Loss 173.655548 || Total Training Translation Loss 0.010805\n",
            "2025-07-10 14:07:18,646 Epoch 232:\n",
            "\t\t\tTotal Training Recognition Loss 189.072708 || Total Training Translation Loss 0.006609\n",
            "2025-07-10 14:07:18,816 Epoch 233:\n",
            "\t\t\tTotal Training Recognition Loss 165.836090 || Total Training Translation Loss 0.009491\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-07-10 14:07:18,987 Epoch 234:\n",
            "\t\t\tTotal Training Recognition Loss 166.346344 || Total Training Translation Loss 0.009526\n",
            "2025-07-10 14:07:19,157 Epoch 235:\n",
            "\t\t\tTotal Training Recognition Loss 162.416275 || Total Training Translation Loss 0.008465\n",
            "2025-07-10 14:07:19,326 Epoch 236:\n",
            "\t\t\tTotal Training Recognition Loss 151.671738 || Total Training Translation Loss 0.010063\n",
            "2025-07-10 14:07:19,496 Epoch 237:\n",
            "\t\t\tTotal Training Recognition Loss 152.834778 || Total Training Translation Loss 0.008812\n",
            "2025-07-10 14:07:19,665 Epoch 238:\n",
            "\t\t\tTotal Training Recognition Loss 127.307716 || Total Training Translation Loss 0.010752\n",
            "2025-07-10 14:07:19,836 Epoch 239:\n",
            "\t\t\tTotal Training Recognition Loss 140.454147 || Total Training Translation Loss 0.080785\n",
            "2025-07-10 14:07:20,004 Epoch 240:\n",
            "\t\t\tTotal Training Recognition Loss 177.562393 || Total Training Translation Loss 0.008718\n",
            "2025-07-10 14:07:20,172 Epoch 241:\n",
            "\t\t\tTotal Training Recognition Loss 140.804489 || Total Training Translation Loss 0.008938\n",
            "2025-07-10 14:07:20,341 Epoch 242:\n",
            "\t\t\tTotal Training Recognition Loss 137.869675 || Total Training Translation Loss 0.010417\n",
            "2025-07-10 14:07:20,509 Epoch 243:\n",
            "\t\t\tTotal Training Recognition Loss 146.649887 || Total Training Translation Loss 0.008248\n",
            "2025-07-10 14:07:20,678 Epoch 244:\n",
            "\t\t\tTotal Training Recognition Loss 139.650314 || Total Training Translation Loss 0.009956\n",
            "2025-07-10 14:07:20,847 Epoch 245:\n",
            "\t\t\tTotal Training Recognition Loss 119.742752 || Total Training Translation Loss 0.015000\n",
            "2025-07-10 14:07:21,015 Epoch 246:\n",
            "\t\t\tTotal Training Recognition Loss 156.687943 || Total Training Translation Loss 0.022724\n",
            "2025-07-10 14:07:21,184 Epoch 247:\n",
            "\t\t\tTotal Training Recognition Loss 136.195465 || Total Training Translation Loss 0.010807\n",
            "2025-07-10 14:07:21,353 Epoch 248:\n",
            "\t\t\tTotal Training Recognition Loss 136.439560 || Total Training Translation Loss 0.013751\n",
            "2025-07-10 14:07:21,522 Epoch 249:\n",
            "\t\t\tTotal Training Recognition Loss 139.536392 || Total Training Translation Loss 0.019729\n",
            "2025-07-10 14:07:21,692 Epoch 250:\n",
            "\t\t\tTotal Training Recognition Loss 123.741608 || Total Training Translation Loss 0.012914\n",
            "2025-07-10 14:07:21,861 Epoch 251:\n",
            "\t\t\tTotal Training Recognition Loss 110.432182 || Total Training Translation Loss 0.011064\n",
            "2025-07-10 14:07:22,031 Epoch 252:\n",
            "\t\t\tTotal Training Recognition Loss 126.005623 || Total Training Translation Loss 0.013550\n",
            "2025-07-10 14:07:22,204 Epoch 253:\n",
            "\t\t\tTotal Training Recognition Loss 106.171921 || Total Training Translation Loss 0.017338\n",
            "2025-07-10 14:07:22,375 Epoch 254:\n",
            "\t\t\tTotal Training Recognition Loss 144.157211 || Total Training Translation Loss 0.013213\n",
            "2025-07-10 14:07:22,545 Epoch 255:\n",
            "\t\t\tTotal Training Recognition Loss 116.577309 || Total Training Translation Loss 0.019093\n",
            "2025-07-10 14:07:22,715 Epoch 256:\n",
            "\t\t\tTotal Training Recognition Loss 116.614128 || Total Training Translation Loss 0.013059\n",
            "2025-07-10 14:07:22,885 Epoch 257:\n",
            "\t\t\tTotal Training Recognition Loss 97.034042 || Total Training Translation Loss 0.007484\n",
            "2025-07-10 14:07:23,055 Epoch 258:\n",
            "\t\t\tTotal Training Recognition Loss 112.758148 || Total Training Translation Loss 0.006698\n",
            "2025-07-10 14:07:23,223 Epoch 259:\n",
            "\t\t\tTotal Training Recognition Loss 87.703857 || Total Training Translation Loss 0.012283\n",
            "2025-07-10 14:07:23,392 Epoch 260:\n",
            "\t\t\tTotal Training Recognition Loss 81.040581 || Total Training Translation Loss 0.010536\n",
            "2025-07-10 14:07:23,564 Epoch 261:\n",
            "\t\t\tTotal Training Recognition Loss 66.711899 || Total Training Translation Loss 0.011847\n",
            "2025-07-10 14:07:23,735 Epoch 262:\n",
            "\t\t\tTotal Training Recognition Loss 66.565811 || Total Training Translation Loss 0.008341\n",
            "2025-07-10 14:07:23,908 Epoch 263:\n",
            "\t\t\tTotal Training Recognition Loss 81.696198 || Total Training Translation Loss 0.010299\n",
            "2025-07-10 14:07:24,076 Epoch 264:\n",
            "\t\t\tTotal Training Recognition Loss 53.287964 || Total Training Translation Loss 0.008692\n",
            "2025-07-10 14:07:24,245 Epoch 265:\n",
            "\t\t\tTotal Training Recognition Loss 107.508087 || Total Training Translation Loss 0.009286\n",
            "2025-07-10 14:07:24,415 Epoch 266:\n",
            "\t\t\tTotal Training Recognition Loss 57.775280 || Total Training Translation Loss 0.007592\n",
            "2025-07-10 14:07:24,584 Epoch 267:\n",
            "\t\t\tTotal Training Recognition Loss 50.321632 || Total Training Translation Loss 0.007426\n",
            "2025-07-10 14:07:24,752 Epoch 268:\n",
            "\t\t\tTotal Training Recognition Loss 42.758213 || Total Training Translation Loss 0.007242\n",
            "2025-07-10 14:07:24,920 Epoch 269:\n",
            "\t\t\tTotal Training Recognition Loss 62.469471 || Total Training Translation Loss 0.012161\n",
            "2025-07-10 14:07:25,089 Epoch 270:\n",
            "\t\t\tTotal Training Recognition Loss 66.981537 || Total Training Translation Loss 0.007955\n",
            "2025-07-10 14:07:25,258 Epoch 271:\n",
            "\t\t\tTotal Training Recognition Loss 46.532322 || Total Training Translation Loss 0.009580\n",
            "2025-07-10 14:07:25,427 Epoch 272:\n",
            "\t\t\tTotal Training Recognition Loss 104.671616 || Total Training Translation Loss 0.007246\n",
            "2025-07-10 14:07:25,596 Epoch 273:\n",
            "\t\t\tTotal Training Recognition Loss 62.487919 || Total Training Translation Loss 0.005599\n",
            "2025-07-10 14:07:25,767 Epoch 274:\n",
            "\t\t\tTotal Training Recognition Loss 37.584137 || Total Training Translation Loss 0.008355\n",
            "2025-07-10 14:07:25,937 Epoch 275:\n",
            "\t\t\tTotal Training Recognition Loss 42.590122 || Total Training Translation Loss 0.007951\n",
            "2025-07-10 14:07:26,105 Epoch 276:\n",
            "\t\t\tTotal Training Recognition Loss 29.760983 || Total Training Translation Loss 0.008386\n",
            "2025-07-10 14:07:26,274 Epoch 277:\n",
            "\t\t\tTotal Training Recognition Loss 33.809242 || Total Training Translation Loss 0.007592\n",
            "2025-07-10 14:07:26,442 Epoch 278:\n",
            "\t\t\tTotal Training Recognition Loss 38.425785 || Total Training Translation Loss 0.007521\n",
            "2025-07-10 14:07:26,611 Epoch 279:\n",
            "\t\t\tTotal Training Recognition Loss 32.600594 || Total Training Translation Loss 0.008052\n",
            "2025-07-10 14:07:26,779 Epoch 280:\n",
            "\t\t\tTotal Training Recognition Loss 32.039825 || Total Training Translation Loss 0.007665\n",
            "2025-07-10 14:07:26,948 Epoch 281:\n",
            "\t\t\tTotal Training Recognition Loss 50.536732 || Total Training Translation Loss 0.007345\n",
            "2025-07-10 14:07:27,117 Epoch 282:\n",
            "\t\t\tTotal Training Recognition Loss 43.810509 || Total Training Translation Loss 0.008081\n",
            "2025-07-10 14:07:27,288 Epoch 283:\n",
            "\t\t\tTotal Training Recognition Loss 28.203655 || Total Training Translation Loss 0.007112\n",
            "2025-07-10 14:07:27,460 Epoch 284:\n",
            "\t\t\tTotal Training Recognition Loss 37.071896 || Total Training Translation Loss 0.006188\n",
            "2025-07-10 14:07:27,631 Epoch 285:\n",
            "\t\t\tTotal Training Recognition Loss 28.935083 || Total Training Translation Loss 0.005591\n",
            "2025-07-10 14:07:27,803 Epoch 286:\n",
            "\t\t\tTotal Training Recognition Loss 35.936630 || Total Training Translation Loss 0.008853\n",
            "2025-07-10 14:07:27,974 Epoch 287:\n",
            "\t\t\tTotal Training Recognition Loss 32.741268 || Total Training Translation Loss 0.006533\n",
            "2025-07-10 14:07:28,146 Epoch 288:\n",
            "\t\t\tTotal Training Recognition Loss 63.505268 || Total Training Translation Loss 0.006452\n",
            "2025-07-10 14:07:28,318 Epoch 289:\n",
            "\t\t\tTotal Training Recognition Loss 26.734009 || Total Training Translation Loss 0.006692\n",
            "2025-07-10 14:07:28,490 Epoch 290:\n",
            "\t\t\tTotal Training Recognition Loss 94.645515 || Total Training Translation Loss 0.007357\n",
            "2025-07-10 14:07:28,661 Epoch 291:\n",
            "\t\t\tTotal Training Recognition Loss 53.624035 || Total Training Translation Loss 0.005970\n",
            "2025-07-10 14:07:28,832 Epoch 292:\n",
            "\t\t\tTotal Training Recognition Loss 21.328342 || Total Training Translation Loss 0.006404\n",
            "2025-07-10 14:07:29,003 Epoch 293:\n",
            "\t\t\tTotal Training Recognition Loss 76.894241 || Total Training Translation Loss 0.006015\n",
            "2025-07-10 14:07:29,175 Epoch 294:\n",
            "\t\t\tTotal Training Recognition Loss 22.504665 || Total Training Translation Loss 0.006124\n",
            "2025-07-10 14:07:29,346 Epoch 295:\n",
            "\t\t\tTotal Training Recognition Loss 39.904083 || Total Training Translation Loss 0.005191\n",
            "2025-07-10 14:07:29,518 Epoch 296:\n",
            "\t\t\tTotal Training Recognition Loss 21.203999 || Total Training Translation Loss 0.004938\n",
            "2025-07-10 14:07:29,688 Epoch 297:\n",
            "\t\t\tTotal Training Recognition Loss 25.505688 || Total Training Translation Loss 0.006301\n",
            "2025-07-10 14:07:29,860 Epoch 298:\n",
            "\t\t\tTotal Training Recognition Loss 39.908188 || Total Training Translation Loss 0.007804\n",
            "2025-07-10 14:07:30,031 Epoch 299:\n",
            "\t\t\tTotal Training Recognition Loss 27.066973 || Total Training Translation Loss 0.007784\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-07-10 14:07:30,205 [Epoch: 300 Step: 00000300] Batch Recognition Loss:  57.910343 => Gls Tokens per Sec:      214 || Batch Translation Loss:   0.007069 => Txt Tokens per Sec:      578 || Lr: 0.001000\n",
            "2025-07-10 14:07:30,493 Hooray! New best validation result [eval_metric]!\n",
            "2025-07-10 14:07:30,496 Saving new checkpoint.\n",
            "2025-07-10 14:07:30,596 Validation result at epoch 300, step      300: duration: 0.3903s\n",
            "\tRecognition Beam Size: 1\tTranslation Beam Size: 1\tTranslation Beam Alpha: -1\n",
            "\tRecognition Loss: 5965.93945\tTranslation Loss: 252.95355\tPPL: 19.60774\n",
            "\tEval Metric: BLEU\n",
            "\tWER 111.43\t(DEL: 40.00,\tINS: 20.00,\tSUB: 51.43)\n",
            "\tBLEU-4 2.05\t(BLEU-1: 8.00,\tBLEU-2: 4.07,\tBLEU-3: 2.87,\tBLEU-4: 2.05)\n",
            "\tCHRF 26.05\tROUGE 11.03\tFID 0.00\n",
            "2025-07-10 14:07:30,597 Logging Recognition and Translation Outputs\n",
            "2025-07-10 14:07:30,597 ========================================================================================\n",
            "2025-07-10 14:07:30,597 Logging Sequence: 11August_2010_Wednesday_tagesschau-2\n",
            "2025-07-10 14:07:30,597 \tGloss Reference :\tDRUCK TIEF KOMMEN\n",
            "2025-07-10 14:07:30,598 \tGloss Hypothesis:\t***** **** ******\n",
            "2025-07-10 14:07:30,598 \tGloss Alignment :\tD     D    D     \n",
            "2025-07-10 14:07:30,598 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:07:30,599 \tText Reference  :\t** *** **** ** ********** *** **** ******** ******* tiefer luftdruck bestimmt in ******* ******** ****** ********* *********** ***** ***** ***** ***** ***** ***** ***** den   nächsten tagen unser wetter\n",
            "2025-07-10 14:07:30,599 \tText Hypothesis :\tam tag gibt es verbreitet zum teil kräftige schauer oder   gewitter  und      in manchen regionen fallen ergiebige regenmengen <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>    <unk> <unk> <unk> \n",
            "2025-07-10 14:07:30,599 \tText Alignment  :\tI  I   I    I  I          I   I    I        I       S      S         S           I       I        I      I         I           I     I     I     I     I     I     I     S     S        S     S     S     \n",
            "2025-07-10 14:07:30,600 ========================================================================================\n",
            "2025-07-10 14:07:30,600 Logging Sequence: 11August_2010_Wednesday_tagesschau-3\n",
            "2025-07-10 14:07:30,600 \tGloss Reference :\tES-BEDEUTET VIEL WOLKE UND KOENNEN REGEN    GEWITTER KOENNEN\n",
            "2025-07-10 14:07:30,601 \tGloss Hypothesis:\t*********** **** ***** *** ******* NORDWEST HEUTE    KOENNEN\n",
            "2025-07-10 14:07:30,601 \tGloss Alignment :\tD           D    D     D   D       S        S               \n",
            "2025-07-10 14:07:30,601 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:07:30,603 \tText Reference  :\tdas bedeutet viele wolken und  immer wieder     zum teil kräftige schauer **** ******** und ** ******* ******** ****** ********* *********** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** gewitter\n",
            "2025-07-10 14:07:30,603 \tText Hypothesis :\t*** ******** am    tag    gibt es    verbreitet zum teil kräftige schauer oder gewitter und in manchen regionen fallen ergiebige regenmengen <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>   \n",
            "2025-07-10 14:07:30,603 \tText Alignment  :\tD   D        S     S      S    S     S                                    I    I            I  I       I        I      I         I           I     I     I     I     I     I     I     I     I     I     I     S       \n",
            "2025-07-10 14:07:30,604 ========================================================================================\n",
            "2025-07-10 14:07:30,604 Logging Sequence: 11August_2010_Wednesday_tagesschau-8\n",
            "2025-07-10 14:07:30,604 \tGloss Reference :\tWIND MAESSIG SCHWACH         REGION   WENN     GEWITTER WIND            KOENNEN \n",
            "2025-07-10 14:07:30,605 \tGloss Hypothesis:\t**** ORT     UEBERSCHWEMMUNG NORDWEST SPEZIELL NORDWEST UEBERSCHWEMMUNG NORDWEST\n",
            "2025-07-10 14:07:30,605 \tGloss Alignment :\tD    S       S               S        S        S        S               S       \n",
            "2025-07-10 14:07:30,605 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:07:30,607 \tText Reference  :\tmeist weht nur ein  schwacher wind       aus unterschiedlichen richtungen der     bei  schauern und ** ******* ******** ****** ********* *********** ***** ***** ***** ***** ***** ***** ***** gewittern stark böig  sein  kann \n",
            "2025-07-10 14:07:30,607 \tText Hypothesis :\t***** am   tag gibt es        verbreitet zum teil              kräftige   schauer oder gewitter und in manchen regionen fallen ergiebige regenmengen <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>     <unk> <unk> <unk> <unk>\n",
            "2025-07-10 14:07:30,608 \tText Alignment  :\tD     S    S   S    S         S          S   S                 S          S       S    S            I  I       I        I      I         I           I     I     I     I     I     I     I     S         S     S     S     S    \n",
            "2025-07-10 14:07:30,608 ========================================================================================\n",
            "2025-07-10 14:07:30,608 Logging Sequence: 25October_2010_Monday_tagesschau-22\n",
            "2025-07-10 14:07:30,608 \tGloss Reference :\t***** ********** ***** ******** ******** ********** *** MITTWOCH REGEN    KOENNEN NORDWEST WAHRSCHEINLICH NORD     STARK WIND \n",
            "2025-07-10 14:07:30,609 \tGloss Hypothesis:\tJETZT DONNERSTAG DURCH GEWITTER SUEDWEST DONNERSTAG ORT MORGEN   SUEDWEST JETZT   SUEDWEST MORGEN         MANCHMAL ORT   JETZT\n",
            "2025-07-10 14:07:30,609 \tGloss Alignment :\tI     I          I     I        I        I          I   S        S        S       S        S              S        S     S    \n",
            "2025-07-10 14:07:30,609 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:07:30,610 \tText Reference  :\tam *** **** ** ********** *** **** ******** ******* mittwoch hier     und ** ******* ******** ****** ********* *********** ***** ***** da    nieselregen in    der   nordwesthälfte an    den   küsten kräftiger wind \n",
            "2025-07-10 14:07:30,611 \tText Hypothesis :\tam tag gibt es verbreitet zum teil kräftige schauer oder     gewitter und in manchen regionen fallen ergiebige regenmengen <unk> <unk> <unk> <unk>       <unk> <unk> <unk>          <unk> <unk> <unk>  <unk>     <unk>\n",
            "2025-07-10 14:07:30,611 \tText Alignment  :\t   I   I    I  I          I   I    I        I       S        S            I  I       I        I      I         I           I     I     S     S           S     S     S              S     S     S      S         S    \n",
            "2025-07-10 14:07:30,611 ========================================================================================\n",
            "2025-07-10 14:07:30,611 Logging Sequence: 05May_2011_Thursday_tagesschau-25\n",
            "2025-07-10 14:07:30,611 \tGloss Reference :\tJETZT WETTER WIE-AUSSEHEN MORGEN FREITAG SECHSTE MAI ZEIGEN-BILDSCHIRM\n",
            "2025-07-10 14:07:30,612 \tGloss Hypothesis:\tJETZT WETTER ************ ****** ******* ******* *** DONNERSTAG       \n",
            "2025-07-10 14:07:30,612 \tGloss Alignment :\t             D            D      D       D       D   S                \n",
            "2025-07-10 14:07:30,612 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:07:30,613 \tText Reference  :\t** *** **** ** ********** *** **** ******** ******* **** ******** und ** ******* ******** ****** ********* *********** ***** ***** ***** nun   die   wettervorhersage für   morgen freitag den   sechsten mai  \n",
            "2025-07-10 14:07:30,613 \tText Hypothesis :\tam tag gibt es verbreitet zum teil kräftige schauer oder gewitter und in manchen regionen fallen ergiebige regenmengen <unk> <unk> <unk> <unk> <unk> <unk>            <unk> <unk>  <unk>   <unk> <unk>    <unk>\n",
            "2025-07-10 14:07:30,614 \tText Alignment  :\tI  I   I    I  I          I   I    I        I       I    I            I  I       I        I      I         I           I     I     I     S     S     S                S     S      S       S     S        S    \n",
            "2025-07-10 14:07:30,614 ========================================================================================\n",
            "2025-07-10 14:07:30,614 Epoch 300:\n",
            "\t\t\tTotal Training Recognition Loss 57.910343 || Total Training Translation Loss 0.007069\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-07-10 14:07:30,825 Epoch 301:\n",
            "\t\t\tTotal Training Recognition Loss 41.170067 || Total Training Translation Loss 0.008842\n",
            "2025-07-10 14:07:31,033 Epoch 302:\n",
            "\t\t\tTotal Training Recognition Loss 27.240196 || Total Training Translation Loss 0.006984\n",
            "2025-07-10 14:07:31,241 Epoch 303:\n",
            "\t\t\tTotal Training Recognition Loss 23.824463 || Total Training Translation Loss 0.005787\n",
            "2025-07-10 14:07:31,421 Epoch 304:\n",
            "\t\t\tTotal Training Recognition Loss 47.531368 || Total Training Translation Loss 0.005109\n",
            "2025-07-10 14:07:31,593 Epoch 305:\n",
            "\t\t\tTotal Training Recognition Loss 33.778526 || Total Training Translation Loss 0.006977\n",
            "2025-07-10 14:07:31,765 Epoch 306:\n",
            "\t\t\tTotal Training Recognition Loss 38.008690 || Total Training Translation Loss 0.007628\n",
            "2025-07-10 14:07:31,938 Epoch 307:\n",
            "\t\t\tTotal Training Recognition Loss 21.347105 || Total Training Translation Loss 0.007519\n",
            "2025-07-10 14:07:32,109 Epoch 308:\n",
            "\t\t\tTotal Training Recognition Loss 28.529421 || Total Training Translation Loss 0.007675\n",
            "2025-07-10 14:07:32,282 Epoch 309:\n",
            "\t\t\tTotal Training Recognition Loss 47.176105 || Total Training Translation Loss 0.006317\n",
            "2025-07-10 14:07:32,454 Epoch 310:\n",
            "\t\t\tTotal Training Recognition Loss 54.565918 || Total Training Translation Loss 0.004599\n",
            "2025-07-10 14:07:32,626 Epoch 311:\n",
            "\t\t\tTotal Training Recognition Loss 29.582941 || Total Training Translation Loss 0.006760\n",
            "2025-07-10 14:07:32,799 Epoch 312:\n",
            "\t\t\tTotal Training Recognition Loss 24.562695 || Total Training Translation Loss 0.005071\n",
            "2025-07-10 14:07:32,971 Epoch 313:\n",
            "\t\t\tTotal Training Recognition Loss 34.344971 || Total Training Translation Loss 0.004758\n",
            "2025-07-10 14:07:33,143 Epoch 314:\n",
            "\t\t\tTotal Training Recognition Loss 25.048521 || Total Training Translation Loss 0.005438\n",
            "2025-07-10 14:07:33,355 Epoch 315:\n",
            "\t\t\tTotal Training Recognition Loss 25.698895 || Total Training Translation Loss 0.005047\n",
            "2025-07-10 14:07:33,568 Epoch 316:\n",
            "\t\t\tTotal Training Recognition Loss 39.514946 || Total Training Translation Loss 0.004515\n",
            "2025-07-10 14:07:33,778 Epoch 317:\n",
            "\t\t\tTotal Training Recognition Loss 24.846790 || Total Training Translation Loss 0.004696\n",
            "2025-07-10 14:07:33,986 Epoch 318:\n",
            "\t\t\tTotal Training Recognition Loss 37.855297 || Total Training Translation Loss 0.005542\n",
            "2025-07-10 14:07:34,194 Epoch 319:\n",
            "\t\t\tTotal Training Recognition Loss 29.111267 || Total Training Translation Loss 0.004760\n",
            "2025-07-10 14:07:34,400 Epoch 320:\n",
            "\t\t\tTotal Training Recognition Loss 24.213818 || Total Training Translation Loss 0.005304\n",
            "2025-07-10 14:07:34,607 Epoch 321:\n",
            "\t\t\tTotal Training Recognition Loss 23.453926 || Total Training Translation Loss 0.003675\n",
            "2025-07-10 14:07:34,814 Epoch 322:\n",
            "\t\t\tTotal Training Recognition Loss 23.905979 || Total Training Translation Loss 0.004673\n",
            "2025-07-10 14:07:35,020 Epoch 323:\n",
            "\t\t\tTotal Training Recognition Loss 27.577168 || Total Training Translation Loss 0.004515\n",
            "2025-07-10 14:07:35,226 Epoch 324:\n",
            "\t\t\tTotal Training Recognition Loss 34.556774 || Total Training Translation Loss 0.009452\n",
            "2025-07-10 14:07:35,433 Epoch 325:\n",
            "\t\t\tTotal Training Recognition Loss 22.719061 || Total Training Translation Loss 0.005452\n",
            "2025-07-10 14:07:35,639 Epoch 326:\n",
            "\t\t\tTotal Training Recognition Loss 19.544161 || Total Training Translation Loss 0.004432\n",
            "2025-07-10 14:07:35,848 Epoch 327:\n",
            "\t\t\tTotal Training Recognition Loss 20.789854 || Total Training Translation Loss 0.005054\n",
            "2025-07-10 14:07:36,057 Epoch 328:\n",
            "\t\t\tTotal Training Recognition Loss 60.028351 || Total Training Translation Loss 0.005929\n",
            "2025-07-10 14:07:36,264 Epoch 329:\n",
            "\t\t\tTotal Training Recognition Loss 28.682634 || Total Training Translation Loss 0.003951\n",
            "2025-07-10 14:07:36,471 Epoch 330:\n",
            "\t\t\tTotal Training Recognition Loss 38.804131 || Total Training Translation Loss 0.005153\n",
            "2025-07-10 14:07:36,678 Epoch 331:\n",
            "\t\t\tTotal Training Recognition Loss 31.159830 || Total Training Translation Loss 0.005564\n",
            "2025-07-10 14:07:36,857 Epoch 332:\n",
            "\t\t\tTotal Training Recognition Loss 28.794329 || Total Training Translation Loss 0.006277\n",
            "2025-07-10 14:07:37,028 Epoch 333:\n",
            "\t\t\tTotal Training Recognition Loss 23.158028 || Total Training Translation Loss 0.005620\n",
            "2025-07-10 14:07:37,202 Epoch 334:\n",
            "\t\t\tTotal Training Recognition Loss 23.414124 || Total Training Translation Loss 0.004412\n",
            "2025-07-10 14:07:37,374 Epoch 335:\n",
            "\t\t\tTotal Training Recognition Loss 22.914015 || Total Training Translation Loss 0.005720\n",
            "2025-07-10 14:07:37,546 Epoch 336:\n",
            "\t\t\tTotal Training Recognition Loss 29.454216 || Total Training Translation Loss 0.005161\n",
            "2025-07-10 14:07:37,718 Epoch 337:\n",
            "\t\t\tTotal Training Recognition Loss 21.282995 || Total Training Translation Loss 0.006145\n",
            "2025-07-10 14:07:37,891 Epoch 338:\n",
            "\t\t\tTotal Training Recognition Loss 24.082254 || Total Training Translation Loss 0.004409\n",
            "2025-07-10 14:07:38,064 Epoch 339:\n",
            "\t\t\tTotal Training Recognition Loss 25.024136 || Total Training Translation Loss 0.005708\n",
            "2025-07-10 14:07:38,235 Epoch 340:\n",
            "\t\t\tTotal Training Recognition Loss 27.882586 || Total Training Translation Loss 0.005417\n",
            "2025-07-10 14:07:38,434 Epoch 341:\n",
            "\t\t\tTotal Training Recognition Loss 18.854641 || Total Training Translation Loss 0.005129\n",
            "2025-07-10 14:07:38,642 Epoch 342:\n",
            "\t\t\tTotal Training Recognition Loss 20.417486 || Total Training Translation Loss 0.007113\n",
            "2025-07-10 14:07:38,847 Epoch 343:\n",
            "\t\t\tTotal Training Recognition Loss 34.829163 || Total Training Translation Loss 0.006458\n",
            "2025-07-10 14:07:39,053 Epoch 344:\n",
            "\t\t\tTotal Training Recognition Loss 30.306473 || Total Training Translation Loss 0.005807\n",
            "2025-07-10 14:07:39,260 Epoch 345:\n",
            "\t\t\tTotal Training Recognition Loss 23.450344 || Total Training Translation Loss 0.007599\n",
            "2025-07-10 14:07:39,467 Epoch 346:\n",
            "\t\t\tTotal Training Recognition Loss 57.036720 || Total Training Translation Loss 0.004568\n",
            "2025-07-10 14:07:39,673 Epoch 347:\n",
            "\t\t\tTotal Training Recognition Loss 34.218975 || Total Training Translation Loss 0.005595\n",
            "2025-07-10 14:07:39,847 Epoch 348:\n",
            "\t\t\tTotal Training Recognition Loss 30.191734 || Total Training Translation Loss 0.004522\n",
            "2025-07-10 14:07:40,017 Epoch 349:\n",
            "\t\t\tTotal Training Recognition Loss 42.976776 || Total Training Translation Loss 0.005371\n",
            "2025-07-10 14:07:40,186 Epoch 350:\n",
            "\t\t\tTotal Training Recognition Loss 19.256681 || Total Training Translation Loss 0.005036\n",
            "2025-07-10 14:07:40,357 Epoch 351:\n",
            "\t\t\tTotal Training Recognition Loss 24.341879 || Total Training Translation Loss 0.004054\n",
            "2025-07-10 14:07:40,528 Epoch 352:\n",
            "\t\t\tTotal Training Recognition Loss 18.462769 || Total Training Translation Loss 0.003981\n",
            "2025-07-10 14:07:40,698 Epoch 353:\n",
            "\t\t\tTotal Training Recognition Loss 19.140249 || Total Training Translation Loss 0.005923\n",
            "2025-07-10 14:07:40,870 Epoch 354:\n",
            "\t\t\tTotal Training Recognition Loss 23.990561 || Total Training Translation Loss 0.005087\n",
            "2025-07-10 14:07:41,039 Epoch 355:\n",
            "\t\t\tTotal Training Recognition Loss 19.348072 || Total Training Translation Loss 0.004489\n",
            "2025-07-10 14:07:41,210 Epoch 356:\n",
            "\t\t\tTotal Training Recognition Loss 19.685287 || Total Training Translation Loss 0.005682\n",
            "2025-07-10 14:07:41,380 Epoch 357:\n",
            "\t\t\tTotal Training Recognition Loss 89.961723 || Total Training Translation Loss 0.007030\n",
            "2025-07-10 14:07:41,553 Epoch 358:\n",
            "\t\t\tTotal Training Recognition Loss 22.347712 || Total Training Translation Loss 0.005736\n",
            "2025-07-10 14:07:41,724 Epoch 359:\n",
            "\t\t\tTotal Training Recognition Loss 23.350676 || Total Training Translation Loss 0.004805\n",
            "2025-07-10 14:07:41,894 Epoch 360:\n",
            "\t\t\tTotal Training Recognition Loss 37.554829 || Total Training Translation Loss 0.005649\n",
            "2025-07-10 14:07:42,064 Epoch 361:\n",
            "\t\t\tTotal Training Recognition Loss 22.758678 || Total Training Translation Loss 0.003554\n",
            "2025-07-10 14:07:42,233 Epoch 362:\n",
            "\t\t\tTotal Training Recognition Loss 21.451895 || Total Training Translation Loss 0.003820\n",
            "2025-07-10 14:07:42,402 Epoch 363:\n",
            "\t\t\tTotal Training Recognition Loss 40.189125 || Total Training Translation Loss 0.004379\n",
            "2025-07-10 14:07:42,573 Epoch 364:\n",
            "\t\t\tTotal Training Recognition Loss 20.682314 || Total Training Translation Loss 0.004924\n",
            "2025-07-10 14:07:42,742 Epoch 365:\n",
            "\t\t\tTotal Training Recognition Loss 25.248522 || Total Training Translation Loss 0.003657\n",
            "2025-07-10 14:07:42,911 Epoch 366:\n",
            "\t\t\tTotal Training Recognition Loss 48.161274 || Total Training Translation Loss 0.003907\n",
            "2025-07-10 14:07:43,119 Epoch 367:\n",
            "\t\t\tTotal Training Recognition Loss 22.053543 || Total Training Translation Loss 0.004625\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-07-10 14:07:43,327 Epoch 368:\n",
            "\t\t\tTotal Training Recognition Loss 23.329962 || Total Training Translation Loss 0.006105\n",
            "2025-07-10 14:07:43,537 Epoch 369:\n",
            "\t\t\tTotal Training Recognition Loss 61.841228 || Total Training Translation Loss 0.005311\n",
            "2025-07-10 14:07:43,744 Epoch 370:\n",
            "\t\t\tTotal Training Recognition Loss 27.680698 || Total Training Translation Loss 0.004970\n",
            "2025-07-10 14:07:43,951 Epoch 371:\n",
            "\t\t\tTotal Training Recognition Loss 26.516230 || Total Training Translation Loss 0.005644\n",
            "2025-07-10 14:07:44,159 Epoch 372:\n",
            "\t\t\tTotal Training Recognition Loss 57.541142 || Total Training Translation Loss 0.005672\n",
            "2025-07-10 14:07:44,368 Epoch 373:\n",
            "\t\t\tTotal Training Recognition Loss 19.454630 || Total Training Translation Loss 0.004188\n",
            "2025-07-10 14:07:44,575 Epoch 374:\n",
            "\t\t\tTotal Training Recognition Loss 146.049820 || Total Training Translation Loss 0.004277\n",
            "2025-07-10 14:07:44,751 Epoch 375:\n",
            "\t\t\tTotal Training Recognition Loss 45.166908 || Total Training Translation Loss 0.006008\n",
            "2025-07-10 14:07:44,920 Epoch 376:\n",
            "\t\t\tTotal Training Recognition Loss 30.397873 || Total Training Translation Loss 0.006397\n",
            "2025-07-10 14:07:45,089 Epoch 377:\n",
            "\t\t\tTotal Training Recognition Loss 43.685619 || Total Training Translation Loss 0.006086\n",
            "2025-07-10 14:07:45,258 Epoch 378:\n",
            "\t\t\tTotal Training Recognition Loss 25.284327 || Total Training Translation Loss 0.005735\n",
            "2025-07-10 14:07:45,429 Epoch 379:\n",
            "\t\t\tTotal Training Recognition Loss 37.564198 || Total Training Translation Loss 0.004856\n",
            "2025-07-10 14:07:45,599 Epoch 380:\n",
            "\t\t\tTotal Training Recognition Loss 23.462389 || Total Training Translation Loss 0.004621\n",
            "2025-07-10 14:07:45,770 Epoch 381:\n",
            "\t\t\tTotal Training Recognition Loss 30.398983 || Total Training Translation Loss 0.006699\n",
            "2025-07-10 14:07:45,940 Epoch 382:\n",
            "\t\t\tTotal Training Recognition Loss 25.500927 || Total Training Translation Loss 0.005448\n",
            "2025-07-10 14:07:46,112 Epoch 383:\n",
            "\t\t\tTotal Training Recognition Loss 53.302479 || Total Training Translation Loss 0.006064\n",
            "2025-07-10 14:07:46,283 Epoch 384:\n",
            "\t\t\tTotal Training Recognition Loss 22.613085 || Total Training Translation Loss 0.005713\n",
            "2025-07-10 14:07:46,452 Epoch 385:\n",
            "\t\t\tTotal Training Recognition Loss 19.735712 || Total Training Translation Loss 0.004655\n",
            "2025-07-10 14:07:46,626 Epoch 386:\n",
            "\t\t\tTotal Training Recognition Loss 22.042988 || Total Training Translation Loss 0.005446\n",
            "2025-07-10 14:07:46,796 Epoch 387:\n",
            "\t\t\tTotal Training Recognition Loss 24.411739 || Total Training Translation Loss 0.005818\n",
            "2025-07-10 14:07:46,965 Epoch 388:\n",
            "\t\t\tTotal Training Recognition Loss 17.308310 || Total Training Translation Loss 0.004873\n",
            "2025-07-10 14:07:47,134 Epoch 389:\n",
            "\t\t\tTotal Training Recognition Loss 19.254004 || Total Training Translation Loss 0.004276\n",
            "2025-07-10 14:07:47,303 Epoch 390:\n",
            "\t\t\tTotal Training Recognition Loss 22.382998 || Total Training Translation Loss 0.004126\n",
            "2025-07-10 14:07:47,472 Epoch 391:\n",
            "\t\t\tTotal Training Recognition Loss 26.131990 || Total Training Translation Loss 0.005038\n",
            "2025-07-10 14:07:47,641 Epoch 392:\n",
            "\t\t\tTotal Training Recognition Loss 33.100277 || Total Training Translation Loss 0.005040\n",
            "2025-07-10 14:07:47,810 Epoch 393:\n",
            "\t\t\tTotal Training Recognition Loss 35.903477 || Total Training Translation Loss 0.004322\n",
            "2025-07-10 14:07:47,979 Epoch 394:\n",
            "\t\t\tTotal Training Recognition Loss 17.672058 || Total Training Translation Loss 0.004641\n",
            "2025-07-10 14:07:48,149 Epoch 395:\n",
            "\t\t\tTotal Training Recognition Loss 22.359991 || Total Training Translation Loss 0.004724\n",
            "2025-07-10 14:07:48,317 Epoch 396:\n",
            "\t\t\tTotal Training Recognition Loss 18.475082 || Total Training Translation Loss 0.008158\n",
            "2025-07-10 14:07:48,486 Epoch 397:\n",
            "\t\t\tTotal Training Recognition Loss 17.340254 || Total Training Translation Loss 0.004464\n",
            "2025-07-10 14:07:48,656 Epoch 398:\n",
            "\t\t\tTotal Training Recognition Loss 29.151213 || Total Training Translation Loss 0.004245\n",
            "2025-07-10 14:07:48,869 Epoch 399:\n",
            "\t\t\tTotal Training Recognition Loss 27.914663 || Total Training Translation Loss 0.004953\n",
            "2025-07-10 14:07:49,086 [Epoch: 400 Step: 00000400] Batch Recognition Loss:  16.826426 => Gls Tokens per Sec:      172 || Batch Translation Loss:   0.004581 => Txt Tokens per Sec:      464 || Lr: 0.001000\n",
            "2025-07-10 14:07:49,390 Validation result at epoch 400, step      400: duration: 0.3033s\n",
            "\tRecognition Beam Size: 1\tTranslation Beam Size: 1\tTranslation Beam Alpha: -1\n",
            "\tRecognition Loss: 5704.57568\tTranslation Loss: 261.05164\tPPL: 21.56767\n",
            "\tEval Metric: BLEU\n",
            "\tWER 91.43\t(DEL: 48.57,\tINS: 2.86,\tSUB: 40.00)\n",
            "\tBLEU-4 2.05\t(BLEU-1: 8.00,\tBLEU-2: 4.07,\tBLEU-3: 2.87,\tBLEU-4: 2.05)\n",
            "\tCHRF 26.05\tROUGE 11.03\tFID 0.00\n",
            "2025-07-10 14:07:49,391 Logging Recognition and Translation Outputs\n",
            "2025-07-10 14:07:49,392 ========================================================================================\n",
            "2025-07-10 14:07:49,392 Logging Sequence: 11August_2010_Wednesday_tagesschau-2\n",
            "2025-07-10 14:07:49,393 \tGloss Reference :\tDRUCK TIEF KOMMEN\n",
            "2025-07-10 14:07:49,393 \tGloss Hypothesis:\t***** **** ******\n",
            "2025-07-10 14:07:49,393 \tGloss Alignment :\tD     D    D     \n",
            "2025-07-10 14:07:49,393 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:07:49,396 \tText Reference  :\t** *** **** ** ********** *** **** ******** ******* tiefer luftdruck bestimmt in ******* ******** ****** ********* *********** ***** ***** ***** ***** ***** ***** ***** den   nächsten tagen unser wetter\n",
            "2025-07-10 14:07:49,397 \tText Hypothesis :\tam tag gibt es verbreitet zum teil kräftige schauer oder   gewitter  und      in manchen regionen fallen ergiebige regenmengen <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>    <unk> <unk> <unk> \n",
            "2025-07-10 14:07:49,397 \tText Alignment  :\tI  I   I    I  I          I   I    I        I       S      S         S           I       I        I      I         I           I     I     I     I     I     I     I     S     S        S     S     S     \n",
            "2025-07-10 14:07:49,397 ========================================================================================\n",
            "2025-07-10 14:07:49,397 Logging Sequence: 11August_2010_Wednesday_tagesschau-3\n",
            "2025-07-10 14:07:49,398 \tGloss Reference :\tES-BEDEUTET VIEL WOLKE UND KOENNEN REGEN GEWITTER KOENNEN\n",
            "2025-07-10 14:07:49,398 \tGloss Hypothesis:\t*********** **** ***** *** ******* ***** HEUTE    KOENNEN\n",
            "2025-07-10 14:07:49,398 \tGloss Alignment :\tD           D    D     D   D       D     S               \n",
            "2025-07-10 14:07:49,399 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:07:49,402 \tText Reference  :\tdas bedeutet viele wolken und  immer wieder     zum teil kräftige schauer **** ******** und ** ******* ******** ****** ********* *********** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** gewitter\n",
            "2025-07-10 14:07:49,403 \tText Hypothesis :\t*** ******** am    tag    gibt es    verbreitet zum teil kräftige schauer oder gewitter und in manchen regionen fallen ergiebige regenmengen <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>   \n",
            "2025-07-10 14:07:49,404 \tText Alignment  :\tD   D        S     S      S    S     S                                    I    I            I  I       I        I      I         I           I     I     I     I     I     I     I     I     I     I     I     S       \n",
            "2025-07-10 14:07:49,404 ========================================================================================\n",
            "2025-07-10 14:07:49,404 Logging Sequence: 11August_2010_Wednesday_tagesschau-8\n",
            "2025-07-10 14:07:49,405 \tGloss Reference :\tWIND MAESSIG SCHWACH REGION   WENN     GEWITTER WIND            KOENNEN ********\n",
            "2025-07-10 14:07:49,406 \tGloss Hypothesis:\t**** ******* ******* NORDWEST SPEZIELL NORDWEST UEBERSCHWEMMUNG KOENNEN NORDWEST\n",
            "2025-07-10 14:07:49,406 \tGloss Alignment :\tD    D       D       S        S        S        S                       I       \n",
            "2025-07-10 14:07:49,406 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:07:49,411 \tText Reference  :\tmeist weht nur ein  schwacher wind       aus unterschiedlichen richtungen der     bei  schauern und ** ******* ******** ****** ********* *********** ***** ***** ***** ***** ***** ***** ***** gewittern stark böig  sein  kann \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-07-10 14:07:49,412 \tText Hypothesis :\t***** am   tag gibt es        verbreitet zum teil              kräftige   schauer oder gewitter und in manchen regionen fallen ergiebige regenmengen <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>     <unk> <unk> <unk> <unk>\n",
            "2025-07-10 14:07:49,412 \tText Alignment  :\tD     S    S   S    S         S          S   S                 S          S       S    S            I  I       I        I      I         I           I     I     I     I     I     I     I     S         S     S     S     S    \n",
            "2025-07-10 14:07:49,412 ========================================================================================\n",
            "2025-07-10 14:07:49,412 Logging Sequence: 25October_2010_Monday_tagesschau-22\n",
            "2025-07-10 14:07:49,413 \tGloss Reference :\tMITTWOCH REGEN  KOENNEN    NORDWEST WAHRSCHEINLICH NORD   STARK WIND \n",
            "2025-07-10 14:07:49,413 \tGloss Hypothesis:\tJETZT    WETTER DONNERSTAG ORT      JETZT          MORGEN ORT   JETZT\n",
            "2025-07-10 14:07:49,414 \tGloss Alignment :\tS        S      S          S        S              S      S     S    \n",
            "2025-07-10 14:07:49,414 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:07:49,418 \tText Reference  :\tam *** **** ** ********** *** **** ******** ******* mittwoch hier     und ** ******* ******** ****** ********* *********** ***** ***** da    nieselregen in    der   nordwesthälfte an    den   küsten kräftiger wind \n",
            "2025-07-10 14:07:49,418 \tText Hypothesis :\tam tag gibt es verbreitet zum teil kräftige schauer oder     gewitter und in manchen regionen fallen ergiebige regenmengen <unk> <unk> <unk> <unk>       <unk> <unk> <unk>          <unk> <unk> <unk>  <unk>     <unk>\n",
            "2025-07-10 14:07:49,418 \tText Alignment  :\t   I   I    I  I          I   I    I        I       S        S            I  I       I        I      I         I           I     I     S     S           S     S     S              S     S     S      S         S    \n",
            "2025-07-10 14:07:49,419 ========================================================================================\n",
            "2025-07-10 14:07:49,419 Logging Sequence: 05May_2011_Thursday_tagesschau-25\n",
            "2025-07-10 14:07:49,419 \tGloss Reference :\tJETZT WETTER WIE-AUSSEHEN MORGEN FREITAG SECHSTE MAI ZEIGEN-BILDSCHIRM\n",
            "2025-07-10 14:07:49,420 \tGloss Hypothesis:\tJETZT WETTER ************ ****** ******* ******* *** DONNERSTAG       \n",
            "2025-07-10 14:07:49,420 \tGloss Alignment :\t             D            D      D       D       D   S                \n",
            "2025-07-10 14:07:49,420 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:07:49,423 \tText Reference  :\t** *** **** ** ********** *** **** ******** ******* **** ******** und ** ******* ******** ****** ********* *********** ***** ***** ***** nun   die   wettervorhersage für   morgen freitag den   sechsten mai  \n",
            "2025-07-10 14:07:49,423 \tText Hypothesis :\tam tag gibt es verbreitet zum teil kräftige schauer oder gewitter und in manchen regionen fallen ergiebige regenmengen <unk> <unk> <unk> <unk> <unk> <unk>            <unk> <unk>  <unk>   <unk> <unk>    <unk>\n",
            "2025-07-10 14:07:49,423 \tText Alignment  :\tI  I   I    I  I          I   I    I        I       I    I            I  I       I        I      I         I           I     I     I     S     S     S                S     S      S       S     S        S    \n",
            "2025-07-10 14:07:49,424 ========================================================================================\n",
            "2025-07-10 14:07:49,426 Epoch 400:\n",
            "\t\t\tTotal Training Recognition Loss 16.826426 || Total Training Translation Loss 0.004581\n",
            "2025-07-10 14:07:49,639 Epoch 401:\n",
            "\t\t\tTotal Training Recognition Loss 26.158382 || Total Training Translation Loss 0.005523\n",
            "2025-07-10 14:07:49,857 Epoch 402:\n",
            "\t\t\tTotal Training Recognition Loss 17.015421 || Total Training Translation Loss 0.004117\n",
            "2025-07-10 14:07:50,075 Epoch 403:\n",
            "\t\t\tTotal Training Recognition Loss 22.752462 || Total Training Translation Loss 0.004498\n",
            "2025-07-10 14:07:50,293 Epoch 404:\n",
            "\t\t\tTotal Training Recognition Loss 19.053722 || Total Training Translation Loss 0.004266\n",
            "2025-07-10 14:07:50,511 Epoch 405:\n",
            "\t\t\tTotal Training Recognition Loss 25.386482 || Total Training Translation Loss 0.004871\n",
            "2025-07-10 14:07:50,728 Epoch 406:\n",
            "\t\t\tTotal Training Recognition Loss 19.669058 || Total Training Translation Loss 0.004821\n",
            "2025-07-10 14:07:50,946 Epoch 407:\n",
            "\t\t\tTotal Training Recognition Loss 15.962511 || Total Training Translation Loss 0.003599\n",
            "2025-07-10 14:07:51,164 Epoch 408:\n",
            "\t\t\tTotal Training Recognition Loss 19.933661 || Total Training Translation Loss 0.003274\n",
            "2025-07-10 14:07:51,380 Epoch 409:\n",
            "\t\t\tTotal Training Recognition Loss 19.083609 || Total Training Translation Loss 0.004797\n",
            "2025-07-10 14:07:51,598 Epoch 410:\n",
            "\t\t\tTotal Training Recognition Loss 16.573536 || Total Training Translation Loss 0.005199\n",
            "2025-07-10 14:07:51,814 Epoch 411:\n",
            "\t\t\tTotal Training Recognition Loss 23.305687 || Total Training Translation Loss 0.004473\n",
            "2025-07-10 14:07:52,032 Epoch 412:\n",
            "\t\t\tTotal Training Recognition Loss 16.175293 || Total Training Translation Loss 0.005287\n",
            "2025-07-10 14:07:52,248 Epoch 413:\n",
            "\t\t\tTotal Training Recognition Loss 47.170364 || Total Training Translation Loss 0.004036\n",
            "2025-07-10 14:07:52,466 Epoch 414:\n",
            "\t\t\tTotal Training Recognition Loss 13.266648 || Total Training Translation Loss 0.004390\n",
            "2025-07-10 14:07:52,642 Epoch 415:\n",
            "\t\t\tTotal Training Recognition Loss 17.756168 || Total Training Translation Loss 0.005566\n",
            "2025-07-10 14:07:52,813 Epoch 416:\n",
            "\t\t\tTotal Training Recognition Loss 19.322071 || Total Training Translation Loss 0.003891\n",
            "2025-07-10 14:07:52,985 Epoch 417:\n",
            "\t\t\tTotal Training Recognition Loss 22.650578 || Total Training Translation Loss 0.003743\n",
            "2025-07-10 14:07:53,157 Epoch 418:\n",
            "\t\t\tTotal Training Recognition Loss 54.396027 || Total Training Translation Loss 0.004691\n",
            "2025-07-10 14:07:53,329 Epoch 419:\n",
            "\t\t\tTotal Training Recognition Loss 27.852154 || Total Training Translation Loss 0.006985\n",
            "2025-07-10 14:07:53,501 Epoch 420:\n",
            "\t\t\tTotal Training Recognition Loss 102.161026 || Total Training Translation Loss 0.006508\n",
            "2025-07-10 14:07:53,676 Epoch 421:\n",
            "\t\t\tTotal Training Recognition Loss 41.097977 || Total Training Translation Loss 0.006032\n",
            "2025-07-10 14:07:53,847 Epoch 422:\n",
            "\t\t\tTotal Training Recognition Loss 17.879555 || Total Training Translation Loss 0.006930\n",
            "2025-07-10 14:07:54,021 Epoch 423:\n",
            "\t\t\tTotal Training Recognition Loss 29.447447 || Total Training Translation Loss 0.006148\n",
            "2025-07-10 14:07:54,191 Epoch 424:\n",
            "\t\t\tTotal Training Recognition Loss 20.260695 || Total Training Translation Loss 0.006952\n",
            "2025-07-10 14:07:54,362 Epoch 425:\n",
            "\t\t\tTotal Training Recognition Loss 18.724871 || Total Training Translation Loss 0.004859\n",
            "2025-07-10 14:07:54,533 Epoch 426:\n",
            "\t\t\tTotal Training Recognition Loss 18.422466 || Total Training Translation Loss 0.004526\n",
            "2025-07-10 14:07:54,703 Epoch 427:\n",
            "\t\t\tTotal Training Recognition Loss 18.413727 || Total Training Translation Loss 0.003653\n",
            "2025-07-10 14:07:54,874 Epoch 428:\n",
            "\t\t\tTotal Training Recognition Loss 17.960026 || Total Training Translation Loss 0.004689\n",
            "2025-07-10 14:07:55,045 Epoch 429:\n",
            "\t\t\tTotal Training Recognition Loss 14.124091 || Total Training Translation Loss 0.004253\n",
            "2025-07-10 14:07:55,217 Epoch 430:\n",
            "\t\t\tTotal Training Recognition Loss 15.678624 || Total Training Translation Loss 0.004083\n",
            "2025-07-10 14:07:55,387 Epoch 431:\n",
            "\t\t\tTotal Training Recognition Loss 17.988216 || Total Training Translation Loss 0.004349\n",
            "2025-07-10 14:07:55,558 Epoch 432:\n",
            "\t\t\tTotal Training Recognition Loss 33.335693 || Total Training Translation Loss 0.003664\n",
            "2025-07-10 14:07:55,729 Epoch 433:\n",
            "\t\t\tTotal Training Recognition Loss 7.029301 || Total Training Translation Loss 0.004547\n",
            "2025-07-10 14:07:55,899 Epoch 434:\n",
            "\t\t\tTotal Training Recognition Loss 34.069176 || Total Training Translation Loss 0.005557\n",
            "2025-07-10 14:07:56,070 Epoch 435:\n",
            "\t\t\tTotal Training Recognition Loss 9.539509 || Total Training Translation Loss 0.005175\n",
            "2025-07-10 14:07:56,290 Epoch 436:\n",
            "\t\t\tTotal Training Recognition Loss 47.383091 || Total Training Translation Loss 0.004538\n",
            "2025-07-10 14:07:56,507 Epoch 437:\n",
            "\t\t\tTotal Training Recognition Loss 8.706409 || Total Training Translation Loss 0.004505\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-07-10 14:07:56,726 Epoch 438:\n",
            "\t\t\tTotal Training Recognition Loss 20.413469 || Total Training Translation Loss 0.004017\n",
            "2025-07-10 14:07:56,942 Epoch 439:\n",
            "\t\t\tTotal Training Recognition Loss 13.640618 || Total Training Translation Loss 0.005730\n",
            "2025-07-10 14:07:57,160 Epoch 440:\n",
            "\t\t\tTotal Training Recognition Loss 12.410967 || Total Training Translation Loss 0.005016\n",
            "2025-07-10 14:07:57,377 Epoch 441:\n",
            "\t\t\tTotal Training Recognition Loss 9.821864 || Total Training Translation Loss 0.004906\n",
            "2025-07-10 14:07:57,593 Epoch 442:\n",
            "\t\t\tTotal Training Recognition Loss 16.220194 || Total Training Translation Loss 0.005515\n",
            "2025-07-10 14:07:57,808 Epoch 443:\n",
            "\t\t\tTotal Training Recognition Loss 5.755795 || Total Training Translation Loss 0.004392\n",
            "2025-07-10 14:07:58,025 Epoch 444:\n",
            "\t\t\tTotal Training Recognition Loss 11.504919 || Total Training Translation Loss 0.004451\n",
            "2025-07-10 14:07:58,243 Epoch 445:\n",
            "\t\t\tTotal Training Recognition Loss 15.559539 || Total Training Translation Loss 0.004439\n",
            "2025-07-10 14:07:58,460 Epoch 446:\n",
            "\t\t\tTotal Training Recognition Loss 4.601390 || Total Training Translation Loss 0.004022\n",
            "2025-07-10 14:07:58,676 Epoch 447:\n",
            "\t\t\tTotal Training Recognition Loss 4.323662 || Total Training Translation Loss 0.005076\n",
            "2025-07-10 14:07:58,893 Epoch 448:\n",
            "\t\t\tTotal Training Recognition Loss 12.613153 || Total Training Translation Loss 0.005715\n",
            "2025-07-10 14:07:59,109 Epoch 449:\n",
            "\t\t\tTotal Training Recognition Loss 7.309347 || Total Training Translation Loss 0.004133\n",
            "2025-07-10 14:07:59,326 Epoch 450:\n",
            "\t\t\tTotal Training Recognition Loss 6.220922 || Total Training Translation Loss 0.005584\n",
            "2025-07-10 14:07:59,542 Epoch 451:\n",
            "\t\t\tTotal Training Recognition Loss 3.531580 || Total Training Translation Loss 0.004032\n",
            "2025-07-10 14:07:59,759 Epoch 452:\n",
            "\t\t\tTotal Training Recognition Loss 4.648194 || Total Training Translation Loss 0.003713\n",
            "2025-07-10 14:07:59,931 Epoch 453:\n",
            "\t\t\tTotal Training Recognition Loss 26.376408 || Total Training Translation Loss 0.003878\n",
            "2025-07-10 14:08:00,102 Epoch 454:\n",
            "\t\t\tTotal Training Recognition Loss 7.689344 || Total Training Translation Loss 0.004594\n",
            "2025-07-10 14:08:00,321 Epoch 455:\n",
            "\t\t\tTotal Training Recognition Loss 13.450468 || Total Training Translation Loss 0.006113\n",
            "2025-07-10 14:08:00,537 Epoch 456:\n",
            "\t\t\tTotal Training Recognition Loss 69.268723 || Total Training Translation Loss 0.005022\n",
            "2025-07-10 14:08:00,754 Epoch 457:\n",
            "\t\t\tTotal Training Recognition Loss 7.807894 || Total Training Translation Loss 0.004039\n",
            "2025-07-10 14:08:00,969 Epoch 458:\n",
            "\t\t\tTotal Training Recognition Loss 3.704543 || Total Training Translation Loss 0.005107\n",
            "2025-07-10 14:08:01,187 Epoch 459:\n",
            "\t\t\tTotal Training Recognition Loss 2.877353 || Total Training Translation Loss 0.004444\n",
            "2025-07-10 14:08:01,404 Epoch 460:\n",
            "\t\t\tTotal Training Recognition Loss 2.101044 || Total Training Translation Loss 0.007501\n",
            "2025-07-10 14:08:01,622 Epoch 461:\n",
            "\t\t\tTotal Training Recognition Loss 6.282054 || Total Training Translation Loss 0.003930\n",
            "2025-07-10 14:08:01,838 Epoch 462:\n",
            "\t\t\tTotal Training Recognition Loss 15.700866 || Total Training Translation Loss 0.004036\n",
            "2025-07-10 14:08:02,056 Epoch 463:\n",
            "\t\t\tTotal Training Recognition Loss 4.999396 || Total Training Translation Loss 0.004996\n",
            "2025-07-10 14:08:02,273 Epoch 464:\n",
            "\t\t\tTotal Training Recognition Loss 11.224982 || Total Training Translation Loss 0.004820\n",
            "2025-07-10 14:08:02,490 Epoch 465:\n",
            "\t\t\tTotal Training Recognition Loss 4.330856 || Total Training Translation Loss 0.007875\n",
            "2025-07-10 14:08:02,708 Epoch 466:\n",
            "\t\t\tTotal Training Recognition Loss 3.378353 || Total Training Translation Loss 0.006606\n",
            "2025-07-10 14:08:02,925 Epoch 467:\n",
            "\t\t\tTotal Training Recognition Loss 5.913414 || Total Training Translation Loss 0.004172\n",
            "2025-07-10 14:08:03,142 Epoch 468:\n",
            "\t\t\tTotal Training Recognition Loss 3.492555 || Total Training Translation Loss 0.004894\n",
            "2025-07-10 14:08:03,359 Epoch 469:\n",
            "\t\t\tTotal Training Recognition Loss 7.198094 || Total Training Translation Loss 0.004934\n",
            "2025-07-10 14:08:03,576 Epoch 470:\n",
            "\t\t\tTotal Training Recognition Loss 2.670890 || Total Training Translation Loss 0.004579\n",
            "2025-07-10 14:08:03,792 Epoch 471:\n",
            "\t\t\tTotal Training Recognition Loss 8.514957 || Total Training Translation Loss 0.004209\n",
            "2025-07-10 14:08:04,010 Epoch 472:\n",
            "\t\t\tTotal Training Recognition Loss 1.370023 || Total Training Translation Loss 0.004288\n",
            "2025-07-10 14:08:04,227 Epoch 473:\n",
            "\t\t\tTotal Training Recognition Loss 31.702988 || Total Training Translation Loss 0.003146\n",
            "2025-07-10 14:08:04,445 Epoch 474:\n",
            "\t\t\tTotal Training Recognition Loss 1.453231 || Total Training Translation Loss 0.004510\n",
            "2025-07-10 14:08:04,662 Epoch 475:\n",
            "\t\t\tTotal Training Recognition Loss 5.828093 || Total Training Translation Loss 0.004357\n",
            "2025-07-10 14:08:04,877 Epoch 476:\n",
            "\t\t\tTotal Training Recognition Loss 10.210636 || Total Training Translation Loss 0.004726\n",
            "2025-07-10 14:08:05,094 Epoch 477:\n",
            "\t\t\tTotal Training Recognition Loss 55.526119 || Total Training Translation Loss 0.004578\n",
            "2025-07-10 14:08:05,312 Epoch 478:\n",
            "\t\t\tTotal Training Recognition Loss 26.324860 || Total Training Translation Loss 0.006492\n",
            "2025-07-10 14:08:05,529 Epoch 479:\n",
            "\t\t\tTotal Training Recognition Loss 4.902362 || Total Training Translation Loss 0.004990\n",
            "2025-07-10 14:08:05,746 Epoch 480:\n",
            "\t\t\tTotal Training Recognition Loss 3.340652 || Total Training Translation Loss 0.004483\n",
            "2025-07-10 14:08:05,965 Epoch 481:\n",
            "\t\t\tTotal Training Recognition Loss 5.242035 || Total Training Translation Loss 0.004441\n",
            "2025-07-10 14:08:06,154 Epoch 482:\n",
            "\t\t\tTotal Training Recognition Loss 2.049482 || Total Training Translation Loss 0.004968\n",
            "2025-07-10 14:08:06,327 Epoch 483:\n",
            "\t\t\tTotal Training Recognition Loss 10.061250 || Total Training Translation Loss 0.004087\n",
            "2025-07-10 14:08:06,500 Epoch 484:\n",
            "\t\t\tTotal Training Recognition Loss 3.799905 || Total Training Translation Loss 0.004728\n",
            "2025-07-10 14:08:06,672 Epoch 485:\n",
            "\t\t\tTotal Training Recognition Loss 4.487200 || Total Training Translation Loss 0.003765\n",
            "2025-07-10 14:08:06,847 Epoch 486:\n",
            "\t\t\tTotal Training Recognition Loss 11.111441 || Total Training Translation Loss 0.004403\n",
            "2025-07-10 14:08:07,023 Epoch 487:\n",
            "\t\t\tTotal Training Recognition Loss 3.105705 || Total Training Translation Loss 0.004141\n",
            "2025-07-10 14:08:07,198 Epoch 488:\n",
            "\t\t\tTotal Training Recognition Loss 30.815006 || Total Training Translation Loss 0.004032\n",
            "2025-07-10 14:08:07,372 Epoch 489:\n",
            "\t\t\tTotal Training Recognition Loss 2.090776 || Total Training Translation Loss 0.004020\n",
            "2025-07-10 14:08:07,544 Epoch 490:\n",
            "\t\t\tTotal Training Recognition Loss 7.338651 || Total Training Translation Loss 0.003681\n",
            "2025-07-10 14:08:07,719 Epoch 491:\n",
            "\t\t\tTotal Training Recognition Loss 6.640372 || Total Training Translation Loss 0.004400\n",
            "2025-07-10 14:08:07,895 Epoch 492:\n",
            "\t\t\tTotal Training Recognition Loss 13.201992 || Total Training Translation Loss 0.003973\n",
            "2025-07-10 14:08:08,069 Epoch 493:\n",
            "\t\t\tTotal Training Recognition Loss 20.291210 || Total Training Translation Loss 0.003073\n",
            "2025-07-10 14:08:08,246 Epoch 494:\n",
            "\t\t\tTotal Training Recognition Loss 3.267250 || Total Training Translation Loss 0.003880\n",
            "2025-07-10 14:08:08,467 Epoch 495:\n",
            "\t\t\tTotal Training Recognition Loss 5.549060 || Total Training Translation Loss 0.004233\n",
            "2025-07-10 14:08:08,685 Epoch 496:\n",
            "\t\t\tTotal Training Recognition Loss 3.641793 || Total Training Translation Loss 0.003577\n",
            "2025-07-10 14:08:08,903 Epoch 497:\n",
            "\t\t\tTotal Training Recognition Loss 2.414588 || Total Training Translation Loss 0.004326\n",
            "2025-07-10 14:08:09,086 Epoch 498:\n",
            "\t\t\tTotal Training Recognition Loss 2.117382 || Total Training Translation Loss 0.003980\n",
            "2025-07-10 14:08:09,261 Epoch 499:\n",
            "\t\t\tTotal Training Recognition Loss 16.911203 || Total Training Translation Loss 0.005604\n",
            "2025-07-10 14:08:09,437 [Epoch: 500 Step: 00000500] Batch Recognition Loss:   6.510443 => Gls Tokens per Sec:      212 || Batch Translation Loss:   0.004409 => Txt Tokens per Sec:      573 || Lr: 0.001000\n",
            "2025-07-10 14:08:09,674 Validation result at epoch 500, step      500: duration: 0.2365s\n",
            "\tRecognition Beam Size: 1\tTranslation Beam Size: 1\tTranslation Beam Alpha: -1\n",
            "\tRecognition Loss: 5325.88672\tTranslation Loss: 262.86859\tPPL: 22.03367\n",
            "\tEval Metric: BLEU\n",
            "\tWER 88.57\t(DEL: 37.14,\tINS: 2.86,\tSUB: 48.57)\n",
            "\tBLEU-4 2.00\t(BLEU-1: 7.33,\tBLEU-2: 3.90,\tBLEU-3: 2.79,\tBLEU-4: 2.00)\n",
            "\tCHRF 22.16\tROUGE 10.16\tFID 0.00\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-07-10 14:08:09,675 Logging Recognition and Translation Outputs\n",
            "2025-07-10 14:08:09,676 ========================================================================================\n",
            "2025-07-10 14:08:09,676 Logging Sequence: 11August_2010_Wednesday_tagesschau-2\n",
            "2025-07-10 14:08:09,676 \tGloss Reference :\tDRUCK TIEF KOMMEN\n",
            "2025-07-10 14:08:09,677 \tGloss Hypothesis:\t***** **** ******\n",
            "2025-07-10 14:08:09,677 \tGloss Alignment :\tD     D    D     \n",
            "2025-07-10 14:08:09,677 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:08:09,679 \tText Reference  :\t** *** **** ** ********** *** **** ******** ******* tiefer luftdruck bestimmt in ******* ******** ****** ********* *********** ***** ***** ***** ***** ***** ***** ***** den   nächsten tagen unser wetter\n",
            "2025-07-10 14:08:09,680 \tText Hypothesis :\tam tag gibt es verbreitet zum teil kräftige schauer oder   gewitter  und      in manchen regionen fallen ergiebige regenmengen <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>    <unk> <unk> <unk> \n",
            "2025-07-10 14:08:09,680 \tText Alignment  :\tI  I   I    I  I          I   I    I        I       S      S         S           I       I        I      I         I           I     I     I     I     I     I     I     S     S        S     S     S     \n",
            "2025-07-10 14:08:09,681 ========================================================================================\n",
            "2025-07-10 14:08:09,681 Logging Sequence: 11August_2010_Wednesday_tagesschau-3\n",
            "2025-07-10 14:08:09,682 \tGloss Reference :\tES-BEDEUTET VIEL WOLKE UND KOENNEN REGEN GEWITTER KOENNEN\n",
            "2025-07-10 14:08:09,682 \tGloss Hypothesis:\t*********** **** ***** *** ******* ***** LOCH     KOENNEN\n",
            "2025-07-10 14:08:09,682 \tGloss Alignment :\tD           D    D     D   D       D     S               \n",
            "2025-07-10 14:08:09,682 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:08:09,685 \tText Reference  :\tdas bedeutet viele wolken und  immer wieder     zum teil kräftige schauer **** ******** und ** ******* ******** ****** ********* *********** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** gewitter\n",
            "2025-07-10 14:08:09,686 \tText Hypothesis :\t*** ******** am    tag    gibt es    verbreitet zum teil kräftige schauer oder gewitter und in manchen regionen fallen ergiebige regenmengen <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>   \n",
            "2025-07-10 14:08:09,686 \tText Alignment  :\tD   D        S     S      S    S     S                                    I    I            I  I       I        I      I         I           I     I     I     I     I     I     I     I     I     I     I     S       \n",
            "2025-07-10 14:08:09,686 ========================================================================================\n",
            "2025-07-10 14:08:09,687 Logging Sequence: 11August_2010_Wednesday_tagesschau-8\n",
            "2025-07-10 14:08:09,687 \tGloss Reference :\tWIND    MAESSIG SCHWACH REGION  WENN     GEWITTER WIND            KOENNEN ********\n",
            "2025-07-10 14:08:09,687 \tGloss Hypothesis:\tTROCKEN ORT     TROCKEN BLEIBEN NORDWEST KOENNEN  UEBERSCHWEMMUNG KOENNEN NORDWEST\n",
            "2025-07-10 14:08:09,688 \tGloss Alignment :\tS       S       S       S       S        S        S                       I       \n",
            "2025-07-10 14:08:09,688 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:08:09,690 \tText Reference  :\t************ ****** **** *** ***** ** ********** ***** ***** ***** ***** ***** meist weht  nur   ein   schwacher wind  aus   unterschiedlichen richtungen der   bei   schauern und   gewittern stark böig  sein  kann \n",
            "2025-07-10 14:08:09,691 \tText Hypothesis :\twolkenlücken finden sich vor allem im nordwesten <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>     <unk> <unk> <unk>             <unk>      <unk> <unk> <unk>    <unk> <unk>     <unk> <unk> <unk> <unk>\n",
            "2025-07-10 14:08:09,691 \tText Alignment  :\tI            I      I    I   I     I  I          I     I     I     I     I     S     S     S     S     S         S     S     S                 S          S     S     S        S     S         S     S     S     S    \n",
            "2025-07-10 14:08:09,691 ========================================================================================\n",
            "2025-07-10 14:08:09,691 Logging Sequence: 25October_2010_Monday_tagesschau-22\n",
            "2025-07-10 14:08:09,692 \tGloss Reference :\tMITTWOCH REGEN KOENNEN NORDWEST WAHRSCHEINLICH NORD STARK WIND   \n",
            "2025-07-10 14:08:09,692 \tGloss Hypothesis:\tJETZT    REGEN FEBRUAR JETZT    MORGEN         ORT  JETZT KOENNEN\n",
            "2025-07-10 14:08:09,693 \tGloss Alignment :\tS              S       S        S              S    S     S      \n",
            "2025-07-10 14:08:09,693 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:08:09,695 \tText Reference  :\tam *** **** ** ********** *** **** ******** ******* mittwoch hier     und ** ******* ******** ****** ********* *********** ***** ***** da    nieselregen in    der   nordwesthälfte an    den   küsten kräftiger wind \n",
            "2025-07-10 14:08:09,695 \tText Hypothesis :\tam tag gibt es verbreitet zum teil kräftige schauer oder     gewitter und in manchen regionen fallen ergiebige regenmengen <unk> <unk> <unk> <unk>       <unk> <unk> <unk>          <unk> <unk> <unk>  <unk>     <unk>\n",
            "2025-07-10 14:08:09,696 \tText Alignment  :\t   I   I    I  I          I   I    I        I       S        S            I  I       I        I      I         I           I     I     S     S           S     S     S              S     S     S      S         S    \n",
            "2025-07-10 14:08:09,696 ========================================================================================\n",
            "2025-07-10 14:08:09,697 Logging Sequence: 05May_2011_Thursday_tagesschau-25\n",
            "2025-07-10 14:08:09,697 \tGloss Reference :\tJETZT WETTER WIE-AUSSEHEN MORGEN FREITAG SECHSTE MAI ZEIGEN-BILDSCHIRM\n",
            "2025-07-10 14:08:09,697 \tGloss Hypothesis:\tJETZT WETTER ************ ****** ******* ******* ORT DONNERSTAG       \n",
            "2025-07-10 14:08:09,698 \tGloss Alignment :\t             D            D      D       D       S   S                \n",
            "2025-07-10 14:08:09,698 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:08:09,700 \tText Reference  :\t** *** **** ** ********** *** **** ******** ******* **** ******** und ** ******* ******** ****** ********* *********** ***** ***** ***** nun   die   wettervorhersage für   morgen freitag den   sechsten mai  \n",
            "2025-07-10 14:08:09,700 \tText Hypothesis :\tam tag gibt es verbreitet zum teil kräftige schauer oder gewitter und in manchen regionen fallen ergiebige regenmengen <unk> <unk> <unk> <unk> <unk> <unk>            <unk> <unk>  <unk>   <unk> <unk>    <unk>\n",
            "2025-07-10 14:08:09,700 \tText Alignment  :\tI  I   I    I  I          I   I    I        I       I    I            I  I       I        I      I         I           I     I     I     S     S     S                S     S      S       S     S        S    \n",
            "2025-07-10 14:08:09,700 ========================================================================================\n",
            "2025-07-10 14:08:09,701 Epoch 500:\n",
            "\t\t\tTotal Training Recognition Loss 6.510443 || Total Training Translation Loss 0.004409\n",
            "2025-07-10 14:08:09,874 Epoch 501:\n",
            "\t\t\tTotal Training Recognition Loss 9.279638 || Total Training Translation Loss 0.003995\n",
            "2025-07-10 14:08:10,053 Epoch 502:\n",
            "\t\t\tTotal Training Recognition Loss 8.535012 || Total Training Translation Loss 0.005454\n",
            "2025-07-10 14:08:10,232 Epoch 503:\n",
            "\t\t\tTotal Training Recognition Loss 3.183066 || Total Training Translation Loss 0.005408\n",
            "2025-07-10 14:08:10,405 Epoch 504:\n",
            "\t\t\tTotal Training Recognition Loss 9.703262 || Total Training Translation Loss 0.004694\n",
            "2025-07-10 14:08:10,579 Epoch 505:\n",
            "\t\t\tTotal Training Recognition Loss 4.012748 || Total Training Translation Loss 0.003657\n",
            "2025-07-10 14:08:10,754 Epoch 506:\n",
            "\t\t\tTotal Training Recognition Loss 3.373139 || Total Training Translation Loss 0.005047\n",
            "2025-07-10 14:08:10,927 Epoch 507:\n",
            "\t\t\tTotal Training Recognition Loss 12.737954 || Total Training Translation Loss 0.004634\n",
            "2025-07-10 14:08:11,101 Epoch 508:\n",
            "\t\t\tTotal Training Recognition Loss 3.825868 || Total Training Translation Loss 0.004545\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-07-10 14:08:11,274 Epoch 509:\n",
            "\t\t\tTotal Training Recognition Loss 3.867943 || Total Training Translation Loss 0.004765\n",
            "2025-07-10 14:08:11,490 Epoch 510:\n",
            "\t\t\tTotal Training Recognition Loss 9.407827 || Total Training Translation Loss 0.004297\n",
            "2025-07-10 14:08:11,665 Epoch 511:\n",
            "\t\t\tTotal Training Recognition Loss 2.933731 || Total Training Translation Loss 0.004060\n",
            "2025-07-10 14:08:11,884 Epoch 512:\n",
            "\t\t\tTotal Training Recognition Loss 2.017596 || Total Training Translation Loss 0.005797\n",
            "2025-07-10 14:08:12,106 Epoch 513:\n",
            "\t\t\tTotal Training Recognition Loss 13.633778 || Total Training Translation Loss 0.004709\n",
            "2025-07-10 14:08:12,327 Epoch 514:\n",
            "\t\t\tTotal Training Recognition Loss 4.754662 || Total Training Translation Loss 0.004252\n",
            "2025-07-10 14:08:12,549 Epoch 515:\n",
            "\t\t\tTotal Training Recognition Loss 22.436306 || Total Training Translation Loss 0.005005\n",
            "2025-07-10 14:08:12,772 Epoch 516:\n",
            "\t\t\tTotal Training Recognition Loss 9.280334 || Total Training Translation Loss 0.003642\n",
            "2025-07-10 14:08:12,954 Epoch 517:\n",
            "\t\t\tTotal Training Recognition Loss 2.057564 || Total Training Translation Loss 0.004564\n",
            "2025-07-10 14:08:13,128 Epoch 518:\n",
            "\t\t\tTotal Training Recognition Loss 0.927609 || Total Training Translation Loss 0.004206\n",
            "2025-07-10 14:08:13,300 Epoch 519:\n",
            "\t\t\tTotal Training Recognition Loss 2.706894 || Total Training Translation Loss 0.004197\n",
            "2025-07-10 14:08:13,472 Epoch 520:\n",
            "\t\t\tTotal Training Recognition Loss 0.674425 || Total Training Translation Loss 0.005012\n",
            "2025-07-10 14:08:13,645 Epoch 521:\n",
            "\t\t\tTotal Training Recognition Loss 0.984738 || Total Training Translation Loss 0.004358\n",
            "2025-07-10 14:08:13,818 Epoch 522:\n",
            "\t\t\tTotal Training Recognition Loss 10.325653 || Total Training Translation Loss 0.005596\n",
            "2025-07-10 14:08:13,992 Epoch 523:\n",
            "\t\t\tTotal Training Recognition Loss 1.653692 || Total Training Translation Loss 0.004750\n",
            "2025-07-10 14:08:14,178 Epoch 524:\n",
            "\t\t\tTotal Training Recognition Loss 5.138901 || Total Training Translation Loss 0.004497\n",
            "2025-07-10 14:08:14,365 Epoch 525:\n",
            "\t\t\tTotal Training Recognition Loss 2.642190 || Total Training Translation Loss 0.004179\n",
            "2025-07-10 14:08:14,541 Epoch 526:\n",
            "\t\t\tTotal Training Recognition Loss 2.188320 || Total Training Translation Loss 0.005093\n",
            "2025-07-10 14:08:14,715 Epoch 527:\n",
            "\t\t\tTotal Training Recognition Loss 0.797064 || Total Training Translation Loss 0.004405\n",
            "2025-07-10 14:08:14,900 Epoch 528:\n",
            "\t\t\tTotal Training Recognition Loss 1.325711 || Total Training Translation Loss 0.004006\n",
            "2025-07-10 14:08:15,077 Epoch 529:\n",
            "\t\t\tTotal Training Recognition Loss 1.487840 || Total Training Translation Loss 0.004561\n",
            "2025-07-10 14:08:15,250 Epoch 530:\n",
            "\t\t\tTotal Training Recognition Loss 5.527576 || Total Training Translation Loss 0.004317\n",
            "2025-07-10 14:08:15,422 Epoch 531:\n",
            "\t\t\tTotal Training Recognition Loss 4.442476 || Total Training Translation Loss 0.005617\n",
            "2025-07-10 14:08:15,597 Epoch 532:\n",
            "\t\t\tTotal Training Recognition Loss 1.238912 || Total Training Translation Loss 0.004405\n",
            "2025-07-10 14:08:15,770 Epoch 533:\n",
            "\t\t\tTotal Training Recognition Loss 1.197015 || Total Training Translation Loss 0.008619\n",
            "2025-07-10 14:08:15,992 Epoch 534:\n",
            "\t\t\tTotal Training Recognition Loss 1.961331 || Total Training Translation Loss 0.003175\n",
            "2025-07-10 14:08:16,209 Epoch 535:\n",
            "\t\t\tTotal Training Recognition Loss 1.540817 || Total Training Translation Loss 0.003612\n",
            "2025-07-10 14:08:16,427 Epoch 536:\n",
            "\t\t\tTotal Training Recognition Loss 0.921735 || Total Training Translation Loss 0.003567\n",
            "2025-07-10 14:08:16,647 Epoch 537:\n",
            "\t\t\tTotal Training Recognition Loss 1.440079 || Total Training Translation Loss 0.003554\n",
            "2025-07-10 14:08:16,827 Epoch 538:\n",
            "\t\t\tTotal Training Recognition Loss 10.991858 || Total Training Translation Loss 0.003279\n",
            "2025-07-10 14:08:17,001 Epoch 539:\n",
            "\t\t\tTotal Training Recognition Loss 0.774786 || Total Training Translation Loss 0.004054\n",
            "2025-07-10 14:08:17,174 Epoch 540:\n",
            "\t\t\tTotal Training Recognition Loss 0.812291 || Total Training Translation Loss 0.004600\n",
            "2025-07-10 14:08:17,347 Epoch 541:\n",
            "\t\t\tTotal Training Recognition Loss 1.101952 || Total Training Translation Loss 0.003949\n",
            "2025-07-10 14:08:17,520 Epoch 542:\n",
            "\t\t\tTotal Training Recognition Loss 2.949552 || Total Training Translation Loss 0.004215\n",
            "2025-07-10 14:08:17,696 Epoch 543:\n",
            "\t\t\tTotal Training Recognition Loss 26.409790 || Total Training Translation Loss 0.005444\n",
            "2025-07-10 14:08:17,871 Epoch 544:\n",
            "\t\t\tTotal Training Recognition Loss 2.180399 || Total Training Translation Loss 0.004815\n",
            "2025-07-10 14:08:18,099 Epoch 545:\n",
            "\t\t\tTotal Training Recognition Loss 2.427204 || Total Training Translation Loss 0.004177\n",
            "2025-07-10 14:08:18,326 Epoch 546:\n",
            "\t\t\tTotal Training Recognition Loss 62.409630 || Total Training Translation Loss 0.005191\n",
            "2025-07-10 14:08:18,547 Epoch 547:\n",
            "\t\t\tTotal Training Recognition Loss 1.324693 || Total Training Translation Loss 0.005830\n",
            "2025-07-10 14:08:18,768 Epoch 548:\n",
            "\t\t\tTotal Training Recognition Loss 2.300545 || Total Training Translation Loss 0.005056\n",
            "2025-07-10 14:08:18,997 Epoch 549:\n",
            "\t\t\tTotal Training Recognition Loss 6.587664 || Total Training Translation Loss 0.004057\n",
            "2025-07-10 14:08:19,226 Epoch 550:\n",
            "\t\t\tTotal Training Recognition Loss 19.245073 || Total Training Translation Loss 0.004004\n",
            "2025-07-10 14:08:19,411 Epoch 551:\n",
            "\t\t\tTotal Training Recognition Loss 17.227198 || Total Training Translation Loss 0.004604\n",
            "2025-07-10 14:08:19,592 Epoch 552:\n",
            "\t\t\tTotal Training Recognition Loss 0.835079 || Total Training Translation Loss 0.006247\n",
            "2025-07-10 14:08:19,767 Epoch 553:\n",
            "\t\t\tTotal Training Recognition Loss 0.904627 || Total Training Translation Loss 0.004024\n",
            "2025-07-10 14:08:19,943 Epoch 554:\n",
            "\t\t\tTotal Training Recognition Loss 5.267550 || Total Training Translation Loss 0.003168\n",
            "2025-07-10 14:08:20,119 Epoch 555:\n",
            "\t\t\tTotal Training Recognition Loss 1.318656 || Total Training Translation Loss 0.004183\n",
            "2025-07-10 14:08:20,293 Epoch 556:\n",
            "\t\t\tTotal Training Recognition Loss 2.764093 || Total Training Translation Loss 0.005399\n",
            "2025-07-10 14:08:20,513 Epoch 557:\n",
            "\t\t\tTotal Training Recognition Loss 21.734852 || Total Training Translation Loss 0.004897\n",
            "2025-07-10 14:08:20,732 Epoch 558:\n",
            "\t\t\tTotal Training Recognition Loss 2.239939 || Total Training Translation Loss 0.005330\n",
            "2025-07-10 14:08:20,952 Epoch 559:\n",
            "\t\t\tTotal Training Recognition Loss 11.360506 || Total Training Translation Loss 0.004975\n",
            "2025-07-10 14:08:21,171 Epoch 560:\n",
            "\t\t\tTotal Training Recognition Loss 3.352682 || Total Training Translation Loss 0.003561\n",
            "2025-07-10 14:08:21,364 Epoch 561:\n",
            "\t\t\tTotal Training Recognition Loss 1.399234 || Total Training Translation Loss 0.003860\n",
            "2025-07-10 14:08:21,538 Epoch 562:\n",
            "\t\t\tTotal Training Recognition Loss 9.003283 || Total Training Translation Loss 0.004437\n",
            "2025-07-10 14:08:21,713 Epoch 563:\n",
            "\t\t\tTotal Training Recognition Loss 2.348505 || Total Training Translation Loss 0.005373\n",
            "2025-07-10 14:08:21,891 Epoch 564:\n",
            "\t\t\tTotal Training Recognition Loss 7.261487 || Total Training Translation Loss 0.005446\n",
            "2025-07-10 14:08:22,067 Epoch 565:\n",
            "\t\t\tTotal Training Recognition Loss 3.272459 || Total Training Translation Loss 0.003480\n",
            "2025-07-10 14:08:22,246 Epoch 566:\n",
            "\t\t\tTotal Training Recognition Loss 2.505491 || Total Training Translation Loss 0.004150\n",
            "2025-07-10 14:08:22,428 Epoch 567:\n",
            "\t\t\tTotal Training Recognition Loss 0.863655 || Total Training Translation Loss 0.003976\n",
            "2025-07-10 14:08:22,616 Epoch 568:\n",
            "\t\t\tTotal Training Recognition Loss 2.568719 || Total Training Translation Loss 0.005289\n",
            "2025-07-10 14:08:22,808 Epoch 569:\n",
            "\t\t\tTotal Training Recognition Loss 0.902349 || Total Training Translation Loss 0.004058\n",
            "2025-07-10 14:08:22,986 Epoch 570:\n",
            "\t\t\tTotal Training Recognition Loss 1.317760 || Total Training Translation Loss 0.003738\n",
            "2025-07-10 14:08:23,169 Epoch 571:\n",
            "\t\t\tTotal Training Recognition Loss 14.195623 || Total Training Translation Loss 0.005318\n",
            "2025-07-10 14:08:23,350 Epoch 572:\n",
            "\t\t\tTotal Training Recognition Loss 4.131476 || Total Training Translation Loss 0.005188\n",
            "2025-07-10 14:08:23,574 Epoch 573:\n",
            "\t\t\tTotal Training Recognition Loss 3.400099 || Total Training Translation Loss 0.005142\n",
            "2025-07-10 14:08:23,771 Epoch 574:\n",
            "\t\t\tTotal Training Recognition Loss 9.898872 || Total Training Translation Loss 0.004744\n",
            "2025-07-10 14:08:23,942 Epoch 575:\n",
            "\t\t\tTotal Training Recognition Loss 0.667593 || Total Training Translation Loss 0.004384\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-07-10 14:08:24,114 Epoch 576:\n",
            "\t\t\tTotal Training Recognition Loss 0.609922 || Total Training Translation Loss 0.003970\n",
            "2025-07-10 14:08:24,287 Epoch 577:\n",
            "\t\t\tTotal Training Recognition Loss 0.703361 || Total Training Translation Loss 0.004222\n",
            "2025-07-10 14:08:24,458 Epoch 578:\n",
            "\t\t\tTotal Training Recognition Loss 3.145539 || Total Training Translation Loss 0.003232\n",
            "2025-07-10 14:08:24,629 Epoch 579:\n",
            "\t\t\tTotal Training Recognition Loss 1.777926 || Total Training Translation Loss 0.004104\n",
            "2025-07-10 14:08:24,800 Epoch 580:\n",
            "\t\t\tTotal Training Recognition Loss 0.771608 || Total Training Translation Loss 0.003824\n",
            "2025-07-10 14:08:24,970 Epoch 581:\n",
            "\t\t\tTotal Training Recognition Loss 1.063578 || Total Training Translation Loss 0.004201\n",
            "2025-07-10 14:08:25,142 Epoch 582:\n",
            "\t\t\tTotal Training Recognition Loss 0.994236 || Total Training Translation Loss 0.003652\n",
            "2025-07-10 14:08:25,312 Epoch 583:\n",
            "\t\t\tTotal Training Recognition Loss 0.798634 || Total Training Translation Loss 0.003873\n",
            "2025-07-10 14:08:25,482 Epoch 584:\n",
            "\t\t\tTotal Training Recognition Loss 0.865627 || Total Training Translation Loss 0.004105\n",
            "2025-07-10 14:08:25,652 Epoch 585:\n",
            "\t\t\tTotal Training Recognition Loss 0.906108 || Total Training Translation Loss 0.003465\n",
            "2025-07-10 14:08:25,822 Epoch 586:\n",
            "\t\t\tTotal Training Recognition Loss 1.529121 || Total Training Translation Loss 0.004412\n",
            "2025-07-10 14:08:25,992 Epoch 587:\n",
            "\t\t\tTotal Training Recognition Loss 4.951191 || Total Training Translation Loss 0.004200\n",
            "2025-07-10 14:08:26,163 Epoch 588:\n",
            "\t\t\tTotal Training Recognition Loss 1.501426 || Total Training Translation Loss 0.004836\n",
            "2025-07-10 14:08:26,333 Epoch 589:\n",
            "\t\t\tTotal Training Recognition Loss 5.656511 || Total Training Translation Loss 0.004298\n",
            "2025-07-10 14:08:26,503 Epoch 590:\n",
            "\t\t\tTotal Training Recognition Loss 1.067724 || Total Training Translation Loss 0.004124\n",
            "2025-07-10 14:08:26,674 Epoch 591:\n",
            "\t\t\tTotal Training Recognition Loss 6.298454 || Total Training Translation Loss 0.003715\n",
            "2025-07-10 14:08:26,844 Epoch 592:\n",
            "\t\t\tTotal Training Recognition Loss 0.723480 || Total Training Translation Loss 0.004353\n",
            "2025-07-10 14:08:27,013 Epoch 593:\n",
            "\t\t\tTotal Training Recognition Loss 1.866696 || Total Training Translation Loss 0.003746\n",
            "2025-07-10 14:08:27,183 Epoch 594:\n",
            "\t\t\tTotal Training Recognition Loss 0.654320 || Total Training Translation Loss 0.003619\n",
            "2025-07-10 14:08:27,353 Epoch 595:\n",
            "\t\t\tTotal Training Recognition Loss 1.076657 || Total Training Translation Loss 0.004059\n",
            "2025-07-10 14:08:27,523 Epoch 596:\n",
            "\t\t\tTotal Training Recognition Loss 1.337782 || Total Training Translation Loss 0.004803\n",
            "2025-07-10 14:08:27,695 Epoch 597:\n",
            "\t\t\tTotal Training Recognition Loss 1.224018 || Total Training Translation Loss 0.003622\n",
            "2025-07-10 14:08:27,865 Epoch 598:\n",
            "\t\t\tTotal Training Recognition Loss 1.678560 || Total Training Translation Loss 0.003834\n",
            "2025-07-10 14:08:28,036 Epoch 599:\n",
            "\t\t\tTotal Training Recognition Loss 1.020716 || Total Training Translation Loss 0.003993\n",
            "2025-07-10 14:08:28,207 [Epoch: 600 Step: 00000600] Batch Recognition Loss:   2.433061 => Gls Tokens per Sec:      219 || Batch Translation Loss:   0.003459 => Txt Tokens per Sec:      591 || Lr: 0.001000\n",
            "2025-07-10 14:08:28,515 Validation result at epoch 600, step      600: duration: 0.3072s\n",
            "\tRecognition Beam Size: 1\tTranslation Beam Size: 1\tTranslation Beam Alpha: -1\n",
            "\tRecognition Loss: 9025.18652\tTranslation Loss: 255.46931\tPPL: 20.19674\n",
            "\tEval Metric: BLEU\n",
            "\tWER 102.86\t(DEL: 22.86,\tINS: 20.00,\tSUB: 60.00)\n",
            "\tBLEU-4 2.00\t(BLEU-1: 7.33,\tBLEU-2: 3.90,\tBLEU-3: 2.79,\tBLEU-4: 2.00)\n",
            "\tCHRF 22.16\tROUGE 10.16\tFID 0.00\n",
            "2025-07-10 14:08:28,516 Logging Recognition and Translation Outputs\n",
            "2025-07-10 14:08:28,516 ========================================================================================\n",
            "2025-07-10 14:08:28,517 Logging Sequence: 11August_2010_Wednesday_tagesschau-2\n",
            "2025-07-10 14:08:28,517 \tGloss Reference :\tDRUCK TIEF KOMMEN\n",
            "2025-07-10 14:08:28,518 \tGloss Hypothesis:\t***** **** LOCH  \n",
            "2025-07-10 14:08:28,518 \tGloss Alignment :\tD     D    S     \n",
            "2025-07-10 14:08:28,518 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:08:28,521 \tText Reference  :\t** *** **** ** ********** *** **** ******** ******* tiefer luftdruck bestimmt in ******* ******** ****** ********* *********** ***** ***** ***** ***** ***** ***** ***** den   nächsten tagen unser wetter\n",
            "2025-07-10 14:08:28,522 \tText Hypothesis :\tam tag gibt es verbreitet zum teil kräftige schauer oder   gewitter  und      in manchen regionen fallen ergiebige regenmengen <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>    <unk> <unk> <unk> \n",
            "2025-07-10 14:08:28,522 \tText Alignment  :\tI  I   I    I  I          I   I    I        I       S      S         S           I       I        I      I         I           I     I     I     I     I     I     I     S     S        S     S     S     \n",
            "2025-07-10 14:08:28,523 ========================================================================================\n",
            "2025-07-10 14:08:28,523 Logging Sequence: 11August_2010_Wednesday_tagesschau-3\n",
            "2025-07-10 14:08:28,523 \tGloss Reference :\tES-BEDEUTET VIEL WOLKE UND KOENNEN REGEN GEWITTER KOENNEN\n",
            "2025-07-10 14:08:28,524 \tGloss Hypothesis:\t*********** **** ***** *** ******* HEUTE BLEIBEN  KOENNEN\n",
            "2025-07-10 14:08:28,524 \tGloss Alignment :\tD           D    D     D   D       S     S               \n",
            "2025-07-10 14:08:28,524 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:08:28,528 \tText Reference  :\tdas bedeutet viele wolken und  immer wieder     zum teil kräftige schauer **** ******** und ** ******* ******** ****** ********* *********** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** gewitter\n",
            "2025-07-10 14:08:28,528 \tText Hypothesis :\t*** ******** am    tag    gibt es    verbreitet zum teil kräftige schauer oder gewitter und in manchen regionen fallen ergiebige regenmengen <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>   \n",
            "2025-07-10 14:08:28,529 \tText Alignment  :\tD   D        S     S      S    S     S                                    I    I            I  I       I        I      I         I           I     I     I     I     I     I     I     I     I     I     I     S       \n",
            "2025-07-10 14:08:28,529 ========================================================================================\n",
            "2025-07-10 14:08:28,529 Logging Sequence: 11August_2010_Wednesday_tagesschau-8\n",
            "2025-07-10 14:08:28,531 \tGloss Reference :\tWIND MAESSIG SCHWACH REGION   WENN    GEWITTER WIND            KOENNEN ******** *** ***************\n",
            "2025-07-10 14:08:28,531 \tGloss Hypothesis:\t**** ORT     BLEIBEN NORDWEST BLEIBEN NORDWEST UEBERSCHWEMMUNG KOENNEN NORDWEST ORT UEBERSCHWEMMUNG\n",
            "2025-07-10 14:08:28,531 \tGloss Alignment :\tD    S       S       S        S       S        S                       I        I   I              \n",
            "2025-07-10 14:08:28,531 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:08:28,537 \tText Reference  :\t************ ****** **** *** ***** ** ********** ***** ***** ***** ***** ***** meist weht  nur   ein   schwacher wind  aus   unterschiedlichen richtungen der   bei   schauern und   gewittern stark böig  sein  kann \n",
            "2025-07-10 14:08:28,537 \tText Hypothesis :\twolkenlücken finden sich vor allem im nordwesten <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>     <unk> <unk> <unk>             <unk>      <unk> <unk> <unk>    <unk> <unk>     <unk> <unk> <unk> <unk>\n",
            "2025-07-10 14:08:28,537 \tText Alignment  :\tI            I      I    I   I     I  I          I     I     I     I     I     S     S     S     S     S         S     S     S                 S          S     S     S        S     S         S     S     S     S    \n",
            "2025-07-10 14:08:28,537 ========================================================================================\n",
            "2025-07-10 14:08:28,537 Logging Sequence: 25October_2010_Monday_tagesschau-22\n",
            "2025-07-10 14:08:28,538 \tGloss Reference :\t***** ******* MITTWOCH REGEN    KOENNEN ***** ******** NORDWEST WAHRSCHEINLICH NORD STARK WIND   \n",
            "2025-07-10 14:08:28,539 \tGloss Hypothesis:\tJETZT FEBRUAR KOENNEN  SUEDWEST KOENNEN JETZT SUEDWEST JETZT    MANCHMAL       ORT  JETZT KOENNEN\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-07-10 14:08:28,539 \tGloss Alignment :\tI     I       S        S                I     I        S        S              S    S     S      \n",
            "2025-07-10 14:08:28,539 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:08:28,543 \tText Reference  :\tam *** **** ** ********** *** **** ******** ******* mittwoch hier     und ** ******* ******** ****** ********* *********** ***** ***** da    nieselregen in    der   nordwesthälfte an    den   küsten kräftiger wind \n",
            "2025-07-10 14:08:28,544 \tText Hypothesis :\tam tag gibt es verbreitet zum teil kräftige schauer oder     gewitter und in manchen regionen fallen ergiebige regenmengen <unk> <unk> <unk> <unk>       <unk> <unk> <unk>          <unk> <unk> <unk>  <unk>     <unk>\n",
            "2025-07-10 14:08:28,544 \tText Alignment  :\t   I   I    I  I          I   I    I        I       S        S            I  I       I        I      I         I           I     I     S     S           S     S     S              S     S     S      S         S    \n",
            "2025-07-10 14:08:28,544 ========================================================================================\n",
            "2025-07-10 14:08:28,544 Logging Sequence: 05May_2011_Thursday_tagesschau-25\n",
            "2025-07-10 14:08:28,545 \tGloss Reference :\tJETZT WETTER WIE-AUSSEHEN MORGEN FREITAG    SECHSTE MAI    ZEIGEN-BILDSCHIRM\n",
            "2025-07-10 14:08:28,545 \tGloss Hypothesis:\tJETZT WETTER ORT          MORGEN DONNERSTAG TROCKEN MORGEN BLEIBEN          \n",
            "2025-07-10 14:08:28,545 \tGloss Alignment :\t             S                   S          S       S      S                \n",
            "2025-07-10 14:08:28,546 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:08:28,549 \tText Reference  :\t** *** **** ** ********** *** **** ******** ******* **** ******** und ** ******* ******** ****** ********* *********** ***** ***** ***** nun   die   wettervorhersage für   morgen freitag den   sechsten mai  \n",
            "2025-07-10 14:08:28,549 \tText Hypothesis :\tam tag gibt es verbreitet zum teil kräftige schauer oder gewitter und in manchen regionen fallen ergiebige regenmengen <unk> <unk> <unk> <unk> <unk> <unk>            <unk> <unk>  <unk>   <unk> <unk>    <unk>\n",
            "2025-07-10 14:08:28,549 \tText Alignment  :\tI  I   I    I  I          I   I    I        I       I    I            I  I       I        I      I         I           I     I     I     S     S     S                S     S      S       S     S        S    \n",
            "2025-07-10 14:08:28,549 ========================================================================================\n",
            "2025-07-10 14:08:28,550 Epoch 600:\n",
            "\t\t\tTotal Training Recognition Loss 2.433061 || Total Training Translation Loss 0.003459\n",
            "2025-07-10 14:08:28,763 Epoch 601:\n",
            "\t\t\tTotal Training Recognition Loss 1.078804 || Total Training Translation Loss 0.003479\n",
            "2025-07-10 14:08:28,935 Epoch 602:\n",
            "\t\t\tTotal Training Recognition Loss 0.937197 || Total Training Translation Loss 0.004205\n",
            "2025-07-10 14:08:29,105 Epoch 603:\n",
            "\t\t\tTotal Training Recognition Loss 1.346968 || Total Training Translation Loss 0.003991\n",
            "2025-07-10 14:08:29,274 Epoch 604:\n",
            "\t\t\tTotal Training Recognition Loss 1.625264 || Total Training Translation Loss 0.003855\n",
            "2025-07-10 14:08:29,445 Epoch 605:\n",
            "\t\t\tTotal Training Recognition Loss 1.102073 || Total Training Translation Loss 0.003815\n",
            "2025-07-10 14:08:29,618 Epoch 606:\n",
            "\t\t\tTotal Training Recognition Loss 0.810440 || Total Training Translation Loss 0.003163\n",
            "2025-07-10 14:08:29,791 Epoch 607:\n",
            "\t\t\tTotal Training Recognition Loss 0.671896 || Total Training Translation Loss 0.003580\n",
            "2025-07-10 14:08:29,968 Epoch 608:\n",
            "\t\t\tTotal Training Recognition Loss 1.002939 || Total Training Translation Loss 0.003304\n",
            "2025-07-10 14:08:30,140 Epoch 609:\n",
            "\t\t\tTotal Training Recognition Loss 0.769034 || Total Training Translation Loss 0.003830\n",
            "2025-07-10 14:08:30,310 Epoch 610:\n",
            "\t\t\tTotal Training Recognition Loss 0.961962 || Total Training Translation Loss 0.003491\n",
            "2025-07-10 14:08:30,479 Epoch 611:\n",
            "\t\t\tTotal Training Recognition Loss 0.860479 || Total Training Translation Loss 0.003395\n",
            "2025-07-10 14:08:30,651 Epoch 612:\n",
            "\t\t\tTotal Training Recognition Loss 0.966631 || Total Training Translation Loss 0.004825\n",
            "2025-07-10 14:08:30,821 Epoch 613:\n",
            "\t\t\tTotal Training Recognition Loss 3.251992 || Total Training Translation Loss 0.004249\n",
            "2025-07-10 14:08:30,992 Epoch 614:\n",
            "\t\t\tTotal Training Recognition Loss 0.775337 || Total Training Translation Loss 0.003573\n",
            "2025-07-10 14:08:31,165 Epoch 615:\n",
            "\t\t\tTotal Training Recognition Loss 0.807038 || Total Training Translation Loss 0.005240\n",
            "2025-07-10 14:08:31,340 Epoch 616:\n",
            "\t\t\tTotal Training Recognition Loss 1.211993 || Total Training Translation Loss 0.004104\n",
            "2025-07-10 14:08:31,513 Epoch 617:\n",
            "\t\t\tTotal Training Recognition Loss 0.842308 || Total Training Translation Loss 0.005137\n",
            "2025-07-10 14:08:31,686 Epoch 618:\n",
            "\t\t\tTotal Training Recognition Loss 0.726352 || Total Training Translation Loss 0.003716\n",
            "2025-07-10 14:08:31,858 Epoch 619:\n",
            "\t\t\tTotal Training Recognition Loss 0.893266 || Total Training Translation Loss 0.003627\n",
            "2025-07-10 14:08:32,031 Epoch 620:\n",
            "\t\t\tTotal Training Recognition Loss 0.630546 || Total Training Translation Loss 0.003857\n",
            "2025-07-10 14:08:32,206 Epoch 621:\n",
            "\t\t\tTotal Training Recognition Loss 0.581234 || Total Training Translation Loss 0.003448\n",
            "2025-07-10 14:08:32,379 Epoch 622:\n",
            "\t\t\tTotal Training Recognition Loss 0.631041 || Total Training Translation Loss 0.003940\n",
            "2025-07-10 14:08:32,552 Epoch 623:\n",
            "\t\t\tTotal Training Recognition Loss 0.867557 || Total Training Translation Loss 0.003600\n",
            "2025-07-10 14:08:32,725 Epoch 624:\n",
            "\t\t\tTotal Training Recognition Loss 0.565121 || Total Training Translation Loss 0.003845\n",
            "2025-07-10 14:08:32,899 Epoch 625:\n",
            "\t\t\tTotal Training Recognition Loss 0.570170 || Total Training Translation Loss 0.003502\n",
            "2025-07-10 14:08:33,073 Epoch 626:\n",
            "\t\t\tTotal Training Recognition Loss 5.784879 || Total Training Translation Loss 0.003678\n",
            "2025-07-10 14:08:33,248 Epoch 627:\n",
            "\t\t\tTotal Training Recognition Loss 1.175987 || Total Training Translation Loss 0.003943\n",
            "2025-07-10 14:08:33,425 Epoch 628:\n",
            "\t\t\tTotal Training Recognition Loss 0.516277 || Total Training Translation Loss 0.004698\n",
            "2025-07-10 14:08:33,604 Epoch 629:\n",
            "\t\t\tTotal Training Recognition Loss 0.366610 || Total Training Translation Loss 0.003525\n",
            "2025-07-10 14:08:33,781 Epoch 630:\n",
            "\t\t\tTotal Training Recognition Loss 0.918328 || Total Training Translation Loss 0.003441\n",
            "2025-07-10 14:08:33,958 Epoch 631:\n",
            "\t\t\tTotal Training Recognition Loss 42.705334 || Total Training Translation Loss 0.004536\n",
            "2025-07-10 14:08:34,133 Epoch 632:\n",
            "\t\t\tTotal Training Recognition Loss 0.400673 || Total Training Translation Loss 0.006912\n",
            "2025-07-10 14:08:34,321 Epoch 633:\n",
            "\t\t\tTotal Training Recognition Loss 1.482087 || Total Training Translation Loss 0.003610\n",
            "2025-07-10 14:08:34,500 Epoch 634:\n",
            "\t\t\tTotal Training Recognition Loss 0.763729 || Total Training Translation Loss 0.004407\n",
            "2025-07-10 14:08:34,685 Epoch 635:\n",
            "\t\t\tTotal Training Recognition Loss 115.845200 || Total Training Translation Loss 0.004188\n",
            "2025-07-10 14:08:34,865 Epoch 636:\n",
            "\t\t\tTotal Training Recognition Loss 6.380620 || Total Training Translation Loss 0.003815\n",
            "2025-07-10 14:08:35,089 Epoch 637:\n",
            "\t\t\tTotal Training Recognition Loss 38.879963 || Total Training Translation Loss 0.003881\n",
            "2025-07-10 14:08:35,314 Epoch 638:\n",
            "\t\t\tTotal Training Recognition Loss 0.578730 || Total Training Translation Loss 0.004253\n",
            "2025-07-10 14:08:35,530 Epoch 639:\n",
            "\t\t\tTotal Training Recognition Loss 0.871826 || Total Training Translation Loss 0.003352\n",
            "2025-07-10 14:08:35,744 Epoch 640:\n",
            "\t\t\tTotal Training Recognition Loss 0.664121 || Total Training Translation Loss 0.003103\n",
            "2025-07-10 14:08:35,921 Epoch 641:\n",
            "\t\t\tTotal Training Recognition Loss 3.718676 || Total Training Translation Loss 0.003745\n",
            "2025-07-10 14:08:36,098 Epoch 642:\n",
            "\t\t\tTotal Training Recognition Loss 11.257799 || Total Training Translation Loss 0.004127\n",
            "2025-07-10 14:08:36,271 Epoch 643:\n",
            "\t\t\tTotal Training Recognition Loss 5.855096 || Total Training Translation Loss 0.004780\n",
            "2025-07-10 14:08:36,444 Epoch 644:\n",
            "\t\t\tTotal Training Recognition Loss 3.688128 || Total Training Translation Loss 0.006106\n",
            "2025-07-10 14:08:36,619 Epoch 645:\n",
            "\t\t\tTotal Training Recognition Loss 23.240225 || Total Training Translation Loss 0.004881\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-07-10 14:08:36,792 Epoch 646:\n",
            "\t\t\tTotal Training Recognition Loss 8.224388 || Total Training Translation Loss 0.004517\n",
            "2025-07-10 14:08:36,963 Epoch 647:\n",
            "\t\t\tTotal Training Recognition Loss 2.723512 || Total Training Translation Loss 0.004677\n",
            "2025-07-10 14:08:37,135 Epoch 648:\n",
            "\t\t\tTotal Training Recognition Loss 4.864373 || Total Training Translation Loss 0.004778\n",
            "2025-07-10 14:08:37,306 Epoch 649:\n",
            "\t\t\tTotal Training Recognition Loss 2.887859 || Total Training Translation Loss 0.004262\n",
            "2025-07-10 14:08:37,476 Epoch 650:\n",
            "\t\t\tTotal Training Recognition Loss 5.489636 || Total Training Translation Loss 0.003506\n",
            "2025-07-10 14:08:37,648 Epoch 651:\n",
            "\t\t\tTotal Training Recognition Loss 3.742092 || Total Training Translation Loss 0.004526\n",
            "2025-07-10 14:08:37,820 Epoch 652:\n",
            "\t\t\tTotal Training Recognition Loss 3.487948 || Total Training Translation Loss 0.003644\n",
            "2025-07-10 14:08:37,993 Epoch 653:\n",
            "\t\t\tTotal Training Recognition Loss 8.085845 || Total Training Translation Loss 0.003812\n",
            "2025-07-10 14:08:38,164 Epoch 654:\n",
            "\t\t\tTotal Training Recognition Loss 4.702446 || Total Training Translation Loss 0.005447\n",
            "2025-07-10 14:08:38,334 Epoch 655:\n",
            "\t\t\tTotal Training Recognition Loss 2.353487 || Total Training Translation Loss 0.004207\n",
            "2025-07-10 14:08:38,505 Epoch 656:\n",
            "\t\t\tTotal Training Recognition Loss 1.493385 || Total Training Translation Loss 0.004255\n",
            "2025-07-10 14:08:38,676 Epoch 657:\n",
            "\t\t\tTotal Training Recognition Loss 8.489951 || Total Training Translation Loss 0.003988\n",
            "2025-07-10 14:08:38,847 Epoch 658:\n",
            "\t\t\tTotal Training Recognition Loss 1.356585 || Total Training Translation Loss 0.004093\n",
            "2025-07-10 14:08:39,021 Epoch 659:\n",
            "\t\t\tTotal Training Recognition Loss 50.839214 || Total Training Translation Loss 0.004610\n",
            "2025-07-10 14:08:39,195 Epoch 660:\n",
            "\t\t\tTotal Training Recognition Loss 1.515841 || Total Training Translation Loss 0.003975\n",
            "2025-07-10 14:08:39,367 Epoch 661:\n",
            "\t\t\tTotal Training Recognition Loss 3.452814 || Total Training Translation Loss 0.004175\n",
            "2025-07-10 14:08:39,541 Epoch 662:\n",
            "\t\t\tTotal Training Recognition Loss 3.526674 || Total Training Translation Loss 0.003748\n",
            "2025-07-10 14:08:39,715 Epoch 663:\n",
            "\t\t\tTotal Training Recognition Loss 2.999683 || Total Training Translation Loss 0.003930\n",
            "2025-07-10 14:08:39,888 Epoch 664:\n",
            "\t\t\tTotal Training Recognition Loss 44.089504 || Total Training Translation Loss 0.004608\n",
            "2025-07-10 14:08:40,059 Epoch 665:\n",
            "\t\t\tTotal Training Recognition Loss 16.993275 || Total Training Translation Loss 0.004293\n",
            "2025-07-10 14:08:40,277 Epoch 666:\n",
            "\t\t\tTotal Training Recognition Loss 3.904242 || Total Training Translation Loss 0.003966\n",
            "2025-07-10 14:08:40,494 Epoch 667:\n",
            "\t\t\tTotal Training Recognition Loss 4.707276 || Total Training Translation Loss 0.005834\n",
            "2025-07-10 14:08:40,712 Epoch 668:\n",
            "\t\t\tTotal Training Recognition Loss 52.367813 || Total Training Translation Loss 0.005742\n",
            "2025-07-10 14:08:40,929 Epoch 669:\n",
            "\t\t\tTotal Training Recognition Loss 8.543999 || Total Training Translation Loss 0.005544\n",
            "2025-07-10 14:08:41,146 Epoch 670:\n",
            "\t\t\tTotal Training Recognition Loss 97.875641 || Total Training Translation Loss 0.005416\n",
            "2025-07-10 14:08:41,363 Epoch 671:\n",
            "\t\t\tTotal Training Recognition Loss 4.842009 || Total Training Translation Loss 0.004304\n",
            "2025-07-10 14:08:41,580 Epoch 672:\n",
            "\t\t\tTotal Training Recognition Loss 30.712734 || Total Training Translation Loss 0.004662\n",
            "2025-07-10 14:08:41,797 Epoch 673:\n",
            "\t\t\tTotal Training Recognition Loss 41.238552 || Total Training Translation Loss 0.005357\n",
            "2025-07-10 14:08:42,014 Epoch 674:\n",
            "\t\t\tTotal Training Recognition Loss 42.419201 || Total Training Translation Loss 0.004616\n",
            "2025-07-10 14:08:42,231 Epoch 675:\n",
            "\t\t\tTotal Training Recognition Loss 24.466566 || Total Training Translation Loss 0.004428\n",
            "2025-07-10 14:08:42,447 Epoch 676:\n",
            "\t\t\tTotal Training Recognition Loss 8.739253 || Total Training Translation Loss 0.003725\n",
            "2025-07-10 14:08:42,664 Epoch 677:\n",
            "\t\t\tTotal Training Recognition Loss 2.034402 || Total Training Translation Loss 0.004035\n",
            "2025-07-10 14:08:42,881 Epoch 678:\n",
            "\t\t\tTotal Training Recognition Loss 3.191752 || Total Training Translation Loss 0.003872\n",
            "2025-07-10 14:08:43,098 Epoch 679:\n",
            "\t\t\tTotal Training Recognition Loss 4.148183 || Total Training Translation Loss 0.004748\n",
            "2025-07-10 14:08:43,317 Epoch 680:\n",
            "\t\t\tTotal Training Recognition Loss 6.249690 || Total Training Translation Loss 0.004102\n",
            "2025-07-10 14:08:43,535 Epoch 681:\n",
            "\t\t\tTotal Training Recognition Loss 5.366966 || Total Training Translation Loss 0.005453\n",
            "2025-07-10 14:08:43,751 Epoch 682:\n",
            "\t\t\tTotal Training Recognition Loss 3.894321 || Total Training Translation Loss 0.006043\n",
            "2025-07-10 14:08:43,969 Epoch 683:\n",
            "\t\t\tTotal Training Recognition Loss 4.427865 || Total Training Translation Loss 0.005818\n",
            "2025-07-10 14:08:44,186 Epoch 684:\n",
            "\t\t\tTotal Training Recognition Loss 3.496309 || Total Training Translation Loss 0.006090\n",
            "2025-07-10 14:08:44,403 Epoch 685:\n",
            "\t\t\tTotal Training Recognition Loss 8.914053 || Total Training Translation Loss 0.005363\n",
            "2025-07-10 14:08:44,620 Epoch 686:\n",
            "\t\t\tTotal Training Recognition Loss 47.674694 || Total Training Translation Loss 0.008601\n",
            "2025-07-10 14:08:44,837 Epoch 687:\n",
            "\t\t\tTotal Training Recognition Loss 2.513898 || Total Training Translation Loss 0.005940\n",
            "2025-07-10 14:08:45,053 Epoch 688:\n",
            "\t\t\tTotal Training Recognition Loss 6.233074 || Total Training Translation Loss 0.007209\n",
            "2025-07-10 14:08:45,270 Epoch 689:\n",
            "\t\t\tTotal Training Recognition Loss 1.319897 || Total Training Translation Loss 0.007252\n",
            "2025-07-10 14:08:45,488 Epoch 690:\n",
            "\t\t\tTotal Training Recognition Loss 9.377191 || Total Training Translation Loss 0.004893\n",
            "2025-07-10 14:08:45,706 Epoch 691:\n",
            "\t\t\tTotal Training Recognition Loss 27.081520 || Total Training Translation Loss 0.005245\n",
            "2025-07-10 14:08:45,923 Epoch 692:\n",
            "\t\t\tTotal Training Recognition Loss 3.544267 || Total Training Translation Loss 0.004332\n",
            "2025-07-10 14:08:46,141 Epoch 693:\n",
            "\t\t\tTotal Training Recognition Loss 4.660795 || Total Training Translation Loss 0.004667\n",
            "2025-07-10 14:08:46,359 Epoch 694:\n",
            "\t\t\tTotal Training Recognition Loss 4.881479 || Total Training Translation Loss 0.004732\n",
            "2025-07-10 14:08:46,575 Epoch 695:\n",
            "\t\t\tTotal Training Recognition Loss 3.897371 || Total Training Translation Loss 0.004569\n",
            "2025-07-10 14:08:46,792 Epoch 696:\n",
            "\t\t\tTotal Training Recognition Loss 3.093256 || Total Training Translation Loss 0.004594\n",
            "2025-07-10 14:08:47,008 Epoch 697:\n",
            "\t\t\tTotal Training Recognition Loss 8.020966 || Total Training Translation Loss 0.004198\n",
            "2025-07-10 14:08:47,185 Epoch 698:\n",
            "\t\t\tTotal Training Recognition Loss 4.762212 || Total Training Translation Loss 0.006651\n",
            "2025-07-10 14:08:47,387 Epoch 699:\n",
            "\t\t\tTotal Training Recognition Loss 4.445360 || Total Training Translation Loss 0.004952\n",
            "2025-07-10 14:08:47,605 [Epoch: 700 Step: 00000700] Batch Recognition Loss:   5.616250 => Gls Tokens per Sec:      171 || Batch Translation Loss:   0.006523 => Txt Tokens per Sec:      461 || Lr: 0.001000\n",
            "2025-07-10 14:08:47,916 Hooray! New best validation result [eval_metric]!\n",
            "2025-07-10 14:08:47,919 Saving new checkpoint.\n",
            "2025-07-10 14:08:48,021 Validation result at epoch 700, step      700: duration: 0.4143s\n",
            "\tRecognition Beam Size: 1\tTranslation Beam Size: 1\tTranslation Beam Alpha: -1\n",
            "\tRecognition Loss: 10895.81445\tTranslation Loss: 260.56070\tPPL: 21.44346\n",
            "\tEval Metric: BLEU\n",
            "\tWER 97.14\t(DEL: 17.14,\tINS: 14.29,\tSUB: 65.71)\n",
            "\tBLEU-4 2.83\t(BLEU-1: 7.33,\tBLEU-2: 4.50,\tBLEU-3: 3.51,\tBLEU-4: 2.83)\n",
            "\tCHRF 24.05\tROUGE 11.65\tFID 0.00\n",
            "2025-07-10 14:08:48,021 Logging Recognition and Translation Outputs\n",
            "2025-07-10 14:08:48,022 ========================================================================================\n",
            "2025-07-10 14:08:48,022 Logging Sequence: 11August_2010_Wednesday_tagesschau-2\n",
            "2025-07-10 14:08:48,022 \tGloss Reference :\tDRUCK TIEF     KOMMEN\n",
            "2025-07-10 14:08:48,023 \tGloss Hypothesis:\t***** NORDWEST LOCH  \n",
            "2025-07-10 14:08:48,023 \tGloss Alignment :\tD     S        S     \n",
            "2025-07-10 14:08:48,024 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:08:48,025 \tText Reference  :\t** *** **** ** ********** *** **** ******** ******* tiefer luftdruck bestimmt in ******* ******** ****** ********* *********** ***** ***** ***** ***** ***** ***** ***** den   nächsten tagen unser wetter\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-07-10 14:08:48,025 \tText Hypothesis :\tam tag gibt es verbreitet zum teil kräftige schauer oder   gewitter  und      in manchen regionen fallen ergiebige regenmengen <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>    <unk> <unk> <unk> \n",
            "2025-07-10 14:08:48,026 \tText Alignment  :\tI  I   I    I  I          I   I    I        I       S      S         S           I       I        I      I         I           I     I     I     I     I     I     I     S     S        S     S     S     \n",
            "2025-07-10 14:08:48,026 ========================================================================================\n",
            "2025-07-10 14:08:48,026 Logging Sequence: 11August_2010_Wednesday_tagesschau-3\n",
            "2025-07-10 14:08:48,026 \tGloss Reference :\tES-BEDEUTET VIEL WOLKE    UND KOENNEN REGEN    GEWITTER KOENNEN\n",
            "2025-07-10 14:08:48,026 \tGloss Hypothesis:\t*********** **** NORDWEST ORT HEUTE   NORDWEST BLEIBEN  KOENNEN\n",
            "2025-07-10 14:08:48,026 \tGloss Alignment :\tD           D    S        S   S       S        S               \n",
            "2025-07-10 14:08:48,027 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:08:48,028 \tText Reference  :\t****** ** ***** ***** ***** ******* ***** **** *** ********* ********* ******** ********* das    bedeutet viele wolken und ****** ****** ****** ****** immer  wieder zum    teil   kräftige schauer   und    gewitter\n",
            "2025-07-10 14:08:48,028 \tText Hypothesis :\tbleibt es heute nacht meist trocken sonst muss mit teilweise kräftigen schauern gerechnet werden örtlich  mit   blitz  und donner donner donner donner donner donner donner donner donner   gerechnet werden örtlich \n",
            "2025-07-10 14:08:48,029 \tText Alignment  :\tI      I  I     I     I     I       I     I    I   I         I         I        I         S      S        S     S          I      I      I      I      S      S      S      S      S        S         S      S       \n",
            "2025-07-10 14:08:48,029 ========================================================================================\n",
            "2025-07-10 14:08:48,029 Logging Sequence: 11August_2010_Wednesday_tagesschau-8\n",
            "2025-07-10 14:08:48,029 \tGloss Reference :\tWIND MAESSIG SCHWACH  REGION  WENN     GEWITTER WIND     KOENNEN ******** ***\n",
            "2025-07-10 14:08:48,030 \tGloss Hypothesis:\t**** ORT     SPEZIELL BLEIBEN NORDWEST BLEIBEN  SUEDWEST KOENNEN NORDWEST ORT\n",
            "2025-07-10 14:08:48,030 \tGloss Alignment :\tD    S       S        S       S        S        S                I        I  \n",
            "2025-07-10 14:08:48,030 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:08:48,032 \tText Reference  :\t************ ****** **** *** ***** ** ********** ***** ***** ***** ***** ***** meist weht  nur   ein   schwacher wind  aus   unterschiedlichen richtungen der   bei   schauern und   gewittern stark böig  sein  kann \n",
            "2025-07-10 14:08:48,032 \tText Hypothesis :\twolkenlücken finden sich vor allem im nordwesten <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>     <unk> <unk> <unk>             <unk>      <unk> <unk> <unk>    <unk> <unk>     <unk> <unk> <unk> <unk>\n",
            "2025-07-10 14:08:48,032 \tText Alignment  :\tI            I      I    I   I     I  I          I     I     I     I     I     S     S     S     S     S         S     S     S                 S          S     S     S        S     S         S     S     S     S    \n",
            "2025-07-10 14:08:48,032 ========================================================================================\n",
            "2025-07-10 14:08:48,032 Logging Sequence: 25October_2010_Monday_tagesschau-22\n",
            "2025-07-10 14:08:48,033 \tGloss Reference :\t***** MITTWOCH REGEN ******* ******** KOENNEN NORDWEST WAHRSCHEINLICH NORD STARK   WIND\n",
            "2025-07-10 14:08:48,033 \tGloss Hypothesis:\tJETZT ORT      REGEN FEBRUAR SUEDWEST KOENNEN ******** SUEDWEST       ORT  KOENNEN VIEL\n",
            "2025-07-10 14:08:48,033 \tGloss Alignment :\tI     S              I       I                D        S              S    S       S   \n",
            "2025-07-10 14:08:48,033 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:08:48,035 \tText Reference  :\tam *** **** ** ********** *** **** ******** ******* mittwoch hier     und ** ******* ******** ****** ********* *********** ***** ***** da    nieselregen in    der   nordwesthälfte an    den   küsten kräftiger wind \n",
            "2025-07-10 14:08:48,035 \tText Hypothesis :\tam tag gibt es verbreitet zum teil kräftige schauer oder     gewitter und in manchen regionen fallen ergiebige regenmengen <unk> <unk> <unk> <unk>       <unk> <unk> <unk>          <unk> <unk> <unk>  <unk>     <unk>\n",
            "2025-07-10 14:08:48,035 \tText Alignment  :\t   I   I    I  I          I   I    I        I       S        S            I  I       I        I      I         I           I     I     S     S           S     S     S              S     S     S      S         S    \n",
            "2025-07-10 14:08:48,036 ========================================================================================\n",
            "2025-07-10 14:08:48,036 Logging Sequence: 05May_2011_Thursday_tagesschau-25\n",
            "2025-07-10 14:08:48,036 \tGloss Reference :\tJETZT WETTER WIE-AUSSEHEN MORGEN FREITAG    SECHSTE MAI    ZEIGEN-BILDSCHIRM\n",
            "2025-07-10 14:08:48,036 \tGloss Hypothesis:\tJETZT WETTER ************ ORT    DONNERSTAG ZWOELF  MORGEN BLEIBEN          \n",
            "2025-07-10 14:08:48,036 \tGloss Alignment :\t             D            S      S          S       S      S                \n",
            "2025-07-10 14:08:48,036 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:08:48,038 \tText Reference  :\tund nun die wettervorhersage für morgen freitag    den ******** ****** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** sechsten mai  \n",
            "2025-07-10 14:08:48,038 \tText Hypothesis :\t*** nun die wettervorhersage für morgen donnerstag den zwölften august <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>    <unk>\n",
            "2025-07-10 14:08:48,038 \tText Alignment  :\tD                                       S              I        I      I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     S        S    \n",
            "2025-07-10 14:08:48,038 ========================================================================================\n",
            "2025-07-10 14:08:48,038 Epoch 700:\n",
            "\t\t\tTotal Training Recognition Loss 5.616250 || Total Training Translation Loss 0.006523\n",
            "2025-07-10 14:08:48,214 Epoch 701:\n",
            "\t\t\tTotal Training Recognition Loss 4.019866 || Total Training Translation Loss 0.006310\n",
            "2025-07-10 14:08:48,388 Epoch 702:\n",
            "\t\t\tTotal Training Recognition Loss 3.528444 || Total Training Translation Loss 0.005966\n",
            "2025-07-10 14:08:48,560 Epoch 703:\n",
            "\t\t\tTotal Training Recognition Loss 2.182284 || Total Training Translation Loss 0.005227\n",
            "2025-07-10 14:08:48,732 Epoch 704:\n",
            "\t\t\tTotal Training Recognition Loss 1.679382 || Total Training Translation Loss 0.005923\n",
            "2025-07-10 14:08:48,905 Epoch 705:\n",
            "\t\t\tTotal Training Recognition Loss 5.775594 || Total Training Translation Loss 0.005435\n",
            "2025-07-10 14:08:49,078 Epoch 706:\n",
            "\t\t\tTotal Training Recognition Loss 3.693064 || Total Training Translation Loss 0.005656\n",
            "2025-07-10 14:08:49,250 Epoch 707:\n",
            "\t\t\tTotal Training Recognition Loss 1.868204 || Total Training Translation Loss 0.005940\n",
            "2025-07-10 14:08:49,422 Epoch 708:\n",
            "\t\t\tTotal Training Recognition Loss 2.665892 || Total Training Translation Loss 0.006288\n",
            "2025-07-10 14:08:49,595 Epoch 709:\n",
            "\t\t\tTotal Training Recognition Loss 2.050860 || Total Training Translation Loss 0.005718\n",
            "2025-07-10 14:08:49,768 Epoch 710:\n",
            "\t\t\tTotal Training Recognition Loss 42.180180 || Total Training Translation Loss 0.004194\n",
            "2025-07-10 14:08:49,941 Epoch 711:\n",
            "\t\t\tTotal Training Recognition Loss 58.832638 || Total Training Translation Loss 0.005471\n",
            "2025-07-10 14:08:50,115 Epoch 712:\n",
            "\t\t\tTotal Training Recognition Loss 6.033291 || Total Training Translation Loss 0.004425\n",
            "2025-07-10 14:08:50,289 Epoch 713:\n",
            "\t\t\tTotal Training Recognition Loss 83.279472 || Total Training Translation Loss 0.005667\n",
            "2025-07-10 14:08:50,460 Epoch 714:\n",
            "\t\t\tTotal Training Recognition Loss 1.702465 || Total Training Translation Loss 0.006225\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-07-10 14:08:50,633 Epoch 715:\n",
            "\t\t\tTotal Training Recognition Loss 7.600799 || Total Training Translation Loss 0.005253\n",
            "2025-07-10 14:08:50,804 Epoch 716:\n",
            "\t\t\tTotal Training Recognition Loss 135.695251 || Total Training Translation Loss 0.007001\n",
            "2025-07-10 14:08:50,982 Epoch 717:\n",
            "\t\t\tTotal Training Recognition Loss 65.289528 || Total Training Translation Loss 0.007210\n",
            "2025-07-10 14:08:51,160 Epoch 718:\n",
            "\t\t\tTotal Training Recognition Loss 12.107892 || Total Training Translation Loss 0.007727\n",
            "2025-07-10 14:08:51,336 Epoch 719:\n",
            "\t\t\tTotal Training Recognition Loss 16.451771 || Total Training Translation Loss 0.005039\n",
            "2025-07-10 14:08:51,508 Epoch 720:\n",
            "\t\t\tTotal Training Recognition Loss 10.197160 || Total Training Translation Loss 0.009622\n",
            "2025-07-10 14:08:51,680 Epoch 721:\n",
            "\t\t\tTotal Training Recognition Loss 12.586990 || Total Training Translation Loss 0.008507\n",
            "2025-07-10 14:08:51,855 Epoch 722:\n",
            "\t\t\tTotal Training Recognition Loss 50.822697 || Total Training Translation Loss 0.010290\n",
            "2025-07-10 14:08:52,032 Epoch 723:\n",
            "\t\t\tTotal Training Recognition Loss 32.418781 || Total Training Translation Loss 0.010125\n",
            "2025-07-10 14:08:52,203 Epoch 724:\n",
            "\t\t\tTotal Training Recognition Loss 13.309710 || Total Training Translation Loss 0.014498\n",
            "2025-07-10 14:08:52,374 Epoch 725:\n",
            "\t\t\tTotal Training Recognition Loss 7.796384 || Total Training Translation Loss 0.011054\n",
            "2025-07-10 14:08:52,548 Epoch 726:\n",
            "\t\t\tTotal Training Recognition Loss 17.358425 || Total Training Translation Loss 0.006875\n",
            "2025-07-10 14:08:52,719 Epoch 727:\n",
            "\t\t\tTotal Training Recognition Loss 92.927223 || Total Training Translation Loss 0.008766\n",
            "2025-07-10 14:08:52,895 Epoch 728:\n",
            "\t\t\tTotal Training Recognition Loss 17.165894 || Total Training Translation Loss 0.006734\n",
            "2025-07-10 14:08:53,067 Epoch 729:\n",
            "\t\t\tTotal Training Recognition Loss 6.573817 || Total Training Translation Loss 0.006472\n",
            "2025-07-10 14:08:53,282 Epoch 730:\n",
            "\t\t\tTotal Training Recognition Loss 4.916734 || Total Training Translation Loss 0.005519\n",
            "2025-07-10 14:08:53,494 Epoch 731:\n",
            "\t\t\tTotal Training Recognition Loss 2.404011 || Total Training Translation Loss 0.005252\n",
            "2025-07-10 14:08:53,666 Epoch 732:\n",
            "\t\t\tTotal Training Recognition Loss 3.171519 || Total Training Translation Loss 0.005393\n",
            "2025-07-10 14:08:53,839 Epoch 733:\n",
            "\t\t\tTotal Training Recognition Loss 2.660797 || Total Training Translation Loss 0.007719\n",
            "2025-07-10 14:08:54,010 Epoch 734:\n",
            "\t\t\tTotal Training Recognition Loss 3.764096 || Total Training Translation Loss 0.005586\n",
            "2025-07-10 14:08:54,184 Epoch 735:\n",
            "\t\t\tTotal Training Recognition Loss 2.454766 || Total Training Translation Loss 0.006849\n",
            "2025-07-10 14:08:54,356 Epoch 736:\n",
            "\t\t\tTotal Training Recognition Loss 2.203027 || Total Training Translation Loss 0.005230\n",
            "2025-07-10 14:08:54,528 Epoch 737:\n",
            "\t\t\tTotal Training Recognition Loss 2.010528 || Total Training Translation Loss 0.006068\n",
            "2025-07-10 14:08:54,700 Epoch 738:\n",
            "\t\t\tTotal Training Recognition Loss 5.290282 || Total Training Translation Loss 0.004492\n",
            "2025-07-10 14:08:54,871 Epoch 739:\n",
            "\t\t\tTotal Training Recognition Loss 4.165027 || Total Training Translation Loss 0.005024\n",
            "2025-07-10 14:08:55,042 Epoch 740:\n",
            "\t\t\tTotal Training Recognition Loss 7.899870 || Total Training Translation Loss 0.007737\n",
            "2025-07-10 14:08:55,211 Epoch 741:\n",
            "\t\t\tTotal Training Recognition Loss 4.676499 || Total Training Translation Loss 0.004928\n",
            "2025-07-10 14:08:55,382 Epoch 742:\n",
            "\t\t\tTotal Training Recognition Loss 2.789870 || Total Training Translation Loss 0.006083\n",
            "2025-07-10 14:08:55,552 Epoch 743:\n",
            "\t\t\tTotal Training Recognition Loss 3.116366 || Total Training Translation Loss 0.005643\n",
            "2025-07-10 14:08:55,752 Epoch 744:\n",
            "\t\t\tTotal Training Recognition Loss 2.715798 || Total Training Translation Loss 0.005476\n",
            "2025-07-10 14:08:55,926 Epoch 745:\n",
            "\t\t\tTotal Training Recognition Loss 5.224919 || Total Training Translation Loss 0.005713\n",
            "2025-07-10 14:08:56,098 Epoch 746:\n",
            "\t\t\tTotal Training Recognition Loss 2.182392 || Total Training Translation Loss 0.004208\n",
            "2025-07-10 14:08:56,296 Epoch 747:\n",
            "\t\t\tTotal Training Recognition Loss 1.784302 || Total Training Translation Loss 0.003898\n",
            "2025-07-10 14:08:56,468 Epoch 748:\n",
            "\t\t\tTotal Training Recognition Loss 2.419300 || Total Training Translation Loss 0.005571\n",
            "2025-07-10 14:08:56,645 Epoch 749:\n",
            "\t\t\tTotal Training Recognition Loss 2.819404 || Total Training Translation Loss 0.004856\n",
            "2025-07-10 14:08:56,817 Epoch 750:\n",
            "\t\t\tTotal Training Recognition Loss 1.783021 || Total Training Translation Loss 0.007137\n",
            "2025-07-10 14:08:56,988 Epoch 751:\n",
            "\t\t\tTotal Training Recognition Loss 2.055772 || Total Training Translation Loss 0.004965\n",
            "2025-07-10 14:08:57,159 Epoch 752:\n",
            "\t\t\tTotal Training Recognition Loss 1.646687 || Total Training Translation Loss 0.004832\n",
            "2025-07-10 14:08:57,334 Epoch 753:\n",
            "\t\t\tTotal Training Recognition Loss 1.707423 || Total Training Translation Loss 0.005891\n",
            "2025-07-10 14:08:57,513 Epoch 754:\n",
            "\t\t\tTotal Training Recognition Loss 0.996084 || Total Training Translation Loss 0.004655\n",
            "2025-07-10 14:08:57,700 Epoch 755:\n",
            "\t\t\tTotal Training Recognition Loss 0.824805 || Total Training Translation Loss 0.004522\n",
            "2025-07-10 14:08:57,884 Epoch 756:\n",
            "\t\t\tTotal Training Recognition Loss 1.042438 || Total Training Translation Loss 0.005266\n",
            "2025-07-10 14:08:58,060 Epoch 757:\n",
            "\t\t\tTotal Training Recognition Loss 0.986573 || Total Training Translation Loss 0.004329\n",
            "2025-07-10 14:08:58,284 Epoch 758:\n",
            "\t\t\tTotal Training Recognition Loss 1.053885 || Total Training Translation Loss 0.003467\n",
            "2025-07-10 14:08:58,501 Epoch 759:\n",
            "\t\t\tTotal Training Recognition Loss 1.097188 || Total Training Translation Loss 0.005064\n",
            "2025-07-10 14:08:58,716 Epoch 760:\n",
            "\t\t\tTotal Training Recognition Loss 0.823324 || Total Training Translation Loss 0.003957\n",
            "2025-07-10 14:08:58,931 Epoch 761:\n",
            "\t\t\tTotal Training Recognition Loss 0.953846 || Total Training Translation Loss 0.005555\n",
            "2025-07-10 14:08:59,146 Epoch 762:\n",
            "\t\t\tTotal Training Recognition Loss 1.343685 || Total Training Translation Loss 0.004420\n",
            "2025-07-10 14:08:59,365 Epoch 763:\n",
            "\t\t\tTotal Training Recognition Loss 0.564991 || Total Training Translation Loss 0.002937\n",
            "2025-07-10 14:08:59,580 Epoch 764:\n",
            "\t\t\tTotal Training Recognition Loss 0.474246 || Total Training Translation Loss 0.004118\n",
            "2025-07-10 14:08:59,794 Epoch 765:\n",
            "\t\t\tTotal Training Recognition Loss 1.178265 || Total Training Translation Loss 0.003505\n",
            "2025-07-10 14:09:00,009 Epoch 766:\n",
            "\t\t\tTotal Training Recognition Loss 6.527322 || Total Training Translation Loss 0.004207\n",
            "2025-07-10 14:09:00,223 Epoch 767:\n",
            "\t\t\tTotal Training Recognition Loss 0.489762 || Total Training Translation Loss 0.003483\n",
            "2025-07-10 14:09:00,438 Epoch 768:\n",
            "\t\t\tTotal Training Recognition Loss 0.900607 || Total Training Translation Loss 0.004247\n",
            "2025-07-10 14:09:00,652 Epoch 769:\n",
            "\t\t\tTotal Training Recognition Loss 2.836786 || Total Training Translation Loss 0.004120\n",
            "2025-07-10 14:09:00,869 Epoch 770:\n",
            "\t\t\tTotal Training Recognition Loss 0.691562 || Total Training Translation Loss 0.003316\n",
            "2025-07-10 14:09:01,085 Epoch 771:\n",
            "\t\t\tTotal Training Recognition Loss 0.668685 || Total Training Translation Loss 0.003907\n",
            "2025-07-10 14:09:01,303 Epoch 772:\n",
            "\t\t\tTotal Training Recognition Loss 0.359090 || Total Training Translation Loss 0.004296\n",
            "2025-07-10 14:09:01,519 Epoch 773:\n",
            "\t\t\tTotal Training Recognition Loss 2.282114 || Total Training Translation Loss 0.003919\n",
            "2025-07-10 14:09:01,736 Epoch 774:\n",
            "\t\t\tTotal Training Recognition Loss 0.788561 || Total Training Translation Loss 0.003248\n",
            "2025-07-10 14:09:01,955 Epoch 775:\n",
            "\t\t\tTotal Training Recognition Loss 1.671034 || Total Training Translation Loss 0.004991\n",
            "2025-07-10 14:09:02,162 Epoch 776:\n",
            "\t\t\tTotal Training Recognition Loss 0.421097 || Total Training Translation Loss 0.003441\n",
            "2025-07-10 14:09:02,377 Epoch 777:\n",
            "\t\t\tTotal Training Recognition Loss 0.883492 || Total Training Translation Loss 0.003325\n",
            "2025-07-10 14:09:02,595 Epoch 778:\n",
            "\t\t\tTotal Training Recognition Loss 1.760866 || Total Training Translation Loss 0.005690\n",
            "2025-07-10 14:09:02,811 Epoch 779:\n",
            "\t\t\tTotal Training Recognition Loss 0.502434 || Total Training Translation Loss 0.003969\n",
            "2025-07-10 14:09:02,998 Epoch 780:\n",
            "\t\t\tTotal Training Recognition Loss 0.627546 || Total Training Translation Loss 0.004154\n",
            "2025-07-10 14:09:03,193 Epoch 781:\n",
            "\t\t\tTotal Training Recognition Loss 0.687554 || Total Training Translation Loss 0.003583\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-07-10 14:09:03,371 Epoch 782:\n",
            "\t\t\tTotal Training Recognition Loss 0.508879 || Total Training Translation Loss 0.003155\n",
            "2025-07-10 14:09:03,542 Epoch 783:\n",
            "\t\t\tTotal Training Recognition Loss 0.531613 || Total Training Translation Loss 0.003756\n",
            "2025-07-10 14:09:03,719 Epoch 784:\n",
            "\t\t\tTotal Training Recognition Loss 0.619841 || Total Training Translation Loss 0.004645\n",
            "2025-07-10 14:09:03,895 Epoch 785:\n",
            "\t\t\tTotal Training Recognition Loss 0.653011 || Total Training Translation Loss 0.002944\n",
            "2025-07-10 14:09:04,065 Epoch 786:\n",
            "\t\t\tTotal Training Recognition Loss 0.824762 || Total Training Translation Loss 0.003803\n",
            "2025-07-10 14:09:04,240 Epoch 787:\n",
            "\t\t\tTotal Training Recognition Loss 1.155347 || Total Training Translation Loss 0.003821\n",
            "2025-07-10 14:09:04,416 Epoch 788:\n",
            "\t\t\tTotal Training Recognition Loss 0.461505 || Total Training Translation Loss 0.004012\n",
            "2025-07-10 14:09:04,590 Epoch 789:\n",
            "\t\t\tTotal Training Recognition Loss 1.170264 || Total Training Translation Loss 0.004824\n",
            "2025-07-10 14:09:04,763 Epoch 790:\n",
            "\t\t\tTotal Training Recognition Loss 0.597657 || Total Training Translation Loss 0.004685\n",
            "2025-07-10 14:09:04,941 Epoch 791:\n",
            "\t\t\tTotal Training Recognition Loss 0.380171 || Total Training Translation Loss 0.004161\n",
            "2025-07-10 14:09:05,148 Epoch 792:\n",
            "\t\t\tTotal Training Recognition Loss 2.631664 || Total Training Translation Loss 0.004138\n",
            "2025-07-10 14:09:05,322 Epoch 793:\n",
            "\t\t\tTotal Training Recognition Loss 17.869663 || Total Training Translation Loss 0.003494\n",
            "2025-07-10 14:09:05,497 Epoch 794:\n",
            "\t\t\tTotal Training Recognition Loss 0.488889 || Total Training Translation Loss 0.003795\n",
            "2025-07-10 14:09:05,676 Epoch 795:\n",
            "\t\t\tTotal Training Recognition Loss 0.533733 || Total Training Translation Loss 0.005799\n",
            "2025-07-10 14:09:05,850 Epoch 796:\n",
            "\t\t\tTotal Training Recognition Loss 0.471178 || Total Training Translation Loss 0.003962\n",
            "2025-07-10 14:09:06,024 Epoch 797:\n",
            "\t\t\tTotal Training Recognition Loss 0.913867 || Total Training Translation Loss 0.003964\n",
            "2025-07-10 14:09:06,198 Epoch 798:\n",
            "\t\t\tTotal Training Recognition Loss 1.949975 || Total Training Translation Loss 0.004283\n",
            "2025-07-10 14:09:06,371 Epoch 799:\n",
            "\t\t\tTotal Training Recognition Loss 3.594063 || Total Training Translation Loss 0.004354\n",
            "2025-07-10 14:09:06,546 [Epoch: 800 Step: 00000800] Batch Recognition Loss:   0.463196 => Gls Tokens per Sec:      213 || Batch Translation Loss:   0.003469 => Txt Tokens per Sec:      575 || Lr: 0.001000\n",
            "2025-07-10 14:09:06,815 Validation result at epoch 800, step      800: duration: 0.2672s\n",
            "\tRecognition Beam Size: 1\tTranslation Beam Size: 1\tTranslation Beam Alpha: -1\n",
            "\tRecognition Loss: 8143.97607\tTranslation Loss: 254.03732\tPPL: 19.85934\n",
            "\tEval Metric: BLEU\n",
            "\tWER 102.86\t(DEL: 25.71,\tINS: 20.00,\tSUB: 57.14)\n",
            "\tBLEU-4 2.05\t(BLEU-1: 8.00,\tBLEU-2: 4.07,\tBLEU-3: 2.87,\tBLEU-4: 2.05)\n",
            "\tCHRF 26.05\tROUGE 11.03\tFID 0.00\n",
            "2025-07-10 14:09:06,815 Logging Recognition and Translation Outputs\n",
            "2025-07-10 14:09:06,816 ========================================================================================\n",
            "2025-07-10 14:09:06,816 Logging Sequence: 11August_2010_Wednesday_tagesschau-2\n",
            "2025-07-10 14:09:06,816 \tGloss Reference :\tDRUCK TIEF     KOMMEN\n",
            "2025-07-10 14:09:06,817 \tGloss Hypothesis:\t***** NORDWEST LOCH  \n",
            "2025-07-10 14:09:06,817 \tGloss Alignment :\tD     S        S     \n",
            "2025-07-10 14:09:06,817 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:09:06,820 \tText Reference  :\t** *** **** ** ********** *** **** ******** ******* tiefer luftdruck bestimmt in ******* ******** ****** ********* *********** ***** ***** ***** ***** ***** ***** ***** den   nächsten tagen unser wetter\n",
            "2025-07-10 14:09:06,820 \tText Hypothesis :\tam tag gibt es verbreitet zum teil kräftige schauer oder   gewitter  und      in manchen regionen fallen ergiebige regenmengen <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>    <unk> <unk> <unk> \n",
            "2025-07-10 14:09:06,820 \tText Alignment  :\tI  I   I    I  I          I   I    I        I       S      S         S           I       I        I      I         I           I     I     I     I     I     I     I     S     S        S     S     S     \n",
            "2025-07-10 14:09:06,820 ========================================================================================\n",
            "2025-07-10 14:09:06,820 Logging Sequence: 11August_2010_Wednesday_tagesschau-3\n",
            "2025-07-10 14:09:06,821 \tGloss Reference :\tES-BEDEUTET VIEL WOLKE UND KOENNEN REGEN GEWITTER KOENNEN\n",
            "2025-07-10 14:09:06,821 \tGloss Hypothesis:\t*********** **** ***** *** ******* HEUTE BLEIBEN  KOENNEN\n",
            "2025-07-10 14:09:06,822 \tGloss Alignment :\tD           D    D     D   D       S     S               \n",
            "2025-07-10 14:09:06,822 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:09:06,824 \tText Reference  :\tdas bedeutet viele wolken und  immer wieder     zum teil kräftige schauer **** ******** und ** ******* ******** ****** ********* *********** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** gewitter\n",
            "2025-07-10 14:09:06,824 \tText Hypothesis :\t*** ******** am    tag    gibt es    verbreitet zum teil kräftige schauer oder gewitter und in manchen regionen fallen ergiebige regenmengen <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>   \n",
            "2025-07-10 14:09:06,824 \tText Alignment  :\tD   D        S     S      S    S     S                                    I    I            I  I       I        I      I         I           I     I     I     I     I     I     I     I     I     I     I     S       \n",
            "2025-07-10 14:09:06,824 ========================================================================================\n",
            "2025-07-10 14:09:06,825 Logging Sequence: 11August_2010_Wednesday_tagesschau-8\n",
            "2025-07-10 14:09:06,825 \tGloss Reference :\tWIND MAESSIG SCHWACH REGION  WENN     GEWITTER WIND     KOENNEN ********\n",
            "2025-07-10 14:09:06,825 \tGloss Hypothesis:\t**** ******* DURCH   BLEIBEN NORDWEST BLEIBEN  SUEDWEST KOENNEN NORDWEST\n",
            "2025-07-10 14:09:06,825 \tGloss Alignment :\tD    D       S       S       S        S        S                I       \n",
            "2025-07-10 14:09:06,826 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:09:06,828 \tText Reference  :\tmeist weht nur ein  schwacher wind       aus unterschiedlichen richtungen der     bei  schauern und ** ******* ******** ****** ********* *********** ***** ***** ***** ***** ***** ***** ***** gewittern stark böig  sein  kann \n",
            "2025-07-10 14:09:06,828 \tText Hypothesis :\t***** am   tag gibt es        verbreitet zum teil              kräftige   schauer oder gewitter und in manchen regionen fallen ergiebige regenmengen <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>     <unk> <unk> <unk> <unk>\n",
            "2025-07-10 14:09:06,829 \tText Alignment  :\tD     S    S   S    S         S          S   S                 S          S       S    S            I  I       I        I      I         I           I     I     I     I     I     I     I     S         S     S     S     S    \n",
            "2025-07-10 14:09:06,829 ========================================================================================\n",
            "2025-07-10 14:09:06,829 Logging Sequence: 25October_2010_Monday_tagesschau-22\n",
            "2025-07-10 14:09:06,830 \tGloss Reference :\t*** ******* ******* MITTWOCH REGEN   KOENNEN ********** ******** ****** NORDWEST WAHRSCHEINLICH NORD       STARK   WIND\n",
            "2025-07-10 14:09:06,830 \tGloss Hypothesis:\tOFT FEBRUAR BLEIBEN GEWITTER BLEIBEN KOENNEN DONNERSTAG SUEDWEST MORGEN SUEDWEST OFT            DONNERSTAG KOENNEN VIEL\n",
            "2025-07-10 14:09:06,830 \tGloss Alignment :\tI   I       I       S        S               I          I        I      S        S              S          S       S   \n",
            "2025-07-10 14:09:06,830 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:09:06,832 \tText Reference  :\tam *** **** ** ********** *** **** ******** ******* mittwoch hier     und ** ******* ******** ****** ********* *********** ***** ***** da    nieselregen in    der   nordwesthälfte an    den   küsten kräftiger wind \n",
            "2025-07-10 14:09:06,832 \tText Hypothesis :\tam tag gibt es verbreitet zum teil kräftige schauer oder     gewitter und in manchen regionen fallen ergiebige regenmengen <unk> <unk> <unk> <unk>       <unk> <unk> <unk>          <unk> <unk> <unk>  <unk>     <unk>\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-07-10 14:09:06,832 \tText Alignment  :\t   I   I    I  I          I   I    I        I       S        S            I  I       I        I      I         I           I     I     S     S           S     S     S              S     S     S      S         S    \n",
            "2025-07-10 14:09:06,832 ========================================================================================\n",
            "2025-07-10 14:09:06,832 Logging Sequence: 05May_2011_Thursday_tagesschau-25\n",
            "2025-07-10 14:09:06,833 \tGloss Reference :\tJETZT WETTER WIE-AUSSEHEN MORGEN FREITAG    SECHSTE MAI     ZEIGEN-BILDSCHIRM\n",
            "2025-07-10 14:09:06,833 \tGloss Hypothesis:\tJETZT WETTER ************ MORGEN DONNERSTAG MORGEN  BLEIBEN FEBRUAR          \n",
            "2025-07-10 14:09:06,833 \tGloss Alignment :\t             D                   S          S       S       S                \n",
            "2025-07-10 14:09:06,833 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:09:06,835 \tText Reference  :\t** *** **** ** ********** *** **** ******** ******* **** ******** und ** ******* ******** ****** ********* *********** ***** ***** ***** nun   die   wettervorhersage für   morgen freitag den   sechsten mai  \n",
            "2025-07-10 14:09:06,835 \tText Hypothesis :\tam tag gibt es verbreitet zum teil kräftige schauer oder gewitter und in manchen regionen fallen ergiebige regenmengen <unk> <unk> <unk> <unk> <unk> <unk>            <unk> <unk>  <unk>   <unk> <unk>    <unk>\n",
            "2025-07-10 14:09:06,835 \tText Alignment  :\tI  I   I    I  I          I   I    I        I       I    I            I  I       I        I      I         I           I     I     I     S     S     S                S     S      S       S     S        S    \n",
            "2025-07-10 14:09:06,835 ========================================================================================\n",
            "2025-07-10 14:09:06,836 Epoch 800:\n",
            "\t\t\tTotal Training Recognition Loss 0.463196 || Total Training Translation Loss 0.003469\n",
            "2025-07-10 14:09:07,010 Epoch 801:\n",
            "\t\t\tTotal Training Recognition Loss 0.600495 || Total Training Translation Loss 0.003926\n",
            "2025-07-10 14:09:07,184 Epoch 802:\n",
            "\t\t\tTotal Training Recognition Loss 0.517479 || Total Training Translation Loss 0.005421\n",
            "2025-07-10 14:09:07,366 Epoch 803:\n",
            "\t\t\tTotal Training Recognition Loss 3.044092 || Total Training Translation Loss 0.003547\n",
            "2025-07-10 14:09:07,544 Epoch 804:\n",
            "\t\t\tTotal Training Recognition Loss 0.565343 || Total Training Translation Loss 0.003810\n",
            "2025-07-10 14:09:07,719 Epoch 805:\n",
            "\t\t\tTotal Training Recognition Loss 0.660225 || Total Training Translation Loss 0.003852\n",
            "2025-07-10 14:09:07,892 Epoch 806:\n",
            "\t\t\tTotal Training Recognition Loss 0.423450 || Total Training Translation Loss 0.003236\n",
            "2025-07-10 14:09:08,067 Epoch 807:\n",
            "\t\t\tTotal Training Recognition Loss 0.529469 || Total Training Translation Loss 0.003740\n",
            "2025-07-10 14:09:08,239 Epoch 808:\n",
            "\t\t\tTotal Training Recognition Loss 0.524907 || Total Training Translation Loss 0.004401\n",
            "2025-07-10 14:09:08,449 Epoch 809:\n",
            "\t\t\tTotal Training Recognition Loss 1.889328 || Total Training Translation Loss 0.003806\n",
            "2025-07-10 14:09:08,626 Epoch 810:\n",
            "\t\t\tTotal Training Recognition Loss 7.816559 || Total Training Translation Loss 0.004761\n",
            "2025-07-10 14:09:08,804 Epoch 811:\n",
            "\t\t\tTotal Training Recognition Loss 0.678061 || Total Training Translation Loss 0.003353\n",
            "2025-07-10 14:09:08,980 Epoch 812:\n",
            "\t\t\tTotal Training Recognition Loss 0.266669 || Total Training Translation Loss 0.003909\n",
            "2025-07-10 14:09:09,158 Epoch 813:\n",
            "\t\t\tTotal Training Recognition Loss 0.371088 || Total Training Translation Loss 0.004498\n",
            "2025-07-10 14:09:09,335 Epoch 814:\n",
            "\t\t\tTotal Training Recognition Loss 0.489837 || Total Training Translation Loss 0.003387\n",
            "2025-07-10 14:09:09,509 Epoch 815:\n",
            "\t\t\tTotal Training Recognition Loss 0.252416 || Total Training Translation Loss 0.003210\n",
            "2025-07-10 14:09:09,685 Epoch 816:\n",
            "\t\t\tTotal Training Recognition Loss 0.521457 || Total Training Translation Loss 0.004629\n",
            "2025-07-10 14:09:09,860 Epoch 817:\n",
            "\t\t\tTotal Training Recognition Loss 0.556164 || Total Training Translation Loss 0.003804\n",
            "2025-07-10 14:09:10,034 Epoch 818:\n",
            "\t\t\tTotal Training Recognition Loss 0.371019 || Total Training Translation Loss 0.003637\n",
            "2025-07-10 14:09:10,209 Epoch 819:\n",
            "\t\t\tTotal Training Recognition Loss 1.031248 || Total Training Translation Loss 0.003680\n",
            "2025-07-10 14:09:10,384 Epoch 820:\n",
            "\t\t\tTotal Training Recognition Loss 0.329523 || Total Training Translation Loss 0.003757\n",
            "2025-07-10 14:09:10,558 Epoch 821:\n",
            "\t\t\tTotal Training Recognition Loss 0.445786 || Total Training Translation Loss 0.004694\n",
            "2025-07-10 14:09:10,734 Epoch 822:\n",
            "\t\t\tTotal Training Recognition Loss 0.512284 || Total Training Translation Loss 0.004855\n",
            "2025-07-10 14:09:10,909 Epoch 823:\n",
            "\t\t\tTotal Training Recognition Loss 8.360917 || Total Training Translation Loss 0.003598\n",
            "2025-07-10 14:09:11,084 Epoch 824:\n",
            "\t\t\tTotal Training Recognition Loss 0.860353 || Total Training Translation Loss 0.004125\n",
            "2025-07-10 14:09:11,259 Epoch 825:\n",
            "\t\t\tTotal Training Recognition Loss 0.725249 || Total Training Translation Loss 0.003674\n",
            "2025-07-10 14:09:11,434 Epoch 826:\n",
            "\t\t\tTotal Training Recognition Loss 0.910018 || Total Training Translation Loss 0.004721\n",
            "2025-07-10 14:09:11,656 Epoch 827:\n",
            "\t\t\tTotal Training Recognition Loss 0.539941 || Total Training Translation Loss 0.003517\n",
            "2025-07-10 14:09:11,873 Epoch 828:\n",
            "\t\t\tTotal Training Recognition Loss 2.815435 || Total Training Translation Loss 0.004369\n",
            "2025-07-10 14:09:12,090 Epoch 829:\n",
            "\t\t\tTotal Training Recognition Loss 1.244350 || Total Training Translation Loss 0.003988\n",
            "2025-07-10 14:09:12,266 Epoch 830:\n",
            "\t\t\tTotal Training Recognition Loss 0.373363 || Total Training Translation Loss 0.004424\n",
            "2025-07-10 14:09:12,483 Epoch 831:\n",
            "\t\t\tTotal Training Recognition Loss 0.268496 || Total Training Translation Loss 0.003489\n",
            "2025-07-10 14:09:12,659 Epoch 832:\n",
            "\t\t\tTotal Training Recognition Loss 0.454535 || Total Training Translation Loss 0.004535\n",
            "2025-07-10 14:09:12,832 Epoch 833:\n",
            "\t\t\tTotal Training Recognition Loss 0.536373 || Total Training Translation Loss 0.004302\n",
            "2025-07-10 14:09:13,005 Epoch 834:\n",
            "\t\t\tTotal Training Recognition Loss 0.390103 || Total Training Translation Loss 0.003055\n",
            "2025-07-10 14:09:13,178 Epoch 835:\n",
            "\t\t\tTotal Training Recognition Loss 0.430015 || Total Training Translation Loss 0.003750\n",
            "2025-07-10 14:09:13,352 Epoch 836:\n",
            "\t\t\tTotal Training Recognition Loss 4.188282 || Total Training Translation Loss 0.004570\n",
            "2025-07-10 14:09:13,525 Epoch 837:\n",
            "\t\t\tTotal Training Recognition Loss 2.025390 || Total Training Translation Loss 0.003005\n",
            "2025-07-10 14:09:13,700 Epoch 838:\n",
            "\t\t\tTotal Training Recognition Loss 0.576240 || Total Training Translation Loss 0.004318\n",
            "2025-07-10 14:09:13,874 Epoch 839:\n",
            "\t\t\tTotal Training Recognition Loss 0.477281 || Total Training Translation Loss 0.004817\n",
            "2025-07-10 14:09:14,048 Epoch 840:\n",
            "\t\t\tTotal Training Recognition Loss 0.367007 || Total Training Translation Loss 0.004182\n",
            "2025-07-10 14:09:14,222 Epoch 841:\n",
            "\t\t\tTotal Training Recognition Loss 0.395517 || Total Training Translation Loss 0.003569\n",
            "2025-07-10 14:09:14,394 Epoch 842:\n",
            "\t\t\tTotal Training Recognition Loss 0.370539 || Total Training Translation Loss 0.003986\n",
            "2025-07-10 14:09:14,567 Epoch 843:\n",
            "\t\t\tTotal Training Recognition Loss 0.448631 || Total Training Translation Loss 0.003380\n",
            "2025-07-10 14:09:14,739 Epoch 844:\n",
            "\t\t\tTotal Training Recognition Loss 0.745840 || Total Training Translation Loss 0.004718\n",
            "2025-07-10 14:09:14,912 Epoch 845:\n",
            "\t\t\tTotal Training Recognition Loss 0.484174 || Total Training Translation Loss 0.003474\n",
            "2025-07-10 14:09:15,086 Epoch 846:\n",
            "\t\t\tTotal Training Recognition Loss 0.631552 || Total Training Translation Loss 0.004176\n",
            "2025-07-10 14:09:15,259 Epoch 847:\n",
            "\t\t\tTotal Training Recognition Loss 0.663586 || Total Training Translation Loss 0.003942\n",
            "2025-07-10 14:09:15,434 Epoch 848:\n",
            "\t\t\tTotal Training Recognition Loss 0.646859 || Total Training Translation Loss 0.004612\n",
            "2025-07-10 14:09:15,607 Epoch 849:\n",
            "\t\t\tTotal Training Recognition Loss 0.518388 || Total Training Translation Loss 0.004924\n",
            "2025-07-10 14:09:15,779 Epoch 850:\n",
            "\t\t\tTotal Training Recognition Loss 0.579171 || Total Training Translation Loss 0.003867\n",
            "2025-07-10 14:09:15,952 Epoch 851:\n",
            "\t\t\tTotal Training Recognition Loss 1.099913 || Total Training Translation Loss 0.004676\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-07-10 14:09:16,126 Epoch 852:\n",
            "\t\t\tTotal Training Recognition Loss 0.898485 || Total Training Translation Loss 0.004201\n",
            "2025-07-10 14:09:16,300 Epoch 853:\n",
            "\t\t\tTotal Training Recognition Loss 0.716383 || Total Training Translation Loss 0.003826\n",
            "2025-07-10 14:09:16,480 Epoch 854:\n",
            "\t\t\tTotal Training Recognition Loss 0.447704 || Total Training Translation Loss 0.004637\n",
            "2025-07-10 14:09:16,651 Epoch 855:\n",
            "\t\t\tTotal Training Recognition Loss 0.454568 || Total Training Translation Loss 0.003733\n",
            "2025-07-10 14:09:16,820 Epoch 856:\n",
            "\t\t\tTotal Training Recognition Loss 0.466260 || Total Training Translation Loss 0.004290\n",
            "2025-07-10 14:09:16,990 Epoch 857:\n",
            "\t\t\tTotal Training Recognition Loss 0.841957 || Total Training Translation Loss 0.003781\n",
            "2025-07-10 14:09:17,201 Epoch 858:\n",
            "\t\t\tTotal Training Recognition Loss 0.403950 || Total Training Translation Loss 0.004229\n",
            "2025-07-10 14:09:17,417 Epoch 859:\n",
            "\t\t\tTotal Training Recognition Loss 0.370640 || Total Training Translation Loss 0.003227\n",
            "2025-07-10 14:09:17,633 Epoch 860:\n",
            "\t\t\tTotal Training Recognition Loss 0.655949 || Total Training Translation Loss 0.003636\n",
            "2025-07-10 14:09:17,849 Epoch 861:\n",
            "\t\t\tTotal Training Recognition Loss 0.328831 || Total Training Translation Loss 0.004272\n",
            "2025-07-10 14:09:18,023 Epoch 862:\n",
            "\t\t\tTotal Training Recognition Loss 3.980720 || Total Training Translation Loss 0.005928\n",
            "2025-07-10 14:09:18,202 Epoch 863:\n",
            "\t\t\tTotal Training Recognition Loss 0.523842 || Total Training Translation Loss 0.004561\n",
            "2025-07-10 14:09:18,375 Epoch 864:\n",
            "\t\t\tTotal Training Recognition Loss 0.560868 || Total Training Translation Loss 0.004311\n",
            "2025-07-10 14:09:18,548 Epoch 865:\n",
            "\t\t\tTotal Training Recognition Loss 0.277762 || Total Training Translation Loss 0.003873\n",
            "2025-07-10 14:09:18,720 Epoch 866:\n",
            "\t\t\tTotal Training Recognition Loss 0.274630 || Total Training Translation Loss 0.004583\n",
            "2025-07-10 14:09:18,892 Epoch 867:\n",
            "\t\t\tTotal Training Recognition Loss 0.842566 || Total Training Translation Loss 0.004458\n",
            "2025-07-10 14:09:19,064 Epoch 868:\n",
            "\t\t\tTotal Training Recognition Loss 0.206024 || Total Training Translation Loss 0.003382\n",
            "2025-07-10 14:09:19,237 Epoch 869:\n",
            "\t\t\tTotal Training Recognition Loss 3.621111 || Total Training Translation Loss 0.004444\n",
            "2025-07-10 14:09:19,410 Epoch 870:\n",
            "\t\t\tTotal Training Recognition Loss 0.269153 || Total Training Translation Loss 0.003734\n",
            "2025-07-10 14:09:19,583 Epoch 871:\n",
            "\t\t\tTotal Training Recognition Loss 0.249740 || Total Training Translation Loss 0.003337\n",
            "2025-07-10 14:09:19,755 Epoch 872:\n",
            "\t\t\tTotal Training Recognition Loss 2.367179 || Total Training Translation Loss 0.003826\n",
            "2025-07-10 14:09:19,927 Epoch 873:\n",
            "\t\t\tTotal Training Recognition Loss 0.382185 || Total Training Translation Loss 0.004694\n",
            "2025-07-10 14:09:20,100 Epoch 874:\n",
            "\t\t\tTotal Training Recognition Loss 0.556519 || Total Training Translation Loss 0.004525\n",
            "2025-07-10 14:09:20,272 Epoch 875:\n",
            "\t\t\tTotal Training Recognition Loss 9.368422 || Total Training Translation Loss 0.003488\n",
            "2025-07-10 14:09:20,447 Epoch 876:\n",
            "\t\t\tTotal Training Recognition Loss 0.554628 || Total Training Translation Loss 0.004559\n",
            "2025-07-10 14:09:20,618 Epoch 877:\n",
            "\t\t\tTotal Training Recognition Loss 0.588895 || Total Training Translation Loss 0.004244\n",
            "2025-07-10 14:09:20,786 Epoch 878:\n",
            "\t\t\tTotal Training Recognition Loss 0.557212 || Total Training Translation Loss 0.003964\n",
            "2025-07-10 14:09:20,955 Epoch 879:\n",
            "\t\t\tTotal Training Recognition Loss 0.258631 || Total Training Translation Loss 0.005567\n",
            "2025-07-10 14:09:21,173 Epoch 880:\n",
            "\t\t\tTotal Training Recognition Loss 6.802190 || Total Training Translation Loss 0.004121\n",
            "2025-07-10 14:09:21,389 Epoch 881:\n",
            "\t\t\tTotal Training Recognition Loss 0.557501 || Total Training Translation Loss 0.004425\n",
            "2025-07-10 14:09:21,604 Epoch 882:\n",
            "\t\t\tTotal Training Recognition Loss 1.061392 || Total Training Translation Loss 0.005534\n",
            "2025-07-10 14:09:21,819 Epoch 883:\n",
            "\t\t\tTotal Training Recognition Loss 0.517357 || Total Training Translation Loss 0.004958\n",
            "2025-07-10 14:09:21,990 Epoch 884:\n",
            "\t\t\tTotal Training Recognition Loss 46.003784 || Total Training Translation Loss 0.004599\n",
            "2025-07-10 14:09:22,160 Epoch 885:\n",
            "\t\t\tTotal Training Recognition Loss 0.289437 || Total Training Translation Loss 0.003655\n",
            "2025-07-10 14:09:22,330 Epoch 886:\n",
            "\t\t\tTotal Training Recognition Loss 63.183064 || Total Training Translation Loss 0.005302\n",
            "2025-07-10 14:09:22,499 Epoch 887:\n",
            "\t\t\tTotal Training Recognition Loss 4.317222 || Total Training Translation Loss 0.004935\n",
            "2025-07-10 14:09:22,671 Epoch 888:\n",
            "\t\t\tTotal Training Recognition Loss 10.987489 || Total Training Translation Loss 0.005008\n",
            "2025-07-10 14:09:22,843 Epoch 889:\n",
            "\t\t\tTotal Training Recognition Loss 97.025558 || Total Training Translation Loss 0.005106\n",
            "2025-07-10 14:09:23,016 Epoch 890:\n",
            "\t\t\tTotal Training Recognition Loss 55.193268 || Total Training Translation Loss 0.005073\n",
            "2025-07-10 14:09:23,188 Epoch 891:\n",
            "\t\t\tTotal Training Recognition Loss 18.969852 || Total Training Translation Loss 0.006136\n",
            "2025-07-10 14:09:23,363 Epoch 892:\n",
            "\t\t\tTotal Training Recognition Loss 13.396920 || Total Training Translation Loss 0.004965\n",
            "2025-07-10 14:09:23,535 Epoch 893:\n",
            "\t\t\tTotal Training Recognition Loss 112.359978 || Total Training Translation Loss 0.005483\n",
            "2025-07-10 14:09:23,707 Epoch 894:\n",
            "\t\t\tTotal Training Recognition Loss 20.567907 || Total Training Translation Loss 0.006599\n",
            "2025-07-10 14:09:23,877 Epoch 895:\n",
            "\t\t\tTotal Training Recognition Loss 28.809111 || Total Training Translation Loss 0.007550\n",
            "2025-07-10 14:09:24,047 Epoch 896:\n",
            "\t\t\tTotal Training Recognition Loss 93.021011 || Total Training Translation Loss 0.009066\n",
            "2025-07-10 14:09:24,219 Epoch 897:\n",
            "\t\t\tTotal Training Recognition Loss 105.623123 || Total Training Translation Loss 0.097727\n",
            "2025-07-10 14:09:24,389 Epoch 898:\n",
            "\t\t\tTotal Training Recognition Loss 88.949379 || Total Training Translation Loss 0.017269\n",
            "2025-07-10 14:09:24,559 Epoch 899:\n",
            "\t\t\tTotal Training Recognition Loss 54.641701 || Total Training Translation Loss 0.553467\n",
            "2025-07-10 14:09:24,729 [Epoch: 900 Step: 00000900] Batch Recognition Loss:  36.143974 => Gls Tokens per Sec:      219 || Batch Translation Loss:   0.367115 => Txt Tokens per Sec:      592 || Lr: 0.001000\n",
            "2025-07-10 14:09:24,965 Validation result at epoch 900, step      900: duration: 0.2353s\n",
            "\tRecognition Beam Size: 1\tTranslation Beam Size: 1\tTranslation Beam Alpha: -1\n",
            "\tRecognition Loss: 9806.02734\tTranslation Loss: 250.47284\tPPL: 19.04375\n",
            "\tEval Metric: BLEU\n",
            "\tWER 105.71\t(DEL: 25.71,\tINS: 20.00,\tSUB: 60.00)\n",
            "\tBLEU-4 0.00\t(BLEU-1: 3.33,\tBLEU-2: 0.00,\tBLEU-3: 0.00,\tBLEU-4: 0.00)\n",
            "\tCHRF 17.73\tROUGE 4.95\tFID 0.00\n",
            "2025-07-10 14:09:24,966 Logging Recognition and Translation Outputs\n",
            "2025-07-10 14:09:24,966 ========================================================================================\n",
            "2025-07-10 14:09:24,967 Logging Sequence: 11August_2010_Wednesday_tagesschau-2\n",
            "2025-07-10 14:09:24,967 \tGloss Reference :\tDRUCK TIEF KOMMEN  \n",
            "2025-07-10 14:09:24,967 \tGloss Hypothesis:\t***** **** NORDWEST\n",
            "2025-07-10 14:09:24,967 \tGloss Alignment :\tD     D    S       \n",
            "2025-07-10 14:09:24,968 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:09:24,969 \tText Reference  :\t****** ** **** ****** *** ******** **** ****** **************** **** ****** ******* ***** ***** ***** ***** ***** ***** ***** ***** ***** tiefer luftdruck bestimmt in    den   nächsten tagen unser wetter\n",
            "2025-07-10 14:09:24,969 \tText Hypothesis :\tregnet es auch länger und ergiebig auch lokale überschwemmungen sind wieder möglich <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>  <unk>     <unk>    <unk> <unk> <unk>    <unk> <unk> <unk> \n",
            "2025-07-10 14:09:24,970 \tText Alignment  :\tI      I  I    I      I   I        I    I      I                I    I      I       I     I     I     I     I     I     I     I     I     S      S         S        S     S     S        S     S     S     \n",
            "2025-07-10 14:09:24,970 ========================================================================================\n",
            "2025-07-10 14:09:24,970 Logging Sequence: 11August_2010_Wednesday_tagesschau-3\n",
            "2025-07-10 14:09:24,971 \tGloss Reference :\tES-BEDEUTET VIEL  WOLKE  UND     KOENNEN REGEN GEWITTER KOENNEN ******\n",
            "2025-07-10 14:09:24,971 \tGloss Hypothesis:\t*********** HEUTE ZWOELF TROCKEN KOENNEN ***** VIEL     KOENNEN ZWOELF\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-07-10 14:09:24,971 \tGloss Alignment :\tD           S     S      S               D     S                I     \n",
            "2025-07-10 14:09:24,971 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:09:24,973 \tText Reference  :\tdas    bedeutet viele wolken und ******** **** ****** **************** immer wieder ******* ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** zum   teil  kräftige schauer und   gewitter\n",
            "2025-07-10 14:09:24,973 \tText Hypothesis :\tregnet es       auch  länger und ergiebig auch lokale überschwemmungen sind  wieder möglich <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>    <unk>   <unk> <unk>   \n",
            "2025-07-10 14:09:24,973 \tText Alignment  :\tS      S        S     S          I        I    I      I                S            I       I     I     I     I     I     I     I     I     I     I     I     I     S     S     S        S       S     S       \n",
            "2025-07-10 14:09:24,973 ========================================================================================\n",
            "2025-07-10 14:09:24,974 Logging Sequence: 11August_2010_Wednesday_tagesschau-8\n",
            "2025-07-10 14:09:24,974 \tGloss Reference :\tWIND MAESSIG SCHWACH REGION WENN GEWITTER WIND    KOENNEN \n",
            "2025-07-10 14:09:24,974 \tGloss Hypothesis:\t**** ******* ******* ****** ORT  NORDWEST TROCKEN NORDWEST\n",
            "2025-07-10 14:09:24,974 \tGloss Alignment :\tD    D       D       D      S    S        S       S       \n",
            "2025-07-10 14:09:24,975 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:09:24,977 \tText Reference  :\t****** ** **** ****** *** ******** **** ****** **************** **** ****** ******* meist weht  nur   ein   schwacher wind  aus   unterschiedlichen richtungen der   bei   schauern und   gewittern stark böig  sein  kann \n",
            "2025-07-10 14:09:24,977 \tText Hypothesis :\tregnet es auch länger und ergiebig auch lokale überschwemmungen sind wieder möglich <unk> <unk> <unk> <unk> <unk>     <unk> <unk> <unk>             <unk>      <unk> <unk> <unk>    <unk> <unk>     <unk> <unk> <unk> <unk>\n",
            "2025-07-10 14:09:24,977 \tText Alignment  :\tI      I  I    I      I   I        I    I      I                I    I      I       S     S     S     S     S         S     S     S                 S          S     S     S        S     S         S     S     S     S    \n",
            "2025-07-10 14:09:24,978 ========================================================================================\n",
            "2025-07-10 14:09:24,978 Logging Sequence: 25October_2010_Monday_tagesschau-22\n",
            "2025-07-10 14:09:24,978 \tGloss Reference :\t****** *** ******* ******** MITTWOCH REGEN    KOENNEN NORDWEST WAHRSCHEINLICH NORD STARK   WIND\n",
            "2025-07-10 14:09:24,978 \tGloss Hypothesis:\tWETTER ORT FEBRUAR GEWITTER ORT      SUEDWEST MORGEN  ORT      SUEDWEST       VIEL TROCKEN ORT \n",
            "2025-07-10 14:09:24,979 \tGloss Alignment :\tI      I   I       I        S        S        S       S        S              S    S       S   \n",
            "2025-07-10 14:09:24,979 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:09:24,980 \tText Reference  :\t****** am mittwoch hier   und ******** **** ****** **************** **** ****** ******* ***** ***** ***** ***** ***** ***** ***** ***** da    nieselregen in    der   nordwesthälfte an    den   küsten kräftiger wind \n",
            "2025-07-10 14:09:24,981 \tText Hypothesis :\tregnet es auch     länger und ergiebig auch lokale überschwemmungen sind wieder möglich <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>       <unk> <unk> <unk>          <unk> <unk> <unk>  <unk>     <unk>\n",
            "2025-07-10 14:09:24,981 \tText Alignment  :\tI      S  S        S          I        I    I      I                I    I      I       I     I     I     I     I     I     I     I     S     S           S     S     S              S     S     S      S         S    \n",
            "2025-07-10 14:09:24,981 ========================================================================================\n",
            "2025-07-10 14:09:24,981 Logging Sequence: 05May_2011_Thursday_tagesschau-25\n",
            "2025-07-10 14:09:24,981 \tGloss Reference :\t****** JETZT WETTER WIE-AUSSEHEN MORGEN ********** FREITAG SECHSTE MAI    ZEIGEN-BILDSCHIRM\n",
            "2025-07-10 14:09:24,982 \tGloss Hypothesis:\tZWOELF JETZT WETTER ************ MORGEN DONNERSTAG ZWOELF  TROCKEN MORGEN FEBRUAR          \n",
            "2025-07-10 14:09:24,982 \tGloss Alignment :\tI                   D                   I          S       S       S      S                \n",
            "2025-07-10 14:09:24,982 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:09:24,983 \tText Reference  :\t****** ** **** ****** und ******** **** ****** **************** **** ****** ******* ***** ***** ***** ***** ***** ***** ***** ***** ***** nun   die   wettervorhersage für   morgen freitag den   sechsten mai  \n",
            "2025-07-10 14:09:24,983 \tText Hypothesis :\tregnet es auch länger und ergiebig auch lokale überschwemmungen sind wieder möglich <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>            <unk> <unk>  <unk>   <unk> <unk>    <unk>\n",
            "2025-07-10 14:09:24,983 \tText Alignment  :\tI      I  I    I          I        I    I      I                I    I      I       I     I     I     I     I     I     I     I     I     S     S     S                S     S      S       S     S        S    \n",
            "2025-07-10 14:09:24,984 ========================================================================================\n",
            "2025-07-10 14:09:24,985 Epoch 900:\n",
            "\t\t\tTotal Training Recognition Loss 36.143974 || Total Training Translation Loss 0.367115\n",
            "2025-07-10 14:09:25,153 Epoch 901:\n",
            "\t\t\tTotal Training Recognition Loss 26.054466 || Total Training Translation Loss 1.387276\n",
            "2025-07-10 14:09:25,324 Epoch 902:\n",
            "\t\t\tTotal Training Recognition Loss 36.404186 || Total Training Translation Loss 0.046330\n",
            "2025-07-10 14:09:25,494 Epoch 903:\n",
            "\t\t\tTotal Training Recognition Loss 67.818947 || Total Training Translation Loss 0.299619\n",
            "2025-07-10 14:09:25,664 Epoch 904:\n",
            "\t\t\tTotal Training Recognition Loss 32.941063 || Total Training Translation Loss 0.411390\n",
            "2025-07-10 14:09:25,836 Epoch 905:\n",
            "\t\t\tTotal Training Recognition Loss 34.053356 || Total Training Translation Loss 0.151994\n",
            "2025-07-10 14:09:26,007 Epoch 906:\n",
            "\t\t\tTotal Training Recognition Loss 9.118994 || Total Training Translation Loss 0.241432\n",
            "2025-07-10 14:09:26,177 Epoch 907:\n",
            "\t\t\tTotal Training Recognition Loss 18.321741 || Total Training Translation Loss 0.199143\n",
            "2025-07-10 14:09:26,347 Epoch 908:\n",
            "\t\t\tTotal Training Recognition Loss 8.112231 || Total Training Translation Loss 0.287596\n",
            "2025-07-10 14:09:26,518 Epoch 909:\n",
            "\t\t\tTotal Training Recognition Loss 70.267242 || Total Training Translation Loss 0.320294\n",
            "2025-07-10 14:09:26,687 Epoch 910:\n",
            "\t\t\tTotal Training Recognition Loss 35.714798 || Total Training Translation Loss 0.078731\n",
            "2025-07-10 14:09:26,857 Epoch 911:\n",
            "\t\t\tTotal Training Recognition Loss 128.861816 || Total Training Translation Loss 0.072462\n",
            "2025-07-10 14:09:27,027 Epoch 912:\n",
            "\t\t\tTotal Training Recognition Loss 17.431595 || Total Training Translation Loss 0.075341\n",
            "2025-07-10 14:09:27,198 Epoch 913:\n",
            "\t\t\tTotal Training Recognition Loss 35.703766 || Total Training Translation Loss 0.121630\n",
            "2025-07-10 14:09:27,368 Epoch 914:\n",
            "\t\t\tTotal Training Recognition Loss 18.482578 || Total Training Translation Loss 0.118194\n",
            "2025-07-10 14:09:27,541 Epoch 915:\n",
            "\t\t\tTotal Training Recognition Loss 18.473648 || Total Training Translation Loss 0.067512\n",
            "2025-07-10 14:09:27,711 Epoch 916:\n",
            "\t\t\tTotal Training Recognition Loss 29.296329 || Total Training Translation Loss 0.043351\n",
            "2025-07-10 14:09:27,882 Epoch 917:\n",
            "\t\t\tTotal Training Recognition Loss 61.884888 || Total Training Translation Loss 0.059443\n",
            "2025-07-10 14:09:28,052 Epoch 918:\n",
            "\t\t\tTotal Training Recognition Loss 28.616226 || Total Training Translation Loss 0.047937\n",
            "2025-07-10 14:09:28,224 Epoch 919:\n",
            "\t\t\tTotal Training Recognition Loss 10.967745 || Total Training Translation Loss 0.061379\n",
            "2025-07-10 14:09:28,395 Epoch 920:\n",
            "\t\t\tTotal Training Recognition Loss 32.369083 || Total Training Translation Loss 0.045363\n",
            "2025-07-10 14:09:28,565 Epoch 921:\n",
            "\t\t\tTotal Training Recognition Loss 7.619688 || Total Training Translation Loss 0.050742\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-07-10 14:09:28,734 Epoch 922:\n",
            "\t\t\tTotal Training Recognition Loss 18.549648 || Total Training Translation Loss 0.048454\n",
            "2025-07-10 14:09:28,904 Epoch 923:\n",
            "\t\t\tTotal Training Recognition Loss 35.697674 || Total Training Translation Loss 0.047609\n",
            "2025-07-10 14:09:29,075 Epoch 924:\n",
            "\t\t\tTotal Training Recognition Loss 16.612890 || Total Training Translation Loss 0.019110\n",
            "2025-07-10 14:09:29,246 Epoch 925:\n",
            "\t\t\tTotal Training Recognition Loss 14.221874 || Total Training Translation Loss 0.023140\n",
            "2025-07-10 14:09:29,422 Epoch 926:\n",
            "\t\t\tTotal Training Recognition Loss 11.606022 || Total Training Translation Loss 0.024209\n",
            "2025-07-10 14:09:29,592 Epoch 927:\n",
            "\t\t\tTotal Training Recognition Loss 17.605837 || Total Training Translation Loss 0.020191\n",
            "2025-07-10 14:09:29,763 Epoch 928:\n",
            "\t\t\tTotal Training Recognition Loss 21.324818 || Total Training Translation Loss 0.039715\n",
            "2025-07-10 14:09:29,933 Epoch 929:\n",
            "\t\t\tTotal Training Recognition Loss 101.452766 || Total Training Translation Loss 0.016342\n",
            "2025-07-10 14:09:30,104 Epoch 930:\n",
            "\t\t\tTotal Training Recognition Loss 8.143274 || Total Training Translation Loss 0.023937\n",
            "2025-07-10 14:09:30,274 Epoch 931:\n",
            "\t\t\tTotal Training Recognition Loss 3.079363 || Total Training Translation Loss 0.014647\n",
            "2025-07-10 14:09:30,445 Epoch 932:\n",
            "\t\t\tTotal Training Recognition Loss 3.517602 || Total Training Translation Loss 0.015683\n",
            "2025-07-10 14:09:30,616 Epoch 933:\n",
            "\t\t\tTotal Training Recognition Loss 2.406245 || Total Training Translation Loss 0.020562\n",
            "2025-07-10 14:09:30,788 Epoch 934:\n",
            "\t\t\tTotal Training Recognition Loss 7.963521 || Total Training Translation Loss 0.017752\n",
            "2025-07-10 14:09:30,958 Epoch 935:\n",
            "\t\t\tTotal Training Recognition Loss 5.122390 || Total Training Translation Loss 0.017480\n",
            "2025-07-10 14:09:31,130 Epoch 936:\n",
            "\t\t\tTotal Training Recognition Loss 2.177730 || Total Training Translation Loss 0.015184\n",
            "2025-07-10 14:09:31,309 Epoch 937:\n",
            "\t\t\tTotal Training Recognition Loss 3.880842 || Total Training Translation Loss 0.011797\n",
            "2025-07-10 14:09:31,480 Epoch 938:\n",
            "\t\t\tTotal Training Recognition Loss 2.023199 || Total Training Translation Loss 0.010603\n",
            "2025-07-10 14:09:31,651 Epoch 939:\n",
            "\t\t\tTotal Training Recognition Loss 3.850218 || Total Training Translation Loss 0.009694\n",
            "2025-07-10 14:09:31,822 Epoch 940:\n",
            "\t\t\tTotal Training Recognition Loss 3.472288 || Total Training Translation Loss 0.012007\n",
            "2025-07-10 14:09:31,992 Epoch 941:\n",
            "\t\t\tTotal Training Recognition Loss 48.251080 || Total Training Translation Loss 0.011798\n",
            "2025-07-10 14:09:32,162 Epoch 942:\n",
            "\t\t\tTotal Training Recognition Loss 2.637667 || Total Training Translation Loss 0.008827\n",
            "2025-07-10 14:09:32,332 Epoch 943:\n",
            "\t\t\tTotal Training Recognition Loss 5.392570 || Total Training Translation Loss 0.008242\n",
            "2025-07-10 14:09:32,504 Epoch 944:\n",
            "\t\t\tTotal Training Recognition Loss 7.764322 || Total Training Translation Loss 0.009283\n",
            "2025-07-10 14:09:32,676 Epoch 945:\n",
            "\t\t\tTotal Training Recognition Loss 5.507241 || Total Training Translation Loss 0.006168\n",
            "2025-07-10 14:09:32,849 Epoch 946:\n",
            "\t\t\tTotal Training Recognition Loss 17.930496 || Total Training Translation Loss 0.006566\n",
            "2025-07-10 14:09:33,021 Epoch 947:\n",
            "\t\t\tTotal Training Recognition Loss 17.649282 || Total Training Translation Loss 0.006701\n",
            "2025-07-10 14:09:33,193 Epoch 948:\n",
            "\t\t\tTotal Training Recognition Loss 3.090275 || Total Training Translation Loss 0.006690\n",
            "2025-07-10 14:09:33,364 Epoch 949:\n",
            "\t\t\tTotal Training Recognition Loss 17.285769 || Total Training Translation Loss 0.006673\n",
            "2025-07-10 14:09:33,536 Epoch 950:\n",
            "\t\t\tTotal Training Recognition Loss 3.965285 || Total Training Translation Loss 0.006203\n",
            "2025-07-10 14:09:33,707 Epoch 951:\n",
            "\t\t\tTotal Training Recognition Loss 2.859327 || Total Training Translation Loss 0.006369\n",
            "2025-07-10 14:09:33,884 Epoch 952:\n",
            "\t\t\tTotal Training Recognition Loss 6.074327 || Total Training Translation Loss 0.006371\n",
            "2025-07-10 14:09:34,058 Epoch 953:\n",
            "\t\t\tTotal Training Recognition Loss 4.336389 || Total Training Translation Loss 0.007088\n",
            "2025-07-10 14:09:34,241 Epoch 954:\n",
            "\t\t\tTotal Training Recognition Loss 8.559653 || Total Training Translation Loss 0.009027\n",
            "2025-07-10 14:09:34,414 Epoch 955:\n",
            "\t\t\tTotal Training Recognition Loss 21.178255 || Total Training Translation Loss 0.005717\n",
            "2025-07-10 14:09:34,588 Epoch 956:\n",
            "\t\t\tTotal Training Recognition Loss 18.745644 || Total Training Translation Loss 0.005468\n",
            "2025-07-10 14:09:34,762 Epoch 957:\n",
            "\t\t\tTotal Training Recognition Loss 21.143976 || Total Training Translation Loss 0.006223\n",
            "2025-07-10 14:09:34,933 Epoch 958:\n",
            "\t\t\tTotal Training Recognition Loss 33.235703 || Total Training Translation Loss 0.006510\n",
            "2025-07-10 14:09:35,105 Epoch 959:\n",
            "\t\t\tTotal Training Recognition Loss 9.265198 || Total Training Translation Loss 0.005616\n",
            "2025-07-10 14:09:35,275 Epoch 960:\n",
            "\t\t\tTotal Training Recognition Loss 7.971948 || Total Training Translation Loss 0.004384\n",
            "2025-07-10 14:09:35,445 Epoch 961:\n",
            "\t\t\tTotal Training Recognition Loss 13.838129 || Total Training Translation Loss 0.004083\n",
            "2025-07-10 14:09:35,616 Epoch 962:\n",
            "\t\t\tTotal Training Recognition Loss 9.719553 || Total Training Translation Loss 0.005140\n",
            "2025-07-10 14:09:35,787 Epoch 963:\n",
            "\t\t\tTotal Training Recognition Loss 9.646212 || Total Training Translation Loss 0.004461\n",
            "2025-07-10 14:09:35,958 Epoch 964:\n",
            "\t\t\tTotal Training Recognition Loss 4.374810 || Total Training Translation Loss 0.003920\n",
            "2025-07-10 14:09:36,130 Epoch 965:\n",
            "\t\t\tTotal Training Recognition Loss 24.846096 || Total Training Translation Loss 0.004118\n",
            "2025-07-10 14:09:36,303 Epoch 966:\n",
            "\t\t\tTotal Training Recognition Loss 5.176950 || Total Training Translation Loss 0.005051\n",
            "2025-07-10 14:09:36,474 Epoch 967:\n",
            "\t\t\tTotal Training Recognition Loss 1.630285 || Total Training Translation Loss 0.003838\n",
            "2025-07-10 14:09:36,651 Epoch 968:\n",
            "\t\t\tTotal Training Recognition Loss 6.294447 || Total Training Translation Loss 0.004767\n",
            "2025-07-10 14:09:36,823 Epoch 969:\n",
            "\t\t\tTotal Training Recognition Loss 2.393557 || Total Training Translation Loss 0.005058\n",
            "2025-07-10 14:09:36,994 Epoch 970:\n",
            "\t\t\tTotal Training Recognition Loss 1.111766 || Total Training Translation Loss 0.004600\n",
            "2025-07-10 14:09:37,194 Epoch 971:\n",
            "\t\t\tTotal Training Recognition Loss 1.729303 || Total Training Translation Loss 0.003771\n",
            "2025-07-10 14:09:37,414 Epoch 972:\n",
            "\t\t\tTotal Training Recognition Loss 18.140182 || Total Training Translation Loss 0.003542\n",
            "2025-07-10 14:09:37,636 Epoch 973:\n",
            "\t\t\tTotal Training Recognition Loss 1.987994 || Total Training Translation Loss 0.003862\n",
            "2025-07-10 14:09:37,855 Epoch 974:\n",
            "\t\t\tTotal Training Recognition Loss 1.734683 || Total Training Translation Loss 0.003367\n",
            "2025-07-10 14:09:38,030 Epoch 975:\n",
            "\t\t\tTotal Training Recognition Loss 11.476697 || Total Training Translation Loss 0.003360\n",
            "2025-07-10 14:09:38,203 Epoch 976:\n",
            "\t\t\tTotal Training Recognition Loss 6.517409 || Total Training Translation Loss 0.005157\n",
            "2025-07-10 14:09:38,376 Epoch 977:\n",
            "\t\t\tTotal Training Recognition Loss 1.991261 || Total Training Translation Loss 0.003622\n",
            "2025-07-10 14:09:38,546 Epoch 978:\n",
            "\t\t\tTotal Training Recognition Loss 5.201770 || Total Training Translation Loss 0.005287\n",
            "2025-07-10 14:09:38,749 Epoch 979:\n",
            "\t\t\tTotal Training Recognition Loss 1.020689 || Total Training Translation Loss 0.004144\n",
            "2025-07-10 14:09:38,931 Epoch 980:\n",
            "\t\t\tTotal Training Recognition Loss 0.921453 || Total Training Translation Loss 0.004167\n",
            "2025-07-10 14:09:39,109 Epoch 981:\n",
            "\t\t\tTotal Training Recognition Loss 1.061079 || Total Training Translation Loss 0.003934\n",
            "2025-07-10 14:09:39,288 Epoch 982:\n",
            "\t\t\tTotal Training Recognition Loss 2.954298 || Total Training Translation Loss 0.003485\n",
            "2025-07-10 14:09:39,467 Epoch 983:\n",
            "\t\t\tTotal Training Recognition Loss 0.983708 || Total Training Translation Loss 0.003191\n",
            "2025-07-10 14:09:39,646 Epoch 984:\n",
            "\t\t\tTotal Training Recognition Loss 3.152793 || Total Training Translation Loss 0.003127\n",
            "2025-07-10 14:09:39,829 Epoch 985:\n",
            "\t\t\tTotal Training Recognition Loss 2.567849 || Total Training Translation Loss 0.003302\n",
            "2025-07-10 14:09:40,003 Epoch 986:\n",
            "\t\t\tTotal Training Recognition Loss 5.070701 || Total Training Translation Loss 0.003377\n",
            "2025-07-10 14:09:40,178 Epoch 987:\n",
            "\t\t\tTotal Training Recognition Loss 1.288710 || Total Training Translation Loss 0.004968\n",
            "2025-07-10 14:09:40,352 Epoch 988:\n",
            "\t\t\tTotal Training Recognition Loss 1.522231 || Total Training Translation Loss 0.004809\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-07-10 14:09:40,529 Epoch 989:\n",
            "\t\t\tTotal Training Recognition Loss 1.910345 || Total Training Translation Loss 0.003401\n",
            "2025-07-10 14:09:40,703 Epoch 990:\n",
            "\t\t\tTotal Training Recognition Loss 5.519510 || Total Training Translation Loss 0.002913\n",
            "2025-07-10 14:09:40,874 Epoch 991:\n",
            "\t\t\tTotal Training Recognition Loss 1.311464 || Total Training Translation Loss 0.003093\n",
            "2025-07-10 14:09:41,045 Epoch 992:\n",
            "\t\t\tTotal Training Recognition Loss 0.614817 || Total Training Translation Loss 0.003199\n",
            "2025-07-10 14:09:41,215 Epoch 993:\n",
            "\t\t\tTotal Training Recognition Loss 1.804067 || Total Training Translation Loss 0.002636\n",
            "2025-07-10 14:09:41,390 Epoch 994:\n",
            "\t\t\tTotal Training Recognition Loss 0.602722 || Total Training Translation Loss 0.003192\n",
            "2025-07-10 14:09:41,580 Epoch 995:\n",
            "\t\t\tTotal Training Recognition Loss 0.644181 || Total Training Translation Loss 0.003556\n",
            "2025-07-10 14:09:41,760 Epoch 996:\n",
            "\t\t\tTotal Training Recognition Loss 0.463793 || Total Training Translation Loss 0.003543\n",
            "2025-07-10 14:09:41,935 Epoch 997:\n",
            "\t\t\tTotal Training Recognition Loss 0.575187 || Total Training Translation Loss 0.002979\n",
            "2025-07-10 14:09:42,110 Epoch 998:\n",
            "\t\t\tTotal Training Recognition Loss 0.764137 || Total Training Translation Loss 0.002950\n",
            "2025-07-10 14:09:42,330 Epoch 999:\n",
            "\t\t\tTotal Training Recognition Loss 1.205422 || Total Training Translation Loss 0.004123\n",
            "2025-07-10 14:09:42,547 [Epoch: 1000 Step: 00001000] Batch Recognition Loss:   1.014391 => Gls Tokens per Sec:      172 || Batch Translation Loss:   0.002892 => Txt Tokens per Sec:      466 || Lr: 0.001000\n",
            "2025-07-10 14:09:42,849 Validation result at epoch 1000, step     1000: duration: 0.3008s\n",
            "\tRecognition Beam Size: 1\tTranslation Beam Size: 1\tTranslation Beam Alpha: -1\n",
            "\tRecognition Loss: 8601.85547\tTranslation Loss: 230.63806\tPPL: 15.08029\n",
            "\tEval Metric: BLEU\n",
            "\tWER 97.14\t(DEL: 45.71,\tINS: 5.71,\tSUB: 45.71)\n",
            "\tBLEU-4 2.05\t(BLEU-1: 8.00,\tBLEU-2: 4.07,\tBLEU-3: 2.87,\tBLEU-4: 2.05)\n",
            "\tCHRF 23.39\tROUGE 11.03\tFID 0.00\n",
            "2025-07-10 14:09:42,849 Logging Recognition and Translation Outputs\n",
            "2025-07-10 14:09:42,850 ========================================================================================\n",
            "2025-07-10 14:09:42,850 Logging Sequence: 11August_2010_Wednesday_tagesschau-2\n",
            "2025-07-10 14:09:42,850 \tGloss Reference :\tDRUCK TIEF   KOMMEN  \n",
            "2025-07-10 14:09:42,851 \tGloss Hypothesis:\t***** ZWOELF NORDWEST\n",
            "2025-07-10 14:09:42,851 \tGloss Alignment :\tD     S      S       \n",
            "2025-07-10 14:09:42,852 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:09:42,855 \tText Reference  :\t** *** **** ** ********** *** **** ******** ******* tiefer luftdruck bestimmt in ******* ******** ****** ********* *********** ***** ***** ***** ***** ***** ***** ***** den   nächsten tagen unser wetter\n",
            "2025-07-10 14:09:42,855 \tText Hypothesis :\tam tag gibt es verbreitet zum teil kräftige schauer oder   gewitter  und      in manchen regionen fallen ergiebige regenmengen <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>    <unk> <unk> <unk> \n",
            "2025-07-10 14:09:42,855 \tText Alignment  :\tI  I   I    I  I          I   I    I        I       S      S         S           I       I        I      I         I           I     I     I     I     I     I     I     S     S        S     S     S     \n",
            "2025-07-10 14:09:42,855 ========================================================================================\n",
            "2025-07-10 14:09:42,856 Logging Sequence: 11August_2010_Wednesday_tagesschau-3\n",
            "2025-07-10 14:09:42,856 \tGloss Reference :\tES-BEDEUTET VIEL WOLKE UND KOENNEN REGEN GEWITTER KOENNEN\n",
            "2025-07-10 14:09:42,856 \tGloss Hypothesis:\t*********** **** ***** *** ******* ***** HEUTE    KOENNEN\n",
            "2025-07-10 14:09:42,857 \tGloss Alignment :\tD           D    D     D   D       D     S               \n",
            "2025-07-10 14:09:42,857 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:09:42,861 \tText Reference  :\tdas bedeutet viele wolken und  immer wieder     zum teil kräftige schauer **** ******** und ** ******* ******** ****** ********* *********** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** gewitter\n",
            "2025-07-10 14:09:42,862 \tText Hypothesis :\t*** ******** am    tag    gibt es    verbreitet zum teil kräftige schauer oder gewitter und in manchen regionen fallen ergiebige regenmengen <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>   \n",
            "2025-07-10 14:09:42,862 \tText Alignment  :\tD   D        S     S      S    S     S                                    I    I            I  I       I        I      I         I           I     I     I     I     I     I     I     I     I     I     I     S       \n",
            "2025-07-10 14:09:42,862 ========================================================================================\n",
            "2025-07-10 14:09:42,862 Logging Sequence: 11August_2010_Wednesday_tagesschau-8\n",
            "2025-07-10 14:09:42,863 \tGloss Reference :\tWIND MAESSIG SCHWACH REGION   WENN    GEWITTER WIND     KOENNEN \n",
            "2025-07-10 14:09:42,863 \tGloss Hypothesis:\t**** ORT     TROCKEN NORDWEST BLEIBEN NORDWEST SUEDWEST NORDWEST\n",
            "2025-07-10 14:09:42,864 \tGloss Alignment :\tD    S       S       S        S       S        S        S       \n",
            "2025-07-10 14:09:42,864 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:09:42,869 \tText Reference  :\t** **** ****** *** ******** **** ****** **************** **** ****** ******* ***** meist weht  nur   ein   schwacher wind  aus   unterschiedlichen richtungen der   bei   schauern und   gewittern stark böig  sein  kann \n",
            "2025-07-10 14:09:42,869 \tText Hypothesis :\tes auch länger und ergiebig auch lokale überschwemmungen sind wieder möglich <unk> <unk> <unk> <unk> <unk> <unk>     <unk> <unk> <unk>             <unk>      <unk> <unk> <unk>    <unk> <unk>     <unk> <unk> <unk> <unk>\n",
            "2025-07-10 14:09:42,870 \tText Alignment  :\tI  I    I      I   I        I    I      I                I    I      I       I     S     S     S     S     S         S     S     S                 S          S     S     S        S     S         S     S     S     S    \n",
            "2025-07-10 14:09:42,870 ========================================================================================\n",
            "2025-07-10 14:09:42,870 Logging Sequence: 25October_2010_Monday_tagesschau-22\n",
            "2025-07-10 14:09:42,871 \tGloss Reference :\tMITTWOCH REGEN KOENNEN NORDWEST WAHRSCHEINLICH NORD    STARK    WIND \n",
            "2025-07-10 14:09:42,871 \tGloss Hypothesis:\t******** ***** ******* ******** JETZT          FEBRUAR GEWITTER JETZT\n",
            "2025-07-10 14:09:42,871 \tGloss Alignment :\tD        D     D       D        S              S       S        S    \n",
            "2025-07-10 14:09:42,871 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:09:42,876 \tText Reference  :\tam *** **** ** ********** *** **** ******** ******* mittwoch hier     und ** ******* ******** ****** ********* *********** ***** ***** da    nieselregen in    der   nordwesthälfte an    den   küsten kräftiger wind \n",
            "2025-07-10 14:09:42,876 \tText Hypothesis :\tam tag gibt es verbreitet zum teil kräftige schauer oder     gewitter und in manchen regionen fallen ergiebige regenmengen <unk> <unk> <unk> <unk>       <unk> <unk> <unk>          <unk> <unk> <unk>  <unk>     <unk>\n",
            "2025-07-10 14:09:42,876 \tText Alignment  :\t   I   I    I  I          I   I    I        I       S        S            I  I       I        I      I         I           I     I     S     S           S     S     S              S     S     S      S         S    \n",
            "2025-07-10 14:09:42,876 ========================================================================================\n",
            "2025-07-10 14:09:42,876 Logging Sequence: 05May_2011_Thursday_tagesschau-25\n",
            "2025-07-10 14:09:42,877 \tGloss Reference :\t****** JETZT ****** WETTER WIE-AUSSEHEN MORGEN FREITAG SECHSTE MAI        ZEIGEN-BILDSCHIRM\n",
            "2025-07-10 14:09:42,877 \tGloss Hypothesis:\tZWOELF JETZT ZWOELF WETTER ************ ****** ******* ******* DONNERSTAG ZWOELF           \n",
            "2025-07-10 14:09:42,877 \tGloss Alignment :\tI            I             D            D      D       D       S          S                \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-07-10 14:09:42,877 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:09:42,878 \tText Reference  :\t** **** ****** und ******** **** ****** **************** **** ****** ******* ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** nun   die   wettervorhersage für   morgen freitag den   sechsten mai  \n",
            "2025-07-10 14:09:42,879 \tText Hypothesis :\tes auch länger und ergiebig auch lokale überschwemmungen sind wieder möglich <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>            <unk> <unk>  <unk>   <unk> <unk>    <unk>\n",
            "2025-07-10 14:09:42,879 \tText Alignment  :\tI  I    I          I        I    I      I                I    I      I       I     I     I     I     I     I     I     I     I     I     S     S     S                S     S      S       S     S        S    \n",
            "2025-07-10 14:09:42,879 ========================================================================================\n",
            "2025-07-10 14:09:42,880 Epoch 1000:\n",
            "\t\t\tTotal Training Recognition Loss 1.014391 || Total Training Translation Loss 0.002892\n",
            "2025-07-10 14:09:43,093 Epoch 1001:\n",
            "\t\t\tTotal Training Recognition Loss 60.589005 || Total Training Translation Loss 0.003707\n",
            "2025-07-10 14:09:43,309 Epoch 1002:\n",
            "\t\t\tTotal Training Recognition Loss 1.074234 || Total Training Translation Loss 0.003000\n",
            "2025-07-10 14:09:43,524 Epoch 1003:\n",
            "\t\t\tTotal Training Recognition Loss 1.236604 || Total Training Translation Loss 0.003287\n",
            "2025-07-10 14:09:43,739 Epoch 1004:\n",
            "\t\t\tTotal Training Recognition Loss 1.153844 || Total Training Translation Loss 0.002947\n",
            "2025-07-10 14:09:43,957 Epoch 1005:\n",
            "\t\t\tTotal Training Recognition Loss 0.451546 || Total Training Translation Loss 0.003036\n",
            "2025-07-10 14:09:44,172 Epoch 1006:\n",
            "\t\t\tTotal Training Recognition Loss 1.294895 || Total Training Translation Loss 0.003304\n",
            "2025-07-10 14:09:44,390 Epoch 1007:\n",
            "\t\t\tTotal Training Recognition Loss 8.670630 || Total Training Translation Loss 0.002801\n",
            "2025-07-10 14:09:44,607 Epoch 1008:\n",
            "\t\t\tTotal Training Recognition Loss 1.306663 || Total Training Translation Loss 0.003050\n",
            "2025-07-10 14:09:44,824 Epoch 1009:\n",
            "\t\t\tTotal Training Recognition Loss 0.884502 || Total Training Translation Loss 0.002686\n",
            "2025-07-10 14:09:45,040 Epoch 1010:\n",
            "\t\t\tTotal Training Recognition Loss 0.798112 || Total Training Translation Loss 0.003472\n",
            "2025-07-10 14:09:45,258 Epoch 1011:\n",
            "\t\t\tTotal Training Recognition Loss 3.393490 || Total Training Translation Loss 0.003323\n",
            "2025-07-10 14:09:45,475 Epoch 1012:\n",
            "\t\t\tTotal Training Recognition Loss 1.721589 || Total Training Translation Loss 0.002677\n",
            "2025-07-10 14:09:45,653 Epoch 1013:\n",
            "\t\t\tTotal Training Recognition Loss 2.274004 || Total Training Translation Loss 0.003392\n",
            "2025-07-10 14:09:45,825 Epoch 1014:\n",
            "\t\t\tTotal Training Recognition Loss 31.798351 || Total Training Translation Loss 0.003188\n",
            "2025-07-10 14:09:45,996 Epoch 1015:\n",
            "\t\t\tTotal Training Recognition Loss 2.140562 || Total Training Translation Loss 0.006865\n",
            "2025-07-10 14:09:46,169 Epoch 1016:\n",
            "\t\t\tTotal Training Recognition Loss 13.612848 || Total Training Translation Loss 0.004422\n",
            "2025-07-10 14:09:46,342 Epoch 1017:\n",
            "\t\t\tTotal Training Recognition Loss 0.895343 || Total Training Translation Loss 0.003661\n",
            "2025-07-10 14:09:46,515 Epoch 1018:\n",
            "\t\t\tTotal Training Recognition Loss 1.174085 || Total Training Translation Loss 0.002798\n",
            "2025-07-10 14:09:46,688 Epoch 1019:\n",
            "\t\t\tTotal Training Recognition Loss 1.706562 || Total Training Translation Loss 0.003138\n",
            "2025-07-10 14:09:46,860 Epoch 1020:\n",
            "\t\t\tTotal Training Recognition Loss 0.758947 || Total Training Translation Loss 0.003385\n",
            "2025-07-10 14:09:47,034 Epoch 1021:\n",
            "\t\t\tTotal Training Recognition Loss 3.735655 || Total Training Translation Loss 0.003546\n",
            "2025-07-10 14:09:47,207 Epoch 1022:\n",
            "\t\t\tTotal Training Recognition Loss 2.224734 || Total Training Translation Loss 0.004600\n",
            "2025-07-10 14:09:47,380 Epoch 1023:\n",
            "\t\t\tTotal Training Recognition Loss 3.329223 || Total Training Translation Loss 0.003047\n",
            "2025-07-10 14:09:47,553 Epoch 1024:\n",
            "\t\t\tTotal Training Recognition Loss 1.091762 || Total Training Translation Loss 0.002648\n",
            "2025-07-10 14:09:47,727 Epoch 1025:\n",
            "\t\t\tTotal Training Recognition Loss 1.134610 || Total Training Translation Loss 0.003276\n",
            "2025-07-10 14:09:47,900 Epoch 1026:\n",
            "\t\t\tTotal Training Recognition Loss 0.813822 || Total Training Translation Loss 0.002934\n",
            "2025-07-10 14:09:48,073 Epoch 1027:\n",
            "\t\t\tTotal Training Recognition Loss 1.047166 || Total Training Translation Loss 0.003195\n",
            "2025-07-10 14:09:48,247 Epoch 1028:\n",
            "\t\t\tTotal Training Recognition Loss 0.979910 || Total Training Translation Loss 0.002625\n",
            "2025-07-10 14:09:48,420 Epoch 1029:\n",
            "\t\t\tTotal Training Recognition Loss 1.573277 || Total Training Translation Loss 0.002567\n",
            "2025-07-10 14:09:48,593 Epoch 1030:\n",
            "\t\t\tTotal Training Recognition Loss 8.372995 || Total Training Translation Loss 0.002959\n",
            "2025-07-10 14:09:48,766 Epoch 1031:\n",
            "\t\t\tTotal Training Recognition Loss 23.413033 || Total Training Translation Loss 0.003034\n",
            "2025-07-10 14:09:48,938 Epoch 1032:\n",
            "\t\t\tTotal Training Recognition Loss 85.624100 || Total Training Translation Loss 0.003412\n",
            "2025-07-10 14:09:49,111 Epoch 1033:\n",
            "\t\t\tTotal Training Recognition Loss 0.736874 || Total Training Translation Loss 0.002867\n",
            "2025-07-10 14:09:49,283 Epoch 1034:\n",
            "\t\t\tTotal Training Recognition Loss 36.722469 || Total Training Translation Loss 0.003000\n",
            "2025-07-10 14:09:49,456 Epoch 1035:\n",
            "\t\t\tTotal Training Recognition Loss 1.792404 || Total Training Translation Loss 0.002835\n",
            "2025-07-10 14:09:49,628 Epoch 1036:\n",
            "\t\t\tTotal Training Recognition Loss 1.119885 || Total Training Translation Loss 0.003580\n",
            "2025-07-10 14:09:49,801 Epoch 1037:\n",
            "\t\t\tTotal Training Recognition Loss 0.515668 || Total Training Translation Loss 0.004067\n",
            "2025-07-10 14:09:49,974 Epoch 1038:\n",
            "\t\t\tTotal Training Recognition Loss 1.492049 || Total Training Translation Loss 0.003841\n",
            "2025-07-10 14:09:50,148 Epoch 1039:\n",
            "\t\t\tTotal Training Recognition Loss 1.321465 || Total Training Translation Loss 0.004370\n",
            "2025-07-10 14:09:50,320 Epoch 1040:\n",
            "\t\t\tTotal Training Recognition Loss 5.874410 || Total Training Translation Loss 0.004731\n",
            "2025-07-10 14:09:50,493 Epoch 1041:\n",
            "\t\t\tTotal Training Recognition Loss 3.233646 || Total Training Translation Loss 0.004504\n",
            "2025-07-10 14:09:50,665 Epoch 1042:\n",
            "\t\t\tTotal Training Recognition Loss 4.656203 || Total Training Translation Loss 0.003640\n",
            "2025-07-10 14:09:50,839 Epoch 1043:\n",
            "\t\t\tTotal Training Recognition Loss 1.909108 || Total Training Translation Loss 0.004354\n",
            "2025-07-10 14:09:51,012 Epoch 1044:\n",
            "\t\t\tTotal Training Recognition Loss 18.393908 || Total Training Translation Loss 0.003786\n",
            "2025-07-10 14:09:51,184 Epoch 1045:\n",
            "\t\t\tTotal Training Recognition Loss 0.647744 || Total Training Translation Loss 0.003230\n",
            "2025-07-10 14:09:51,356 Epoch 1046:\n",
            "\t\t\tTotal Training Recognition Loss 120.919182 || Total Training Translation Loss 0.004590\n",
            "2025-07-10 14:09:51,568 Epoch 1047:\n",
            "\t\t\tTotal Training Recognition Loss 463.999329 || Total Training Translation Loss 0.004837\n",
            "2025-07-10 14:09:51,785 Epoch 1048:\n",
            "\t\t\tTotal Training Recognition Loss 66.377052 || Total Training Translation Loss 0.008324\n",
            "2025-07-10 14:09:52,001 Epoch 1049:\n",
            "\t\t\tTotal Training Recognition Loss 265.129517 || Total Training Translation Loss 0.004012\n",
            "2025-07-10 14:09:52,218 Epoch 1050:\n",
            "\t\t\tTotal Training Recognition Loss 360.633423 || Total Training Translation Loss 0.004064\n",
            "2025-07-10 14:09:52,436 Epoch 1051:\n",
            "\t\t\tTotal Training Recognition Loss 325.070099 || Total Training Translation Loss 0.005750\n",
            "2025-07-10 14:09:52,653 Epoch 1052:\n",
            "\t\t\tTotal Training Recognition Loss 295.501312 || Total Training Translation Loss 0.006244\n",
            "2025-07-10 14:09:52,874 Epoch 1053:\n",
            "\t\t\tTotal Training Recognition Loss 268.146698 || Total Training Translation Loss 0.006438\n",
            "2025-07-10 14:09:53,056 Epoch 1054:\n",
            "\t\t\tTotal Training Recognition Loss 252.071442 || Total Training Translation Loss 0.005829\n",
            "2025-07-10 14:09:53,228 Epoch 1055:\n",
            "\t\t\tTotal Training Recognition Loss 329.363922 || Total Training Translation Loss 0.008803\n",
            "2025-07-10 14:09:53,402 Epoch 1056:\n",
            "\t\t\tTotal Training Recognition Loss 240.920944 || Total Training Translation Loss 0.006381\n",
            "2025-07-10 14:09:53,575 Epoch 1057:\n",
            "\t\t\tTotal Training Recognition Loss 196.517197 || Total Training Translation Loss 0.006622\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-07-10 14:09:53,748 Epoch 1058:\n",
            "\t\t\tTotal Training Recognition Loss 198.506409 || Total Training Translation Loss 0.006461\n",
            "2025-07-10 14:09:53,921 Epoch 1059:\n",
            "\t\t\tTotal Training Recognition Loss 176.776810 || Total Training Translation Loss 0.005675\n",
            "2025-07-10 14:09:54,093 Epoch 1060:\n",
            "\t\t\tTotal Training Recognition Loss 170.642883 || Total Training Translation Loss 0.006610\n",
            "2025-07-10 14:09:54,266 Epoch 1061:\n",
            "\t\t\tTotal Training Recognition Loss 82.309097 || Total Training Translation Loss 0.007339\n",
            "2025-07-10 14:09:54,484 Epoch 1062:\n",
            "\t\t\tTotal Training Recognition Loss 57.545116 || Total Training Translation Loss 0.009398\n",
            "2025-07-10 14:09:54,699 Epoch 1063:\n",
            "\t\t\tTotal Training Recognition Loss 61.877323 || Total Training Translation Loss 0.008118\n",
            "2025-07-10 14:09:54,915 Epoch 1064:\n",
            "\t\t\tTotal Training Recognition Loss 152.731186 || Total Training Translation Loss 0.008329\n",
            "2025-07-10 14:09:55,130 Epoch 1065:\n",
            "\t\t\tTotal Training Recognition Loss 52.642178 || Total Training Translation Loss 0.007226\n",
            "2025-07-10 14:09:55,348 Epoch 1066:\n",
            "\t\t\tTotal Training Recognition Loss 38.471317 || Total Training Translation Loss 0.006088\n",
            "2025-07-10 14:09:55,565 Epoch 1067:\n",
            "\t\t\tTotal Training Recognition Loss 38.652966 || Total Training Translation Loss 0.007941\n",
            "2025-07-10 14:09:55,780 Epoch 1068:\n",
            "\t\t\tTotal Training Recognition Loss 52.503685 || Total Training Translation Loss 0.006534\n",
            "2025-07-10 14:09:55,996 Epoch 1069:\n",
            "\t\t\tTotal Training Recognition Loss 76.846893 || Total Training Translation Loss 0.006403\n",
            "2025-07-10 14:09:56,213 Epoch 1070:\n",
            "\t\t\tTotal Training Recognition Loss 48.534405 || Total Training Translation Loss 0.004525\n",
            "2025-07-10 14:09:56,429 Epoch 1071:\n",
            "\t\t\tTotal Training Recognition Loss 55.121712 || Total Training Translation Loss 0.005850\n",
            "2025-07-10 14:09:56,642 Epoch 1072:\n",
            "\t\t\tTotal Training Recognition Loss 37.422787 || Total Training Translation Loss 0.004992\n",
            "2025-07-10 14:09:56,859 Epoch 1073:\n",
            "\t\t\tTotal Training Recognition Loss 54.650227 || Total Training Translation Loss 0.004610\n",
            "2025-07-10 14:09:57,077 Epoch 1074:\n",
            "\t\t\tTotal Training Recognition Loss 25.455673 || Total Training Translation Loss 0.006523\n",
            "2025-07-10 14:09:57,293 Epoch 1075:\n",
            "\t\t\tTotal Training Recognition Loss 41.040478 || Total Training Translation Loss 0.003980\n",
            "2025-07-10 14:09:57,508 Epoch 1076:\n",
            "\t\t\tTotal Training Recognition Loss 57.355560 || Total Training Translation Loss 0.005471\n",
            "2025-07-10 14:09:57,724 Epoch 1077:\n",
            "\t\t\tTotal Training Recognition Loss 30.932730 || Total Training Translation Loss 0.004088\n",
            "2025-07-10 14:09:57,939 Epoch 1078:\n",
            "\t\t\tTotal Training Recognition Loss 28.296490 || Total Training Translation Loss 0.005419\n",
            "2025-07-10 14:09:58,156 Epoch 1079:\n",
            "\t\t\tTotal Training Recognition Loss 28.293407 || Total Training Translation Loss 0.004370\n",
            "2025-07-10 14:09:58,331 Epoch 1080:\n",
            "\t\t\tTotal Training Recognition Loss 22.671911 || Total Training Translation Loss 0.004349\n",
            "2025-07-10 14:09:58,504 Epoch 1081:\n",
            "\t\t\tTotal Training Recognition Loss 25.951342 || Total Training Translation Loss 0.003999\n",
            "2025-07-10 14:09:58,678 Epoch 1082:\n",
            "\t\t\tTotal Training Recognition Loss 42.066769 || Total Training Translation Loss 0.009533\n",
            "2025-07-10 14:09:58,851 Epoch 1083:\n",
            "\t\t\tTotal Training Recognition Loss 32.939545 || Total Training Translation Loss 0.004266\n",
            "2025-07-10 14:09:59,024 Epoch 1084:\n",
            "\t\t\tTotal Training Recognition Loss 16.162832 || Total Training Translation Loss 0.003638\n",
            "2025-07-10 14:09:59,198 Epoch 1085:\n",
            "\t\t\tTotal Training Recognition Loss 31.510227 || Total Training Translation Loss 0.003723\n",
            "2025-07-10 14:09:59,372 Epoch 1086:\n",
            "\t\t\tTotal Training Recognition Loss 13.545364 || Total Training Translation Loss 0.003501\n",
            "2025-07-10 14:09:59,545 Epoch 1087:\n",
            "\t\t\tTotal Training Recognition Loss 40.498466 || Total Training Translation Loss 0.004249\n",
            "2025-07-10 14:09:59,718 Epoch 1088:\n",
            "\t\t\tTotal Training Recognition Loss 9.215263 || Total Training Translation Loss 0.003849\n",
            "2025-07-10 14:09:59,892 Epoch 1089:\n",
            "\t\t\tTotal Training Recognition Loss 6.026457 || Total Training Translation Loss 0.003645\n",
            "2025-07-10 14:10:00,065 Epoch 1090:\n",
            "\t\t\tTotal Training Recognition Loss 14.589427 || Total Training Translation Loss 0.003427\n",
            "2025-07-10 14:10:00,239 Epoch 1091:\n",
            "\t\t\tTotal Training Recognition Loss 13.065748 || Total Training Translation Loss 0.003805\n",
            "2025-07-10 14:10:00,413 Epoch 1092:\n",
            "\t\t\tTotal Training Recognition Loss 11.757698 || Total Training Translation Loss 0.010438\n",
            "2025-07-10 14:10:00,590 Epoch 1093:\n",
            "\t\t\tTotal Training Recognition Loss 8.836130 || Total Training Translation Loss 0.006673\n",
            "2025-07-10 14:10:00,765 Epoch 1094:\n",
            "\t\t\tTotal Training Recognition Loss 6.039555 || Total Training Translation Loss 0.004531\n",
            "2025-07-10 14:10:00,939 Epoch 1095:\n",
            "\t\t\tTotal Training Recognition Loss 37.554359 || Total Training Translation Loss 0.003837\n",
            "2025-07-10 14:10:01,113 Epoch 1096:\n",
            "\t\t\tTotal Training Recognition Loss 4.679376 || Total Training Translation Loss 0.003673\n",
            "2025-07-10 14:10:01,286 Epoch 1097:\n",
            "\t\t\tTotal Training Recognition Loss 7.721550 || Total Training Translation Loss 0.003781\n",
            "2025-07-10 14:10:01,460 Epoch 1098:\n",
            "\t\t\tTotal Training Recognition Loss 4.658004 || Total Training Translation Loss 0.004489\n",
            "2025-07-10 14:10:01,636 Epoch 1099:\n",
            "\t\t\tTotal Training Recognition Loss 39.284973 || Total Training Translation Loss 0.004109\n",
            "2025-07-10 14:10:01,810 [Epoch: 1100 Step: 00001100] Batch Recognition Loss:   7.817578 => Gls Tokens per Sec:      215 || Batch Translation Loss:   0.003662 => Txt Tokens per Sec:      580 || Lr: 0.001000\n",
            "2025-07-10 14:10:02,045 Validation result at epoch 1100, step     1100: duration: 0.2344s\n",
            "\tRecognition Beam Size: 1\tTranslation Beam Size: 1\tTranslation Beam Alpha: -1\n",
            "\tRecognition Loss: 6178.58887\tTranslation Loss: 233.76227\tPPL: 15.64489\n",
            "\tEval Metric: BLEU\n",
            "\tWER 88.57\t(DEL: 34.29,\tINS: 2.86,\tSUB: 51.43)\n",
            "\tBLEU-4 2.00\t(BLEU-1: 7.33,\tBLEU-2: 3.90,\tBLEU-3: 2.79,\tBLEU-4: 2.00)\n",
            "\tCHRF 24.75\tROUGE 9.90\tFID 0.00\n",
            "2025-07-10 14:10:02,045 Logging Recognition and Translation Outputs\n",
            "2025-07-10 14:10:02,046 ========================================================================================\n",
            "2025-07-10 14:10:02,046 Logging Sequence: 11August_2010_Wednesday_tagesschau-2\n",
            "2025-07-10 14:10:02,047 \tGloss Reference :\tDRUCK TIEF     KOMMEN  \n",
            "2025-07-10 14:10:02,047 \tGloss Hypothesis:\t***** NORDWEST SPEZIELL\n",
            "2025-07-10 14:10:02,047 \tGloss Alignment :\tD     S        S       \n",
            "2025-07-10 14:10:02,048 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:10:02,051 \tText Reference  :\t********** ****** ** ***** ***** ***** ******* ***** **** *** ********* ********* ******** ********* ****** ******* *** ***** *** ****** ****** tiefer luftdruck bestimmt in    den   nächsten tagen unser wetter\n",
            "2025-07-10 14:10:02,051 \tText Hypothesis :\tnordwesten bleibt es heute nacht meist trocken sonst muss mit teilweise kräftigen schauern gerechnet werden örtlich mit blitz und donner donner <unk>  <unk>     <unk>    <unk> <unk> <unk>    <unk> <unk> <unk> \n",
            "2025-07-10 14:10:02,051 \tText Alignment  :\tI          I      I  I     I     I     I       I     I    I   I         I         I        I         I      I       I   I     I   I      I      S      S         S        S     S     S        S     S     S     \n",
            "2025-07-10 14:10:02,052 ========================================================================================\n",
            "2025-07-10 14:10:02,052 Logging Sequence: 11August_2010_Wednesday_tagesschau-3\n",
            "2025-07-10 14:10:02,053 \tGloss Reference :\tES-BEDEUTET VIEL WOLKE UND KOENNEN REGEN    GEWITTER KOENNEN\n",
            "2025-07-10 14:10:02,053 \tGloss Hypothesis:\t*********** **** ***** *** ******* NORDWEST ORT      KOENNEN\n",
            "2025-07-10 14:10:02,053 \tGloss Alignment :\tD           D    D     D   D       S        S               \n",
            "2025-07-10 14:10:02,053 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:10:02,058 \tText Reference  :\tdas bedeutet viele wolken und  immer wieder     zum teil kräftige schauer **** ******** und ** ******* ******** ****** ********* *********** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** gewitter\n",
            "2025-07-10 14:10:02,058 \tText Hypothesis :\t*** ******** am    tag    gibt es    verbreitet zum teil kräftige schauer oder gewitter und in manchen regionen fallen ergiebige regenmengen <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>   \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-07-10 14:10:02,058 \tText Alignment  :\tD   D        S     S      S    S     S                                    I    I            I  I       I        I      I         I           I     I     I     I     I     I     I     I     I     I     I     S       \n",
            "2025-07-10 14:10:02,059 ========================================================================================\n",
            "2025-07-10 14:10:02,059 Logging Sequence: 11August_2010_Wednesday_tagesschau-8\n",
            "2025-07-10 14:10:02,060 \tGloss Reference :\tWIND MAESSIG SCHWACH REGION  WENN     GEWITTER WIND     KOENNEN \n",
            "2025-07-10 14:10:02,060 \tGloss Hypothesis:\tORT  LOCH    TROCKEN BLEIBEN SPEZIELL NORDWEST SUEDWEST NORDWEST\n",
            "2025-07-10 14:10:02,061 \tGloss Alignment :\tS    S       S       S       S        S        S        S       \n",
            "2025-07-10 14:10:02,061 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:10:02,066 \tText Reference  :\t****** ** **** ****** *** ******** **** ****** **************** **** ****** ******* meist weht  nur   ein   schwacher wind  aus   unterschiedlichen richtungen der   bei   schauern und   gewittern stark böig  sein  kann \n",
            "2025-07-10 14:10:02,067 \tText Hypothesis :\tregnet es auch länger und ergiebig auch lokale überschwemmungen sind wieder möglich <unk> <unk> <unk> <unk> <unk>     <unk> <unk> <unk>             <unk>      <unk> <unk> <unk>    <unk> <unk>     <unk> <unk> <unk> <unk>\n",
            "2025-07-10 14:10:02,067 \tText Alignment  :\tI      I  I    I      I   I        I    I      I                I    I      I       S     S     S     S     S         S     S     S                 S          S     S     S        S     S         S     S     S     S    \n",
            "2025-07-10 14:10:02,067 ========================================================================================\n",
            "2025-07-10 14:10:02,067 Logging Sequence: 25October_2010_Monday_tagesschau-22\n",
            "2025-07-10 14:10:02,068 \tGloss Reference :\tMITTWOCH REGEN KOENNEN NORDWEST WAHRSCHEINLICH NORD     STARK    WIND    \n",
            "2025-07-10 14:10:02,068 \tGloss Hypothesis:\t******** ***** ORT     NORDWEST SUEDWEST       NORDWEST SUEDWEST GEWITTER\n",
            "2025-07-10 14:10:02,068 \tGloss Alignment :\tD        D     S                S              S        S        S       \n",
            "2025-07-10 14:10:02,069 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:10:02,073 \tText Reference  :\tam *** **** ** ********** *** **** ******** ******* mittwoch hier     und ** ******* ******** ****** ********* *********** ***** ***** da    nieselregen in    der   nordwesthälfte an    den   küsten kräftiger wind \n",
            "2025-07-10 14:10:02,073 \tText Hypothesis :\tam tag gibt es verbreitet zum teil kräftige schauer oder     gewitter und in manchen regionen fallen ergiebige regenmengen <unk> <unk> <unk> <unk>       <unk> <unk> <unk>          <unk> <unk> <unk>  <unk>     <unk>\n",
            "2025-07-10 14:10:02,073 \tText Alignment  :\t   I   I    I  I          I   I    I        I       S        S            I  I       I        I      I         I           I     I     S     S           S     S     S              S     S     S      S         S    \n",
            "2025-07-10 14:10:02,073 ========================================================================================\n",
            "2025-07-10 14:10:02,074 Logging Sequence: 05May_2011_Thursday_tagesschau-25\n",
            "2025-07-10 14:10:02,074 \tGloss Reference :\t****** JETZT WETTER WIE-AUSSEHEN MORGEN FREITAG SECHSTE MAI ZEIGEN-BILDSCHIRM\n",
            "2025-07-10 14:10:02,074 \tGloss Hypothesis:\tZWOELF JETZT WETTER ************ MORGEN ******* ******* *** NORDWEST         \n",
            "2025-07-10 14:10:02,075 \tGloss Alignment :\tI                   D                   D       D       D   S                \n",
            "2025-07-10 14:10:02,075 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:10:02,077 \tText Reference  :\t** *** **** ** ********** *** **** ******** ******* **** ******** und ** ******* ******** ****** ********* *********** ***** ***** ***** nun   die   wettervorhersage für   morgen freitag den   sechsten mai  \n",
            "2025-07-10 14:10:02,077 \tText Hypothesis :\tam tag gibt es verbreitet zum teil kräftige schauer oder gewitter und in manchen regionen fallen ergiebige regenmengen <unk> <unk> <unk> <unk> <unk> <unk>            <unk> <unk>  <unk>   <unk> <unk>    <unk>\n",
            "2025-07-10 14:10:02,077 \tText Alignment  :\tI  I   I    I  I          I   I    I        I       I    I            I  I       I        I      I         I           I     I     I     S     S     S                S     S      S       S     S        S    \n",
            "2025-07-10 14:10:02,077 ========================================================================================\n",
            "2025-07-10 14:10:02,078 Epoch 1100:\n",
            "\t\t\tTotal Training Recognition Loss 7.817578 || Total Training Translation Loss 0.003662\n",
            "2025-07-10 14:10:02,252 Epoch 1101:\n",
            "\t\t\tTotal Training Recognition Loss 17.446600 || Total Training Translation Loss 0.006358\n",
            "2025-07-10 14:10:02,427 Epoch 1102:\n",
            "\t\t\tTotal Training Recognition Loss 6.217752 || Total Training Translation Loss 0.004288\n",
            "2025-07-10 14:10:02,604 Epoch 1103:\n",
            "\t\t\tTotal Training Recognition Loss 4.825593 || Total Training Translation Loss 0.004065\n",
            "2025-07-10 14:10:02,779 Epoch 1104:\n",
            "\t\t\tTotal Training Recognition Loss 11.687543 || Total Training Translation Loss 0.006350\n",
            "2025-07-10 14:10:02,952 Epoch 1105:\n",
            "\t\t\tTotal Training Recognition Loss 8.576488 || Total Training Translation Loss 0.003870\n",
            "2025-07-10 14:10:03,125 Epoch 1106:\n",
            "\t\t\tTotal Training Recognition Loss 3.573873 || Total Training Translation Loss 0.003830\n",
            "2025-07-10 14:10:03,299 Epoch 1107:\n",
            "\t\t\tTotal Training Recognition Loss 7.437507 || Total Training Translation Loss 0.003431\n",
            "2025-07-10 14:10:03,472 Epoch 1108:\n",
            "\t\t\tTotal Training Recognition Loss 5.020717 || Total Training Translation Loss 0.005611\n",
            "2025-07-10 14:10:03,645 Epoch 1109:\n",
            "\t\t\tTotal Training Recognition Loss 4.337987 || Total Training Translation Loss 0.003411\n",
            "2025-07-10 14:10:03,819 Epoch 1110:\n",
            "\t\t\tTotal Training Recognition Loss 5.409615 || Total Training Translation Loss 0.004632\n",
            "2025-07-10 14:10:04,037 Epoch 1111:\n",
            "\t\t\tTotal Training Recognition Loss 7.553554 || Total Training Translation Loss 0.004175\n",
            "2025-07-10 14:10:04,253 Epoch 1112:\n",
            "\t\t\tTotal Training Recognition Loss 2.476523 || Total Training Translation Loss 0.003050\n",
            "2025-07-10 14:10:04,469 Epoch 1113:\n",
            "\t\t\tTotal Training Recognition Loss 3.034964 || Total Training Translation Loss 0.003171\n",
            "2025-07-10 14:10:04,684 Epoch 1114:\n",
            "\t\t\tTotal Training Recognition Loss 10.381618 || Total Training Translation Loss 0.003063\n",
            "2025-07-10 14:10:04,900 Epoch 1115:\n",
            "\t\t\tTotal Training Recognition Loss 2.065933 || Total Training Translation Loss 0.002565\n",
            "2025-07-10 14:10:05,118 Epoch 1116:\n",
            "\t\t\tTotal Training Recognition Loss 5.641184 || Total Training Translation Loss 0.003366\n",
            "2025-07-10 14:10:05,295 Epoch 1117:\n",
            "\t\t\tTotal Training Recognition Loss 4.042194 || Total Training Translation Loss 0.003295\n",
            "2025-07-10 14:10:05,468 Epoch 1118:\n",
            "\t\t\tTotal Training Recognition Loss 0.800625 || Total Training Translation Loss 0.002767\n",
            "2025-07-10 14:10:05,642 Epoch 1119:\n",
            "\t\t\tTotal Training Recognition Loss 0.900219 || Total Training Translation Loss 0.003011\n",
            "2025-07-10 14:10:05,816 Epoch 1120:\n",
            "\t\t\tTotal Training Recognition Loss 0.801860 || Total Training Translation Loss 0.002871\n",
            "2025-07-10 14:10:05,988 Epoch 1121:\n",
            "\t\t\tTotal Training Recognition Loss 14.536904 || Total Training Translation Loss 0.002846\n",
            "2025-07-10 14:10:06,163 Epoch 1122:\n",
            "\t\t\tTotal Training Recognition Loss 1.221429 || Total Training Translation Loss 0.002805\n",
            "2025-07-10 14:10:06,337 Epoch 1123:\n",
            "\t\t\tTotal Training Recognition Loss 0.821484 || Total Training Translation Loss 0.004064\n",
            "2025-07-10 14:10:06,511 Epoch 1124:\n",
            "\t\t\tTotal Training Recognition Loss 25.894772 || Total Training Translation Loss 0.003901\n",
            "2025-07-10 14:10:06,684 Epoch 1125:\n",
            "\t\t\tTotal Training Recognition Loss 2.239977 || Total Training Translation Loss 0.003213\n",
            "2025-07-10 14:10:06,857 Epoch 1126:\n",
            "\t\t\tTotal Training Recognition Loss 1.904404 || Total Training Translation Loss 0.003356\n",
            "2025-07-10 14:10:07,035 Epoch 1127:\n",
            "\t\t\tTotal Training Recognition Loss 5.051507 || Total Training Translation Loss 0.003698\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-07-10 14:10:07,253 Epoch 1128:\n",
            "\t\t\tTotal Training Recognition Loss 3.749665 || Total Training Translation Loss 0.003219\n",
            "2025-07-10 14:10:07,440 Epoch 1129:\n",
            "\t\t\tTotal Training Recognition Loss 5.363411 || Total Training Translation Loss 0.004111\n",
            "2025-07-10 14:10:07,613 Epoch 1130:\n",
            "\t\t\tTotal Training Recognition Loss 4.516915 || Total Training Translation Loss 0.003126\n",
            "2025-07-10 14:10:07,788 Epoch 1131:\n",
            "\t\t\tTotal Training Recognition Loss 6.882771 || Total Training Translation Loss 0.003255\n",
            "2025-07-10 14:10:07,960 Epoch 1132:\n",
            "\t\t\tTotal Training Recognition Loss 2.776995 || Total Training Translation Loss 0.003205\n",
            "2025-07-10 14:10:08,133 Epoch 1133:\n",
            "\t\t\tTotal Training Recognition Loss 11.724346 || Total Training Translation Loss 0.003226\n",
            "2025-07-10 14:10:08,307 Epoch 1134:\n",
            "\t\t\tTotal Training Recognition Loss 5.102599 || Total Training Translation Loss 0.003103\n",
            "2025-07-10 14:10:08,480 Epoch 1135:\n",
            "\t\t\tTotal Training Recognition Loss 28.976112 || Total Training Translation Loss 0.002878\n",
            "2025-07-10 14:10:08,653 Epoch 1136:\n",
            "\t\t\tTotal Training Recognition Loss 5.351663 || Total Training Translation Loss 0.002149\n",
            "2025-07-10 14:10:08,826 Epoch 1137:\n",
            "\t\t\tTotal Training Recognition Loss 4.260504 || Total Training Translation Loss 0.003096\n",
            "2025-07-10 14:10:08,999 Epoch 1138:\n",
            "\t\t\tTotal Training Recognition Loss 6.035081 || Total Training Translation Loss 0.002372\n",
            "2025-07-10 14:10:09,173 Epoch 1139:\n",
            "\t\t\tTotal Training Recognition Loss 6.681976 || Total Training Translation Loss 0.003529\n",
            "2025-07-10 14:10:09,348 Epoch 1140:\n",
            "\t\t\tTotal Training Recognition Loss 112.583282 || Total Training Translation Loss 0.003139\n",
            "2025-07-10 14:10:09,521 Epoch 1141:\n",
            "\t\t\tTotal Training Recognition Loss 74.515991 || Total Training Translation Loss 0.002740\n",
            "2025-07-10 14:10:09,694 Epoch 1142:\n",
            "\t\t\tTotal Training Recognition Loss 26.600142 || Total Training Translation Loss 0.002850\n",
            "2025-07-10 14:10:09,867 Epoch 1143:\n",
            "\t\t\tTotal Training Recognition Loss 5.084153 || Total Training Translation Loss 0.002752\n",
            "2025-07-10 14:10:10,040 Epoch 1144:\n",
            "\t\t\tTotal Training Recognition Loss 10.078071 || Total Training Translation Loss 0.002968\n",
            "2025-07-10 14:10:10,213 Epoch 1145:\n",
            "\t\t\tTotal Training Recognition Loss 7.683125 || Total Training Translation Loss 0.002668\n",
            "2025-07-10 14:10:10,388 Epoch 1146:\n",
            "\t\t\tTotal Training Recognition Loss 9.965914 || Total Training Translation Loss 0.002958\n",
            "2025-07-10 14:10:10,561 Epoch 1147:\n",
            "\t\t\tTotal Training Recognition Loss 130.809402 || Total Training Translation Loss 0.002991\n",
            "2025-07-10 14:10:10,734 Epoch 1148:\n",
            "\t\t\tTotal Training Recognition Loss 9.586920 || Total Training Translation Loss 0.003099\n",
            "2025-07-10 14:10:10,908 Epoch 1149:\n",
            "\t\t\tTotal Training Recognition Loss 77.719521 || Total Training Translation Loss 0.003021\n",
            "2025-07-10 14:10:11,081 Epoch 1150:\n",
            "\t\t\tTotal Training Recognition Loss 21.246681 || Total Training Translation Loss 0.003571\n",
            "2025-07-10 14:10:11,261 Epoch 1151:\n",
            "\t\t\tTotal Training Recognition Loss 15.236040 || Total Training Translation Loss 0.003895\n",
            "2025-07-10 14:10:11,435 Epoch 1152:\n",
            "\t\t\tTotal Training Recognition Loss 15.933590 || Total Training Translation Loss 0.003310\n",
            "2025-07-10 14:10:11,608 Epoch 1153:\n",
            "\t\t\tTotal Training Recognition Loss 138.817337 || Total Training Translation Loss 0.003513\n",
            "2025-07-10 14:10:11,783 Epoch 1154:\n",
            "\t\t\tTotal Training Recognition Loss 10.389014 || Total Training Translation Loss 0.003143\n",
            "2025-07-10 14:10:11,955 Epoch 1155:\n",
            "\t\t\tTotal Training Recognition Loss 21.985664 || Total Training Translation Loss 0.005236\n",
            "2025-07-10 14:10:12,128 Epoch 1156:\n",
            "\t\t\tTotal Training Recognition Loss 23.264866 || Total Training Translation Loss 0.003325\n",
            "2025-07-10 14:10:12,301 Epoch 1157:\n",
            "\t\t\tTotal Training Recognition Loss 21.204895 || Total Training Translation Loss 0.002870\n",
            "2025-07-10 14:10:12,475 Epoch 1158:\n",
            "\t\t\tTotal Training Recognition Loss 10.044580 || Total Training Translation Loss 0.003565\n",
            "2025-07-10 14:10:12,651 Epoch 1159:\n",
            "\t\t\tTotal Training Recognition Loss 15.219336 || Total Training Translation Loss 0.003745\n",
            "2025-07-10 14:10:12,824 Epoch 1160:\n",
            "\t\t\tTotal Training Recognition Loss 42.194450 || Total Training Translation Loss 0.003226\n",
            "2025-07-10 14:10:12,998 Epoch 1161:\n",
            "\t\t\tTotal Training Recognition Loss 10.301141 || Total Training Translation Loss 0.003431\n",
            "2025-07-10 14:10:13,174 Epoch 1162:\n",
            "\t\t\tTotal Training Recognition Loss 14.968852 || Total Training Translation Loss 0.002925\n",
            "2025-07-10 14:10:13,348 Epoch 1163:\n",
            "\t\t\tTotal Training Recognition Loss 24.332081 || Total Training Translation Loss 0.003532\n",
            "2025-07-10 14:10:13,521 Epoch 1164:\n",
            "\t\t\tTotal Training Recognition Loss 9.713886 || Total Training Translation Loss 0.003039\n",
            "2025-07-10 14:10:13,694 Epoch 1165:\n",
            "\t\t\tTotal Training Recognition Loss 8.612922 || Total Training Translation Loss 0.003561\n",
            "2025-07-10 14:10:13,867 Epoch 1166:\n",
            "\t\t\tTotal Training Recognition Loss 5.077559 || Total Training Translation Loss 0.003477\n",
            "2025-07-10 14:10:14,040 Epoch 1167:\n",
            "\t\t\tTotal Training Recognition Loss 8.723697 || Total Training Translation Loss 0.003724\n",
            "2025-07-10 14:10:14,212 Epoch 1168:\n",
            "\t\t\tTotal Training Recognition Loss 15.335959 || Total Training Translation Loss 0.003350\n",
            "2025-07-10 14:10:14,386 Epoch 1169:\n",
            "\t\t\tTotal Training Recognition Loss 5.414653 || Total Training Translation Loss 0.003819\n",
            "2025-07-10 14:10:14,559 Epoch 1170:\n",
            "\t\t\tTotal Training Recognition Loss 9.453225 || Total Training Translation Loss 0.002940\n",
            "2025-07-10 14:10:14,732 Epoch 1171:\n",
            "\t\t\tTotal Training Recognition Loss 4.589555 || Total Training Translation Loss 0.003529\n",
            "2025-07-10 14:10:14,910 Epoch 1172:\n",
            "\t\t\tTotal Training Recognition Loss 2.081868 || Total Training Translation Loss 0.003940\n",
            "2025-07-10 14:10:15,084 Epoch 1173:\n",
            "\t\t\tTotal Training Recognition Loss 9.351121 || Total Training Translation Loss 0.003492\n",
            "2025-07-10 14:10:15,259 Epoch 1174:\n",
            "\t\t\tTotal Training Recognition Loss 4.281094 || Total Training Translation Loss 0.002985\n",
            "2025-07-10 14:10:15,434 Epoch 1175:\n",
            "\t\t\tTotal Training Recognition Loss 2.892922 || Total Training Translation Loss 0.003563\n",
            "2025-07-10 14:10:15,608 Epoch 1176:\n",
            "\t\t\tTotal Training Recognition Loss 6.157507 || Total Training Translation Loss 0.003009\n",
            "2025-07-10 14:10:15,782 Epoch 1177:\n",
            "\t\t\tTotal Training Recognition Loss 3.201143 || Total Training Translation Loss 0.003899\n",
            "2025-07-10 14:10:15,957 Epoch 1178:\n",
            "\t\t\tTotal Training Recognition Loss 7.151400 || Total Training Translation Loss 0.003362\n",
            "2025-07-10 14:10:16,131 Epoch 1179:\n",
            "\t\t\tTotal Training Recognition Loss 3.515620 || Total Training Translation Loss 0.003301\n",
            "2025-07-10 14:10:16,305 Epoch 1180:\n",
            "\t\t\tTotal Training Recognition Loss 2.435123 || Total Training Translation Loss 0.002506\n",
            "2025-07-10 14:10:16,478 Epoch 1181:\n",
            "\t\t\tTotal Training Recognition Loss 1.530581 || Total Training Translation Loss 0.003189\n",
            "2025-07-10 14:10:16,698 Epoch 1182:\n",
            "\t\t\tTotal Training Recognition Loss 1.139623 || Total Training Translation Loss 0.002743\n",
            "2025-07-10 14:10:16,917 Epoch 1183:\n",
            "\t\t\tTotal Training Recognition Loss 3.352781 || Total Training Translation Loss 0.003274\n",
            "2025-07-10 14:10:17,136 Epoch 1184:\n",
            "\t\t\tTotal Training Recognition Loss 0.995321 || Total Training Translation Loss 0.002619\n",
            "2025-07-10 14:10:17,354 Epoch 1185:\n",
            "\t\t\tTotal Training Recognition Loss 1.445193 || Total Training Translation Loss 0.002599\n",
            "2025-07-10 14:10:17,572 Epoch 1186:\n",
            "\t\t\tTotal Training Recognition Loss 0.927577 || Total Training Translation Loss 0.003627\n",
            "2025-07-10 14:10:17,790 Epoch 1187:\n",
            "\t\t\tTotal Training Recognition Loss 0.837496 || Total Training Translation Loss 0.003556\n",
            "2025-07-10 14:10:18,008 Epoch 1188:\n",
            "\t\t\tTotal Training Recognition Loss 7.652762 || Total Training Translation Loss 0.003112\n",
            "2025-07-10 14:10:18,227 Epoch 1189:\n",
            "\t\t\tTotal Training Recognition Loss 2.653943 || Total Training Translation Loss 0.003233\n",
            "2025-07-10 14:10:18,445 Epoch 1190:\n",
            "\t\t\tTotal Training Recognition Loss 0.929767 || Total Training Translation Loss 0.003594\n",
            "2025-07-10 14:10:18,662 Epoch 1191:\n",
            "\t\t\tTotal Training Recognition Loss 1.109966 || Total Training Translation Loss 0.002871\n",
            "2025-07-10 14:10:18,879 Epoch 1192:\n",
            "\t\t\tTotal Training Recognition Loss 3.763717 || Total Training Translation Loss 0.003027\n",
            "2025-07-10 14:10:19,100 Epoch 1193:\n",
            "\t\t\tTotal Training Recognition Loss 0.575797 || Total Training Translation Loss 0.003233\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-07-10 14:10:19,322 Epoch 1194:\n",
            "\t\t\tTotal Training Recognition Loss 0.815767 || Total Training Translation Loss 0.003419\n",
            "2025-07-10 14:10:19,504 Epoch 1195:\n",
            "\t\t\tTotal Training Recognition Loss 1.931569 || Total Training Translation Loss 0.003325\n",
            "2025-07-10 14:10:19,679 Epoch 1196:\n",
            "\t\t\tTotal Training Recognition Loss 0.657785 || Total Training Translation Loss 0.003411\n",
            "2025-07-10 14:10:19,852 Epoch 1197:\n",
            "\t\t\tTotal Training Recognition Loss 0.473314 || Total Training Translation Loss 0.003423\n",
            "2025-07-10 14:10:20,027 Epoch 1198:\n",
            "\t\t\tTotal Training Recognition Loss 0.737266 || Total Training Translation Loss 0.003332\n",
            "2025-07-10 14:10:20,203 Epoch 1199:\n",
            "\t\t\tTotal Training Recognition Loss 0.658381 || Total Training Translation Loss 0.003137\n",
            "2025-07-10 14:10:20,377 [Epoch: 1200 Step: 00001200] Batch Recognition Loss:   0.823218 => Gls Tokens per Sec:      214 || Batch Translation Loss:   0.002912 => Txt Tokens per Sec:      579 || Lr: 0.001000\n",
            "2025-07-10 14:10:20,614 Validation result at epoch 1200, step     1200: duration: 0.2359s\n",
            "\tRecognition Beam Size: 1\tTranslation Beam Size: 1\tTranslation Beam Alpha: -1\n",
            "\tRecognition Loss: 5215.97754\tTranslation Loss: 236.06860\tPPL: 16.07520\n",
            "\tEval Metric: BLEU\n",
            "\tWER 88.57\t(DEL: 40.00,\tINS: 5.71,\tSUB: 42.86)\n",
            "\tBLEU-4 2.00\t(BLEU-1: 7.33,\tBLEU-2: 3.90,\tBLEU-3: 2.79,\tBLEU-4: 2.00)\n",
            "\tCHRF 24.55\tROUGE 9.90\tFID 0.00\n",
            "2025-07-10 14:10:20,614 Logging Recognition and Translation Outputs\n",
            "2025-07-10 14:10:20,615 ========================================================================================\n",
            "2025-07-10 14:10:20,615 Logging Sequence: 11August_2010_Wednesday_tagesschau-2\n",
            "2025-07-10 14:10:20,615 \tGloss Reference :\tDRUCK TIEF     KOMMEN\n",
            "2025-07-10 14:10:20,615 \tGloss Hypothesis:\t***** SPEZIELL LOCH  \n",
            "2025-07-10 14:10:20,616 \tGloss Alignment :\tD     S        S     \n",
            "2025-07-10 14:10:20,616 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:10:20,617 \tText Reference  :\t********** ****** ** ***** ***** ***** ******* ***** **** *** ********* ********* ******** ********* ****** ******* *** ***** *** ****** ***** tiefer luftdruck bestimmt in    den   nächsten tagen unser wetter\n",
            "2025-07-10 14:10:20,618 \tText Hypothesis :\tnordwesten bleibt es heute nacht meist trocken sonst muss mit teilweise kräftigen schauern gerechnet werden örtlich mit blitz und donner <unk> <unk>  <unk>     <unk>    <unk> <unk> <unk>    <unk> <unk> <unk> \n",
            "2025-07-10 14:10:20,618 \tText Alignment  :\tI          I      I  I     I     I     I       I     I    I   I         I         I        I         I      I       I   I     I   I      I     S      S         S        S     S     S        S     S     S     \n",
            "2025-07-10 14:10:20,618 ========================================================================================\n",
            "2025-07-10 14:10:20,618 Logging Sequence: 11August_2010_Wednesday_tagesschau-3\n",
            "2025-07-10 14:10:20,619 \tGloss Reference :\tES-BEDEUTET VIEL WOLKE UND KOENNEN REGEN GEWITTER KOENNEN\n",
            "2025-07-10 14:10:20,619 \tGloss Hypothesis:\t*********** **** ***** *** ******* ORT   HEUTE    KOENNEN\n",
            "2025-07-10 14:10:20,619 \tGloss Alignment :\tD           D    D     D   D       S     S               \n",
            "2025-07-10 14:10:20,619 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:10:20,621 \tText Reference  :\tdas bedeutet viele wolken und  immer wieder     zum teil kräftige schauer **** ******** und ** ******* ******** ****** ********* *********** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** gewitter\n",
            "2025-07-10 14:10:20,622 \tText Hypothesis :\t*** ******** am    tag    gibt es    verbreitet zum teil kräftige schauer oder gewitter und in manchen regionen fallen ergiebige regenmengen <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>   \n",
            "2025-07-10 14:10:20,622 \tText Alignment  :\tD   D        S     S      S    S     S                                    I    I            I  I       I        I      I         I           I     I     I     I     I     I     I     I     I     I     I     S       \n",
            "2025-07-10 14:10:20,622 ========================================================================================\n",
            "2025-07-10 14:10:20,622 Logging Sequence: 11August_2010_Wednesday_tagesschau-8\n",
            "2025-07-10 14:10:20,623 \tGloss Reference :\tWIND MAESSIG SCHWACH REGION  WENN  GEWITTER WIND     KOENNEN\n",
            "2025-07-10 14:10:20,623 \tGloss Hypothesis:\tLOCH DURCH   TROCKEN BLEIBEN REGEN KOENNEN  SUEDWEST KOENNEN\n",
            "2025-07-10 14:10:20,623 \tGloss Alignment :\tS    S       S       S       S     S        S               \n",
            "2025-07-10 14:10:20,623 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:10:20,625 \tText Reference  :\t** **** ****** *** ******** **** ****** **************** **** ****** ******* ***** meist weht  nur   ein   schwacher wind  aus   unterschiedlichen richtungen der   bei   schauern und   gewittern stark böig  sein  kann \n",
            "2025-07-10 14:10:20,626 \tText Hypothesis :\tes auch länger und ergiebig auch lokale überschwemmungen sind wieder möglich <unk> <unk> <unk> <unk> <unk> <unk>     <unk> <unk> <unk>             <unk>      <unk> <unk> <unk>    <unk> <unk>     <unk> <unk> <unk> <unk>\n",
            "2025-07-10 14:10:20,626 \tText Alignment  :\tI  I    I      I   I        I    I      I                I    I      I       I     S     S     S     S     S         S     S     S                 S          S     S     S        S     S         S     S     S     S    \n",
            "2025-07-10 14:10:20,626 ========================================================================================\n",
            "2025-07-10 14:10:20,626 Logging Sequence: 25October_2010_Monday_tagesschau-22\n",
            "2025-07-10 14:10:20,627 \tGloss Reference :\t******* MITTWOCH   REGEN    KOENNEN NORDWEST WAHRSCHEINLICH NORD STARK WIND\n",
            "2025-07-10 14:10:20,627 \tGloss Hypothesis:\tFEBRUAR DONNERSTAG SUEDWEST KOENNEN ******** ************** **** ***** ****\n",
            "2025-07-10 14:10:20,627 \tGloss Alignment :\tI       S          S                D        D              D    D     D   \n",
            "2025-07-10 14:10:20,628 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:10:20,629 \tText Reference  :\tam *** **** ** ********** *** **** ******** ******* mittwoch hier     und ** ******* ******** ****** ********* *********** ***** ***** da    nieselregen in    der   nordwesthälfte an    den   küsten kräftiger wind \n",
            "2025-07-10 14:10:20,630 \tText Hypothesis :\tam tag gibt es verbreitet zum teil kräftige schauer oder     gewitter und in manchen regionen fallen ergiebige regenmengen <unk> <unk> <unk> <unk>       <unk> <unk> <unk>          <unk> <unk> <unk>  <unk>     <unk>\n",
            "2025-07-10 14:10:20,630 \tText Alignment  :\t   I   I    I  I          I   I    I        I       S        S            I  I       I        I      I         I           I     I     S     S           S     S     S              S     S     S      S         S    \n",
            "2025-07-10 14:10:20,630 ========================================================================================\n",
            "2025-07-10 14:10:20,630 Logging Sequence: 05May_2011_Thursday_tagesschau-25\n",
            "2025-07-10 14:10:20,630 \tGloss Reference :\t****** JETZT WETTER WIE-AUSSEHEN MORGEN FREITAG SECHSTE MAI        ZEIGEN-BILDSCHIRM\n",
            "2025-07-10 14:10:20,630 \tGloss Hypothesis:\tZWOELF JETZT WETTER ************ MORGEN ******* ******* DONNERSTAG FEBRUAR          \n",
            "2025-07-10 14:10:20,631 \tGloss Alignment :\tI                   D                   D       D       S          S                \n",
            "2025-07-10 14:10:20,631 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:10:20,632 \tText Reference  :\t** *** **** ** ********** *** **** ******** ******* **** ******** und ** ******* ******** ****** ********* *********** ***** ***** ***** nun   die   wettervorhersage für   morgen freitag den   sechsten mai  \n",
            "2025-07-10 14:10:20,632 \tText Hypothesis :\tam tag gibt es verbreitet zum teil kräftige schauer oder gewitter und in manchen regionen fallen ergiebige regenmengen <unk> <unk> <unk> <unk> <unk> <unk>            <unk> <unk>  <unk>   <unk> <unk>    <unk>\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-07-10 14:10:20,632 \tText Alignment  :\tI  I   I    I  I          I   I    I        I       I    I            I  I       I        I      I         I           I     I     I     S     S     S                S     S      S       S     S        S    \n",
            "2025-07-10 14:10:20,633 ========================================================================================\n",
            "2025-07-10 14:10:20,633 Epoch 1200:\n",
            "\t\t\tTotal Training Recognition Loss 0.823218 || Total Training Translation Loss 0.002912\n",
            "2025-07-10 14:10:20,806 Epoch 1201:\n",
            "\t\t\tTotal Training Recognition Loss 0.599102 || Total Training Translation Loss 0.003134\n",
            "2025-07-10 14:10:20,983 Epoch 1202:\n",
            "\t\t\tTotal Training Recognition Loss 0.634694 || Total Training Translation Loss 0.002624\n",
            "2025-07-10 14:10:21,157 Epoch 1203:\n",
            "\t\t\tTotal Training Recognition Loss 0.496219 || Total Training Translation Loss 0.003612\n",
            "2025-07-10 14:10:21,329 Epoch 1204:\n",
            "\t\t\tTotal Training Recognition Loss 0.647648 || Total Training Translation Loss 0.003185\n",
            "2025-07-10 14:10:21,506 Epoch 1205:\n",
            "\t\t\tTotal Training Recognition Loss 0.614356 || Total Training Translation Loss 0.003083\n",
            "2025-07-10 14:10:21,684 Epoch 1206:\n",
            "\t\t\tTotal Training Recognition Loss 0.612228 || Total Training Translation Loss 0.002989\n",
            "2025-07-10 14:10:21,859 Epoch 1207:\n",
            "\t\t\tTotal Training Recognition Loss 0.577769 || Total Training Translation Loss 0.003106\n",
            "2025-07-10 14:10:22,032 Epoch 1208:\n",
            "\t\t\tTotal Training Recognition Loss 0.728250 || Total Training Translation Loss 0.004391\n",
            "2025-07-10 14:10:22,205 Epoch 1209:\n",
            "\t\t\tTotal Training Recognition Loss 0.742823 || Total Training Translation Loss 0.003166\n",
            "2025-07-10 14:10:22,378 Epoch 1210:\n",
            "\t\t\tTotal Training Recognition Loss 0.834790 || Total Training Translation Loss 0.003471\n",
            "2025-07-10 14:10:22,553 Epoch 1211:\n",
            "\t\t\tTotal Training Recognition Loss 0.514783 || Total Training Translation Loss 0.003072\n",
            "2025-07-10 14:10:22,726 Epoch 1212:\n",
            "\t\t\tTotal Training Recognition Loss 0.413161 || Total Training Translation Loss 0.002635\n",
            "2025-07-10 14:10:22,899 Epoch 1213:\n",
            "\t\t\tTotal Training Recognition Loss 0.647722 || Total Training Translation Loss 0.003016\n",
            "2025-07-10 14:10:23,072 Epoch 1214:\n",
            "\t\t\tTotal Training Recognition Loss 0.388816 || Total Training Translation Loss 0.002880\n",
            "2025-07-10 14:10:23,245 Epoch 1215:\n",
            "\t\t\tTotal Training Recognition Loss 4.345038 || Total Training Translation Loss 0.003647\n",
            "2025-07-10 14:10:23,418 Epoch 1216:\n",
            "\t\t\tTotal Training Recognition Loss 0.341837 || Total Training Translation Loss 0.002862\n",
            "2025-07-10 14:10:23,591 Epoch 1217:\n",
            "\t\t\tTotal Training Recognition Loss 0.412286 || Total Training Translation Loss 0.003256\n",
            "2025-07-10 14:10:23,763 Epoch 1218:\n",
            "\t\t\tTotal Training Recognition Loss 0.532809 || Total Training Translation Loss 0.002793\n",
            "2025-07-10 14:10:23,936 Epoch 1219:\n",
            "\t\t\tTotal Training Recognition Loss 1.671858 || Total Training Translation Loss 0.003182\n",
            "2025-07-10 14:10:24,109 Epoch 1220:\n",
            "\t\t\tTotal Training Recognition Loss 0.634708 || Total Training Translation Loss 0.002838\n",
            "2025-07-10 14:10:24,282 Epoch 1221:\n",
            "\t\t\tTotal Training Recognition Loss 0.395085 || Total Training Translation Loss 0.004200\n",
            "2025-07-10 14:10:24,455 Epoch 1222:\n",
            "\t\t\tTotal Training Recognition Loss 0.429193 || Total Training Translation Loss 0.003074\n",
            "2025-07-10 14:10:24,628 Epoch 1223:\n",
            "\t\t\tTotal Training Recognition Loss 0.790931 || Total Training Translation Loss 0.003186\n",
            "2025-07-10 14:10:24,802 Epoch 1224:\n",
            "\t\t\tTotal Training Recognition Loss 0.398893 || Total Training Translation Loss 0.003337\n",
            "2025-07-10 14:10:24,976 Epoch 1225:\n",
            "\t\t\tTotal Training Recognition Loss 1.216196 || Total Training Translation Loss 0.003502\n",
            "2025-07-10 14:10:25,149 Epoch 1226:\n",
            "\t\t\tTotal Training Recognition Loss 0.349452 || Total Training Translation Loss 0.003537\n",
            "2025-07-10 14:10:25,323 Epoch 1227:\n",
            "\t\t\tTotal Training Recognition Loss 0.512423 || Total Training Translation Loss 0.002945\n",
            "2025-07-10 14:10:25,496 Epoch 1228:\n",
            "\t\t\tTotal Training Recognition Loss 0.523547 || Total Training Translation Loss 0.003119\n",
            "2025-07-10 14:10:25,717 Epoch 1229:\n",
            "\t\t\tTotal Training Recognition Loss 0.416043 || Total Training Translation Loss 0.002863\n",
            "2025-07-10 14:10:25,931 Epoch 1230:\n",
            "\t\t\tTotal Training Recognition Loss 0.697243 || Total Training Translation Loss 0.003210\n",
            "2025-07-10 14:10:26,147 Epoch 1231:\n",
            "\t\t\tTotal Training Recognition Loss 0.739542 || Total Training Translation Loss 0.002777\n",
            "2025-07-10 14:10:26,363 Epoch 1232:\n",
            "\t\t\tTotal Training Recognition Loss 0.484146 || Total Training Translation Loss 0.002649\n",
            "2025-07-10 14:10:26,585 Epoch 1233:\n",
            "\t\t\tTotal Training Recognition Loss 0.620448 || Total Training Translation Loss 0.003665\n",
            "2025-07-10 14:10:26,806 Epoch 1234:\n",
            "\t\t\tTotal Training Recognition Loss 3.453300 || Total Training Translation Loss 0.003422\n",
            "2025-07-10 14:10:27,030 Epoch 1235:\n",
            "\t\t\tTotal Training Recognition Loss 0.254459 || Total Training Translation Loss 0.003626\n",
            "2025-07-10 14:10:27,226 Epoch 1236:\n",
            "\t\t\tTotal Training Recognition Loss 0.194077 || Total Training Translation Loss 0.003388\n",
            "2025-07-10 14:10:27,402 Epoch 1237:\n",
            "\t\t\tTotal Training Recognition Loss 0.297975 || Total Training Translation Loss 0.002976\n",
            "2025-07-10 14:10:27,575 Epoch 1238:\n",
            "\t\t\tTotal Training Recognition Loss 0.291269 || Total Training Translation Loss 0.003160\n",
            "2025-07-10 14:10:27,749 Epoch 1239:\n",
            "\t\t\tTotal Training Recognition Loss 0.344376 || Total Training Translation Loss 0.003080\n",
            "2025-07-10 14:10:27,927 Epoch 1240:\n",
            "\t\t\tTotal Training Recognition Loss 0.237788 || Total Training Translation Loss 0.003447\n",
            "2025-07-10 14:10:28,102 Epoch 1241:\n",
            "\t\t\tTotal Training Recognition Loss 2.015955 || Total Training Translation Loss 0.003159\n",
            "2025-07-10 14:10:28,278 Epoch 1242:\n",
            "\t\t\tTotal Training Recognition Loss 0.461918 || Total Training Translation Loss 0.003139\n",
            "2025-07-10 14:10:28,452 Epoch 1243:\n",
            "\t\t\tTotal Training Recognition Loss 0.234990 || Total Training Translation Loss 0.003188\n",
            "2025-07-10 14:10:28,626 Epoch 1244:\n",
            "\t\t\tTotal Training Recognition Loss 0.397673 || Total Training Translation Loss 0.003945\n",
            "2025-07-10 14:10:28,801 Epoch 1245:\n",
            "\t\t\tTotal Training Recognition Loss 0.426473 || Total Training Translation Loss 0.003131\n",
            "2025-07-10 14:10:28,975 Epoch 1246:\n",
            "\t\t\tTotal Training Recognition Loss 0.316634 || Total Training Translation Loss 0.003089\n",
            "2025-07-10 14:10:29,150 Epoch 1247:\n",
            "\t\t\tTotal Training Recognition Loss 3.756001 || Total Training Translation Loss 0.003260\n",
            "2025-07-10 14:10:29,371 Epoch 1248:\n",
            "\t\t\tTotal Training Recognition Loss 0.791259 || Total Training Translation Loss 0.004522\n",
            "2025-07-10 14:10:29,588 Epoch 1249:\n",
            "\t\t\tTotal Training Recognition Loss 1.198190 || Total Training Translation Loss 0.003168\n",
            "2025-07-10 14:10:29,807 Epoch 1250:\n",
            "\t\t\tTotal Training Recognition Loss 0.366429 || Total Training Translation Loss 0.003001\n",
            "2025-07-10 14:10:30,025 Epoch 1251:\n",
            "\t\t\tTotal Training Recognition Loss 0.500385 || Total Training Translation Loss 0.003661\n",
            "2025-07-10 14:10:30,243 Epoch 1252:\n",
            "\t\t\tTotal Training Recognition Loss 0.810128 || Total Training Translation Loss 0.003461\n",
            "2025-07-10 14:10:30,434 Epoch 1253:\n",
            "\t\t\tTotal Training Recognition Loss 0.354868 || Total Training Translation Loss 0.003368\n",
            "2025-07-10 14:10:30,607 Epoch 1254:\n",
            "\t\t\tTotal Training Recognition Loss 0.266958 || Total Training Translation Loss 0.002878\n",
            "2025-07-10 14:10:30,781 Epoch 1255:\n",
            "\t\t\tTotal Training Recognition Loss 1.021859 || Total Training Translation Loss 0.003215\n",
            "2025-07-10 14:10:30,955 Epoch 1256:\n",
            "\t\t\tTotal Training Recognition Loss 0.214844 || Total Training Translation Loss 0.003265\n",
            "2025-07-10 14:10:31,130 Epoch 1257:\n",
            "\t\t\tTotal Training Recognition Loss 5.309350 || Total Training Translation Loss 0.003347\n",
            "2025-07-10 14:10:31,303 Epoch 1258:\n",
            "\t\t\tTotal Training Recognition Loss 36.186619 || Total Training Translation Loss 0.003111\n",
            "2025-07-10 14:10:31,477 Epoch 1259:\n",
            "\t\t\tTotal Training Recognition Loss 0.216984 || Total Training Translation Loss 0.003615\n",
            "2025-07-10 14:10:31,650 Epoch 1260:\n",
            "\t\t\tTotal Training Recognition Loss 0.300441 || Total Training Translation Loss 0.004255\n",
            "2025-07-10 14:10:31,868 Epoch 1261:\n",
            "\t\t\tTotal Training Recognition Loss 2.533080 || Total Training Translation Loss 0.003209\n",
            "2025-07-10 14:10:32,085 Epoch 1262:\n",
            "\t\t\tTotal Training Recognition Loss 1.027832 || Total Training Translation Loss 0.003473\n",
            "2025-07-10 14:10:32,300 Epoch 1263:\n",
            "\t\t\tTotal Training Recognition Loss 7.597421 || Total Training Translation Loss 0.004283\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-07-10 14:10:32,514 Epoch 1264:\n",
            "\t\t\tTotal Training Recognition Loss 76.839966 || Total Training Translation Loss 0.005011\n",
            "2025-07-10 14:10:32,695 Epoch 1265:\n",
            "\t\t\tTotal Training Recognition Loss 0.749438 || Total Training Translation Loss 0.003103\n",
            "2025-07-10 14:10:32,868 Epoch 1266:\n",
            "\t\t\tTotal Training Recognition Loss 0.593001 || Total Training Translation Loss 0.003708\n",
            "2025-07-10 14:10:33,041 Epoch 1267:\n",
            "\t\t\tTotal Training Recognition Loss 11.387490 || Total Training Translation Loss 0.003138\n",
            "2025-07-10 14:10:33,214 Epoch 1268:\n",
            "\t\t\tTotal Training Recognition Loss 4.679309 || Total Training Translation Loss 0.003636\n",
            "2025-07-10 14:10:33,388 Epoch 1269:\n",
            "\t\t\tTotal Training Recognition Loss 2.968267 || Total Training Translation Loss 0.003384\n",
            "2025-07-10 14:10:33,561 Epoch 1270:\n",
            "\t\t\tTotal Training Recognition Loss 4.740588 || Total Training Translation Loss 0.003856\n",
            "2025-07-10 14:10:33,735 Epoch 1271:\n",
            "\t\t\tTotal Training Recognition Loss 15.457211 || Total Training Translation Loss 0.004155\n",
            "2025-07-10 14:10:33,908 Epoch 1272:\n",
            "\t\t\tTotal Training Recognition Loss 10.561920 || Total Training Translation Loss 0.004553\n",
            "2025-07-10 14:10:34,081 Epoch 1273:\n",
            "\t\t\tTotal Training Recognition Loss 105.970299 || Total Training Translation Loss 0.005192\n",
            "2025-07-10 14:10:34,254 Epoch 1274:\n",
            "\t\t\tTotal Training Recognition Loss 10.077179 || Total Training Translation Loss 0.003974\n",
            "2025-07-10 14:10:34,426 Epoch 1275:\n",
            "\t\t\tTotal Training Recognition Loss 7.714435 || Total Training Translation Loss 0.005371\n",
            "2025-07-10 14:10:34,601 Epoch 1276:\n",
            "\t\t\tTotal Training Recognition Loss 1.726220 || Total Training Translation Loss 0.003922\n",
            "2025-07-10 14:10:34,771 Epoch 1277:\n",
            "\t\t\tTotal Training Recognition Loss 1.719105 || Total Training Translation Loss 0.003767\n",
            "2025-07-10 14:10:34,942 Epoch 1278:\n",
            "\t\t\tTotal Training Recognition Loss 2.076846 || Total Training Translation Loss 0.003949\n",
            "2025-07-10 14:10:35,132 Epoch 1279:\n",
            "\t\t\tTotal Training Recognition Loss 1.890717 || Total Training Translation Loss 0.004019\n",
            "2025-07-10 14:10:35,302 Epoch 1280:\n",
            "\t\t\tTotal Training Recognition Loss 5.497063 || Total Training Translation Loss 0.004344\n",
            "2025-07-10 14:10:35,473 Epoch 1281:\n",
            "\t\t\tTotal Training Recognition Loss 6.023280 || Total Training Translation Loss 0.004713\n",
            "2025-07-10 14:10:35,643 Epoch 1282:\n",
            "\t\t\tTotal Training Recognition Loss 25.030252 || Total Training Translation Loss 0.003923\n",
            "2025-07-10 14:10:35,814 Epoch 1283:\n",
            "\t\t\tTotal Training Recognition Loss 5.090020 || Total Training Translation Loss 0.003890\n",
            "2025-07-10 14:10:36,033 Epoch 1284:\n",
            "\t\t\tTotal Training Recognition Loss 1.760800 || Total Training Translation Loss 0.003949\n",
            "2025-07-10 14:10:36,249 Epoch 1285:\n",
            "\t\t\tTotal Training Recognition Loss 18.705395 || Total Training Translation Loss 0.005039\n",
            "2025-07-10 14:10:36,465 Epoch 1286:\n",
            "\t\t\tTotal Training Recognition Loss 4.231850 || Total Training Translation Loss 0.004716\n",
            "2025-07-10 14:10:36,681 Epoch 1287:\n",
            "\t\t\tTotal Training Recognition Loss 2.686303 || Total Training Translation Loss 0.004013\n",
            "2025-07-10 14:10:36,897 Epoch 1288:\n",
            "\t\t\tTotal Training Recognition Loss 2.192297 || Total Training Translation Loss 0.004069\n",
            "2025-07-10 14:10:37,113 Epoch 1289:\n",
            "\t\t\tTotal Training Recognition Loss 2.898748 || Total Training Translation Loss 0.004019\n",
            "2025-07-10 14:10:37,330 Epoch 1290:\n",
            "\t\t\tTotal Training Recognition Loss 43.436646 || Total Training Translation Loss 0.003177\n",
            "2025-07-10 14:10:37,546 Epoch 1291:\n",
            "\t\t\tTotal Training Recognition Loss 5.458309 || Total Training Translation Loss 0.003497\n",
            "2025-07-10 14:10:37,761 Epoch 1292:\n",
            "\t\t\tTotal Training Recognition Loss 4.333045 || Total Training Translation Loss 0.003696\n",
            "2025-07-10 14:10:37,977 Epoch 1293:\n",
            "\t\t\tTotal Training Recognition Loss 3.421049 || Total Training Translation Loss 0.004300\n",
            "2025-07-10 14:10:38,194 Epoch 1294:\n",
            "\t\t\tTotal Training Recognition Loss 5.777519 || Total Training Translation Loss 0.004405\n",
            "2025-07-10 14:10:38,409 Epoch 1295:\n",
            "\t\t\tTotal Training Recognition Loss 7.478335 || Total Training Translation Loss 0.003578\n",
            "2025-07-10 14:10:38,624 Epoch 1296:\n",
            "\t\t\tTotal Training Recognition Loss 6.184676 || Total Training Translation Loss 0.004317\n",
            "2025-07-10 14:10:38,841 Epoch 1297:\n",
            "\t\t\tTotal Training Recognition Loss 4.399663 || Total Training Translation Loss 0.004720\n",
            "2025-07-10 14:10:39,057 Epoch 1298:\n",
            "\t\t\tTotal Training Recognition Loss 7.874696 || Total Training Translation Loss 0.003913\n",
            "2025-07-10 14:10:39,233 Epoch 1299:\n",
            "\t\t\tTotal Training Recognition Loss 2.665329 || Total Training Translation Loss 0.003670\n",
            "2025-07-10 14:10:39,404 [Epoch: 1300 Step: 00001300] Batch Recognition Loss:   2.014106 => Gls Tokens per Sec:      217 || Batch Translation Loss:   0.003664 => Txt Tokens per Sec:      588 || Lr: 0.001000\n",
            "2025-07-10 14:10:39,639 Validation result at epoch 1300, step     1300: duration: 0.2338s\n",
            "\tRecognition Beam Size: 1\tTranslation Beam Size: 1\tTranslation Beam Alpha: -1\n",
            "\tRecognition Loss: 5690.58154\tTranslation Loss: 229.83841\tPPL: 14.93909\n",
            "\tEval Metric: BLEU\n",
            "\tWER 91.43\t(DEL: 48.57,\tINS: 2.86,\tSUB: 40.00)\n",
            "\tBLEU-4 2.00\t(BLEU-1: 7.33,\tBLEU-2: 3.90,\tBLEU-3: 2.79,\tBLEU-4: 2.00)\n",
            "\tCHRF 26.09\tROUGE 9.90\tFID 0.00\n",
            "2025-07-10 14:10:39,639 Logging Recognition and Translation Outputs\n",
            "2025-07-10 14:10:39,640 ========================================================================================\n",
            "2025-07-10 14:10:39,640 Logging Sequence: 11August_2010_Wednesday_tagesschau-2\n",
            "2025-07-10 14:10:39,640 \tGloss Reference :\tDRUCK TIEF     KOMMEN\n",
            "2025-07-10 14:10:39,641 \tGloss Hypothesis:\t***** SPEZIELL LOCH  \n",
            "2025-07-10 14:10:39,641 \tGloss Alignment :\tD     S        S     \n",
            "2025-07-10 14:10:39,641 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:10:39,643 \tText Reference  :\t********** ****** ** ***** ***** ***** ******* ***** **** *** ********* ********* ******** ********* ****** ******* *** ***** *** ****** ***** tiefer luftdruck bestimmt in    den   nächsten tagen unser wetter\n",
            "2025-07-10 14:10:39,643 \tText Hypothesis :\tnordwesten bleibt es heute nacht meist trocken sonst muss mit teilweise kräftigen schauern gerechnet werden örtlich mit blitz und donner <unk> <unk>  <unk>     <unk>    <unk> <unk> <unk>    <unk> <unk> <unk> \n",
            "2025-07-10 14:10:39,643 \tText Alignment  :\tI          I      I  I     I     I     I       I     I    I   I         I         I        I         I      I       I   I     I   I      I     S      S         S        S     S     S        S     S     S     \n",
            "2025-07-10 14:10:39,643 ========================================================================================\n",
            "2025-07-10 14:10:39,644 Logging Sequence: 11August_2010_Wednesday_tagesschau-3\n",
            "2025-07-10 14:10:39,644 \tGloss Reference :\tES-BEDEUTET VIEL WOLKE UND KOENNEN REGEN GEWITTER KOENNEN\n",
            "2025-07-10 14:10:39,644 \tGloss Hypothesis:\t*********** **** ***** *** ******* ORT   HEUTE    KOENNEN\n",
            "2025-07-10 14:10:39,644 \tGloss Alignment :\tD           D    D     D   D       S     S               \n",
            "2025-07-10 14:10:39,645 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:10:39,646 \tText Reference  :\tdas bedeutet viele wolken und  immer wieder     zum teil kräftige schauer **** ******** und ** ******* ******** ****** ********* *********** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** gewitter\n",
            "2025-07-10 14:10:39,647 \tText Hypothesis :\t*** ******** am    tag    gibt es    verbreitet zum teil kräftige schauer oder gewitter und in manchen regionen fallen ergiebige regenmengen <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>   \n",
            "2025-07-10 14:10:39,647 \tText Alignment  :\tD   D        S     S      S    S     S                                    I    I            I  I       I        I      I         I           I     I     I     I     I     I     I     I     I     I     I     S       \n",
            "2025-07-10 14:10:39,647 ========================================================================================\n",
            "2025-07-10 14:10:39,649 Logging Sequence: 11August_2010_Wednesday_tagesschau-8\n",
            "2025-07-10 14:10:39,649 \tGloss Reference :\tWIND MAESSIG SCHWACH REGION WENN     GEWITTER WIND     KOENNEN ********\n",
            "2025-07-10 14:10:39,649 \tGloss Hypothesis:\t**** ******* REGEN   LOCH   NORDWEST BLEIBEN  NORDWEST KOENNEN NORDWEST\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-07-10 14:10:39,650 \tGloss Alignment :\tD    D       S       S      S        S        S                I       \n",
            "2025-07-10 14:10:39,650 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:10:39,652 \tText Reference  :\tmeist weht nur ein  schwacher wind       aus unterschiedlichen richtungen der     bei  schauern und ** ******* ******** ****** ********* *********** ***** ***** ***** ***** ***** ***** ***** gewittern stark böig  sein  kann \n",
            "2025-07-10 14:10:39,652 \tText Hypothesis :\t***** am   tag gibt es        verbreitet zum teil              kräftige   schauer oder gewitter und in manchen regionen fallen ergiebige regenmengen <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>     <unk> <unk> <unk> <unk>\n",
            "2025-07-10 14:10:39,653 \tText Alignment  :\tD     S    S   S    S         S          S   S                 S          S       S    S            I  I       I        I      I         I           I     I     I     I     I     I     I     S         S     S     S     S    \n",
            "2025-07-10 14:10:39,653 ========================================================================================\n",
            "2025-07-10 14:10:39,653 Logging Sequence: 25October_2010_Monday_tagesschau-22\n",
            "2025-07-10 14:10:39,653 \tGloss Reference :\tMITTWOCH REGEN KOENNEN NORDWEST WAHRSCHEINLICH NORD STARK WIND   \n",
            "2025-07-10 14:10:39,653 \tGloss Hypothesis:\t******** ***** ******* ******** ************** **** ***** FEBRUAR\n",
            "2025-07-10 14:10:39,654 \tGloss Alignment :\tD        D     D       D        D              D    D     S      \n",
            "2025-07-10 14:10:39,654 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:10:39,656 \tText Reference  :\tam *** **** ** ********** *** **** ******** ******* mittwoch hier     und ** ******* ******** ****** ********* *********** ***** ***** da    nieselregen in    der   nordwesthälfte an    den   küsten kräftiger wind \n",
            "2025-07-10 14:10:39,656 \tText Hypothesis :\tam tag gibt es verbreitet zum teil kräftige schauer oder     gewitter und in manchen regionen fallen ergiebige regenmengen <unk> <unk> <unk> <unk>       <unk> <unk> <unk>          <unk> <unk> <unk>  <unk>     <unk>\n",
            "2025-07-10 14:10:39,656 \tText Alignment  :\t   I   I    I  I          I   I    I        I       S        S            I  I       I        I      I         I           I     I     S     S           S     S     S              S     S     S      S         S    \n",
            "2025-07-10 14:10:39,656 ========================================================================================\n",
            "2025-07-10 14:10:39,656 Logging Sequence: 05May_2011_Thursday_tagesschau-25\n",
            "2025-07-10 14:10:39,657 \tGloss Reference :\tJETZT WETTER WIE-AUSSEHEN MORGEN FREITAG SECHSTE    MAI     ZEIGEN-BILDSCHIRM\n",
            "2025-07-10 14:10:39,657 \tGloss Hypothesis:\tJETZT WETTER ************ ****** ORT     DONNERSTAG BLEIBEN FEBRUAR          \n",
            "2025-07-10 14:10:39,657 \tGloss Alignment :\t             D            D      S       S          S       S                \n",
            "2025-07-10 14:10:39,658 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:10:39,659 \tText Reference  :\t** *** **** ** ********** *** **** ******** ******* **** ******** und ** ******* ******** ****** ********* *********** ***** ***** ***** nun   die   wettervorhersage für   morgen freitag den   sechsten mai  \n",
            "2025-07-10 14:10:39,659 \tText Hypothesis :\tam tag gibt es verbreitet zum teil kräftige schauer oder gewitter und in manchen regionen fallen ergiebige regenmengen <unk> <unk> <unk> <unk> <unk> <unk>            <unk> <unk>  <unk>   <unk> <unk>    <unk>\n",
            "2025-07-10 14:10:39,659 \tText Alignment  :\tI  I   I    I  I          I   I    I        I       I    I            I  I       I        I      I         I           I     I     I     S     S     S                S     S      S       S     S        S    \n",
            "2025-07-10 14:10:39,660 ========================================================================================\n",
            "2025-07-10 14:10:39,662 Epoch 1300:\n",
            "\t\t\tTotal Training Recognition Loss 2.014106 || Total Training Translation Loss 0.003664\n",
            "2025-07-10 14:10:39,830 Epoch 1301:\n",
            "\t\t\tTotal Training Recognition Loss 1.322147 || Total Training Translation Loss 0.003958\n",
            "2025-07-10 14:10:40,001 Epoch 1302:\n",
            "\t\t\tTotal Training Recognition Loss 9.052877 || Total Training Translation Loss 0.003943\n",
            "2025-07-10 14:10:40,172 Epoch 1303:\n",
            "\t\t\tTotal Training Recognition Loss 15.762409 || Total Training Translation Loss 0.003574\n",
            "2025-07-10 14:10:40,343 Epoch 1304:\n",
            "\t\t\tTotal Training Recognition Loss 2.072786 || Total Training Translation Loss 0.004813\n",
            "2025-07-10 14:10:40,514 Epoch 1305:\n",
            "\t\t\tTotal Training Recognition Loss 4.980279 || Total Training Translation Loss 0.003788\n",
            "2025-07-10 14:10:40,688 Epoch 1306:\n",
            "\t\t\tTotal Training Recognition Loss 2.002673 || Total Training Translation Loss 0.003749\n",
            "2025-07-10 14:10:40,860 Epoch 1307:\n",
            "\t\t\tTotal Training Recognition Loss 0.741441 || Total Training Translation Loss 0.004185\n",
            "2025-07-10 14:10:41,033 Epoch 1308:\n",
            "\t\t\tTotal Training Recognition Loss 2.355190 || Total Training Translation Loss 0.005210\n",
            "2025-07-10 14:10:41,205 Epoch 1309:\n",
            "\t\t\tTotal Training Recognition Loss 1.028228 || Total Training Translation Loss 0.004302\n",
            "2025-07-10 14:10:41,380 Epoch 1310:\n",
            "\t\t\tTotal Training Recognition Loss 1.251068 || Total Training Translation Loss 0.004574\n",
            "2025-07-10 14:10:41,556 Epoch 1311:\n",
            "\t\t\tTotal Training Recognition Loss 1.403483 || Total Training Translation Loss 0.004100\n",
            "2025-07-10 14:10:41,732 Epoch 1312:\n",
            "\t\t\tTotal Training Recognition Loss 5.702433 || Total Training Translation Loss 0.005168\n",
            "2025-07-10 14:10:41,904 Epoch 1313:\n",
            "\t\t\tTotal Training Recognition Loss 2.407305 || Total Training Translation Loss 0.004669\n",
            "2025-07-10 14:10:42,082 Epoch 1314:\n",
            "\t\t\tTotal Training Recognition Loss 3.090453 || Total Training Translation Loss 0.005353\n",
            "2025-07-10 14:10:42,263 Epoch 1315:\n",
            "\t\t\tTotal Training Recognition Loss 0.847661 || Total Training Translation Loss 0.005963\n",
            "2025-07-10 14:10:42,442 Epoch 1316:\n",
            "\t\t\tTotal Training Recognition Loss 2.715280 || Total Training Translation Loss 0.004317\n",
            "2025-07-10 14:10:42,623 Epoch 1317:\n",
            "\t\t\tTotal Training Recognition Loss 3.117933 || Total Training Translation Loss 0.005320\n",
            "2025-07-10 14:10:42,814 Epoch 1318:\n",
            "\t\t\tTotal Training Recognition Loss 0.861811 || Total Training Translation Loss 0.004873\n",
            "2025-07-10 14:10:42,994 Epoch 1319:\n",
            "\t\t\tTotal Training Recognition Loss 0.827495 || Total Training Translation Loss 0.004632\n",
            "2025-07-10 14:10:43,176 Epoch 1320:\n",
            "\t\t\tTotal Training Recognition Loss 1.888814 || Total Training Translation Loss 0.004359\n",
            "2025-07-10 14:10:43,391 Epoch 1321:\n",
            "\t\t\tTotal Training Recognition Loss 4.025320 || Total Training Translation Loss 0.004096\n",
            "2025-07-10 14:10:43,588 Epoch 1322:\n",
            "\t\t\tTotal Training Recognition Loss 5.153635 || Total Training Translation Loss 0.004361\n",
            "2025-07-10 14:10:43,764 Epoch 1323:\n",
            "\t\t\tTotal Training Recognition Loss 0.931512 || Total Training Translation Loss 0.005355\n",
            "2025-07-10 14:10:43,943 Epoch 1324:\n",
            "\t\t\tTotal Training Recognition Loss 0.606929 || Total Training Translation Loss 0.006647\n",
            "2025-07-10 14:10:44,127 Epoch 1325:\n",
            "\t\t\tTotal Training Recognition Loss 0.787797 || Total Training Translation Loss 0.004077\n",
            "2025-07-10 14:10:44,349 Epoch 1326:\n",
            "\t\t\tTotal Training Recognition Loss 0.996122 || Total Training Translation Loss 0.003826\n",
            "2025-07-10 14:10:44,574 Epoch 1327:\n",
            "\t\t\tTotal Training Recognition Loss 0.554615 || Total Training Translation Loss 0.004822\n",
            "2025-07-10 14:10:44,757 Epoch 1328:\n",
            "\t\t\tTotal Training Recognition Loss 0.997642 || Total Training Translation Loss 0.005043\n",
            "2025-07-10 14:10:44,939 Epoch 1329:\n",
            "\t\t\tTotal Training Recognition Loss 0.692147 || Total Training Translation Loss 0.004697\n",
            "2025-07-10 14:10:45,124 Epoch 1330:\n",
            "\t\t\tTotal Training Recognition Loss 1.071777 || Total Training Translation Loss 0.003869\n",
            "2025-07-10 14:10:45,306 Epoch 1331:\n",
            "\t\t\tTotal Training Recognition Loss 0.687461 || Total Training Translation Loss 0.003942\n",
            "2025-07-10 14:10:45,487 Epoch 1332:\n",
            "\t\t\tTotal Training Recognition Loss 11.744849 || Total Training Translation Loss 0.003943\n",
            "2025-07-10 14:10:45,665 Epoch 1333:\n",
            "\t\t\tTotal Training Recognition Loss 0.555866 || Total Training Translation Loss 0.004908\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-07-10 14:10:45,844 Epoch 1334:\n",
            "\t\t\tTotal Training Recognition Loss 7.798430 || Total Training Translation Loss 0.004526\n",
            "2025-07-10 14:10:46,026 Epoch 1335:\n",
            "\t\t\tTotal Training Recognition Loss 0.762162 || Total Training Translation Loss 0.004468\n",
            "2025-07-10 14:10:46,206 Epoch 1336:\n",
            "\t\t\tTotal Training Recognition Loss 0.681749 || Total Training Translation Loss 0.004608\n",
            "2025-07-10 14:10:46,387 Epoch 1337:\n",
            "\t\t\tTotal Training Recognition Loss 3.361093 || Total Training Translation Loss 0.004347\n",
            "2025-07-10 14:10:46,562 Epoch 1338:\n",
            "\t\t\tTotal Training Recognition Loss 22.418406 || Total Training Translation Loss 0.005074\n",
            "2025-07-10 14:10:46,738 Epoch 1339:\n",
            "\t\t\tTotal Training Recognition Loss 12.664087 || Total Training Translation Loss 0.005834\n",
            "2025-07-10 14:10:46,912 Epoch 1340:\n",
            "\t\t\tTotal Training Recognition Loss 1.277689 || Total Training Translation Loss 0.004846\n",
            "2025-07-10 14:10:47,089 Epoch 1341:\n",
            "\t\t\tTotal Training Recognition Loss 1.101813 || Total Training Translation Loss 0.004019\n",
            "2025-07-10 14:10:47,269 Epoch 1342:\n",
            "\t\t\tTotal Training Recognition Loss 1.429385 || Total Training Translation Loss 0.004002\n",
            "2025-07-10 14:10:47,445 Epoch 1343:\n",
            "\t\t\tTotal Training Recognition Loss 0.656719 || Total Training Translation Loss 0.004763\n",
            "2025-07-10 14:10:47,661 Epoch 1344:\n",
            "\t\t\tTotal Training Recognition Loss 1.327756 || Total Training Translation Loss 0.005219\n",
            "2025-07-10 14:10:47,856 Epoch 1345:\n",
            "\t\t\tTotal Training Recognition Loss 2.756943 || Total Training Translation Loss 0.004163\n",
            "2025-07-10 14:10:48,031 Epoch 1346:\n",
            "\t\t\tTotal Training Recognition Loss 0.791927 || Total Training Translation Loss 0.004654\n",
            "2025-07-10 14:10:48,206 Epoch 1347:\n",
            "\t\t\tTotal Training Recognition Loss 2.687907 || Total Training Translation Loss 0.005199\n",
            "2025-07-10 14:10:48,384 Epoch 1348:\n",
            "\t\t\tTotal Training Recognition Loss 71.429955 || Total Training Translation Loss 0.003619\n",
            "2025-07-10 14:10:48,556 Epoch 1349:\n",
            "\t\t\tTotal Training Recognition Loss 1.997512 || Total Training Translation Loss 0.004582\n",
            "2025-07-10 14:10:48,730 Epoch 1350:\n",
            "\t\t\tTotal Training Recognition Loss 2.844622 || Total Training Translation Loss 0.004843\n",
            "2025-07-10 14:10:48,904 Epoch 1351:\n",
            "\t\t\tTotal Training Recognition Loss 2.169419 || Total Training Translation Loss 0.004337\n",
            "2025-07-10 14:10:49,082 Epoch 1352:\n",
            "\t\t\tTotal Training Recognition Loss 13.600832 || Total Training Translation Loss 0.005003\n",
            "2025-07-10 14:10:49,256 Epoch 1353:\n",
            "\t\t\tTotal Training Recognition Loss 0.774837 || Total Training Translation Loss 0.004565\n",
            "2025-07-10 14:10:49,432 Epoch 1354:\n",
            "\t\t\tTotal Training Recognition Loss 4.740436 || Total Training Translation Loss 0.004252\n",
            "2025-07-10 14:10:49,606 Epoch 1355:\n",
            "\t\t\tTotal Training Recognition Loss 2.092477 || Total Training Translation Loss 0.004613\n",
            "2025-07-10 14:10:49,784 Epoch 1356:\n",
            "\t\t\tTotal Training Recognition Loss 15.910825 || Total Training Translation Loss 0.005068\n",
            "2025-07-10 14:10:49,962 Epoch 1357:\n",
            "\t\t\tTotal Training Recognition Loss 1.186846 || Total Training Translation Loss 0.004118\n",
            "2025-07-10 14:10:50,136 Epoch 1358:\n",
            "\t\t\tTotal Training Recognition Loss 2.291322 || Total Training Translation Loss 0.004856\n",
            "2025-07-10 14:10:50,342 Epoch 1359:\n",
            "\t\t\tTotal Training Recognition Loss 2.193242 || Total Training Translation Loss 0.004345\n",
            "2025-07-10 14:10:50,523 Epoch 1360:\n",
            "\t\t\tTotal Training Recognition Loss 0.951974 || Total Training Translation Loss 0.004454\n",
            "2025-07-10 14:10:50,697 Epoch 1361:\n",
            "\t\t\tTotal Training Recognition Loss 5.413116 || Total Training Translation Loss 0.005204\n",
            "2025-07-10 14:10:50,873 Epoch 1362:\n",
            "\t\t\tTotal Training Recognition Loss 3.659790 || Total Training Translation Loss 0.003986\n",
            "2025-07-10 14:10:51,045 Epoch 1363:\n",
            "\t\t\tTotal Training Recognition Loss 4.401006 || Total Training Translation Loss 0.004124\n",
            "2025-07-10 14:10:51,217 Epoch 1364:\n",
            "\t\t\tTotal Training Recognition Loss 2.149638 || Total Training Translation Loss 0.004510\n",
            "2025-07-10 14:10:51,390 Epoch 1365:\n",
            "\t\t\tTotal Training Recognition Loss 1.724347 || Total Training Translation Loss 0.004441\n",
            "2025-07-10 14:10:51,609 Epoch 1366:\n",
            "\t\t\tTotal Training Recognition Loss 3.731496 || Total Training Translation Loss 0.004861\n",
            "2025-07-10 14:10:51,829 Epoch 1367:\n",
            "\t\t\tTotal Training Recognition Loss 3.216404 || Total Training Translation Loss 0.005550\n",
            "2025-07-10 14:10:52,004 Epoch 1368:\n",
            "\t\t\tTotal Training Recognition Loss 0.877590 || Total Training Translation Loss 0.004984\n",
            "2025-07-10 14:10:52,175 Epoch 1369:\n",
            "\t\t\tTotal Training Recognition Loss 0.561411 || Total Training Translation Loss 0.003918\n",
            "2025-07-10 14:10:52,360 Epoch 1370:\n",
            "\t\t\tTotal Training Recognition Loss 0.851552 || Total Training Translation Loss 0.004237\n",
            "2025-07-10 14:10:52,535 Epoch 1371:\n",
            "\t\t\tTotal Training Recognition Loss 0.567166 || Total Training Translation Loss 0.004680\n",
            "2025-07-10 14:10:52,708 Epoch 1372:\n",
            "\t\t\tTotal Training Recognition Loss 0.778271 || Total Training Translation Loss 0.004687\n",
            "2025-07-10 14:10:52,882 Epoch 1373:\n",
            "\t\t\tTotal Training Recognition Loss 1.229338 || Total Training Translation Loss 0.004251\n",
            "2025-07-10 14:10:53,054 Epoch 1374:\n",
            "\t\t\tTotal Training Recognition Loss 1.097365 || Total Training Translation Loss 0.004282\n",
            "2025-07-10 14:10:53,228 Epoch 1375:\n",
            "\t\t\tTotal Training Recognition Loss 0.688071 || Total Training Translation Loss 0.003758\n",
            "2025-07-10 14:10:53,402 Epoch 1376:\n",
            "\t\t\tTotal Training Recognition Loss 12.147842 || Total Training Translation Loss 0.004185\n",
            "2025-07-10 14:10:53,574 Epoch 1377:\n",
            "\t\t\tTotal Training Recognition Loss 0.743053 || Total Training Translation Loss 0.005155\n",
            "2025-07-10 14:10:53,747 Epoch 1378:\n",
            "\t\t\tTotal Training Recognition Loss 0.452006 || Total Training Translation Loss 0.004566\n",
            "2025-07-10 14:10:53,921 Epoch 1379:\n",
            "\t\t\tTotal Training Recognition Loss 0.640668 || Total Training Translation Loss 0.004236\n",
            "2025-07-10 14:10:54,096 Epoch 1380:\n",
            "\t\t\tTotal Training Recognition Loss 0.372152 || Total Training Translation Loss 0.004257\n",
            "2025-07-10 14:10:54,270 Epoch 1381:\n",
            "\t\t\tTotal Training Recognition Loss 0.664874 || Total Training Translation Loss 0.004755\n",
            "2025-07-10 14:10:54,443 Epoch 1382:\n",
            "\t\t\tTotal Training Recognition Loss 12.711112 || Total Training Translation Loss 0.004225\n",
            "2025-07-10 14:10:54,617 Epoch 1383:\n",
            "\t\t\tTotal Training Recognition Loss 17.435625 || Total Training Translation Loss 0.004163\n",
            "2025-07-10 14:10:54,790 Epoch 1384:\n",
            "\t\t\tTotal Training Recognition Loss 22.524906 || Total Training Translation Loss 0.003982\n",
            "2025-07-10 14:10:54,965 Epoch 1385:\n",
            "\t\t\tTotal Training Recognition Loss 0.668659 || Total Training Translation Loss 0.004116\n",
            "2025-07-10 14:10:55,137 Epoch 1386:\n",
            "\t\t\tTotal Training Recognition Loss 3.157099 || Total Training Translation Loss 0.004561\n",
            "2025-07-10 14:10:55,315 Epoch 1387:\n",
            "\t\t\tTotal Training Recognition Loss 0.912716 || Total Training Translation Loss 0.005076\n",
            "2025-07-10 14:10:55,488 Epoch 1388:\n",
            "\t\t\tTotal Training Recognition Loss 1.669090 || Total Training Translation Loss 0.004829\n",
            "2025-07-10 14:10:55,659 Epoch 1389:\n",
            "\t\t\tTotal Training Recognition Loss 3.765731 || Total Training Translation Loss 0.004798\n",
            "2025-07-10 14:10:55,832 Epoch 1390:\n",
            "\t\t\tTotal Training Recognition Loss 3.469949 || Total Training Translation Loss 0.004536\n",
            "2025-07-10 14:10:56,004 Epoch 1391:\n",
            "\t\t\tTotal Training Recognition Loss 5.334865 || Total Training Translation Loss 0.004451\n",
            "2025-07-10 14:10:56,177 Epoch 1392:\n",
            "\t\t\tTotal Training Recognition Loss 1.438101 || Total Training Translation Loss 0.004494\n",
            "2025-07-10 14:10:56,349 Epoch 1393:\n",
            "\t\t\tTotal Training Recognition Loss 2.384717 || Total Training Translation Loss 0.006014\n",
            "2025-07-10 14:10:56,521 Epoch 1394:\n",
            "\t\t\tTotal Training Recognition Loss 1.629933 || Total Training Translation Loss 0.005172\n",
            "2025-07-10 14:10:56,693 Epoch 1395:\n",
            "\t\t\tTotal Training Recognition Loss 2.383587 || Total Training Translation Loss 0.004515\n",
            "2025-07-10 14:10:56,867 Epoch 1396:\n",
            "\t\t\tTotal Training Recognition Loss 3.918804 || Total Training Translation Loss 0.004303\n",
            "2025-07-10 14:10:57,039 Epoch 1397:\n",
            "\t\t\tTotal Training Recognition Loss 0.998257 || Total Training Translation Loss 0.004464\n",
            "2025-07-10 14:10:57,215 Epoch 1398:\n",
            "\t\t\tTotal Training Recognition Loss 1.608863 || Total Training Translation Loss 0.003479\n",
            "2025-07-10 14:10:57,391 Epoch 1399:\n",
            "\t\t\tTotal Training Recognition Loss 6.113380 || Total Training Translation Loss 0.004539\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-07-10 14:10:57,565 [Epoch: 1400 Step: 00001400] Batch Recognition Loss:   2.288726 => Gls Tokens per Sec:      214 || Batch Translation Loss:   0.004411 => Txt Tokens per Sec:      579 || Lr: 0.001000\n",
            "2025-07-10 14:10:57,799 Validation result at epoch 1400, step     1400: duration: 0.2333s\n",
            "\tRecognition Beam Size: 1\tTranslation Beam Size: 1\tTranslation Beam Alpha: -1\n",
            "\tRecognition Loss: 6391.32715\tTranslation Loss: 229.72591\tPPL: 14.91933\n",
            "\tEval Metric: BLEU\n",
            "\tWER 100.00\t(DEL: 37.14,\tINS: 14.29,\tSUB: 48.57)\n",
            "\tBLEU-4 0.00\t(BLEU-1: 4.67,\tBLEU-2: 0.00,\tBLEU-3: 0.00,\tBLEU-4: 0.00)\n",
            "\tCHRF 20.33\tROUGE 6.89\tFID 0.00\n",
            "2025-07-10 14:10:57,800 Logging Recognition and Translation Outputs\n",
            "2025-07-10 14:10:57,800 ========================================================================================\n",
            "2025-07-10 14:10:57,800 Logging Sequence: 11August_2010_Wednesday_tagesschau-2\n",
            "2025-07-10 14:10:57,801 \tGloss Reference :\tDRUCK TIEF   KOMMEN  \n",
            "2025-07-10 14:10:57,801 \tGloss Hypothesis:\t***** ZWOELF NORDWEST\n",
            "2025-07-10 14:10:57,802 \tGloss Alignment :\tD     S      S       \n",
            "2025-07-10 14:10:57,802 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:10:57,803 \tText Reference  :\t********** ****** ** ***** ***** ***** ******* ***** **** *** ********* ********* ******** ********* ****** ******* *** ***** *** ****** ***** tiefer luftdruck bestimmt in    den   nächsten tagen unser wetter\n",
            "2025-07-10 14:10:57,804 \tText Hypothesis :\tnordwesten bleibt es heute nacht meist trocken sonst muss mit teilweise kräftigen schauern gerechnet werden örtlich mit blitz und donner <unk> <unk>  <unk>     <unk>    <unk> <unk> <unk>    <unk> <unk> <unk> \n",
            "2025-07-10 14:10:57,804 \tText Alignment  :\tI          I      I  I     I     I     I       I     I    I   I         I         I        I         I      I       I   I     I   I      I     S      S         S        S     S     S        S     S     S     \n",
            "2025-07-10 14:10:57,804 ========================================================================================\n",
            "2025-07-10 14:10:57,804 Logging Sequence: 11August_2010_Wednesday_tagesschau-3\n",
            "2025-07-10 14:10:57,804 \tGloss Reference :\tES-BEDEUTET VIEL WOLKE UND KOENNEN REGEN GEWITTER KOENNEN\n",
            "2025-07-10 14:10:57,805 \tGloss Hypothesis:\t*********** **** ***** *** ******* ***** ZWOELF   KOENNEN\n",
            "2025-07-10 14:10:57,805 \tGloss Alignment :\tD           D    D     D   D       D     S               \n",
            "2025-07-10 14:10:57,805 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:10:57,807 \tText Reference  :\tdas bedeutet viele wolken und ******** **** ****** **************** immer wieder ******* ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** zum   teil  kräftige schauer und   gewitter\n",
            "2025-07-10 14:10:57,807 \tText Hypothesis :\t*** es       auch  länger und ergiebig auch lokale überschwemmungen sind  wieder möglich <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>    <unk>   <unk> <unk>   \n",
            "2025-07-10 14:10:57,807 \tText Alignment  :\tD   S        S     S          I        I    I      I                S            I       I     I     I     I     I     I     I     I     I     I     I     I     I     S     S     S        S       S     S       \n",
            "2025-07-10 14:10:57,808 ========================================================================================\n",
            "2025-07-10 14:10:57,808 Logging Sequence: 11August_2010_Wednesday_tagesschau-8\n",
            "2025-07-10 14:10:57,809 \tGloss Reference :\t**** ******** ******* WIND    MAESSIG  SCHWACH  REGION WENN     GEWITTER WIND    KOENNEN ********\n",
            "2025-07-10 14:10:57,809 \tGloss Hypothesis:\tLOCH NORDWEST TROCKEN BLEIBEN SPEZIELL NORDWEST REGEN  NORDWEST KOENNEN  TROCKEN KOENNEN NORDWEST\n",
            "2025-07-10 14:10:57,809 \tGloss Alignment :\tI    I        I       S       S        S        S      S        S        S               I       \n",
            "2025-07-10 14:10:57,809 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:10:57,811 \tText Reference  :\t****** ** **** ****** *** ******** **** ****** **************** **** ****** ******* meist weht  nur   ein   schwacher wind  aus   unterschiedlichen richtungen der   bei   schauern und   gewittern stark böig  sein  kann \n",
            "2025-07-10 14:10:57,812 \tText Hypothesis :\tregnet es auch länger und ergiebig auch lokale überschwemmungen sind wieder möglich <unk> <unk> <unk> <unk> <unk>     <unk> <unk> <unk>             <unk>      <unk> <unk> <unk>    <unk> <unk>     <unk> <unk> <unk> <unk>\n",
            "2025-07-10 14:10:57,812 \tText Alignment  :\tI      I  I    I      I   I        I    I      I                I    I      I       S     S     S     S     S         S     S     S                 S          S     S     S        S     S         S     S     S     S    \n",
            "2025-07-10 14:10:57,812 ========================================================================================\n",
            "2025-07-10 14:10:57,812 Logging Sequence: 25October_2010_Monday_tagesschau-22\n",
            "2025-07-10 14:10:57,813 \tGloss Reference :\tMITTWOCH REGEN      KOENNEN NORDWEST WAHRSCHEINLICH NORD STARK WIND\n",
            "2025-07-10 14:10:57,813 \tGloss Hypothesis:\tFEBRUAR  DONNERSTAG KOENNEN ******** ************** **** ***** ****\n",
            "2025-07-10 14:10:57,813 \tGloss Alignment :\tS        S                  D        D              D    D     D   \n",
            "2025-07-10 14:10:57,813 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:10:57,815 \tText Reference  :\tam *** **** ** ********** *** **** ******** ******* mittwoch hier     und ** ******* ******** ****** ********* *********** ***** ***** da    nieselregen in    der   nordwesthälfte an    den   küsten kräftiger wind \n",
            "2025-07-10 14:10:57,815 \tText Hypothesis :\tam tag gibt es verbreitet zum teil kräftige schauer oder     gewitter und in manchen regionen fallen ergiebige regenmengen <unk> <unk> <unk> <unk>       <unk> <unk> <unk>          <unk> <unk> <unk>  <unk>     <unk>\n",
            "2025-07-10 14:10:57,815 \tText Alignment  :\t   I   I    I  I          I   I    I        I       S        S            I  I       I        I      I         I           I     I     S     S           S     S     S              S     S     S      S         S    \n",
            "2025-07-10 14:10:57,815 ========================================================================================\n",
            "2025-07-10 14:10:57,815 Logging Sequence: 05May_2011_Thursday_tagesschau-25\n",
            "2025-07-10 14:10:57,816 \tGloss Reference :\t****** JETZT WETTER WIE-AUSSEHEN MORGEN     FREITAG  SECHSTE MAI     ZEIGEN-BILDSCHIRM\n",
            "2025-07-10 14:10:57,816 \tGloss Hypothesis:\tZWOELF JETZT WETTER ************ DONNERSTAG NORDWEST ZWOELF  BLEIBEN FEBRUAR          \n",
            "2025-07-10 14:10:57,817 \tGloss Alignment :\tI                   D            S          S        S       S       S                \n",
            "2025-07-10 14:10:57,817 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:10:57,819 \tText Reference  :\t** *** **** ** ********** *** **** ******** ******* **** ******** und ** ******* ******** ****** ********* *********** ***** ***** ***** nun   die   wettervorhersage für   morgen freitag den   sechsten mai  \n",
            "2025-07-10 14:10:57,819 \tText Hypothesis :\tam tag gibt es verbreitet zum teil kräftige schauer oder gewitter und in manchen regionen fallen ergiebige regenmengen <unk> <unk> <unk> <unk> <unk> <unk>            <unk> <unk>  <unk>   <unk> <unk>    <unk>\n",
            "2025-07-10 14:10:57,819 \tText Alignment  :\tI  I   I    I  I          I   I    I        I       I    I            I  I       I        I      I         I           I     I     I     S     S     S                S     S      S       S     S        S    \n",
            "2025-07-10 14:10:57,819 ========================================================================================\n",
            "2025-07-10 14:10:57,820 Epoch 1400:\n",
            "\t\t\tTotal Training Recognition Loss 2.288726 || Total Training Translation Loss 0.004411\n",
            "2025-07-10 14:10:57,991 Epoch 1401:\n",
            "\t\t\tTotal Training Recognition Loss 0.917047 || Total Training Translation Loss 0.006020\n",
            "2025-07-10 14:10:58,164 Epoch 1402:\n",
            "\t\t\tTotal Training Recognition Loss 2.679722 || Total Training Translation Loss 0.004662\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-07-10 14:10:58,336 Epoch 1403:\n",
            "\t\t\tTotal Training Recognition Loss 1.263742 || Total Training Translation Loss 0.005728\n",
            "2025-07-10 14:10:58,509 Epoch 1404:\n",
            "\t\t\tTotal Training Recognition Loss 0.906381 || Total Training Translation Loss 0.005186\n",
            "2025-07-10 14:10:58,681 Epoch 1405:\n",
            "\t\t\tTotal Training Recognition Loss 3.270339 || Total Training Translation Loss 0.005296\n",
            "2025-07-10 14:10:58,853 Epoch 1406:\n",
            "\t\t\tTotal Training Recognition Loss 1.061525 || Total Training Translation Loss 0.004072\n",
            "2025-07-10 14:10:59,025 Epoch 1407:\n",
            "\t\t\tTotal Training Recognition Loss 0.492291 || Total Training Translation Loss 0.004670\n",
            "2025-07-10 14:10:59,197 Epoch 1408:\n",
            "\t\t\tTotal Training Recognition Loss 0.464686 || Total Training Translation Loss 0.005832\n",
            "2025-07-10 14:10:59,369 Epoch 1409:\n",
            "\t\t\tTotal Training Recognition Loss 0.714664 || Total Training Translation Loss 0.004258\n",
            "2025-07-10 14:10:59,543 Epoch 1410:\n",
            "\t\t\tTotal Training Recognition Loss 0.710792 || Total Training Translation Loss 0.004355\n",
            "2025-07-10 14:10:59,716 Epoch 1411:\n",
            "\t\t\tTotal Training Recognition Loss 1.152770 || Total Training Translation Loss 0.004631\n",
            "2025-07-10 14:10:59,887 Epoch 1412:\n",
            "\t\t\tTotal Training Recognition Loss 6.668658 || Total Training Translation Loss 0.004710\n",
            "2025-07-10 14:11:00,060 Epoch 1413:\n",
            "\t\t\tTotal Training Recognition Loss 0.713339 || Total Training Translation Loss 0.006739\n",
            "2025-07-10 14:11:00,232 Epoch 1414:\n",
            "\t\t\tTotal Training Recognition Loss 0.668019 || Total Training Translation Loss 0.005452\n",
            "2025-07-10 14:11:00,407 Epoch 1415:\n",
            "\t\t\tTotal Training Recognition Loss 1.570814 || Total Training Translation Loss 0.003986\n",
            "2025-07-10 14:11:00,579 Epoch 1416:\n",
            "\t\t\tTotal Training Recognition Loss 0.629842 || Total Training Translation Loss 0.004336\n",
            "2025-07-10 14:11:00,752 Epoch 1417:\n",
            "\t\t\tTotal Training Recognition Loss 1.085580 || Total Training Translation Loss 0.004052\n",
            "2025-07-10 14:11:00,924 Epoch 1418:\n",
            "\t\t\tTotal Training Recognition Loss 3.603955 || Total Training Translation Loss 0.005240\n",
            "2025-07-10 14:11:01,097 Epoch 1419:\n",
            "\t\t\tTotal Training Recognition Loss 0.360848 || Total Training Translation Loss 0.004316\n",
            "2025-07-10 14:11:01,269 Epoch 1420:\n",
            "\t\t\tTotal Training Recognition Loss 0.563757 || Total Training Translation Loss 0.005086\n",
            "2025-07-10 14:11:01,442 Epoch 1421:\n",
            "\t\t\tTotal Training Recognition Loss 1.001721 || Total Training Translation Loss 0.004384\n",
            "2025-07-10 14:11:01,613 Epoch 1422:\n",
            "\t\t\tTotal Training Recognition Loss 0.435875 || Total Training Translation Loss 0.004539\n",
            "2025-07-10 14:11:01,789 Epoch 1423:\n",
            "\t\t\tTotal Training Recognition Loss 1.251417 || Total Training Translation Loss 0.005010\n",
            "2025-07-10 14:11:01,966 Epoch 1424:\n",
            "\t\t\tTotal Training Recognition Loss 0.661947 || Total Training Translation Loss 0.004686\n",
            "2025-07-10 14:11:02,139 Epoch 1425:\n",
            "\t\t\tTotal Training Recognition Loss 0.391438 || Total Training Translation Loss 0.004327\n",
            "2025-07-10 14:11:02,311 Epoch 1426:\n",
            "\t\t\tTotal Training Recognition Loss 0.429863 || Total Training Translation Loss 0.005342\n",
            "2025-07-10 14:11:02,484 Epoch 1427:\n",
            "\t\t\tTotal Training Recognition Loss 0.380246 || Total Training Translation Loss 0.004164\n",
            "2025-07-10 14:11:02,656 Epoch 1428:\n",
            "\t\t\tTotal Training Recognition Loss 0.413043 || Total Training Translation Loss 0.004399\n",
            "2025-07-10 14:11:02,828 Epoch 1429:\n",
            "\t\t\tTotal Training Recognition Loss 0.327292 || Total Training Translation Loss 0.004074\n",
            "2025-07-10 14:11:03,001 Epoch 1430:\n",
            "\t\t\tTotal Training Recognition Loss 0.298449 || Total Training Translation Loss 0.004728\n",
            "2025-07-10 14:11:03,173 Epoch 1431:\n",
            "\t\t\tTotal Training Recognition Loss 0.591387 || Total Training Translation Loss 0.004513\n",
            "2025-07-10 14:11:03,345 Epoch 1432:\n",
            "\t\t\tTotal Training Recognition Loss 0.424549 || Total Training Translation Loss 0.004305\n",
            "2025-07-10 14:11:03,518 Epoch 1433:\n",
            "\t\t\tTotal Training Recognition Loss 0.338044 || Total Training Translation Loss 0.004881\n",
            "2025-07-10 14:11:03,689 Epoch 1434:\n",
            "\t\t\tTotal Training Recognition Loss 0.439012 || Total Training Translation Loss 0.004939\n",
            "2025-07-10 14:11:03,863 Epoch 1435:\n",
            "\t\t\tTotal Training Recognition Loss 0.313069 || Total Training Translation Loss 0.005348\n",
            "2025-07-10 14:11:04,037 Epoch 1436:\n",
            "\t\t\tTotal Training Recognition Loss 0.386139 || Total Training Translation Loss 0.004645\n",
            "2025-07-10 14:11:04,212 Epoch 1437:\n",
            "\t\t\tTotal Training Recognition Loss 0.305114 || Total Training Translation Loss 0.005072\n",
            "2025-07-10 14:11:04,385 Epoch 1438:\n",
            "\t\t\tTotal Training Recognition Loss 0.310373 || Total Training Translation Loss 0.004084\n",
            "2025-07-10 14:11:04,561 Epoch 1439:\n",
            "\t\t\tTotal Training Recognition Loss 0.530238 || Total Training Translation Loss 0.005079\n",
            "2025-07-10 14:11:04,733 Epoch 1440:\n",
            "\t\t\tTotal Training Recognition Loss 1.245889 || Total Training Translation Loss 0.003799\n",
            "2025-07-10 14:11:04,937 Epoch 1441:\n",
            "\t\t\tTotal Training Recognition Loss 0.415549 || Total Training Translation Loss 0.004621\n",
            "2025-07-10 14:11:05,153 Epoch 1442:\n",
            "\t\t\tTotal Training Recognition Loss 0.245493 || Total Training Translation Loss 0.004102\n",
            "2025-07-10 14:11:05,327 Epoch 1443:\n",
            "\t\t\tTotal Training Recognition Loss 0.239569 || Total Training Translation Loss 0.004127\n",
            "2025-07-10 14:11:05,547 Epoch 1444:\n",
            "\t\t\tTotal Training Recognition Loss 0.190164 || Total Training Translation Loss 0.005024\n",
            "2025-07-10 14:11:05,722 Epoch 1445:\n",
            "\t\t\tTotal Training Recognition Loss 0.250410 || Total Training Translation Loss 0.005012\n",
            "2025-07-10 14:11:05,894 Epoch 1446:\n",
            "\t\t\tTotal Training Recognition Loss 1.302373 || Total Training Translation Loss 0.006622\n",
            "2025-07-10 14:11:06,067 Epoch 1447:\n",
            "\t\t\tTotal Training Recognition Loss 0.325617 || Total Training Translation Loss 0.006057\n",
            "2025-07-10 14:11:06,241 Epoch 1448:\n",
            "\t\t\tTotal Training Recognition Loss 0.804360 || Total Training Translation Loss 0.004162\n",
            "2025-07-10 14:11:06,415 Epoch 1449:\n",
            "\t\t\tTotal Training Recognition Loss 0.184221 || Total Training Translation Loss 0.006356\n",
            "2025-07-10 14:11:06,590 Epoch 1450:\n",
            "\t\t\tTotal Training Recognition Loss 0.219354 || Total Training Translation Loss 0.004588\n",
            "2025-07-10 14:11:06,764 Epoch 1451:\n",
            "\t\t\tTotal Training Recognition Loss 0.247595 || Total Training Translation Loss 0.006856\n",
            "2025-07-10 14:11:06,938 Epoch 1452:\n",
            "\t\t\tTotal Training Recognition Loss 0.156349 || Total Training Translation Loss 0.004037\n",
            "2025-07-10 14:11:07,111 Epoch 1453:\n",
            "\t\t\tTotal Training Recognition Loss 2.160045 || Total Training Translation Loss 0.004527\n",
            "2025-07-10 14:11:07,285 Epoch 1454:\n",
            "\t\t\tTotal Training Recognition Loss 0.849594 || Total Training Translation Loss 0.004036\n",
            "2025-07-10 14:11:07,463 Epoch 1455:\n",
            "\t\t\tTotal Training Recognition Loss 0.195308 || Total Training Translation Loss 0.003976\n",
            "2025-07-10 14:11:07,636 Epoch 1456:\n",
            "\t\t\tTotal Training Recognition Loss 0.206363 || Total Training Translation Loss 0.008584\n",
            "2025-07-10 14:11:07,810 Epoch 1457:\n",
            "\t\t\tTotal Training Recognition Loss 0.296979 || Total Training Translation Loss 0.005323\n",
            "2025-07-10 14:11:07,985 Epoch 1458:\n",
            "\t\t\tTotal Training Recognition Loss 0.213768 || Total Training Translation Loss 0.004270\n",
            "2025-07-10 14:11:08,159 Epoch 1459:\n",
            "\t\t\tTotal Training Recognition Loss 0.139741 || Total Training Translation Loss 0.005512\n",
            "2025-07-10 14:11:08,333 Epoch 1460:\n",
            "\t\t\tTotal Training Recognition Loss 0.154261 || Total Training Translation Loss 0.005190\n",
            "2025-07-10 14:11:08,507 Epoch 1461:\n",
            "\t\t\tTotal Training Recognition Loss 0.244145 || Total Training Translation Loss 0.005250\n",
            "2025-07-10 14:11:08,727 Epoch 1462:\n",
            "\t\t\tTotal Training Recognition Loss 0.232509 || Total Training Translation Loss 0.004381\n",
            "2025-07-10 14:11:08,946 Epoch 1463:\n",
            "\t\t\tTotal Training Recognition Loss 1.827220 || Total Training Translation Loss 0.005192\n",
            "2025-07-10 14:11:09,165 Epoch 1464:\n",
            "\t\t\tTotal Training Recognition Loss 0.802612 || Total Training Translation Loss 0.004988\n",
            "2025-07-10 14:11:09,384 Epoch 1465:\n",
            "\t\t\tTotal Training Recognition Loss 0.204181 || Total Training Translation Loss 0.004483\n",
            "2025-07-10 14:11:09,604 Epoch 1466:\n",
            "\t\t\tTotal Training Recognition Loss 0.197794 || Total Training Translation Loss 0.004828\n",
            "2025-07-10 14:11:09,823 Epoch 1467:\n",
            "\t\t\tTotal Training Recognition Loss 0.155919 || Total Training Translation Loss 0.005145\n",
            "2025-07-10 14:11:10,044 Epoch 1468:\n",
            "\t\t\tTotal Training Recognition Loss 0.153116 || Total Training Translation Loss 0.004751\n",
            "2025-07-10 14:11:10,264 Epoch 1469:\n",
            "\t\t\tTotal Training Recognition Loss 0.428697 || Total Training Translation Loss 0.005405\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-07-10 14:11:10,482 Epoch 1470:\n",
            "\t\t\tTotal Training Recognition Loss 0.179202 || Total Training Translation Loss 0.004830\n",
            "2025-07-10 14:11:10,662 Epoch 1471:\n",
            "\t\t\tTotal Training Recognition Loss 0.164346 || Total Training Translation Loss 0.004486\n",
            "2025-07-10 14:11:10,835 Epoch 1472:\n",
            "\t\t\tTotal Training Recognition Loss 0.238473 || Total Training Translation Loss 0.005059\n",
            "2025-07-10 14:11:11,007 Epoch 1473:\n",
            "\t\t\tTotal Training Recognition Loss 0.202207 || Total Training Translation Loss 0.004508\n",
            "2025-07-10 14:11:11,180 Epoch 1474:\n",
            "\t\t\tTotal Training Recognition Loss 0.154596 || Total Training Translation Loss 0.004666\n",
            "2025-07-10 14:11:11,352 Epoch 1475:\n",
            "\t\t\tTotal Training Recognition Loss 0.199923 || Total Training Translation Loss 0.004399\n",
            "2025-07-10 14:11:11,523 Epoch 1476:\n",
            "\t\t\tTotal Training Recognition Loss 0.141517 || Total Training Translation Loss 0.005147\n",
            "2025-07-10 14:11:11,696 Epoch 1477:\n",
            "\t\t\tTotal Training Recognition Loss 28.515478 || Total Training Translation Loss 0.004368\n",
            "2025-07-10 14:11:11,869 Epoch 1478:\n",
            "\t\t\tTotal Training Recognition Loss 0.269297 || Total Training Translation Loss 0.005727\n",
            "2025-07-10 14:11:12,041 Epoch 1479:\n",
            "\t\t\tTotal Training Recognition Loss 0.705003 || Total Training Translation Loss 0.004184\n",
            "2025-07-10 14:11:12,215 Epoch 1480:\n",
            "\t\t\tTotal Training Recognition Loss 0.194132 || Total Training Translation Loss 0.005204\n",
            "2025-07-10 14:11:12,390 Epoch 1481:\n",
            "\t\t\tTotal Training Recognition Loss 0.212360 || Total Training Translation Loss 0.006572\n",
            "2025-07-10 14:11:12,566 Epoch 1482:\n",
            "\t\t\tTotal Training Recognition Loss 0.161523 || Total Training Translation Loss 0.004608\n",
            "2025-07-10 14:11:12,739 Epoch 1483:\n",
            "\t\t\tTotal Training Recognition Loss 0.679141 || Total Training Translation Loss 0.006399\n",
            "2025-07-10 14:11:12,913 Epoch 1484:\n",
            "\t\t\tTotal Training Recognition Loss 4.149733 || Total Training Translation Loss 0.004709\n",
            "2025-07-10 14:11:13,088 Epoch 1485:\n",
            "\t\t\tTotal Training Recognition Loss 0.329193 || Total Training Translation Loss 0.004366\n",
            "2025-07-10 14:11:13,261 Epoch 1486:\n",
            "\t\t\tTotal Training Recognition Loss 0.496992 || Total Training Translation Loss 0.005031\n",
            "2025-07-10 14:11:13,434 Epoch 1487:\n",
            "\t\t\tTotal Training Recognition Loss 0.287440 || Total Training Translation Loss 0.004424\n",
            "2025-07-10 14:11:13,606 Epoch 1488:\n",
            "\t\t\tTotal Training Recognition Loss 0.928434 || Total Training Translation Loss 0.004626\n",
            "2025-07-10 14:11:13,783 Epoch 1489:\n",
            "\t\t\tTotal Training Recognition Loss 0.277595 || Total Training Translation Loss 0.005483\n",
            "2025-07-10 14:11:13,962 Epoch 1490:\n",
            "\t\t\tTotal Training Recognition Loss 0.412786 || Total Training Translation Loss 0.005949\n",
            "2025-07-10 14:11:14,137 Epoch 1491:\n",
            "\t\t\tTotal Training Recognition Loss 0.323493 || Total Training Translation Loss 0.004659\n",
            "2025-07-10 14:11:14,315 Epoch 1492:\n",
            "\t\t\tTotal Training Recognition Loss 0.688367 || Total Training Translation Loss 0.004798\n",
            "2025-07-10 14:11:14,490 Epoch 1493:\n",
            "\t\t\tTotal Training Recognition Loss 0.401496 || Total Training Translation Loss 0.004332\n",
            "2025-07-10 14:11:14,664 Epoch 1494:\n",
            "\t\t\tTotal Training Recognition Loss 0.379306 || Total Training Translation Loss 0.004736\n",
            "2025-07-10 14:11:14,837 Epoch 1495:\n",
            "\t\t\tTotal Training Recognition Loss 0.774437 || Total Training Translation Loss 0.005161\n",
            "2025-07-10 14:11:15,016 Epoch 1496:\n",
            "\t\t\tTotal Training Recognition Loss 0.413852 || Total Training Translation Loss 0.004764\n",
            "2025-07-10 14:11:15,190 Epoch 1497:\n",
            "\t\t\tTotal Training Recognition Loss 0.442218 || Total Training Translation Loss 0.005622\n",
            "2025-07-10 14:11:15,363 Epoch 1498:\n",
            "\t\t\tTotal Training Recognition Loss 8.500080 || Total Training Translation Loss 0.004985\n",
            "2025-07-10 14:11:15,536 Epoch 1499:\n",
            "\t\t\tTotal Training Recognition Loss 0.376922 || Total Training Translation Loss 0.005954\n",
            "2025-07-10 14:11:15,708 [Epoch: 1500 Step: 00001500] Batch Recognition Loss:   0.459034 => Gls Tokens per Sec:      216 || Batch Translation Loss:   0.005630 => Txt Tokens per Sec:      585 || Lr: 0.001000\n",
            "2025-07-10 14:11:15,945 Validation result at epoch 1500, step     1500: duration: 0.2363s\n",
            "\tRecognition Beam Size: 1\tTranslation Beam Size: 1\tTranslation Beam Alpha: -1\n",
            "\tRecognition Loss: 6465.78857\tTranslation Loss: 228.01285\tPPL: 14.62166\n",
            "\tEval Metric: BLEU\n",
            "\tWER 91.43\t(DEL: 28.57,\tINS: 2.86,\tSUB: 60.00)\n",
            "\tBLEU-4 2.76\t(BLEU-1: 6.67,\tBLEU-2: 4.29,\tBLEU-3: 3.40,\tBLEU-4: 2.76)\n",
            "\tCHRF 23.24\tROUGE 10.44\tFID 0.00\n",
            "2025-07-10 14:11:15,946 Logging Recognition and Translation Outputs\n",
            "2025-07-10 14:11:15,946 ========================================================================================\n",
            "2025-07-10 14:11:15,946 Logging Sequence: 11August_2010_Wednesday_tagesschau-2\n",
            "2025-07-10 14:11:15,947 \tGloss Reference :\tDRUCK TIEF     KOMMEN\n",
            "2025-07-10 14:11:15,947 \tGloss Hypothesis:\t***** NORDWEST LOCH  \n",
            "2025-07-10 14:11:15,947 \tGloss Alignment :\tD     S        S     \n",
            "2025-07-10 14:11:15,947 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:11:15,949 \tText Reference  :\t********** ****** ** ***** ***** ***** ******* ***** **** *** ********* ********* ******** ********* ****** ******* *** ***** *** ****** ***** tiefer luftdruck bestimmt in    den   nächsten tagen unser wetter\n",
            "2025-07-10 14:11:15,949 \tText Hypothesis :\tnordwesten bleibt es heute nacht meist trocken sonst muss mit teilweise kräftigen schauern gerechnet werden örtlich mit blitz und donner <unk> <unk>  <unk>     <unk>    <unk> <unk> <unk>    <unk> <unk> <unk> \n",
            "2025-07-10 14:11:15,949 \tText Alignment  :\tI          I      I  I     I     I     I       I     I    I   I         I         I        I         I      I       I   I     I   I      I     S      S         S        S     S     S        S     S     S     \n",
            "2025-07-10 14:11:15,950 ========================================================================================\n",
            "2025-07-10 14:11:15,950 Logging Sequence: 11August_2010_Wednesday_tagesschau-3\n",
            "2025-07-10 14:11:15,950 \tGloss Reference :\tES-BEDEUTET VIEL WOLKE UND   KOENNEN REGEN GEWITTER KOENNEN\n",
            "2025-07-10 14:11:15,950 \tGloss Hypothesis:\t*********** **** ***** HEUTE KOENNEN ***** GEWITTER *******\n",
            "2025-07-10 14:11:15,951 \tGloss Alignment :\tD           D    D     S             D              D      \n",
            "2025-07-10 14:11:15,951 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:11:15,955 \tText Reference  :\tdas bedeutet viele wolken und ******** **** ****** **************** immer wieder ******* ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** zum   teil  kräftige schauer und   gewitter\n",
            "2025-07-10 14:11:15,955 \tText Hypothesis :\t*** es       auch  länger und ergiebig auch lokale überschwemmungen sind  wieder möglich <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>    <unk>   <unk> <unk>   \n",
            "2025-07-10 14:11:15,955 \tText Alignment  :\tD   S        S     S          I        I    I      I                S            I       I     I     I     I     I     I     I     I     I     I     I     I     I     S     S     S        S       S     S       \n",
            "2025-07-10 14:11:15,956 ========================================================================================\n",
            "2025-07-10 14:11:15,956 Logging Sequence: 11August_2010_Wednesday_tagesschau-8\n",
            "2025-07-10 14:11:15,957 \tGloss Reference :\tWIND MAESSIG SCHWACH REGION  WENN  GEWITTER WIND    KOENNEN \n",
            "2025-07-10 14:11:15,957 \tGloss Hypothesis:\t**** LOCH    TROCKEN BLEIBEN REGEN NORDWEST TROCKEN NORDWEST\n",
            "2025-07-10 14:11:15,958 \tGloss Alignment :\tD    S       S       S       S     S        S       S       \n",
            "2025-07-10 14:11:15,958 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:11:15,960 \tText Reference  :\t****** ** **** ****** *** ******** **** ****** **************** **** ****** ******* meist weht  nur   ein   schwacher wind  aus   unterschiedlichen richtungen der   bei   schauern und   gewittern stark böig  sein  kann \n",
            "2025-07-10 14:11:15,961 \tText Hypothesis :\tregnet es auch länger und ergiebig auch lokale überschwemmungen sind wieder möglich <unk> <unk> <unk> <unk> <unk>     <unk> <unk> <unk>             <unk>      <unk> <unk> <unk>    <unk> <unk>     <unk> <unk> <unk> <unk>\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-07-10 14:11:15,962 \tText Alignment  :\tI      I  I    I      I   I        I    I      I                I    I      I       S     S     S     S     S         S     S     S                 S          S     S     S        S     S         S     S     S     S    \n",
            "2025-07-10 14:11:15,962 ========================================================================================\n",
            "2025-07-10 14:11:15,962 Logging Sequence: 25October_2010_Monday_tagesschau-22\n",
            "2025-07-10 14:11:15,963 \tGloss Reference :\tMITTWOCH REGEN KOENNEN NORDWEST        WAHRSCHEINLICH NORD STARK    WIND   \n",
            "2025-07-10 14:11:15,963 \tGloss Hypothesis:\t******** ORT   FEBRUAR UEBERSCHWEMMUNG GEWITTER       DAZU GEWITTER KOENNEN\n",
            "2025-07-10 14:11:15,963 \tGloss Alignment :\tD        S     S       S               S              S    S        S      \n",
            "2025-07-10 14:11:15,963 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:11:15,965 \tText Reference  :\tam mittwoch hier   und ******** **** ****** **************** **** ****** ******* ***** ***** ***** ***** ***** ***** ***** ***** ***** da    nieselregen in    der   nordwesthälfte an    den   küsten kräftiger wind \n",
            "2025-07-10 14:11:15,965 \tText Hypothesis :\tes auch     länger und ergiebig auch lokale überschwemmungen sind wieder möglich <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>       <unk> <unk> <unk>          <unk> <unk> <unk>  <unk>     <unk>\n",
            "2025-07-10 14:11:15,965 \tText Alignment  :\tS  S        S          I        I    I      I                I    I      I       I     I     I     I     I     I     I     I     I     S     S           S     S     S              S     S     S      S         S    \n",
            "2025-07-10 14:11:15,966 ========================================================================================\n",
            "2025-07-10 14:11:15,966 Logging Sequence: 05May_2011_Thursday_tagesschau-25\n",
            "2025-07-10 14:11:15,966 \tGloss Reference :\t****** JETZT WETTER WIE-AUSSEHEN MORGEN FREITAG SECHSTE    MAI    ZEIGEN-BILDSCHIRM\n",
            "2025-07-10 14:11:15,966 \tGloss Hypothesis:\tZWOELF JETZT WETTER ************ ****** ORT     DONNERSTAG ZWOELF BLEIBEN          \n",
            "2025-07-10 14:11:15,967 \tGloss Alignment :\tI                   D            D      S       S          S      S                \n",
            "2025-07-10 14:11:15,968 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:11:15,969 \tText Reference  :\tund nun die wettervorhersage für morgen freitag    den ******** ****** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** sechsten mai  \n",
            "2025-07-10 14:11:15,969 \tText Hypothesis :\t*** nun die wettervorhersage für morgen donnerstag den zwölften august <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>    <unk>\n",
            "2025-07-10 14:11:15,970 \tText Alignment  :\tD                                       S              I        I      I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     S        S    \n",
            "2025-07-10 14:11:15,970 ========================================================================================\n",
            "2025-07-10 14:11:15,971 Epoch 1500:\n",
            "\t\t\tTotal Training Recognition Loss 0.459034 || Total Training Translation Loss 0.005630\n",
            "2025-07-10 14:11:16,186 Epoch 1501:\n",
            "\t\t\tTotal Training Recognition Loss 0.230059 || Total Training Translation Loss 0.004052\n",
            "2025-07-10 14:11:16,404 Epoch 1502:\n",
            "\t\t\tTotal Training Recognition Loss 1.606740 || Total Training Translation Loss 0.005405\n",
            "2025-07-10 14:11:16,623 Epoch 1503:\n",
            "\t\t\tTotal Training Recognition Loss 0.166546 || Total Training Translation Loss 0.004691\n",
            "2025-07-10 14:11:16,796 Epoch 1504:\n",
            "\t\t\tTotal Training Recognition Loss 0.197798 || Total Training Translation Loss 0.005023\n",
            "2025-07-10 14:11:16,966 Epoch 1505:\n",
            "\t\t\tTotal Training Recognition Loss 0.136826 || Total Training Translation Loss 0.005041\n",
            "2025-07-10 14:11:17,137 Epoch 1506:\n",
            "\t\t\tTotal Training Recognition Loss 0.464484 || Total Training Translation Loss 0.005065\n",
            "2025-07-10 14:11:17,310 Epoch 1507:\n",
            "\t\t\tTotal Training Recognition Loss 0.561743 || Total Training Translation Loss 0.006166\n",
            "2025-07-10 14:11:17,522 Epoch 1508:\n",
            "\t\t\tTotal Training Recognition Loss 0.392312 || Total Training Translation Loss 0.006661\n",
            "2025-07-10 14:11:17,699 Epoch 1509:\n",
            "\t\t\tTotal Training Recognition Loss 0.716891 || Total Training Translation Loss 0.005376\n",
            "2025-07-10 14:11:17,873 Epoch 1510:\n",
            "\t\t\tTotal Training Recognition Loss 0.387332 || Total Training Translation Loss 0.005732\n",
            "2025-07-10 14:11:18,047 Epoch 1511:\n",
            "\t\t\tTotal Training Recognition Loss 53.664822 || Total Training Translation Loss 0.005763\n",
            "2025-07-10 14:11:18,224 Epoch 1512:\n",
            "\t\t\tTotal Training Recognition Loss 0.241023 || Total Training Translation Loss 0.004998\n",
            "2025-07-10 14:11:18,398 Epoch 1513:\n",
            "\t\t\tTotal Training Recognition Loss 1.879946 || Total Training Translation Loss 0.005522\n",
            "2025-07-10 14:11:18,572 Epoch 1514:\n",
            "\t\t\tTotal Training Recognition Loss 0.695531 || Total Training Translation Loss 0.004895\n",
            "2025-07-10 14:11:18,745 Epoch 1515:\n",
            "\t\t\tTotal Training Recognition Loss 3.631941 || Total Training Translation Loss 0.004851\n",
            "2025-07-10 14:11:18,919 Epoch 1516:\n",
            "\t\t\tTotal Training Recognition Loss 1.187778 || Total Training Translation Loss 0.006098\n",
            "2025-07-10 14:11:19,093 Epoch 1517:\n",
            "\t\t\tTotal Training Recognition Loss 5.262109 || Total Training Translation Loss 0.005553\n",
            "2025-07-10 14:11:19,267 Epoch 1518:\n",
            "\t\t\tTotal Training Recognition Loss 1.438763 || Total Training Translation Loss 0.007710\n",
            "2025-07-10 14:11:19,441 Epoch 1519:\n",
            "\t\t\tTotal Training Recognition Loss 5.547064 || Total Training Translation Loss 0.004711\n",
            "2025-07-10 14:11:19,614 Epoch 1520:\n",
            "\t\t\tTotal Training Recognition Loss 0.796217 || Total Training Translation Loss 0.006360\n",
            "2025-07-10 14:11:19,788 Epoch 1521:\n",
            "\t\t\tTotal Training Recognition Loss 1.025670 || Total Training Translation Loss 0.005632\n",
            "2025-07-10 14:11:19,961 Epoch 1522:\n",
            "\t\t\tTotal Training Recognition Loss 0.607781 || Total Training Translation Loss 0.006548\n",
            "2025-07-10 14:11:20,134 Epoch 1523:\n",
            "\t\t\tTotal Training Recognition Loss 0.803170 || Total Training Translation Loss 0.005869\n",
            "2025-07-10 14:11:20,307 Epoch 1524:\n",
            "\t\t\tTotal Training Recognition Loss 1.028890 || Total Training Translation Loss 0.005993\n",
            "2025-07-10 14:11:20,480 Epoch 1525:\n",
            "\t\t\tTotal Training Recognition Loss 0.409910 || Total Training Translation Loss 0.005938\n",
            "2025-07-10 14:11:20,653 Epoch 1526:\n",
            "\t\t\tTotal Training Recognition Loss 1.373759 || Total Training Translation Loss 0.005573\n",
            "2025-07-10 14:11:20,826 Epoch 1527:\n",
            "\t\t\tTotal Training Recognition Loss 0.325002 || Total Training Translation Loss 0.008504\n",
            "2025-07-10 14:11:21,000 Epoch 1528:\n",
            "\t\t\tTotal Training Recognition Loss 0.445496 || Total Training Translation Loss 0.008582\n",
            "2025-07-10 14:11:21,173 Epoch 1529:\n",
            "\t\t\tTotal Training Recognition Loss 0.389555 || Total Training Translation Loss 0.007438\n",
            "2025-07-10 14:11:21,346 Epoch 1530:\n",
            "\t\t\tTotal Training Recognition Loss 1.105612 || Total Training Translation Loss 0.005736\n",
            "2025-07-10 14:11:21,520 Epoch 1531:\n",
            "\t\t\tTotal Training Recognition Loss 19.312603 || Total Training Translation Loss 0.006398\n",
            "2025-07-10 14:11:21,693 Epoch 1532:\n",
            "\t\t\tTotal Training Recognition Loss 0.393684 || Total Training Translation Loss 0.005903\n",
            "2025-07-10 14:11:21,867 Epoch 1533:\n",
            "\t\t\tTotal Training Recognition Loss 0.963232 || Total Training Translation Loss 0.005840\n",
            "2025-07-10 14:11:22,043 Epoch 1534:\n",
            "\t\t\tTotal Training Recognition Loss 0.229622 || Total Training Translation Loss 0.006580\n",
            "2025-07-10 14:11:22,216 Epoch 1535:\n",
            "\t\t\tTotal Training Recognition Loss 0.327533 || Total Training Translation Loss 0.005307\n",
            "2025-07-10 14:11:22,389 Epoch 1536:\n",
            "\t\t\tTotal Training Recognition Loss 0.344159 || Total Training Translation Loss 0.006425\n",
            "2025-07-10 14:11:22,562 Epoch 1537:\n",
            "\t\t\tTotal Training Recognition Loss 0.356614 || Total Training Translation Loss 0.006289\n",
            "2025-07-10 14:11:22,735 Epoch 1538:\n",
            "\t\t\tTotal Training Recognition Loss 0.282604 || Total Training Translation Loss 0.006049\n",
            "2025-07-10 14:11:22,908 Epoch 1539:\n",
            "\t\t\tTotal Training Recognition Loss 0.292735 || Total Training Translation Loss 0.006093\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-07-10 14:11:23,081 Epoch 1540:\n",
            "\t\t\tTotal Training Recognition Loss 0.252458 || Total Training Translation Loss 0.007286\n",
            "2025-07-10 14:11:23,254 Epoch 1541:\n",
            "\t\t\tTotal Training Recognition Loss 0.925346 || Total Training Translation Loss 0.004831\n",
            "2025-07-10 14:11:23,427 Epoch 1542:\n",
            "\t\t\tTotal Training Recognition Loss 0.973137 || Total Training Translation Loss 0.005187\n",
            "2025-07-10 14:11:23,601 Epoch 1543:\n",
            "\t\t\tTotal Training Recognition Loss 1.079030 || Total Training Translation Loss 0.005809\n",
            "2025-07-10 14:11:23,774 Epoch 1544:\n",
            "\t\t\tTotal Training Recognition Loss 0.705066 || Total Training Translation Loss 0.005720\n",
            "2025-07-10 14:11:23,947 Epoch 1545:\n",
            "\t\t\tTotal Training Recognition Loss 0.349830 || Total Training Translation Loss 0.006226\n",
            "2025-07-10 14:11:24,123 Epoch 1546:\n",
            "\t\t\tTotal Training Recognition Loss 1.180482 || Total Training Translation Loss 0.004940\n",
            "2025-07-10 14:11:24,298 Epoch 1547:\n",
            "\t\t\tTotal Training Recognition Loss 2.425471 || Total Training Translation Loss 0.008399\n",
            "2025-07-10 14:11:24,471 Epoch 1548:\n",
            "\t\t\tTotal Training Recognition Loss 2.851749 || Total Training Translation Loss 0.005075\n",
            "2025-07-10 14:11:24,644 Epoch 1549:\n",
            "\t\t\tTotal Training Recognition Loss 0.251918 || Total Training Translation Loss 0.006369\n",
            "2025-07-10 14:11:24,817 Epoch 1550:\n",
            "\t\t\tTotal Training Recognition Loss 0.216898 || Total Training Translation Loss 0.005101\n",
            "2025-07-10 14:11:24,990 Epoch 1551:\n",
            "\t\t\tTotal Training Recognition Loss 0.280795 || Total Training Translation Loss 0.005775\n",
            "2025-07-10 14:11:25,162 Epoch 1552:\n",
            "\t\t\tTotal Training Recognition Loss 0.203362 || Total Training Translation Loss 0.005540\n",
            "2025-07-10 14:11:25,336 Epoch 1553:\n",
            "\t\t\tTotal Training Recognition Loss 1.280320 || Total Training Translation Loss 0.004460\n",
            "2025-07-10 14:11:25,554 Epoch 1554:\n",
            "\t\t\tTotal Training Recognition Loss 0.246606 || Total Training Translation Loss 0.005321\n",
            "2025-07-10 14:11:25,732 Epoch 1555:\n",
            "\t\t\tTotal Training Recognition Loss 0.847078 || Total Training Translation Loss 0.006039\n",
            "2025-07-10 14:11:25,905 Epoch 1556:\n",
            "\t\t\tTotal Training Recognition Loss 0.263383 || Total Training Translation Loss 0.005442\n",
            "2025-07-10 14:11:26,079 Epoch 1557:\n",
            "\t\t\tTotal Training Recognition Loss 1.011189 || Total Training Translation Loss 0.005441\n",
            "2025-07-10 14:11:26,251 Epoch 1558:\n",
            "\t\t\tTotal Training Recognition Loss 0.488812 || Total Training Translation Loss 0.004981\n",
            "2025-07-10 14:11:26,422 Epoch 1559:\n",
            "\t\t\tTotal Training Recognition Loss 71.172867 || Total Training Translation Loss 0.004615\n",
            "2025-07-10 14:11:26,594 Epoch 1560:\n",
            "\t\t\tTotal Training Recognition Loss 1.392058 || Total Training Translation Loss 0.005212\n",
            "2025-07-10 14:11:26,765 Epoch 1561:\n",
            "\t\t\tTotal Training Recognition Loss 0.135511 || Total Training Translation Loss 0.004435\n",
            "2025-07-10 14:11:26,939 Epoch 1562:\n",
            "\t\t\tTotal Training Recognition Loss 0.307056 || Total Training Translation Loss 0.005852\n",
            "2025-07-10 14:11:27,111 Epoch 1563:\n",
            "\t\t\tTotal Training Recognition Loss 2.049623 || Total Training Translation Loss 0.006170\n",
            "2025-07-10 14:11:27,282 Epoch 1564:\n",
            "\t\t\tTotal Training Recognition Loss 2.697698 || Total Training Translation Loss 0.005160\n",
            "2025-07-10 14:11:27,453 Epoch 1565:\n",
            "\t\t\tTotal Training Recognition Loss 53.482445 || Total Training Translation Loss 0.007079\n",
            "2025-07-10 14:11:27,626 Epoch 1566:\n",
            "\t\t\tTotal Training Recognition Loss 0.501688 || Total Training Translation Loss 0.004847\n",
            "2025-07-10 14:11:27,799 Epoch 1567:\n",
            "\t\t\tTotal Training Recognition Loss 0.486657 || Total Training Translation Loss 0.006196\n",
            "2025-07-10 14:11:27,972 Epoch 1568:\n",
            "\t\t\tTotal Training Recognition Loss 0.863464 || Total Training Translation Loss 0.004570\n",
            "2025-07-10 14:11:28,146 Epoch 1569:\n",
            "\t\t\tTotal Training Recognition Loss 0.808796 || Total Training Translation Loss 0.006002\n",
            "2025-07-10 14:11:28,319 Epoch 1570:\n",
            "\t\t\tTotal Training Recognition Loss 1.943533 || Total Training Translation Loss 0.006925\n",
            "2025-07-10 14:11:28,491 Epoch 1571:\n",
            "\t\t\tTotal Training Recognition Loss 1.228664 || Total Training Translation Loss 0.005735\n",
            "2025-07-10 14:11:28,663 Epoch 1572:\n",
            "\t\t\tTotal Training Recognition Loss 4.985600 || Total Training Translation Loss 0.005453\n",
            "2025-07-10 14:11:28,834 Epoch 1573:\n",
            "\t\t\tTotal Training Recognition Loss 2.481440 || Total Training Translation Loss 0.005245\n",
            "2025-07-10 14:11:29,009 Epoch 1574:\n",
            "\t\t\tTotal Training Recognition Loss 1.522535 || Total Training Translation Loss 0.004929\n",
            "2025-07-10 14:11:29,183 Epoch 1575:\n",
            "\t\t\tTotal Training Recognition Loss 1.973049 || Total Training Translation Loss 0.006123\n",
            "2025-07-10 14:11:29,359 Epoch 1576:\n",
            "\t\t\tTotal Training Recognition Loss 0.670764 || Total Training Translation Loss 0.006162\n",
            "2025-07-10 14:11:29,531 Epoch 1577:\n",
            "\t\t\tTotal Training Recognition Loss 0.731488 || Total Training Translation Loss 0.004659\n",
            "2025-07-10 14:11:29,704 Epoch 1578:\n",
            "\t\t\tTotal Training Recognition Loss 0.621153 || Total Training Translation Loss 0.005336\n",
            "2025-07-10 14:11:29,878 Epoch 1579:\n",
            "\t\t\tTotal Training Recognition Loss 2.586735 || Total Training Translation Loss 0.004844\n",
            "2025-07-10 14:11:30,052 Epoch 1580:\n",
            "\t\t\tTotal Training Recognition Loss 4.301955 || Total Training Translation Loss 0.005439\n",
            "2025-07-10 14:11:30,225 Epoch 1581:\n",
            "\t\t\tTotal Training Recognition Loss 1.393851 || Total Training Translation Loss 0.005835\n",
            "2025-07-10 14:11:30,398 Epoch 1582:\n",
            "\t\t\tTotal Training Recognition Loss 0.936439 || Total Training Translation Loss 0.005256\n",
            "2025-07-10 14:11:30,571 Epoch 1583:\n",
            "\t\t\tTotal Training Recognition Loss 0.469129 || Total Training Translation Loss 0.005704\n",
            "2025-07-10 14:11:30,744 Epoch 1584:\n",
            "\t\t\tTotal Training Recognition Loss 0.620473 || Total Training Translation Loss 0.005560\n",
            "2025-07-10 14:11:30,917 Epoch 1585:\n",
            "\t\t\tTotal Training Recognition Loss 6.460199 || Total Training Translation Loss 0.006010\n",
            "2025-07-10 14:11:31,090 Epoch 1586:\n",
            "\t\t\tTotal Training Recognition Loss 0.942246 || Total Training Translation Loss 0.006737\n",
            "2025-07-10 14:11:31,308 Epoch 1587:\n",
            "\t\t\tTotal Training Recognition Loss 0.512532 || Total Training Translation Loss 0.004671\n",
            "2025-07-10 14:11:31,528 Epoch 1588:\n",
            "\t\t\tTotal Training Recognition Loss 0.899276 || Total Training Translation Loss 0.007124\n",
            "2025-07-10 14:11:31,747 Epoch 1589:\n",
            "\t\t\tTotal Training Recognition Loss 0.673223 || Total Training Translation Loss 0.005729\n",
            "2025-07-10 14:11:31,959 Epoch 1590:\n",
            "\t\t\tTotal Training Recognition Loss 0.939665 || Total Training Translation Loss 0.004297\n",
            "2025-07-10 14:11:32,183 Epoch 1591:\n",
            "\t\t\tTotal Training Recognition Loss 4.601428 || Total Training Translation Loss 0.006073\n",
            "2025-07-10 14:11:32,369 Epoch 1592:\n",
            "\t\t\tTotal Training Recognition Loss 0.340964 || Total Training Translation Loss 0.006364\n",
            "2025-07-10 14:11:32,542 Epoch 1593:\n",
            "\t\t\tTotal Training Recognition Loss 0.319061 || Total Training Translation Loss 0.005770\n",
            "2025-07-10 14:11:32,715 Epoch 1594:\n",
            "\t\t\tTotal Training Recognition Loss 1.228418 || Total Training Translation Loss 0.005685\n",
            "2025-07-10 14:11:32,889 Epoch 1595:\n",
            "\t\t\tTotal Training Recognition Loss 0.359219 || Total Training Translation Loss 0.005062\n",
            "2025-07-10 14:11:33,064 Epoch 1596:\n",
            "\t\t\tTotal Training Recognition Loss 0.613560 || Total Training Translation Loss 0.006122\n",
            "2025-07-10 14:11:33,236 Epoch 1597:\n",
            "\t\t\tTotal Training Recognition Loss 17.081182 || Total Training Translation Loss 0.006478\n",
            "2025-07-10 14:11:33,410 Epoch 1598:\n",
            "\t\t\tTotal Training Recognition Loss 0.916164 || Total Training Translation Loss 0.005727\n",
            "2025-07-10 14:11:33,583 Epoch 1599:\n",
            "\t\t\tTotal Training Recognition Loss 0.666911 || Total Training Translation Loss 0.005626\n",
            "2025-07-10 14:11:33,756 [Epoch: 1600 Step: 00001600] Batch Recognition Loss:   0.940150 => Gls Tokens per Sec:      215 || Batch Translation Loss:   0.005052 => Txt Tokens per Sec:      582 || Lr: 0.001000\n",
            "2025-07-10 14:11:33,999 Validation result at epoch 1600, step     1600: duration: 0.2425s\n",
            "\tRecognition Beam Size: 1\tTranslation Beam Size: 1\tTranslation Beam Alpha: -1\n",
            "\tRecognition Loss: 11442.28516\tTranslation Loss: 226.57838\tPPL: 14.37698\n",
            "\tEval Metric: BLEU\n",
            "\tWER 120.00\t(DEL: 17.14,\tINS: 34.29,\tSUB: 68.57)\n",
            "\tBLEU-4 2.76\t(BLEU-1: 6.67,\tBLEU-2: 4.29,\tBLEU-3: 3.40,\tBLEU-4: 2.76)\n",
            "\tCHRF 23.48\tROUGE 10.44\tFID 0.00\n",
            "2025-07-10 14:11:34,000 Logging Recognition and Translation Outputs\n",
            "2025-07-10 14:11:34,000 ========================================================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-07-10 14:11:34,000 Logging Sequence: 11August_2010_Wednesday_tagesschau-2\n",
            "2025-07-10 14:11:34,001 \tGloss Reference :\tDRUCK TIEF     KOMMEN\n",
            "2025-07-10 14:11:34,001 \tGloss Hypothesis:\t***** NORDWEST LOCH  \n",
            "2025-07-10 14:11:34,002 \tGloss Alignment :\tD     S        S     \n",
            "2025-07-10 14:11:34,002 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:11:34,004 \tText Reference  :\t********** ****** ** ***** ***** ***** ******* ***** **** *** ********* ********* ******** ********* ****** ******* *** ***** *** ****** ****** tiefer luftdruck bestimmt in    den   nächsten tagen unser wetter\n",
            "2025-07-10 14:11:34,004 \tText Hypothesis :\tnordwesten bleibt es heute nacht meist trocken sonst muss mit teilweise kräftigen schauern gerechnet werden örtlich mit blitz und donner donner <unk>  <unk>     <unk>    <unk> <unk> <unk>    <unk> <unk> <unk> \n",
            "2025-07-10 14:11:34,005 \tText Alignment  :\tI          I      I  I     I     I     I       I     I    I   I         I         I        I         I      I       I   I     I   I      I      S      S         S        S     S     S        S     S     S     \n",
            "2025-07-10 14:11:34,005 ========================================================================================\n",
            "2025-07-10 14:11:34,005 Logging Sequence: 11August_2010_Wednesday_tagesschau-3\n",
            "2025-07-10 14:11:34,005 \tGloss Reference :\tES-BEDEUTET VIEL WOLKE UND KOENNEN  REGEN GEWITTER KOENNEN\n",
            "2025-07-10 14:11:34,006 \tGloss Hypothesis:\t*********** **** ***** *** NORDWEST HEUTE BLEIBEN  KOENNEN\n",
            "2025-07-10 14:11:34,006 \tGloss Alignment :\tD           D    D     D   S        S     S               \n",
            "2025-07-10 14:11:34,006 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:11:34,008 \tText Reference  :\tdas bedeutet viele wolken und ******** **** ****** **************** immer wieder ******* ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** zum   teil  kräftige schauer und   gewitter\n",
            "2025-07-10 14:11:34,008 \tText Hypothesis :\t*** es       auch  länger und ergiebig auch lokale überschwemmungen sind  wieder möglich <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>    <unk>   <unk> <unk>   \n",
            "2025-07-10 14:11:34,008 \tText Alignment  :\tD   S        S     S          I        I    I      I                S            I       I     I     I     I     I     I     I     I     I     I     I     I     I     S     S     S        S       S     S       \n",
            "2025-07-10 14:11:34,008 ========================================================================================\n",
            "2025-07-10 14:11:34,008 Logging Sequence: 11August_2010_Wednesday_tagesschau-8\n",
            "2025-07-10 14:11:34,009 \tGloss Reference :\t**** ******* ******* ******** ******** WIND     MAESSIG SCHWACH  REGION WENN     GEWITTER WIND            KOENNEN \n",
            "2025-07-10 14:11:34,010 \tGloss Hypothesis:\tLOCH TROCKEN BLEIBEN NORDWEST SPEZIELL NORDWEST FEBRUAR NORDWEST REGEN  NORDWEST SPEZIELL UEBERSCHWEMMUNG NORDWEST\n",
            "2025-07-10 14:11:34,010 \tGloss Alignment :\tI    I       I       I        I        S        S       S        S      S        S        S               S       \n",
            "2025-07-10 14:11:34,010 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:11:34,012 \tText Reference  :\t****** ** **** ****** *** ******** **** ****** **************** **** ****** ******* meist weht  nur   ein   schwacher wind  aus   unterschiedlichen richtungen der   bei   schauern und   gewittern stark böig  sein  kann \n",
            "2025-07-10 14:11:34,013 \tText Hypothesis :\tregnet es auch länger und ergiebig auch lokale überschwemmungen sind wieder möglich <unk> <unk> <unk> <unk> <unk>     <unk> <unk> <unk>             <unk>      <unk> <unk> <unk>    <unk> <unk>     <unk> <unk> <unk> <unk>\n",
            "2025-07-10 14:11:34,013 \tText Alignment  :\tI      I  I    I      I   I        I    I      I                I    I      I       S     S     S     S     S         S     S     S                 S          S     S     S        S     S         S     S     S     S    \n",
            "2025-07-10 14:11:34,013 ========================================================================================\n",
            "2025-07-10 14:11:34,013 Logging Sequence: 25October_2010_Monday_tagesschau-22\n",
            "2025-07-10 14:11:34,014 \tGloss Reference :\tMITTWOCH REGEN   KOENNEN    NORDWEST ******** ******* WAHRSCHEINLICH NORD       STARK  WIND   \n",
            "2025-07-10 14:11:34,014 \tGloss Hypothesis:\tNACHT    FEBRUAR DONNERSTAG NORDWEST GEWITTER KOENNEN GEWITTER       DONNERSTAG MORGEN KOENNEN\n",
            "2025-07-10 14:11:34,014 \tGloss Alignment :\tS        S       S                   I        I       S              S          S      S      \n",
            "2025-07-10 14:11:34,015 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:11:34,016 \tText Reference  :\t****** am mittwoch hier   und ******** **** ****** **************** **** ****** ******* ***** ***** ***** ***** ***** ***** ***** ***** da    nieselregen in    der   nordwesthälfte an    den   küsten kräftiger wind \n",
            "2025-07-10 14:11:34,016 \tText Hypothesis :\tregnet es auch     länger und ergiebig auch lokale überschwemmungen sind wieder möglich <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>       <unk> <unk> <unk>          <unk> <unk> <unk>  <unk>     <unk>\n",
            "2025-07-10 14:11:34,017 \tText Alignment  :\tI      S  S        S          I        I    I      I                I    I      I       I     I     I     I     I     I     I     I     S     S           S     S     S              S     S     S      S         S    \n",
            "2025-07-10 14:11:34,017 ========================================================================================\n",
            "2025-07-10 14:11:34,017 Logging Sequence: 05May_2011_Thursday_tagesschau-25\n",
            "2025-07-10 14:11:34,018 \tGloss Reference :\t****** JETZT WETTER WIE-AUSSEHEN MORGEN ********** ******** ********** ******** FREITAG SECHSTE MAI     ZEIGEN-BILDSCHIRM\n",
            "2025-07-10 14:11:34,018 \tGloss Hypothesis:\tZWOELF JETZT WETTER ************ MORGEN DONNERSTAG NORDWEST DONNERSTAG NORDWEST ZWOELF  TROCKEN BLEIBEN FEBRUAR          \n",
            "2025-07-10 14:11:34,018 \tGloss Alignment :\tI                   D                   I          I        I          I        S       S       S       S                \n",
            "2025-07-10 14:11:34,018 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:11:34,020 \tText Reference  :\tund nun die wettervorhersage für morgen freitag    den ******** ****** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** sechsten mai  \n",
            "2025-07-10 14:11:34,020 \tText Hypothesis :\t*** nun die wettervorhersage für morgen donnerstag den zwölften august <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>    <unk>\n",
            "2025-07-10 14:11:34,020 \tText Alignment  :\tD                                       S              I        I      I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     S        S    \n",
            "2025-07-10 14:11:34,020 ========================================================================================\n",
            "2025-07-10 14:11:34,020 Epoch 1600:\n",
            "\t\t\tTotal Training Recognition Loss 0.940150 || Total Training Translation Loss 0.005052\n",
            "2025-07-10 14:11:34,194 Epoch 1601:\n",
            "\t\t\tTotal Training Recognition Loss 1.071457 || Total Training Translation Loss 0.005700\n",
            "2025-07-10 14:11:34,372 Epoch 1602:\n",
            "\t\t\tTotal Training Recognition Loss 1.196856 || Total Training Translation Loss 0.005590\n",
            "2025-07-10 14:11:34,545 Epoch 1603:\n",
            "\t\t\tTotal Training Recognition Loss 1.134052 || Total Training Translation Loss 0.006018\n",
            "2025-07-10 14:11:34,720 Epoch 1604:\n",
            "\t\t\tTotal Training Recognition Loss 77.711380 || Total Training Translation Loss 0.005840\n",
            "2025-07-10 14:11:34,893 Epoch 1605:\n",
            "\t\t\tTotal Training Recognition Loss 0.432750 || Total Training Translation Loss 0.006481\n",
            "2025-07-10 14:11:35,068 Epoch 1606:\n",
            "\t\t\tTotal Training Recognition Loss 0.403479 || Total Training Translation Loss 0.005110\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-07-10 14:11:35,286 Epoch 1607:\n",
            "\t\t\tTotal Training Recognition Loss 1.338198 || Total Training Translation Loss 0.005110\n",
            "2025-07-10 14:11:35,504 Epoch 1608:\n",
            "\t\t\tTotal Training Recognition Loss 0.777731 || Total Training Translation Loss 0.004938\n",
            "2025-07-10 14:11:35,721 Epoch 1609:\n",
            "\t\t\tTotal Training Recognition Loss 0.480773 || Total Training Translation Loss 0.005077\n",
            "2025-07-10 14:11:35,897 Epoch 1610:\n",
            "\t\t\tTotal Training Recognition Loss 0.760827 || Total Training Translation Loss 0.005822\n",
            "2025-07-10 14:11:36,070 Epoch 1611:\n",
            "\t\t\tTotal Training Recognition Loss 0.275743 || Total Training Translation Loss 0.005242\n",
            "2025-07-10 14:11:36,243 Epoch 1612:\n",
            "\t\t\tTotal Training Recognition Loss 0.261198 || Total Training Translation Loss 0.004868\n",
            "2025-07-10 14:11:36,416 Epoch 1613:\n",
            "\t\t\tTotal Training Recognition Loss 0.426577 || Total Training Translation Loss 0.005122\n",
            "2025-07-10 14:11:36,590 Epoch 1614:\n",
            "\t\t\tTotal Training Recognition Loss 1.617399 || Total Training Translation Loss 0.005350\n",
            "2025-07-10 14:11:36,766 Epoch 1615:\n",
            "\t\t\tTotal Training Recognition Loss 1.094169 || Total Training Translation Loss 0.005062\n",
            "2025-07-10 14:11:36,938 Epoch 1616:\n",
            "\t\t\tTotal Training Recognition Loss 0.374224 || Total Training Translation Loss 0.004718\n",
            "2025-07-10 14:11:37,110 Epoch 1617:\n",
            "\t\t\tTotal Training Recognition Loss 0.910656 || Total Training Translation Loss 0.005517\n",
            "2025-07-10 14:11:37,283 Epoch 1618:\n",
            "\t\t\tTotal Training Recognition Loss 0.303384 || Total Training Translation Loss 0.005025\n",
            "2025-07-10 14:11:37,456 Epoch 1619:\n",
            "\t\t\tTotal Training Recognition Loss 0.588824 || Total Training Translation Loss 0.005012\n",
            "2025-07-10 14:11:37,628 Epoch 1620:\n",
            "\t\t\tTotal Training Recognition Loss 0.296022 || Total Training Translation Loss 0.005166\n",
            "2025-07-10 14:11:37,801 Epoch 1621:\n",
            "\t\t\tTotal Training Recognition Loss 0.230390 || Total Training Translation Loss 0.004307\n",
            "2025-07-10 14:11:37,977 Epoch 1622:\n",
            "\t\t\tTotal Training Recognition Loss 0.269692 || Total Training Translation Loss 0.005560\n",
            "2025-07-10 14:11:38,150 Epoch 1623:\n",
            "\t\t\tTotal Training Recognition Loss 0.394935 || Total Training Translation Loss 0.004479\n",
            "2025-07-10 14:11:38,323 Epoch 1624:\n",
            "\t\t\tTotal Training Recognition Loss 0.583423 || Total Training Translation Loss 0.005468\n",
            "2025-07-10 14:11:38,496 Epoch 1625:\n",
            "\t\t\tTotal Training Recognition Loss 0.260002 || Total Training Translation Loss 0.005100\n",
            "2025-07-10 14:11:38,669 Epoch 1626:\n",
            "\t\t\tTotal Training Recognition Loss 0.633457 || Total Training Translation Loss 0.007741\n",
            "2025-07-10 14:11:38,844 Epoch 1627:\n",
            "\t\t\tTotal Training Recognition Loss 0.301066 || Total Training Translation Loss 0.006592\n",
            "2025-07-10 14:11:39,018 Epoch 1628:\n",
            "\t\t\tTotal Training Recognition Loss 0.577978 || Total Training Translation Loss 0.006754\n",
            "2025-07-10 14:11:39,192 Epoch 1629:\n",
            "\t\t\tTotal Training Recognition Loss 0.398279 || Total Training Translation Loss 0.005733\n",
            "2025-07-10 14:11:39,364 Epoch 1630:\n",
            "\t\t\tTotal Training Recognition Loss 0.496167 || Total Training Translation Loss 0.005198\n",
            "2025-07-10 14:11:39,537 Epoch 1631:\n",
            "\t\t\tTotal Training Recognition Loss 0.343753 || Total Training Translation Loss 0.005926\n",
            "2025-07-10 14:11:39,709 Epoch 1632:\n",
            "\t\t\tTotal Training Recognition Loss 0.441440 || Total Training Translation Loss 0.005053\n",
            "2025-07-10 14:11:39,883 Epoch 1633:\n",
            "\t\t\tTotal Training Recognition Loss 0.237019 || Total Training Translation Loss 0.004298\n",
            "2025-07-10 14:11:40,056 Epoch 1634:\n",
            "\t\t\tTotal Training Recognition Loss 0.928711 || Total Training Translation Loss 0.004988\n",
            "2025-07-10 14:11:40,231 Epoch 1635:\n",
            "\t\t\tTotal Training Recognition Loss 0.183241 || Total Training Translation Loss 0.005337\n",
            "2025-07-10 14:11:40,405 Epoch 1636:\n",
            "\t\t\tTotal Training Recognition Loss 0.278895 || Total Training Translation Loss 0.006716\n",
            "2025-07-10 14:11:40,577 Epoch 1637:\n",
            "\t\t\tTotal Training Recognition Loss 0.301078 || Total Training Translation Loss 0.006431\n",
            "2025-07-10 14:11:40,750 Epoch 1638:\n",
            "\t\t\tTotal Training Recognition Loss 0.267852 || Total Training Translation Loss 0.005748\n",
            "2025-07-10 14:11:40,924 Epoch 1639:\n",
            "\t\t\tTotal Training Recognition Loss 0.229571 || Total Training Translation Loss 0.005208\n",
            "2025-07-10 14:11:41,097 Epoch 1640:\n",
            "\t\t\tTotal Training Recognition Loss 1.422280 || Total Training Translation Loss 0.005235\n",
            "2025-07-10 14:11:41,269 Epoch 1641:\n",
            "\t\t\tTotal Training Recognition Loss 0.269025 || Total Training Translation Loss 0.005229\n",
            "2025-07-10 14:11:41,444 Epoch 1642:\n",
            "\t\t\tTotal Training Recognition Loss 0.322903 || Total Training Translation Loss 0.004788\n",
            "2025-07-10 14:11:41,617 Epoch 1643:\n",
            "\t\t\tTotal Training Recognition Loss 0.257718 || Total Training Translation Loss 0.006385\n",
            "2025-07-10 14:11:41,790 Epoch 1644:\n",
            "\t\t\tTotal Training Recognition Loss 0.382934 || Total Training Translation Loss 0.006894\n",
            "2025-07-10 14:11:41,963 Epoch 1645:\n",
            "\t\t\tTotal Training Recognition Loss 0.224262 || Total Training Translation Loss 0.006580\n",
            "2025-07-10 14:11:42,139 Epoch 1646:\n",
            "\t\t\tTotal Training Recognition Loss 0.321538 || Total Training Translation Loss 0.004687\n",
            "2025-07-10 14:11:42,314 Epoch 1647:\n",
            "\t\t\tTotal Training Recognition Loss 0.517547 || Total Training Translation Loss 0.005185\n",
            "2025-07-10 14:11:42,490 Epoch 1648:\n",
            "\t\t\tTotal Training Recognition Loss 0.219112 || Total Training Translation Loss 0.004528\n",
            "2025-07-10 14:11:42,670 Epoch 1649:\n",
            "\t\t\tTotal Training Recognition Loss 0.376108 || Total Training Translation Loss 0.006692\n",
            "2025-07-10 14:11:42,847 Epoch 1650:\n",
            "\t\t\tTotal Training Recognition Loss 0.688500 || Total Training Translation Loss 0.005313\n",
            "2025-07-10 14:11:43,022 Epoch 1651:\n",
            "\t\t\tTotal Training Recognition Loss 0.534522 || Total Training Translation Loss 0.006535\n",
            "2025-07-10 14:11:43,197 Epoch 1652:\n",
            "\t\t\tTotal Training Recognition Loss 0.207290 || Total Training Translation Loss 0.005293\n",
            "2025-07-10 14:11:43,372 Epoch 1653:\n",
            "\t\t\tTotal Training Recognition Loss 0.283114 || Total Training Translation Loss 0.005975\n",
            "2025-07-10 14:11:43,547 Epoch 1654:\n",
            "\t\t\tTotal Training Recognition Loss 0.194313 || Total Training Translation Loss 0.005742\n",
            "2025-07-10 14:11:43,721 Epoch 1655:\n",
            "\t\t\tTotal Training Recognition Loss 0.122370 || Total Training Translation Loss 0.005876\n",
            "2025-07-10 14:11:43,899 Epoch 1656:\n",
            "\t\t\tTotal Training Recognition Loss 0.177553 || Total Training Translation Loss 0.005659\n",
            "2025-07-10 14:11:44,122 Epoch 1657:\n",
            "\t\t\tTotal Training Recognition Loss 0.178994 || Total Training Translation Loss 0.004910\n",
            "2025-07-10 14:11:44,299 Epoch 1658:\n",
            "\t\t\tTotal Training Recognition Loss 0.383609 || Total Training Translation Loss 0.005053\n",
            "2025-07-10 14:11:44,477 Epoch 1659:\n",
            "\t\t\tTotal Training Recognition Loss 0.356896 || Total Training Translation Loss 0.005204\n",
            "2025-07-10 14:11:44,654 Epoch 1660:\n",
            "\t\t\tTotal Training Recognition Loss 0.188294 || Total Training Translation Loss 0.005627\n",
            "2025-07-10 14:11:44,831 Epoch 1661:\n",
            "\t\t\tTotal Training Recognition Loss 1.104090 || Total Training Translation Loss 0.005469\n",
            "2025-07-10 14:11:45,008 Epoch 1662:\n",
            "\t\t\tTotal Training Recognition Loss 0.163158 || Total Training Translation Loss 0.004515\n",
            "2025-07-10 14:11:45,183 Epoch 1663:\n",
            "\t\t\tTotal Training Recognition Loss 0.114074 || Total Training Translation Loss 0.004800\n",
            "2025-07-10 14:11:45,361 Epoch 1664:\n",
            "\t\t\tTotal Training Recognition Loss 0.133340 || Total Training Translation Loss 0.004803\n",
            "2025-07-10 14:11:45,536 Epoch 1665:\n",
            "\t\t\tTotal Training Recognition Loss 0.154010 || Total Training Translation Loss 0.005211\n",
            "2025-07-10 14:11:45,711 Epoch 1666:\n",
            "\t\t\tTotal Training Recognition Loss 0.176231 || Total Training Translation Loss 0.006262\n",
            "2025-07-10 14:11:45,884 Epoch 1667:\n",
            "\t\t\tTotal Training Recognition Loss 0.149457 || Total Training Translation Loss 0.004954\n",
            "2025-07-10 14:11:46,058 Epoch 1668:\n",
            "\t\t\tTotal Training Recognition Loss 0.195887 || Total Training Translation Loss 0.005141\n",
            "2025-07-10 14:11:46,233 Epoch 1669:\n",
            "\t\t\tTotal Training Recognition Loss 0.178885 || Total Training Translation Loss 0.005935\n",
            "2025-07-10 14:11:46,406 Epoch 1670:\n",
            "\t\t\tTotal Training Recognition Loss 0.124727 || Total Training Translation Loss 0.005632\n",
            "2025-07-10 14:11:46,580 Epoch 1671:\n",
            "\t\t\tTotal Training Recognition Loss 0.174116 || Total Training Translation Loss 0.005457\n",
            "2025-07-10 14:11:46,754 Epoch 1672:\n",
            "\t\t\tTotal Training Recognition Loss 0.316888 || Total Training Translation Loss 0.005504\n",
            "2025-07-10 14:11:46,928 Epoch 1673:\n",
            "\t\t\tTotal Training Recognition Loss 0.122424 || Total Training Translation Loss 0.004854\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-07-10 14:11:47,107 Epoch 1674:\n",
            "\t\t\tTotal Training Recognition Loss 0.149011 || Total Training Translation Loss 0.006176\n",
            "2025-07-10 14:11:47,281 Epoch 1675:\n",
            "\t\t\tTotal Training Recognition Loss 0.167660 || Total Training Translation Loss 0.005098\n",
            "2025-07-10 14:11:47,455 Epoch 1676:\n",
            "\t\t\tTotal Training Recognition Loss 0.242930 || Total Training Translation Loss 0.005993\n",
            "2025-07-10 14:11:47,629 Epoch 1677:\n",
            "\t\t\tTotal Training Recognition Loss 0.658015 || Total Training Translation Loss 0.005596\n",
            "2025-07-10 14:11:47,803 Epoch 1678:\n",
            "\t\t\tTotal Training Recognition Loss 0.147113 || Total Training Translation Loss 0.005146\n",
            "2025-07-10 14:11:47,980 Epoch 1679:\n",
            "\t\t\tTotal Training Recognition Loss 0.175548 || Total Training Translation Loss 0.005243\n",
            "2025-07-10 14:11:48,154 Epoch 1680:\n",
            "\t\t\tTotal Training Recognition Loss 0.102710 || Total Training Translation Loss 0.006135\n",
            "2025-07-10 14:11:48,328 Epoch 1681:\n",
            "\t\t\tTotal Training Recognition Loss 0.173700 || Total Training Translation Loss 0.005274\n",
            "2025-07-10 14:11:48,502 Epoch 1682:\n",
            "\t\t\tTotal Training Recognition Loss 0.134566 || Total Training Translation Loss 0.004467\n",
            "2025-07-10 14:11:48,679 Epoch 1683:\n",
            "\t\t\tTotal Training Recognition Loss 0.141612 || Total Training Translation Loss 0.005690\n",
            "2025-07-10 14:11:48,856 Epoch 1684:\n",
            "\t\t\tTotal Training Recognition Loss 0.131933 || Total Training Translation Loss 0.004651\n",
            "2025-07-10 14:11:49,058 Epoch 1685:\n",
            "\t\t\tTotal Training Recognition Loss 0.195446 || Total Training Translation Loss 0.004394\n",
            "2025-07-10 14:11:49,275 Epoch 1686:\n",
            "\t\t\tTotal Training Recognition Loss 0.112979 || Total Training Translation Loss 0.005795\n",
            "2025-07-10 14:11:49,455 Epoch 1687:\n",
            "\t\t\tTotal Training Recognition Loss 0.106028 || Total Training Translation Loss 0.006040\n",
            "2025-07-10 14:11:49,625 Epoch 1688:\n",
            "\t\t\tTotal Training Recognition Loss 0.220528 || Total Training Translation Loss 0.005148\n",
            "2025-07-10 14:11:49,796 Epoch 1689:\n",
            "\t\t\tTotal Training Recognition Loss 0.089224 || Total Training Translation Loss 0.004748\n",
            "2025-07-10 14:11:49,966 Epoch 1690:\n",
            "\t\t\tTotal Training Recognition Loss 0.134311 || Total Training Translation Loss 0.006050\n",
            "2025-07-10 14:11:50,139 Epoch 1691:\n",
            "\t\t\tTotal Training Recognition Loss 0.211681 || Total Training Translation Loss 0.006417\n",
            "2025-07-10 14:11:50,312 Epoch 1692:\n",
            "\t\t\tTotal Training Recognition Loss 0.116714 || Total Training Translation Loss 0.005921\n",
            "2025-07-10 14:11:50,486 Epoch 1693:\n",
            "\t\t\tTotal Training Recognition Loss 0.114270 || Total Training Translation Loss 0.005526\n",
            "2025-07-10 14:11:50,660 Epoch 1694:\n",
            "\t\t\tTotal Training Recognition Loss 0.112093 || Total Training Translation Loss 0.005046\n",
            "2025-07-10 14:11:50,833 Epoch 1695:\n",
            "\t\t\tTotal Training Recognition Loss 0.083253 || Total Training Translation Loss 0.005260\n",
            "2025-07-10 14:11:51,007 Epoch 1696:\n",
            "\t\t\tTotal Training Recognition Loss 0.126271 || Total Training Translation Loss 0.006081\n",
            "2025-07-10 14:11:51,181 Epoch 1697:\n",
            "\t\t\tTotal Training Recognition Loss 1.269951 || Total Training Translation Loss 0.005611\n",
            "2025-07-10 14:11:51,354 Epoch 1698:\n",
            "\t\t\tTotal Training Recognition Loss 0.146488 || Total Training Translation Loss 0.005124\n",
            "2025-07-10 14:11:51,570 Epoch 1699:\n",
            "\t\t\tTotal Training Recognition Loss 0.125063 || Total Training Translation Loss 0.004870\n",
            "2025-07-10 14:11:51,786 [Epoch: 1700 Step: 00001700] Batch Recognition Loss:   0.132297 => Gls Tokens per Sec:      172 || Batch Translation Loss:   0.005695 => Txt Tokens per Sec:      466 || Lr: 0.000700\n",
            "2025-07-10 14:11:52,092 Validation result at epoch 1700, step     1700: duration: 0.3046s\n",
            "\tRecognition Beam Size: 1\tTranslation Beam Size: 1\tTranslation Beam Alpha: -1\n",
            "\tRecognition Loss: 6697.53955\tTranslation Loss: 230.56256\tPPL: 15.06691\n",
            "\tEval Metric: BLEU\n",
            "\tWER 91.43\t(DEL: 28.57,\tINS: 5.71,\tSUB: 57.14)\n",
            "\tBLEU-4 2.76\t(BLEU-1: 6.67,\tBLEU-2: 4.29,\tBLEU-3: 3.40,\tBLEU-4: 2.76)\n",
            "\tCHRF 24.98\tROUGE 10.44\tFID 0.00\n",
            "2025-07-10 14:11:52,092 Logging Recognition and Translation Outputs\n",
            "2025-07-10 14:11:52,093 ========================================================================================\n",
            "2025-07-10 14:11:52,093 Logging Sequence: 11August_2010_Wednesday_tagesschau-2\n",
            "2025-07-10 14:11:52,094 \tGloss Reference :\tDRUCK    TIEF     KOMMEN\n",
            "2025-07-10 14:11:52,094 \tGloss Hypothesis:\tSPEZIELL NORDWEST LOCH  \n",
            "2025-07-10 14:11:52,094 \tGloss Alignment :\tS        S        S     \n",
            "2025-07-10 14:11:52,095 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:11:52,098 \tText Reference  :\t********** ****** ** ***** ***** ***** ******* ***** **** *** ********* ********* ******** ********* ****** ******* *** ***** *** ****** ***** tiefer luftdruck bestimmt in    den   nächsten tagen unser wetter\n",
            "2025-07-10 14:11:52,098 \tText Hypothesis :\tnordwesten bleibt es heute nacht meist trocken sonst muss mit teilweise kräftigen schauern gerechnet werden örtlich mit blitz und donner <unk> <unk>  <unk>     <unk>    <unk> <unk> <unk>    <unk> <unk> <unk> \n",
            "2025-07-10 14:11:52,098 \tText Alignment  :\tI          I      I  I     I     I     I       I     I    I   I         I         I        I         I      I       I   I     I   I      I     S      S         S        S     S     S        S     S     S     \n",
            "2025-07-10 14:11:52,098 ========================================================================================\n",
            "2025-07-10 14:11:52,099 Logging Sequence: 11August_2010_Wednesday_tagesschau-3\n",
            "2025-07-10 14:11:52,099 \tGloss Reference :\tES-BEDEUTET VIEL WOLKE UND KOENNEN REGEN GEWITTER KOENNEN\n",
            "2025-07-10 14:11:52,100 \tGloss Hypothesis:\t*********** **** ***** *** ******* ***** HEUTE    KOENNEN\n",
            "2025-07-10 14:11:52,100 \tGloss Alignment :\tD           D    D     D   D       D     S               \n",
            "2025-07-10 14:11:52,100 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:11:52,104 \tText Reference  :\tdas bedeutet viele wolken und ******** **** ****** **************** immer wieder ******* ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** zum   teil  kräftige schauer und   gewitter\n",
            "2025-07-10 14:11:52,105 \tText Hypothesis :\t*** es       auch  länger und ergiebig auch lokale überschwemmungen sind  wieder möglich <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>    <unk>   <unk> <unk>   \n",
            "2025-07-10 14:11:52,105 \tText Alignment  :\tD   S        S     S          I        I    I      I                S            I       I     I     I     I     I     I     I     I     I     I     I     I     I     S     S     S        S       S     S       \n",
            "2025-07-10 14:11:52,105 ========================================================================================\n",
            "2025-07-10 14:11:52,105 Logging Sequence: 11August_2010_Wednesday_tagesschau-8\n",
            "2025-07-10 14:11:52,106 \tGloss Reference :\tWIND MAESSIG SCHWACH REGION  WENN  GEWITTER WIND            KOENNEN\n",
            "2025-07-10 14:11:52,107 \tGloss Hypothesis:\t**** LOCH    TROCKEN BLEIBEN REGEN VIEL     UEBERSCHWEMMUNG KOENNEN\n",
            "2025-07-10 14:11:52,107 \tGloss Alignment :\tD    S       S       S       S     S        S                      \n",
            "2025-07-10 14:11:52,107 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:11:52,112 \tText Reference  :\t** **** ****** *** ******** **** ****** **************** **** ****** ******* ***** meist weht  nur   ein   schwacher wind  aus   unterschiedlichen richtungen der   bei   schauern und   gewittern stark böig  sein  kann \n",
            "2025-07-10 14:11:52,112 \tText Hypothesis :\tes auch länger und ergiebig auch lokale überschwemmungen sind wieder möglich <unk> <unk> <unk> <unk> <unk> <unk>     <unk> <unk> <unk>             <unk>      <unk> <unk> <unk>    <unk> <unk>     <unk> <unk> <unk> <unk>\n",
            "2025-07-10 14:11:52,113 \tText Alignment  :\tI  I    I      I   I        I    I      I                I    I      I       I     S     S     S     S     S         S     S     S                 S          S     S     S        S     S         S     S     S     S    \n",
            "2025-07-10 14:11:52,113 ========================================================================================\n",
            "2025-07-10 14:11:52,113 Logging Sequence: 25October_2010_Monday_tagesschau-22\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-07-10 14:11:52,114 \tGloss Reference :\t******* MITTWOCH   REGEN    KOENNEN         NORDWEST WAHRSCHEINLICH NORD     STARK  WIND   \n",
            "2025-07-10 14:11:52,114 \tGloss Hypothesis:\tFEBRUAR DONNERSTAG GEWITTER UEBERSCHWEMMUNG GEWITTER KOENNEN        GEWITTER MORGEN KOENNEN\n",
            "2025-07-10 14:11:52,114 \tGloss Alignment :\tI       S          S        S               S        S              S        S      S      \n",
            "2025-07-10 14:11:52,115 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:11:52,118 \tText Reference  :\t** ***** ***** ***** ******* ***** **** *** ********* ********* ******** ********* ****** ******* *** am    mittwoch hier   und ****** da     nieselregen in    der   nordwesthälfte an    den   küsten kräftiger wind \n",
            "2025-07-10 14:11:52,118 \tText Hypothesis :\tes heute nacht meist trocken sonst muss mit teilweise kräftigen schauern gerechnet werden örtlich mit blitz und      donner und donner donner <unk>       <unk> <unk> <unk>          <unk> <unk> <unk>  <unk>     <unk>\n",
            "2025-07-10 14:11:52,118 \tText Alignment  :\tI  I     I     I     I       I     I    I   I         I         I        I         I      I       I   S     S        S          I      S      S           S     S     S              S     S     S      S         S    \n",
            "2025-07-10 14:11:52,119 ========================================================================================\n",
            "2025-07-10 14:11:52,119 Logging Sequence: 05May_2011_Thursday_tagesschau-25\n",
            "2025-07-10 14:11:52,119 \tGloss Reference :\t****** JETZT WETTER WIE-AUSSEHEN MORGEN FREITAG SECHSTE MAI        ZEIGEN-BILDSCHIRM\n",
            "2025-07-10 14:11:52,120 \tGloss Hypothesis:\tZWOELF JETZT WETTER ************ MORGEN ******* ******* DONNERSTAG BLEIBEN          \n",
            "2025-07-10 14:11:52,120 \tGloss Alignment :\tI                   D                   D       D       S          S                \n",
            "2025-07-10 14:11:52,120 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:11:52,122 \tText Reference  :\tund nun die wettervorhersage für morgen freitag    den ******** ****** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** sechsten mai  \n",
            "2025-07-10 14:11:52,122 \tText Hypothesis :\t*** nun die wettervorhersage für morgen donnerstag den zwölften august <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>    <unk>\n",
            "2025-07-10 14:11:52,122 \tText Alignment  :\tD                                       S              I        I      I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     S        S    \n",
            "2025-07-10 14:11:52,122 ========================================================================================\n",
            "2025-07-10 14:11:52,123 Epoch 1700:\n",
            "\t\t\tTotal Training Recognition Loss 0.132297 || Total Training Translation Loss 0.005695\n",
            "2025-07-10 14:11:52,297 Epoch 1701:\n",
            "\t\t\tTotal Training Recognition Loss 0.108071 || Total Training Translation Loss 0.005478\n",
            "2025-07-10 14:11:52,470 Epoch 1702:\n",
            "\t\t\tTotal Training Recognition Loss 0.097406 || Total Training Translation Loss 0.006136\n",
            "2025-07-10 14:11:52,644 Epoch 1703:\n",
            "\t\t\tTotal Training Recognition Loss 0.160354 || Total Training Translation Loss 0.004731\n",
            "2025-07-10 14:11:52,818 Epoch 1704:\n",
            "\t\t\tTotal Training Recognition Loss 0.818500 || Total Training Translation Loss 0.005430\n",
            "2025-07-10 14:11:52,992 Epoch 1705:\n",
            "\t\t\tTotal Training Recognition Loss 0.081362 || Total Training Translation Loss 0.006915\n",
            "2025-07-10 14:11:53,166 Epoch 1706:\n",
            "\t\t\tTotal Training Recognition Loss 0.078313 || Total Training Translation Loss 0.004771\n",
            "2025-07-10 14:11:53,339 Epoch 1707:\n",
            "\t\t\tTotal Training Recognition Loss 0.451877 || Total Training Translation Loss 0.005546\n",
            "2025-07-10 14:11:53,559 Epoch 1708:\n",
            "\t\t\tTotal Training Recognition Loss 0.456120 || Total Training Translation Loss 0.004483\n",
            "2025-07-10 14:11:53,737 Epoch 1709:\n",
            "\t\t\tTotal Training Recognition Loss 0.083680 || Total Training Translation Loss 0.004232\n",
            "2025-07-10 14:11:53,910 Epoch 1710:\n",
            "\t\t\tTotal Training Recognition Loss 0.144746 || Total Training Translation Loss 0.006657\n",
            "2025-07-10 14:11:54,131 Epoch 1711:\n",
            "\t\t\tTotal Training Recognition Loss 0.145683 || Total Training Translation Loss 0.005514\n",
            "2025-07-10 14:11:54,310 Epoch 1712:\n",
            "\t\t\tTotal Training Recognition Loss 0.204415 || Total Training Translation Loss 0.004793\n",
            "2025-07-10 14:11:54,486 Epoch 1713:\n",
            "\t\t\tTotal Training Recognition Loss 0.080147 || Total Training Translation Loss 0.005552\n",
            "2025-07-10 14:11:54,660 Epoch 1714:\n",
            "\t\t\tTotal Training Recognition Loss 0.076106 || Total Training Translation Loss 0.004707\n",
            "2025-07-10 14:11:54,833 Epoch 1715:\n",
            "\t\t\tTotal Training Recognition Loss 0.250332 || Total Training Translation Loss 0.004696\n",
            "2025-07-10 14:11:55,007 Epoch 1716:\n",
            "\t\t\tTotal Training Recognition Loss 0.098524 || Total Training Translation Loss 0.006606\n",
            "2025-07-10 14:11:55,180 Epoch 1717:\n",
            "\t\t\tTotal Training Recognition Loss 0.218744 || Total Training Translation Loss 0.005205\n",
            "2025-07-10 14:11:55,354 Epoch 1718:\n",
            "\t\t\tTotal Training Recognition Loss 0.093056 || Total Training Translation Loss 0.004982\n",
            "2025-07-10 14:11:55,527 Epoch 1719:\n",
            "\t\t\tTotal Training Recognition Loss 0.186559 || Total Training Translation Loss 0.005123\n",
            "2025-07-10 14:11:55,699 Epoch 1720:\n",
            "\t\t\tTotal Training Recognition Loss 0.113989 || Total Training Translation Loss 0.005392\n",
            "2025-07-10 14:11:55,873 Epoch 1721:\n",
            "\t\t\tTotal Training Recognition Loss 0.079317 || Total Training Translation Loss 0.005858\n",
            "2025-07-10 14:11:56,048 Epoch 1722:\n",
            "\t\t\tTotal Training Recognition Loss 0.095029 || Total Training Translation Loss 0.005748\n",
            "2025-07-10 14:11:56,221 Epoch 1723:\n",
            "\t\t\tTotal Training Recognition Loss 2.487750 || Total Training Translation Loss 0.004848\n",
            "2025-07-10 14:11:56,395 Epoch 1724:\n",
            "\t\t\tTotal Training Recognition Loss 0.215912 || Total Training Translation Loss 0.005046\n",
            "2025-07-10 14:11:56,571 Epoch 1725:\n",
            "\t\t\tTotal Training Recognition Loss 6.339112 || Total Training Translation Loss 0.004346\n",
            "2025-07-10 14:11:56,746 Epoch 1726:\n",
            "\t\t\tTotal Training Recognition Loss 0.500622 || Total Training Translation Loss 0.004964\n",
            "2025-07-10 14:11:56,921 Epoch 1727:\n",
            "\t\t\tTotal Training Recognition Loss 0.094676 || Total Training Translation Loss 0.005302\n",
            "2025-07-10 14:11:57,094 Epoch 1728:\n",
            "\t\t\tTotal Training Recognition Loss 0.100741 || Total Training Translation Loss 0.004086\n",
            "2025-07-10 14:11:57,271 Epoch 1729:\n",
            "\t\t\tTotal Training Recognition Loss 0.101732 || Total Training Translation Loss 0.005869\n",
            "2025-07-10 14:11:57,447 Epoch 1730:\n",
            "\t\t\tTotal Training Recognition Loss 0.160928 || Total Training Translation Loss 0.005214\n",
            "2025-07-10 14:11:57,622 Epoch 1731:\n",
            "\t\t\tTotal Training Recognition Loss 0.309340 || Total Training Translation Loss 0.005180\n",
            "2025-07-10 14:11:57,796 Epoch 1732:\n",
            "\t\t\tTotal Training Recognition Loss 0.239898 || Total Training Translation Loss 0.005472\n",
            "2025-07-10 14:11:57,970 Epoch 1733:\n",
            "\t\t\tTotal Training Recognition Loss 1.733088 || Total Training Translation Loss 0.005849\n",
            "2025-07-10 14:11:58,144 Epoch 1734:\n",
            "\t\t\tTotal Training Recognition Loss 0.136981 || Total Training Translation Loss 0.006214\n",
            "2025-07-10 14:11:58,323 Epoch 1735:\n",
            "\t\t\tTotal Training Recognition Loss 0.205001 || Total Training Translation Loss 0.005935\n",
            "2025-07-10 14:11:58,497 Epoch 1736:\n",
            "\t\t\tTotal Training Recognition Loss 0.124433 || Total Training Translation Loss 0.004452\n",
            "2025-07-10 14:11:58,671 Epoch 1737:\n",
            "\t\t\tTotal Training Recognition Loss 0.107239 || Total Training Translation Loss 0.005947\n",
            "2025-07-10 14:11:58,846 Epoch 1738:\n",
            "\t\t\tTotal Training Recognition Loss 0.161932 || Total Training Translation Loss 0.005033\n",
            "2025-07-10 14:11:59,068 Epoch 1739:\n",
            "\t\t\tTotal Training Recognition Loss 0.112135 || Total Training Translation Loss 0.004328\n",
            "2025-07-10 14:11:59,244 Epoch 1740:\n",
            "\t\t\tTotal Training Recognition Loss 0.308690 || Total Training Translation Loss 0.004812\n",
            "2025-07-10 14:11:59,419 Epoch 1741:\n",
            "\t\t\tTotal Training Recognition Loss 0.071268 || Total Training Translation Loss 0.005744\n",
            "2025-07-10 14:11:59,593 Epoch 1742:\n",
            "\t\t\tTotal Training Recognition Loss 0.132187 || Total Training Translation Loss 0.005304\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-07-10 14:11:59,765 Epoch 1743:\n",
            "\t\t\tTotal Training Recognition Loss 44.149994 || Total Training Translation Loss 0.005872\n",
            "2025-07-10 14:11:59,939 Epoch 1744:\n",
            "\t\t\tTotal Training Recognition Loss 0.086311 || Total Training Translation Loss 0.005289\n",
            "2025-07-10 14:12:00,113 Epoch 1745:\n",
            "\t\t\tTotal Training Recognition Loss 0.089521 || Total Training Translation Loss 0.006881\n",
            "2025-07-10 14:12:00,287 Epoch 1746:\n",
            "\t\t\tTotal Training Recognition Loss 0.321978 || Total Training Translation Loss 0.006438\n",
            "2025-07-10 14:12:00,460 Epoch 1747:\n",
            "\t\t\tTotal Training Recognition Loss 0.289910 || Total Training Translation Loss 0.005375\n",
            "2025-07-10 14:12:00,633 Epoch 1748:\n",
            "\t\t\tTotal Training Recognition Loss 1.107866 || Total Training Translation Loss 0.006187\n",
            "2025-07-10 14:12:00,806 Epoch 1749:\n",
            "\t\t\tTotal Training Recognition Loss 24.415787 || Total Training Translation Loss 0.006873\n",
            "2025-07-10 14:12:00,978 Epoch 1750:\n",
            "\t\t\tTotal Training Recognition Loss 1.354241 || Total Training Translation Loss 0.006434\n",
            "2025-07-10 14:12:01,151 Epoch 1751:\n",
            "\t\t\tTotal Training Recognition Loss 0.779559 || Total Training Translation Loss 0.004954\n",
            "2025-07-10 14:12:01,327 Epoch 1752:\n",
            "\t\t\tTotal Training Recognition Loss 0.792874 || Total Training Translation Loss 0.006296\n",
            "2025-07-10 14:12:01,500 Epoch 1753:\n",
            "\t\t\tTotal Training Recognition Loss 2.435287 || Total Training Translation Loss 0.005354\n",
            "2025-07-10 14:12:01,675 Epoch 1754:\n",
            "\t\t\tTotal Training Recognition Loss 0.361790 || Total Training Translation Loss 0.006083\n",
            "2025-07-10 14:12:01,850 Epoch 1755:\n",
            "\t\t\tTotal Training Recognition Loss 0.611805 || Total Training Translation Loss 0.005638\n",
            "2025-07-10 14:12:02,023 Epoch 1756:\n",
            "\t\t\tTotal Training Recognition Loss 0.208292 || Total Training Translation Loss 0.005411\n",
            "2025-07-10 14:12:02,195 Epoch 1757:\n",
            "\t\t\tTotal Training Recognition Loss 0.277171 || Total Training Translation Loss 0.005226\n",
            "2025-07-10 14:12:02,368 Epoch 1758:\n",
            "\t\t\tTotal Training Recognition Loss 0.348701 || Total Training Translation Loss 0.005887\n",
            "2025-07-10 14:12:02,541 Epoch 1759:\n",
            "\t\t\tTotal Training Recognition Loss 3.967487 || Total Training Translation Loss 0.005346\n",
            "2025-07-10 14:12:02,713 Epoch 1760:\n",
            "\t\t\tTotal Training Recognition Loss 0.354726 || Total Training Translation Loss 0.006264\n",
            "2025-07-10 14:12:02,885 Epoch 1761:\n",
            "\t\t\tTotal Training Recognition Loss 0.202717 || Total Training Translation Loss 0.007188\n",
            "2025-07-10 14:12:03,059 Epoch 1762:\n",
            "\t\t\tTotal Training Recognition Loss 0.549531 || Total Training Translation Loss 0.006343\n",
            "2025-07-10 14:12:03,232 Epoch 1763:\n",
            "\t\t\tTotal Training Recognition Loss 0.306260 || Total Training Translation Loss 0.005982\n",
            "2025-07-10 14:12:03,404 Epoch 1764:\n",
            "\t\t\tTotal Training Recognition Loss 0.629848 || Total Training Translation Loss 0.004841\n",
            "2025-07-10 14:12:03,577 Epoch 1765:\n",
            "\t\t\tTotal Training Recognition Loss 1.046355 || Total Training Translation Loss 0.005669\n",
            "2025-07-10 14:12:03,750 Epoch 1766:\n",
            "\t\t\tTotal Training Recognition Loss 0.172310 || Total Training Translation Loss 0.006016\n",
            "2025-07-10 14:12:03,922 Epoch 1767:\n",
            "\t\t\tTotal Training Recognition Loss 0.589712 || Total Training Translation Loss 0.006349\n",
            "2025-07-10 14:12:04,097 Epoch 1768:\n",
            "\t\t\tTotal Training Recognition Loss 0.171602 || Total Training Translation Loss 0.007549\n",
            "2025-07-10 14:12:04,270 Epoch 1769:\n",
            "\t\t\tTotal Training Recognition Loss 4.039715 || Total Training Translation Loss 0.006429\n",
            "2025-07-10 14:12:04,444 Epoch 1770:\n",
            "\t\t\tTotal Training Recognition Loss 0.360450 || Total Training Translation Loss 0.006894\n",
            "2025-07-10 14:12:04,617 Epoch 1771:\n",
            "\t\t\tTotal Training Recognition Loss 26.965996 || Total Training Translation Loss 0.005262\n",
            "2025-07-10 14:12:04,790 Epoch 1772:\n",
            "\t\t\tTotal Training Recognition Loss 0.519252 || Total Training Translation Loss 0.006050\n",
            "2025-07-10 14:12:04,966 Epoch 1773:\n",
            "\t\t\tTotal Training Recognition Loss 0.440631 || Total Training Translation Loss 0.005137\n",
            "2025-07-10 14:12:05,139 Epoch 1774:\n",
            "\t\t\tTotal Training Recognition Loss 0.198033 || Total Training Translation Loss 0.007489\n",
            "2025-07-10 14:12:05,315 Epoch 1775:\n",
            "\t\t\tTotal Training Recognition Loss 0.502676 || Total Training Translation Loss 0.005197\n",
            "2025-07-10 14:12:05,489 Epoch 1776:\n",
            "\t\t\tTotal Training Recognition Loss 0.315127 || Total Training Translation Loss 0.006007\n",
            "2025-07-10 14:12:05,661 Epoch 1777:\n",
            "\t\t\tTotal Training Recognition Loss 0.529136 || Total Training Translation Loss 0.005708\n",
            "2025-07-10 14:12:05,834 Epoch 1778:\n",
            "\t\t\tTotal Training Recognition Loss 1.086267 || Total Training Translation Loss 0.005828\n",
            "2025-07-10 14:12:06,006 Epoch 1779:\n",
            "\t\t\tTotal Training Recognition Loss 0.493811 || Total Training Translation Loss 0.005093\n",
            "2025-07-10 14:12:06,179 Epoch 1780:\n",
            "\t\t\tTotal Training Recognition Loss 9.579238 || Total Training Translation Loss 0.005415\n",
            "2025-07-10 14:12:06,374 Epoch 1781:\n",
            "\t\t\tTotal Training Recognition Loss 8.448707 || Total Training Translation Loss 0.005408\n",
            "2025-07-10 14:12:06,547 Epoch 1782:\n",
            "\t\t\tTotal Training Recognition Loss 0.279388 || Total Training Translation Loss 0.006436\n",
            "2025-07-10 14:12:06,719 Epoch 1783:\n",
            "\t\t\tTotal Training Recognition Loss 0.214324 || Total Training Translation Loss 0.005817\n",
            "2025-07-10 14:12:06,892 Epoch 1784:\n",
            "\t\t\tTotal Training Recognition Loss 8.373400 || Total Training Translation Loss 0.004985\n",
            "2025-07-10 14:12:07,065 Epoch 1785:\n",
            "\t\t\tTotal Training Recognition Loss 0.413687 || Total Training Translation Loss 0.005607\n",
            "2025-07-10 14:12:07,238 Epoch 1786:\n",
            "\t\t\tTotal Training Recognition Loss 0.441961 || Total Training Translation Loss 0.006874\n",
            "2025-07-10 14:12:07,411 Epoch 1787:\n",
            "\t\t\tTotal Training Recognition Loss 4.560621 || Total Training Translation Loss 0.006458\n",
            "2025-07-10 14:12:07,585 Epoch 1788:\n",
            "\t\t\tTotal Training Recognition Loss 14.972637 || Total Training Translation Loss 0.006013\n",
            "2025-07-10 14:12:07,758 Epoch 1789:\n",
            "\t\t\tTotal Training Recognition Loss 2.953618 || Total Training Translation Loss 0.005922\n",
            "2025-07-10 14:12:07,932 Epoch 1790:\n",
            "\t\t\tTotal Training Recognition Loss 11.330978 || Total Training Translation Loss 0.005368\n",
            "2025-07-10 14:12:08,110 Epoch 1791:\n",
            "\t\t\tTotal Training Recognition Loss 1.367416 || Total Training Translation Loss 0.005970\n",
            "2025-07-10 14:12:08,285 Epoch 1792:\n",
            "\t\t\tTotal Training Recognition Loss 0.508099 || Total Training Translation Loss 0.006227\n",
            "2025-07-10 14:12:08,458 Epoch 1793:\n",
            "\t\t\tTotal Training Recognition Loss 0.251974 || Total Training Translation Loss 0.006703\n",
            "2025-07-10 14:12:08,633 Epoch 1794:\n",
            "\t\t\tTotal Training Recognition Loss 0.205651 || Total Training Translation Loss 0.005777\n",
            "2025-07-10 14:12:08,807 Epoch 1795:\n",
            "\t\t\tTotal Training Recognition Loss 6.619438 || Total Training Translation Loss 0.005943\n",
            "2025-07-10 14:12:08,982 Epoch 1796:\n",
            "\t\t\tTotal Training Recognition Loss 0.214051 || Total Training Translation Loss 0.006553\n",
            "2025-07-10 14:12:09,157 Epoch 1797:\n",
            "\t\t\tTotal Training Recognition Loss 19.006359 || Total Training Translation Loss 0.005913\n",
            "2025-07-10 14:12:09,331 Epoch 1798:\n",
            "\t\t\tTotal Training Recognition Loss 1.232129 || Total Training Translation Loss 0.006577\n",
            "2025-07-10 14:12:09,505 Epoch 1799:\n",
            "\t\t\tTotal Training Recognition Loss 0.282085 || Total Training Translation Loss 0.006663\n",
            "2025-07-10 14:12:09,679 [Epoch: 1800 Step: 00001800] Batch Recognition Loss:  35.549000 => Gls Tokens per Sec:      214 || Batch Translation Loss:   0.006643 => Txt Tokens per Sec:      578 || Lr: 0.000700\n",
            "2025-07-10 14:12:09,917 Validation result at epoch 1800, step     1800: duration: 0.2373s\n",
            "\tRecognition Beam Size: 1\tTranslation Beam Size: 1\tTranslation Beam Alpha: -1\n",
            "\tRecognition Loss: 8008.63721\tTranslation Loss: 224.21828\tPPL: 13.98328\n",
            "\tEval Metric: BLEU\n",
            "\tWER 94.29\t(DEL: 14.29,\tINS: 11.43,\tSUB: 68.57)\n",
            "\tBLEU-4 2.76\t(BLEU-1: 6.67,\tBLEU-2: 4.29,\tBLEU-3: 3.40,\tBLEU-4: 2.76)\n",
            "\tCHRF 23.24\tROUGE 10.44\tFID 0.00\n",
            "2025-07-10 14:12:09,918 Logging Recognition and Translation Outputs\n",
            "2025-07-10 14:12:09,918 ========================================================================================\n",
            "2025-07-10 14:12:09,919 Logging Sequence: 11August_2010_Wednesday_tagesschau-2\n",
            "2025-07-10 14:12:09,919 \tGloss Reference :\tDRUCK TIEF     KOMMEN\n",
            "2025-07-10 14:12:09,919 \tGloss Hypothesis:\t***** NORDWEST LOCH  \n",
            "2025-07-10 14:12:09,919 \tGloss Alignment :\tD     S        S     \n",
            "2025-07-10 14:12:09,920 \t--------------------------------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-07-10 14:12:09,921 \tText Reference  :\t********** ****** ** ***** ***** ***** ******* ***** **** *** ********* ********* ******** ********* ****** ******* *** ***** *** ****** ***** tiefer luftdruck bestimmt in    den   nächsten tagen unser wetter\n",
            "2025-07-10 14:12:09,921 \tText Hypothesis :\tnordwesten bleibt es heute nacht meist trocken sonst muss mit teilweise kräftigen schauern gerechnet werden örtlich mit blitz und donner <unk> <unk>  <unk>     <unk>    <unk> <unk> <unk>    <unk> <unk> <unk> \n",
            "2025-07-10 14:12:09,921 \tText Alignment  :\tI          I      I  I     I     I     I       I     I    I   I         I         I        I         I      I       I   I     I   I      I     S      S         S        S     S     S        S     S     S     \n",
            "2025-07-10 14:12:09,922 ========================================================================================\n",
            "2025-07-10 14:12:09,922 Logging Sequence: 11August_2010_Wednesday_tagesschau-3\n",
            "2025-07-10 14:12:09,923 \tGloss Reference :\tES-BEDEUTET VIEL     WOLKE UND KOENNEN REGEN GEWITTER KOENNEN\n",
            "2025-07-10 14:12:09,923 \tGloss Hypothesis:\t*********** NORDWEST WOLKE *** ******* HEUTE ORT      KOENNEN\n",
            "2025-07-10 14:12:09,923 \tGloss Alignment :\tD           S              D   D       S     S               \n",
            "2025-07-10 14:12:09,923 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:12:09,925 \tText Reference  :\tdas bedeutet viele wolken und ******** **** ****** **************** immer wieder ******* ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** zum   teil  kräftige schauer und   gewitter\n",
            "2025-07-10 14:12:09,925 \tText Hypothesis :\t*** es       auch  länger und ergiebig auch lokale überschwemmungen sind  wieder möglich <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>    <unk>   <unk> <unk>   \n",
            "2025-07-10 14:12:09,926 \tText Alignment  :\tD   S        S     S          I        I    I      I                S            I       I     I     I     I     I     I     I     I     I     I     I     I     I     S     S     S        S       S     S       \n",
            "2025-07-10 14:12:09,926 ========================================================================================\n",
            "2025-07-10 14:12:09,926 Logging Sequence: 11August_2010_Wednesday_tagesschau-8\n",
            "2025-07-10 14:12:09,927 \tGloss Reference :\t***** WIND MAESSIG  SCHWACH REGION  WENN  GEWITTER WIND            KOENNEN\n",
            "2025-07-10 14:12:09,927 \tGloss Hypothesis:\tREGEN LOCH NORDWEST TROCKEN BLEIBEN REGEN VIEL     UEBERSCHWEMMUNG KOENNEN\n",
            "2025-07-10 14:12:09,927 \tGloss Alignment :\tI     S    S        S       S       S     S        S                      \n",
            "2025-07-10 14:12:09,927 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:12:09,929 \tText Reference  :\t****** ** **** ****** *** ******** **** ****** **************** **** ****** ******* meist weht  nur   ein   schwacher wind  aus   unterschiedlichen richtungen der   bei   schauern und   gewittern stark böig  sein  kann \n",
            "2025-07-10 14:12:09,930 \tText Hypothesis :\tregnet es auch länger und ergiebig auch lokale überschwemmungen sind wieder möglich <unk> <unk> <unk> <unk> <unk>     <unk> <unk> <unk>             <unk>      <unk> <unk> <unk>    <unk> <unk>     <unk> <unk> <unk> <unk>\n",
            "2025-07-10 14:12:09,930 \tText Alignment  :\tI      I  I    I      I   I        I    I      I                I    I      I       S     S     S     S     S         S     S     S                 S          S     S     S        S     S         S     S     S     S    \n",
            "2025-07-10 14:12:09,930 ========================================================================================\n",
            "2025-07-10 14:12:09,930 Logging Sequence: 25October_2010_Monday_tagesschau-22\n",
            "2025-07-10 14:12:09,931 \tGloss Reference :\t*** MITTWOCH REGEN   KOENNEN  NORDWEST        WAHRSCHEINLICH NORD       STARK    WIND   \n",
            "2025-07-10 14:12:09,931 \tGloss Hypothesis:\tORT OFT      FEBRUAR GEWITTER UEBERSCHWEMMUNG KOENNEN        DONNERSTAG MANCHMAL KOENNEN\n",
            "2025-07-10 14:12:09,932 \tGloss Alignment :\tI   S        S       S        S               S              S          S        S      \n",
            "2025-07-10 14:12:09,932 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:12:09,934 \tText Reference  :\tam mittwoch hier   und ******** **** ****** **************** **** ****** ******* ***** ***** ***** ***** ***** ***** ***** ***** ***** da    nieselregen in    der   nordwesthälfte an    den   küsten kräftiger wind \n",
            "2025-07-10 14:12:09,934 \tText Hypothesis :\tes auch     länger und ergiebig auch lokale überschwemmungen sind wieder möglich <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>       <unk> <unk> <unk>          <unk> <unk> <unk>  <unk>     <unk>\n",
            "2025-07-10 14:12:09,934 \tText Alignment  :\tS  S        S          I        I    I      I                I    I      I       I     I     I     I     I     I     I     I     I     S     S           S     S     S              S     S     S      S         S    \n",
            "2025-07-10 14:12:09,934 ========================================================================================\n",
            "2025-07-10 14:12:09,934 Logging Sequence: 05May_2011_Thursday_tagesschau-25\n",
            "2025-07-10 14:12:09,935 \tGloss Reference :\t****** JETZT WETTER WIE-AUSSEHEN MORGEN ********** FREITAG SECHSTE MAI     ZEIGEN-BILDSCHIRM\n",
            "2025-07-10 14:12:09,935 \tGloss Hypothesis:\tZWOELF JETZT WETTER ************ MORGEN DONNERSTAG REGEN   ZWOELF  BLEIBEN FEBRUAR          \n",
            "2025-07-10 14:12:09,935 \tGloss Alignment :\tI                   D                   I          S       S       S       S                \n",
            "2025-07-10 14:12:09,935 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:12:09,937 \tText Reference  :\tund nun die wettervorhersage für morgen freitag    den ******** ****** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** sechsten mai  \n",
            "2025-07-10 14:12:09,937 \tText Hypothesis :\t*** nun die wettervorhersage für morgen donnerstag den zwölften august <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>    <unk>\n",
            "2025-07-10 14:12:09,937 \tText Alignment  :\tD                                       S              I        I      I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     S        S    \n",
            "2025-07-10 14:12:09,937 ========================================================================================\n",
            "2025-07-10 14:12:09,938 Epoch 1800:\n",
            "\t\t\tTotal Training Recognition Loss 35.549000 || Total Training Translation Loss 0.006643\n",
            "2025-07-10 14:12:10,107 Epoch 1801:\n",
            "\t\t\tTotal Training Recognition Loss 1.067025 || Total Training Translation Loss 0.006150\n",
            "2025-07-10 14:12:10,277 Epoch 1802:\n",
            "\t\t\tTotal Training Recognition Loss 0.774476 || Total Training Translation Loss 0.006456\n",
            "2025-07-10 14:12:10,448 Epoch 1803:\n",
            "\t\t\tTotal Training Recognition Loss 52.720200 || Total Training Translation Loss 0.005960\n",
            "2025-07-10 14:12:10,619 Epoch 1804:\n",
            "\t\t\tTotal Training Recognition Loss 4.994747 || Total Training Translation Loss 0.007570\n",
            "2025-07-10 14:12:10,790 Epoch 1805:\n",
            "\t\t\tTotal Training Recognition Loss 0.624395 || Total Training Translation Loss 0.007183\n",
            "2025-07-10 14:12:10,961 Epoch 1806:\n",
            "\t\t\tTotal Training Recognition Loss 5.610696 || Total Training Translation Loss 0.005484\n",
            "2025-07-10 14:12:11,132 Epoch 1807:\n",
            "\t\t\tTotal Training Recognition Loss 1.478815 || Total Training Translation Loss 0.007445\n",
            "2025-07-10 14:12:11,302 Epoch 1808:\n",
            "\t\t\tTotal Training Recognition Loss 0.772340 || Total Training Translation Loss 0.005889\n",
            "2025-07-10 14:12:11,473 Epoch 1809:\n",
            "\t\t\tTotal Training Recognition Loss 2.097216 || Total Training Translation Loss 0.006954\n",
            "2025-07-10 14:12:11,644 Epoch 1810:\n",
            "\t\t\tTotal Training Recognition Loss 1.690396 || Total Training Translation Loss 0.006114\n",
            "2025-07-10 14:12:11,816 Epoch 1811:\n",
            "\t\t\tTotal Training Recognition Loss 0.836470 || Total Training Translation Loss 0.006135\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-07-10 14:12:11,986 Epoch 1812:\n",
            "\t\t\tTotal Training Recognition Loss 1.582576 || Total Training Translation Loss 0.008664\n",
            "2025-07-10 14:12:12,165 Epoch 1813:\n",
            "\t\t\tTotal Training Recognition Loss 1.633582 || Total Training Translation Loss 0.006250\n",
            "2025-07-10 14:12:12,337 Epoch 1814:\n",
            "\t\t\tTotal Training Recognition Loss 1.293954 || Total Training Translation Loss 0.006982\n",
            "2025-07-10 14:12:12,508 Epoch 1815:\n",
            "\t\t\tTotal Training Recognition Loss 0.481227 || Total Training Translation Loss 0.006129\n",
            "2025-07-10 14:12:12,679 Epoch 1816:\n",
            "\t\t\tTotal Training Recognition Loss 1.575793 || Total Training Translation Loss 0.006639\n",
            "2025-07-10 14:12:12,852 Epoch 1817:\n",
            "\t\t\tTotal Training Recognition Loss 0.494330 || Total Training Translation Loss 0.005651\n",
            "2025-07-10 14:12:13,024 Epoch 1818:\n",
            "\t\t\tTotal Training Recognition Loss 0.514760 || Total Training Translation Loss 0.006078\n",
            "2025-07-10 14:12:13,200 Epoch 1819:\n",
            "\t\t\tTotal Training Recognition Loss 0.693132 || Total Training Translation Loss 0.006845\n",
            "2025-07-10 14:12:13,371 Epoch 1820:\n",
            "\t\t\tTotal Training Recognition Loss 0.539346 || Total Training Translation Loss 0.006332\n",
            "2025-07-10 14:12:13,542 Epoch 1821:\n",
            "\t\t\tTotal Training Recognition Loss 1.107338 || Total Training Translation Loss 0.006291\n",
            "2025-07-10 14:12:13,713 Epoch 1822:\n",
            "\t\t\tTotal Training Recognition Loss 0.413545 || Total Training Translation Loss 0.008445\n",
            "2025-07-10 14:12:13,884 Epoch 1823:\n",
            "\t\t\tTotal Training Recognition Loss 0.270989 || Total Training Translation Loss 0.006470\n",
            "2025-07-10 14:12:14,055 Epoch 1824:\n",
            "\t\t\tTotal Training Recognition Loss 0.461459 || Total Training Translation Loss 0.005697\n",
            "2025-07-10 14:12:14,230 Epoch 1825:\n",
            "\t\t\tTotal Training Recognition Loss 0.253604 || Total Training Translation Loss 0.006117\n",
            "2025-07-10 14:12:14,401 Epoch 1826:\n",
            "\t\t\tTotal Training Recognition Loss 0.375361 || Total Training Translation Loss 0.007000\n",
            "2025-07-10 14:12:14,572 Epoch 1827:\n",
            "\t\t\tTotal Training Recognition Loss 0.795917 || Total Training Translation Loss 0.005037\n",
            "2025-07-10 14:12:14,743 Epoch 1828:\n",
            "\t\t\tTotal Training Recognition Loss 0.734138 || Total Training Translation Loss 0.007595\n",
            "2025-07-10 14:12:14,917 Epoch 1829:\n",
            "\t\t\tTotal Training Recognition Loss 0.354203 || Total Training Translation Loss 0.006469\n",
            "2025-07-10 14:12:15,089 Epoch 1830:\n",
            "\t\t\tTotal Training Recognition Loss 0.327699 || Total Training Translation Loss 0.006464\n",
            "2025-07-10 14:12:15,306 Epoch 1831:\n",
            "\t\t\tTotal Training Recognition Loss 0.293405 || Total Training Translation Loss 0.006573\n",
            "2025-07-10 14:12:15,523 Epoch 1832:\n",
            "\t\t\tTotal Training Recognition Loss 0.400113 || Total Training Translation Loss 0.005197\n",
            "2025-07-10 14:12:15,740 Epoch 1833:\n",
            "\t\t\tTotal Training Recognition Loss 0.380596 || Total Training Translation Loss 0.005502\n",
            "2025-07-10 14:12:15,957 Epoch 1834:\n",
            "\t\t\tTotal Training Recognition Loss 0.315887 || Total Training Translation Loss 0.006104\n",
            "2025-07-10 14:12:16,173 Epoch 1835:\n",
            "\t\t\tTotal Training Recognition Loss 0.289190 || Total Training Translation Loss 0.005372\n",
            "2025-07-10 14:12:16,345 Epoch 1836:\n",
            "\t\t\tTotal Training Recognition Loss 0.209762 || Total Training Translation Loss 0.005531\n",
            "2025-07-10 14:12:16,516 Epoch 1837:\n",
            "\t\t\tTotal Training Recognition Loss 0.441985 || Total Training Translation Loss 0.005664\n",
            "2025-07-10 14:12:16,686 Epoch 1838:\n",
            "\t\t\tTotal Training Recognition Loss 0.182207 || Total Training Translation Loss 0.006556\n",
            "2025-07-10 14:12:16,856 Epoch 1839:\n",
            "\t\t\tTotal Training Recognition Loss 0.781898 || Total Training Translation Loss 0.006172\n",
            "2025-07-10 14:12:17,026 Epoch 1840:\n",
            "\t\t\tTotal Training Recognition Loss 0.451990 || Total Training Translation Loss 0.005031\n",
            "2025-07-10 14:12:17,197 Epoch 1841:\n",
            "\t\t\tTotal Training Recognition Loss 0.615974 || Total Training Translation Loss 0.006713\n",
            "2025-07-10 14:12:17,368 Epoch 1842:\n",
            "\t\t\tTotal Training Recognition Loss 1.222447 || Total Training Translation Loss 0.005761\n",
            "2025-07-10 14:12:17,537 Epoch 1843:\n",
            "\t\t\tTotal Training Recognition Loss 0.268969 || Total Training Translation Loss 0.005751\n",
            "2025-07-10 14:12:17,708 Epoch 1844:\n",
            "\t\t\tTotal Training Recognition Loss 0.222416 || Total Training Translation Loss 0.005723\n",
            "2025-07-10 14:12:17,879 Epoch 1845:\n",
            "\t\t\tTotal Training Recognition Loss 0.215642 || Total Training Translation Loss 0.005037\n",
            "2025-07-10 14:12:18,052 Epoch 1846:\n",
            "\t\t\tTotal Training Recognition Loss 0.186054 || Total Training Translation Loss 0.005938\n",
            "2025-07-10 14:12:18,224 Epoch 1847:\n",
            "\t\t\tTotal Training Recognition Loss 0.186020 || Total Training Translation Loss 0.005535\n",
            "2025-07-10 14:12:18,395 Epoch 1848:\n",
            "\t\t\tTotal Training Recognition Loss 0.303516 || Total Training Translation Loss 0.006731\n",
            "2025-07-10 14:12:18,565 Epoch 1849:\n",
            "\t\t\tTotal Training Recognition Loss 0.301427 || Total Training Translation Loss 0.005679\n",
            "2025-07-10 14:12:18,735 Epoch 1850:\n",
            "\t\t\tTotal Training Recognition Loss 0.283210 || Total Training Translation Loss 0.005690\n",
            "2025-07-10 14:12:18,906 Epoch 1851:\n",
            "\t\t\tTotal Training Recognition Loss 0.218964 || Total Training Translation Loss 0.005192\n",
            "2025-07-10 14:12:19,078 Epoch 1852:\n",
            "\t\t\tTotal Training Recognition Loss 0.424058 || Total Training Translation Loss 0.006844\n",
            "2025-07-10 14:12:19,249 Epoch 1853:\n",
            "\t\t\tTotal Training Recognition Loss 0.245378 || Total Training Translation Loss 0.006142\n",
            "2025-07-10 14:12:19,419 Epoch 1854:\n",
            "\t\t\tTotal Training Recognition Loss 0.521958 || Total Training Translation Loss 0.005468\n",
            "2025-07-10 14:12:19,589 Epoch 1855:\n",
            "\t\t\tTotal Training Recognition Loss 0.190518 || Total Training Translation Loss 0.005791\n",
            "2025-07-10 14:12:19,759 Epoch 1856:\n",
            "\t\t\tTotal Training Recognition Loss 0.223913 || Total Training Translation Loss 0.005209\n",
            "2025-07-10 14:12:19,930 Epoch 1857:\n",
            "\t\t\tTotal Training Recognition Loss 0.300743 || Total Training Translation Loss 0.005189\n",
            "2025-07-10 14:12:20,100 Epoch 1858:\n",
            "\t\t\tTotal Training Recognition Loss 0.177928 || Total Training Translation Loss 0.006839\n",
            "2025-07-10 14:12:20,271 Epoch 1859:\n",
            "\t\t\tTotal Training Recognition Loss 0.137286 || Total Training Translation Loss 0.006290\n",
            "2025-07-10 14:12:20,442 Epoch 1860:\n",
            "\t\t\tTotal Training Recognition Loss 0.168674 || Total Training Translation Loss 0.006116\n",
            "2025-07-10 14:12:20,612 Epoch 1861:\n",
            "\t\t\tTotal Training Recognition Loss 0.194705 || Total Training Translation Loss 0.006215\n",
            "2025-07-10 14:12:20,783 Epoch 1862:\n",
            "\t\t\tTotal Training Recognition Loss 0.230331 || Total Training Translation Loss 0.005836\n",
            "2025-07-10 14:12:20,954 Epoch 1863:\n",
            "\t\t\tTotal Training Recognition Loss 0.349761 || Total Training Translation Loss 0.005762\n",
            "2025-07-10 14:12:21,173 Epoch 1864:\n",
            "\t\t\tTotal Training Recognition Loss 0.963037 || Total Training Translation Loss 0.005705\n",
            "2025-07-10 14:12:21,390 Epoch 1865:\n",
            "\t\t\tTotal Training Recognition Loss 0.250329 || Total Training Translation Loss 0.005999\n",
            "2025-07-10 14:12:21,606 Epoch 1866:\n",
            "\t\t\tTotal Training Recognition Loss 0.431645 || Total Training Translation Loss 0.005647\n",
            "2025-07-10 14:12:21,829 Epoch 1867:\n",
            "\t\t\tTotal Training Recognition Loss 0.172774 || Total Training Translation Loss 0.005645\n",
            "2025-07-10 14:12:22,002 Epoch 1868:\n",
            "\t\t\tTotal Training Recognition Loss 0.308919 || Total Training Translation Loss 0.005620\n",
            "2025-07-10 14:12:22,172 Epoch 1869:\n",
            "\t\t\tTotal Training Recognition Loss 0.226975 || Total Training Translation Loss 0.007674\n",
            "2025-07-10 14:12:22,342 Epoch 1870:\n",
            "\t\t\tTotal Training Recognition Loss 0.333980 || Total Training Translation Loss 0.005261\n",
            "2025-07-10 14:12:22,513 Epoch 1871:\n",
            "\t\t\tTotal Training Recognition Loss 0.195631 || Total Training Translation Loss 0.006172\n",
            "2025-07-10 14:12:22,684 Epoch 1872:\n",
            "\t\t\tTotal Training Recognition Loss 0.156267 || Total Training Translation Loss 0.006365\n",
            "2025-07-10 14:12:22,854 Epoch 1873:\n",
            "\t\t\tTotal Training Recognition Loss 0.117095 || Total Training Translation Loss 0.005787\n",
            "2025-07-10 14:12:23,026 Epoch 1874:\n",
            "\t\t\tTotal Training Recognition Loss 0.103026 || Total Training Translation Loss 0.005729\n",
            "2025-07-10 14:12:23,196 Epoch 1875:\n",
            "\t\t\tTotal Training Recognition Loss 0.151257 || Total Training Translation Loss 0.005359\n",
            "2025-07-10 14:12:23,369 Epoch 1876:\n",
            "\t\t\tTotal Training Recognition Loss 0.442733 || Total Training Translation Loss 0.005351\n",
            "2025-07-10 14:12:23,540 Epoch 1877:\n",
            "\t\t\tTotal Training Recognition Loss 0.260065 || Total Training Translation Loss 0.005986\n",
            "2025-07-10 14:12:23,718 Epoch 1878:\n",
            "\t\t\tTotal Training Recognition Loss 0.170592 || Total Training Translation Loss 0.006089\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-07-10 14:12:23,894 Epoch 1879:\n",
            "\t\t\tTotal Training Recognition Loss 0.238307 || Total Training Translation Loss 0.006415\n",
            "2025-07-10 14:12:24,066 Epoch 1880:\n",
            "\t\t\tTotal Training Recognition Loss 0.143581 || Total Training Translation Loss 0.005459\n",
            "2025-07-10 14:12:24,242 Epoch 1881:\n",
            "\t\t\tTotal Training Recognition Loss 0.143190 || Total Training Translation Loss 0.004573\n",
            "2025-07-10 14:12:24,417 Epoch 1882:\n",
            "\t\t\tTotal Training Recognition Loss 0.122312 || Total Training Translation Loss 0.005727\n",
            "2025-07-10 14:12:24,590 Epoch 1883:\n",
            "\t\t\tTotal Training Recognition Loss 0.239365 || Total Training Translation Loss 0.005108\n",
            "2025-07-10 14:12:24,763 Epoch 1884:\n",
            "\t\t\tTotal Training Recognition Loss 0.152811 || Total Training Translation Loss 0.005925\n",
            "2025-07-10 14:12:24,938 Epoch 1885:\n",
            "\t\t\tTotal Training Recognition Loss 0.191687 || Total Training Translation Loss 0.007124\n",
            "2025-07-10 14:12:25,114 Epoch 1886:\n",
            "\t\t\tTotal Training Recognition Loss 0.214856 || Total Training Translation Loss 0.004784\n",
            "2025-07-10 14:12:25,287 Epoch 1887:\n",
            "\t\t\tTotal Training Recognition Loss 0.303693 || Total Training Translation Loss 0.006001\n",
            "2025-07-10 14:12:25,461 Epoch 1888:\n",
            "\t\t\tTotal Training Recognition Loss 0.117369 || Total Training Translation Loss 0.006474\n",
            "2025-07-10 14:12:25,634 Epoch 1889:\n",
            "\t\t\tTotal Training Recognition Loss 0.134060 || Total Training Translation Loss 0.005615\n",
            "2025-07-10 14:12:25,808 Epoch 1890:\n",
            "\t\t\tTotal Training Recognition Loss 0.158505 || Total Training Translation Loss 0.005821\n",
            "2025-07-10 14:12:25,981 Epoch 1891:\n",
            "\t\t\tTotal Training Recognition Loss 0.115882 || Total Training Translation Loss 0.005130\n",
            "2025-07-10 14:12:26,153 Epoch 1892:\n",
            "\t\t\tTotal Training Recognition Loss 0.220796 || Total Training Translation Loss 0.005295\n",
            "2025-07-10 14:12:26,325 Epoch 1893:\n",
            "\t\t\tTotal Training Recognition Loss 0.255473 || Total Training Translation Loss 0.005906\n",
            "2025-07-10 14:12:26,498 Epoch 1894:\n",
            "\t\t\tTotal Training Recognition Loss 0.152011 || Total Training Translation Loss 0.004787\n",
            "2025-07-10 14:12:26,671 Epoch 1895:\n",
            "\t\t\tTotal Training Recognition Loss 0.149537 || Total Training Translation Loss 0.005459\n",
            "2025-07-10 14:12:26,844 Epoch 1896:\n",
            "\t\t\tTotal Training Recognition Loss 0.191271 || Total Training Translation Loss 0.005287\n",
            "2025-07-10 14:12:27,017 Epoch 1897:\n",
            "\t\t\tTotal Training Recognition Loss 0.159307 || Total Training Translation Loss 0.005167\n",
            "2025-07-10 14:12:27,189 Epoch 1898:\n",
            "\t\t\tTotal Training Recognition Loss 0.117422 || Total Training Translation Loss 0.005196\n",
            "2025-07-10 14:12:27,362 Epoch 1899:\n",
            "\t\t\tTotal Training Recognition Loss 0.315826 || Total Training Translation Loss 0.005237\n",
            "2025-07-10 14:12:27,535 [Epoch: 1900 Step: 00001900] Batch Recognition Loss:   3.577296 => Gls Tokens per Sec:      215 || Batch Translation Loss:   0.005446 => Txt Tokens per Sec:      582 || Lr: 0.000700\n",
            "2025-07-10 14:12:27,770 Validation result at epoch 1900, step     1900: duration: 0.2342s\n",
            "\tRecognition Beam Size: 1\tTranslation Beam Size: 1\tTranslation Beam Alpha: -1\n",
            "\tRecognition Loss: 6544.74561\tTranslation Loss: 237.23859\tPPL: 16.29799\n",
            "\tEval Metric: BLEU\n",
            "\tWER 85.71\t(DEL: 42.86,\tINS: 2.86,\tSUB: 40.00)\n",
            "\tBLEU-4 2.76\t(BLEU-1: 6.67,\tBLEU-2: 4.29,\tBLEU-3: 3.40,\tBLEU-4: 2.76)\n",
            "\tCHRF 23.23\tROUGE 10.44\tFID 0.00\n",
            "2025-07-10 14:12:27,770 Logging Recognition and Translation Outputs\n",
            "2025-07-10 14:12:27,770 ========================================================================================\n",
            "2025-07-10 14:12:27,771 Logging Sequence: 11August_2010_Wednesday_tagesschau-2\n",
            "2025-07-10 14:12:27,771 \tGloss Reference :\tDRUCK TIEF KOMMEN  \n",
            "2025-07-10 14:12:27,771 \tGloss Hypothesis:\t***** **** NORDWEST\n",
            "2025-07-10 14:12:27,771 \tGloss Alignment :\tD     D    S       \n",
            "2025-07-10 14:12:27,772 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:12:27,773 \tText Reference  :\t********** ****** ** ***** ***** ***** ******* ***** **** *** ********* ********* ******** ********* ****** ******* *** ***** *** ****** ****** tiefer luftdruck bestimmt in    den   nächsten tagen unser wetter\n",
            "2025-07-10 14:12:27,773 \tText Hypothesis :\tnordwesten bleibt es heute nacht meist trocken sonst muss mit teilweise kräftigen schauern gerechnet werden örtlich mit blitz und donner donner <unk>  <unk>     <unk>    <unk> <unk> <unk>    <unk> <unk> <unk> \n",
            "2025-07-10 14:12:27,773 \tText Alignment  :\tI          I      I  I     I     I     I       I     I    I   I         I         I        I         I      I       I   I     I   I      I      S      S         S        S     S     S        S     S     S     \n",
            "2025-07-10 14:12:27,774 ========================================================================================\n",
            "2025-07-10 14:12:27,774 Logging Sequence: 11August_2010_Wednesday_tagesschau-3\n",
            "2025-07-10 14:12:27,774 \tGloss Reference :\tES-BEDEUTET VIEL WOLKE UND KOENNEN REGEN GEWITTER KOENNEN\n",
            "2025-07-10 14:12:27,775 \tGloss Hypothesis:\t*********** **** WOLKE *** ******* ***** HEUTE    KOENNEN\n",
            "2025-07-10 14:12:27,775 \tGloss Alignment :\tD           D          D   D       D     S               \n",
            "2025-07-10 14:12:27,775 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:12:27,777 \tText Reference  :\tdas bedeutet viele wolken und ******** **** ****** **************** immer wieder ******* ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** zum   teil  kräftige schauer und   gewitter\n",
            "2025-07-10 14:12:27,777 \tText Hypothesis :\t*** es       auch  länger und ergiebig auch lokale überschwemmungen sind  wieder möglich <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>    <unk>   <unk> <unk>   \n",
            "2025-07-10 14:12:27,777 \tText Alignment  :\tD   S        S     S          I        I    I      I                S            I       I     I     I     I     I     I     I     I     I     I     I     I     I     S     S     S        S       S     S       \n",
            "2025-07-10 14:12:27,777 ========================================================================================\n",
            "2025-07-10 14:12:27,777 Logging Sequence: 11August_2010_Wednesday_tagesschau-8\n",
            "2025-07-10 14:12:27,778 \tGloss Reference :\tWIND MAESSIG SCHWACH REGION WENN    GEWITTER WIND     KOENNEN\n",
            "2025-07-10 14:12:27,778 \tGloss Hypothesis:\tLOCH TROCKEN BLEIBEN REGEN  KOENNEN VIEL     SPEZIELL KOENNEN\n",
            "2025-07-10 14:12:27,778 \tGloss Alignment :\tS    S       S       S      S       S        S               \n",
            "2025-07-10 14:12:27,779 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:12:27,781 \tText Reference  :\t****** ** **** ****** *** ******** **** ****** **************** **** ****** ******* meist weht  nur   ein   schwacher wind  aus   unterschiedlichen richtungen der   bei   schauern und   gewittern stark böig  sein  kann \n",
            "2025-07-10 14:12:27,781 \tText Hypothesis :\tregnet es auch länger und ergiebig auch lokale überschwemmungen sind wieder möglich <unk> <unk> <unk> <unk> <unk>     <unk> <unk> <unk>             <unk>      <unk> <unk> <unk>    <unk> <unk>     <unk> <unk> <unk> <unk>\n",
            "2025-07-10 14:12:27,781 \tText Alignment  :\tI      I  I    I      I   I        I    I      I                I    I      I       S     S     S     S     S         S     S     S                 S          S     S     S        S     S         S     S     S     S    \n",
            "2025-07-10 14:12:27,782 ========================================================================================\n",
            "2025-07-10 14:12:27,782 Logging Sequence: 25October_2010_Monday_tagesschau-22\n",
            "2025-07-10 14:12:27,782 \tGloss Reference :\tMITTWOCH REGEN   KOENNEN NORDWEST WAHRSCHEINLICH NORD STARK WIND\n",
            "2025-07-10 14:12:27,782 \tGloss Hypothesis:\tOFT      FEBRUAR KOENNEN ******** ************** **** ***** ****\n",
            "2025-07-10 14:12:27,782 \tGloss Alignment :\tS        S               D        D              D    D     D   \n",
            "2025-07-10 14:12:27,783 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:12:27,784 \tText Reference  :\tam mittwoch hier   und ******** **** ****** **************** **** ****** ******* ***** ***** ***** ***** ***** ***** ***** ***** ***** da    nieselregen in    der   nordwesthälfte an    den   küsten kräftiger wind \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-07-10 14:12:27,785 \tText Hypothesis :\tes auch     länger und ergiebig auch lokale überschwemmungen sind wieder möglich <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>       <unk> <unk> <unk>          <unk> <unk> <unk>  <unk>     <unk>\n",
            "2025-07-10 14:12:27,785 \tText Alignment  :\tS  S        S          I        I    I      I                I    I      I       I     I     I     I     I     I     I     I     I     S     S           S     S     S              S     S     S      S         S    \n",
            "2025-07-10 14:12:27,785 ========================================================================================\n",
            "2025-07-10 14:12:27,786 Logging Sequence: 05May_2011_Thursday_tagesschau-25\n",
            "2025-07-10 14:12:27,786 \tGloss Reference :\t****** JETZT WETTER WIE-AUSSEHEN MORGEN FREITAG SECHSTE    MAI    ZEIGEN-BILDSCHIRM\n",
            "2025-07-10 14:12:27,786 \tGloss Hypothesis:\tZWOELF JETZT WETTER ************ ****** ******* DONNERSTAG ZWOELF FEBRUAR          \n",
            "2025-07-10 14:12:27,787 \tGloss Alignment :\tI                   D            D      D       S          S      S                \n",
            "2025-07-10 14:12:27,787 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:12:27,788 \tText Reference  :\tund nun die wettervorhersage für morgen freitag    den ******** ****** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** sechsten mai  \n",
            "2025-07-10 14:12:27,788 \tText Hypothesis :\t*** nun die wettervorhersage für morgen donnerstag den zwölften august <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>    <unk>\n",
            "2025-07-10 14:12:27,788 \tText Alignment  :\tD                                       S              I        I      I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     S        S    \n",
            "2025-07-10 14:12:27,789 ========================================================================================\n",
            "2025-07-10 14:12:27,789 Epoch 1900:\n",
            "\t\t\tTotal Training Recognition Loss 3.577296 || Total Training Translation Loss 0.005446\n",
            "2025-07-10 14:12:27,961 Epoch 1901:\n",
            "\t\t\tTotal Training Recognition Loss 0.108572 || Total Training Translation Loss 0.005840\n",
            "2025-07-10 14:12:28,133 Epoch 1902:\n",
            "\t\t\tTotal Training Recognition Loss 0.395459 || Total Training Translation Loss 0.005496\n",
            "2025-07-10 14:12:28,311 Epoch 1903:\n",
            "\t\t\tTotal Training Recognition Loss 0.307152 || Total Training Translation Loss 0.005713\n",
            "2025-07-10 14:12:28,492 Epoch 1904:\n",
            "\t\t\tTotal Training Recognition Loss 14.604246 || Total Training Translation Loss 0.005096\n",
            "2025-07-10 14:12:28,672 Epoch 1905:\n",
            "\t\t\tTotal Training Recognition Loss 0.421676 || Total Training Translation Loss 0.005514\n",
            "2025-07-10 14:12:28,851 Epoch 1906:\n",
            "\t\t\tTotal Training Recognition Loss 0.163455 || Total Training Translation Loss 0.006082\n",
            "2025-07-10 14:12:29,032 Epoch 1907:\n",
            "\t\t\tTotal Training Recognition Loss 10.919343 || Total Training Translation Loss 0.005680\n",
            "2025-07-10 14:12:29,212 Epoch 1908:\n",
            "\t\t\tTotal Training Recognition Loss 0.190979 || Total Training Translation Loss 0.005789\n",
            "2025-07-10 14:12:29,405 Epoch 1909:\n",
            "\t\t\tTotal Training Recognition Loss 0.439733 || Total Training Translation Loss 0.005842\n",
            "2025-07-10 14:12:29,614 Epoch 1910:\n",
            "\t\t\tTotal Training Recognition Loss 0.251681 || Total Training Translation Loss 0.004717\n",
            "2025-07-10 14:12:29,834 Epoch 1911:\n",
            "\t\t\tTotal Training Recognition Loss 0.356152 || Total Training Translation Loss 0.007607\n",
            "2025-07-10 14:12:30,011 Epoch 1912:\n",
            "\t\t\tTotal Training Recognition Loss 1.019736 || Total Training Translation Loss 0.005582\n",
            "2025-07-10 14:12:30,230 Epoch 1913:\n",
            "\t\t\tTotal Training Recognition Loss 0.566283 || Total Training Translation Loss 0.006528\n",
            "2025-07-10 14:12:30,424 Epoch 1914:\n",
            "\t\t\tTotal Training Recognition Loss 3.815845 || Total Training Translation Loss 0.006417\n",
            "2025-07-10 14:12:30,599 Epoch 1915:\n",
            "\t\t\tTotal Training Recognition Loss 2.627715 || Total Training Translation Loss 0.006346\n",
            "2025-07-10 14:12:30,773 Epoch 1916:\n",
            "\t\t\tTotal Training Recognition Loss 27.914509 || Total Training Translation Loss 0.007367\n",
            "2025-07-10 14:12:30,984 Epoch 1917:\n",
            "\t\t\tTotal Training Recognition Loss 20.323355 || Total Training Translation Loss 0.007750\n",
            "2025-07-10 14:12:31,208 Epoch 1918:\n",
            "\t\t\tTotal Training Recognition Loss 24.369839 || Total Training Translation Loss 0.006563\n",
            "2025-07-10 14:12:31,389 Epoch 1919:\n",
            "\t\t\tTotal Training Recognition Loss 0.587656 || Total Training Translation Loss 0.007578\n",
            "2025-07-10 14:12:31,574 Epoch 1920:\n",
            "\t\t\tTotal Training Recognition Loss 0.693984 || Total Training Translation Loss 0.007841\n",
            "2025-07-10 14:12:31,750 Epoch 1921:\n",
            "\t\t\tTotal Training Recognition Loss 1.072222 || Total Training Translation Loss 0.008018\n",
            "2025-07-10 14:12:31,923 Epoch 1922:\n",
            "\t\t\tTotal Training Recognition Loss 3.436120 || Total Training Translation Loss 0.008891\n",
            "2025-07-10 14:12:32,095 Epoch 1923:\n",
            "\t\t\tTotal Training Recognition Loss 1.550724 || Total Training Translation Loss 0.007958\n",
            "2025-07-10 14:12:32,272 Epoch 1924:\n",
            "\t\t\tTotal Training Recognition Loss 8.954907 || Total Training Translation Loss 0.009935\n",
            "2025-07-10 14:12:32,454 Epoch 1925:\n",
            "\t\t\tTotal Training Recognition Loss 6.073323 || Total Training Translation Loss 0.007604\n",
            "2025-07-10 14:12:32,627 Epoch 1926:\n",
            "\t\t\tTotal Training Recognition Loss 1.590025 || Total Training Translation Loss 0.007016\n",
            "2025-07-10 14:12:32,801 Epoch 1927:\n",
            "\t\t\tTotal Training Recognition Loss 21.683861 || Total Training Translation Loss 0.007798\n",
            "2025-07-10 14:12:32,973 Epoch 1928:\n",
            "\t\t\tTotal Training Recognition Loss 1.449867 || Total Training Translation Loss 0.008499\n",
            "2025-07-10 14:12:33,146 Epoch 1929:\n",
            "\t\t\tTotal Training Recognition Loss 63.458874 || Total Training Translation Loss 0.008087\n",
            "2025-07-10 14:12:33,320 Epoch 1930:\n",
            "\t\t\tTotal Training Recognition Loss 3.938098 || Total Training Translation Loss 0.007908\n",
            "2025-07-10 14:12:33,492 Epoch 1931:\n",
            "\t\t\tTotal Training Recognition Loss 7.490934 || Total Training Translation Loss 0.006803\n",
            "2025-07-10 14:12:33,665 Epoch 1932:\n",
            "\t\t\tTotal Training Recognition Loss 2.115766 || Total Training Translation Loss 0.008062\n",
            "2025-07-10 14:12:33,838 Epoch 1933:\n",
            "\t\t\tTotal Training Recognition Loss 34.698227 || Total Training Translation Loss 0.007105\n",
            "2025-07-10 14:12:34,010 Epoch 1934:\n",
            "\t\t\tTotal Training Recognition Loss 86.124245 || Total Training Translation Loss 0.006475\n",
            "2025-07-10 14:12:34,183 Epoch 1935:\n",
            "\t\t\tTotal Training Recognition Loss 2.233101 || Total Training Translation Loss 0.006100\n",
            "2025-07-10 14:12:34,356 Epoch 1936:\n",
            "\t\t\tTotal Training Recognition Loss 6.048414 || Total Training Translation Loss 0.008693\n",
            "2025-07-10 14:12:34,528 Epoch 1937:\n",
            "\t\t\tTotal Training Recognition Loss 2.358031 || Total Training Translation Loss 0.007109\n",
            "2025-07-10 14:12:34,704 Epoch 1938:\n",
            "\t\t\tTotal Training Recognition Loss 43.658215 || Total Training Translation Loss 0.007379\n",
            "2025-07-10 14:12:34,879 Epoch 1939:\n",
            "\t\t\tTotal Training Recognition Loss 6.700209 || Total Training Translation Loss 0.006519\n",
            "2025-07-10 14:12:35,090 Epoch 1940:\n",
            "\t\t\tTotal Training Recognition Loss 5.953857 || Total Training Translation Loss 0.007201\n",
            "2025-07-10 14:12:35,307 Epoch 1941:\n",
            "\t\t\tTotal Training Recognition Loss 14.572504 || Total Training Translation Loss 0.007798\n",
            "2025-07-10 14:12:35,523 Epoch 1942:\n",
            "\t\t\tTotal Training Recognition Loss 7.530807 || Total Training Translation Loss 0.010242\n",
            "2025-07-10 14:12:35,739 Epoch 1943:\n",
            "\t\t\tTotal Training Recognition Loss 22.623568 || Total Training Translation Loss 0.011087\n",
            "2025-07-10 14:12:35,959 Epoch 1944:\n",
            "\t\t\tTotal Training Recognition Loss 3.979647 || Total Training Translation Loss 0.008250\n",
            "2025-07-10 14:12:36,178 Epoch 1945:\n",
            "\t\t\tTotal Training Recognition Loss 44.924416 || Total Training Translation Loss 0.009472\n",
            "2025-07-10 14:12:36,394 Epoch 1946:\n",
            "\t\t\tTotal Training Recognition Loss 22.165510 || Total Training Translation Loss 0.006976\n",
            "2025-07-10 14:12:36,610 Epoch 1947:\n",
            "\t\t\tTotal Training Recognition Loss 109.130539 || Total Training Translation Loss 0.006943\n",
            "2025-07-10 14:12:36,827 Epoch 1948:\n",
            "\t\t\tTotal Training Recognition Loss 23.949181 || Total Training Translation Loss 0.009471\n",
            "2025-07-10 14:12:37,001 Epoch 1949:\n",
            "\t\t\tTotal Training Recognition Loss 36.709915 || Total Training Translation Loss 0.007374\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-07-10 14:12:37,175 Epoch 1950:\n",
            "\t\t\tTotal Training Recognition Loss 11.141244 || Total Training Translation Loss 0.006085\n",
            "2025-07-10 14:12:37,346 Epoch 1951:\n",
            "\t\t\tTotal Training Recognition Loss 12.456604 || Total Training Translation Loss 0.007539\n",
            "2025-07-10 14:12:37,565 Epoch 1952:\n",
            "\t\t\tTotal Training Recognition Loss 24.472235 || Total Training Translation Loss 0.007034\n",
            "2025-07-10 14:12:37,787 Epoch 1953:\n",
            "\t\t\tTotal Training Recognition Loss 24.755772 || Total Training Translation Loss 0.006416\n",
            "2025-07-10 14:12:38,007 Epoch 1954:\n",
            "\t\t\tTotal Training Recognition Loss 6.734270 || Total Training Translation Loss 0.005988\n",
            "2025-07-10 14:12:38,190 Epoch 1955:\n",
            "\t\t\tTotal Training Recognition Loss 2.357254 || Total Training Translation Loss 0.007544\n",
            "2025-07-10 14:12:38,364 Epoch 1956:\n",
            "\t\t\tTotal Training Recognition Loss 0.812857 || Total Training Translation Loss 0.006364\n",
            "2025-07-10 14:12:38,538 Epoch 1957:\n",
            "\t\t\tTotal Training Recognition Loss 1.912330 || Total Training Translation Loss 0.006527\n",
            "2025-07-10 14:12:38,716 Epoch 1958:\n",
            "\t\t\tTotal Training Recognition Loss 2.668369 || Total Training Translation Loss 0.006478\n",
            "2025-07-10 14:12:38,889 Epoch 1959:\n",
            "\t\t\tTotal Training Recognition Loss 3.725404 || Total Training Translation Loss 0.007603\n",
            "2025-07-10 14:12:39,058 Epoch 1960:\n",
            "\t\t\tTotal Training Recognition Loss 2.165138 || Total Training Translation Loss 0.006327\n",
            "2025-07-10 14:12:39,279 Epoch 1961:\n",
            "\t\t\tTotal Training Recognition Loss 6.784485 || Total Training Translation Loss 0.006563\n",
            "2025-07-10 14:12:39,497 Epoch 1962:\n",
            "\t\t\tTotal Training Recognition Loss 7.921373 || Total Training Translation Loss 0.007140\n",
            "2025-07-10 14:12:39,712 Epoch 1963:\n",
            "\t\t\tTotal Training Recognition Loss 5.764312 || Total Training Translation Loss 0.006844\n",
            "2025-07-10 14:12:39,929 Epoch 1964:\n",
            "\t\t\tTotal Training Recognition Loss 2.208252 || Total Training Translation Loss 0.010694\n",
            "2025-07-10 14:12:40,103 Epoch 1965:\n",
            "\t\t\tTotal Training Recognition Loss 2.889956 || Total Training Translation Loss 0.006661\n",
            "2025-07-10 14:12:40,274 Epoch 1966:\n",
            "\t\t\tTotal Training Recognition Loss 1.725672 || Total Training Translation Loss 0.006143\n",
            "2025-07-10 14:12:40,445 Epoch 1967:\n",
            "\t\t\tTotal Training Recognition Loss 6.063169 || Total Training Translation Loss 0.006936\n",
            "2025-07-10 14:12:40,615 Epoch 1968:\n",
            "\t\t\tTotal Training Recognition Loss 1.054358 || Total Training Translation Loss 0.005721\n",
            "2025-07-10 14:12:40,787 Epoch 1969:\n",
            "\t\t\tTotal Training Recognition Loss 1.908398 || Total Training Translation Loss 0.005833\n",
            "2025-07-10 14:12:40,958 Epoch 1970:\n",
            "\t\t\tTotal Training Recognition Loss 1.045970 || Total Training Translation Loss 0.005166\n",
            "2025-07-10 14:12:41,128 Epoch 1971:\n",
            "\t\t\tTotal Training Recognition Loss 1.187986 || Total Training Translation Loss 0.007326\n",
            "2025-07-10 14:12:41,301 Epoch 1972:\n",
            "\t\t\tTotal Training Recognition Loss 0.875461 || Total Training Translation Loss 0.004973\n",
            "2025-07-10 14:12:41,476 Epoch 1973:\n",
            "\t\t\tTotal Training Recognition Loss 5.101257 || Total Training Translation Loss 0.005976\n",
            "2025-07-10 14:12:41,649 Epoch 1974:\n",
            "\t\t\tTotal Training Recognition Loss 1.102973 || Total Training Translation Loss 0.006386\n",
            "2025-07-10 14:12:41,823 Epoch 1975:\n",
            "\t\t\tTotal Training Recognition Loss 0.986804 || Total Training Translation Loss 0.005981\n",
            "2025-07-10 14:12:42,037 Epoch 1976:\n",
            "\t\t\tTotal Training Recognition Loss 1.087823 || Total Training Translation Loss 0.006814\n",
            "2025-07-10 14:12:42,255 Epoch 1977:\n",
            "\t\t\tTotal Training Recognition Loss 0.847665 || Total Training Translation Loss 0.005301\n",
            "2025-07-10 14:12:42,473 Epoch 1978:\n",
            "\t\t\tTotal Training Recognition Loss 2.131945 || Total Training Translation Loss 0.006099\n",
            "2025-07-10 14:12:42,691 Epoch 1979:\n",
            "\t\t\tTotal Training Recognition Loss 0.950804 || Total Training Translation Loss 0.005038\n",
            "2025-07-10 14:12:42,867 Epoch 1980:\n",
            "\t\t\tTotal Training Recognition Loss 0.954821 || Total Training Translation Loss 0.004856\n",
            "2025-07-10 14:12:43,040 Epoch 1981:\n",
            "\t\t\tTotal Training Recognition Loss 2.262105 || Total Training Translation Loss 0.005443\n",
            "2025-07-10 14:12:43,215 Epoch 1982:\n",
            "\t\t\tTotal Training Recognition Loss 0.659320 || Total Training Translation Loss 0.005423\n",
            "2025-07-10 14:12:43,386 Epoch 1983:\n",
            "\t\t\tTotal Training Recognition Loss 1.495396 || Total Training Translation Loss 0.007693\n",
            "2025-07-10 14:12:43,560 Epoch 1984:\n",
            "\t\t\tTotal Training Recognition Loss 1.590086 || Total Training Translation Loss 0.004708\n",
            "2025-07-10 14:12:43,732 Epoch 1985:\n",
            "\t\t\tTotal Training Recognition Loss 2.693923 || Total Training Translation Loss 0.006820\n",
            "2025-07-10 14:12:43,905 Epoch 1986:\n",
            "\t\t\tTotal Training Recognition Loss 0.680411 || Total Training Translation Loss 0.006304\n",
            "2025-07-10 14:12:44,081 Epoch 1987:\n",
            "\t\t\tTotal Training Recognition Loss 0.559599 || Total Training Translation Loss 0.006836\n",
            "2025-07-10 14:12:44,259 Epoch 1988:\n",
            "\t\t\tTotal Training Recognition Loss 1.120904 || Total Training Translation Loss 0.006987\n",
            "2025-07-10 14:12:44,433 Epoch 1989:\n",
            "\t\t\tTotal Training Recognition Loss 0.409375 || Total Training Translation Loss 0.006852\n",
            "2025-07-10 14:12:44,639 Epoch 1990:\n",
            "\t\t\tTotal Training Recognition Loss 17.972019 || Total Training Translation Loss 0.007141\n",
            "2025-07-10 14:12:44,855 Epoch 1991:\n",
            "\t\t\tTotal Training Recognition Loss 4.861243 || Total Training Translation Loss 0.006616\n",
            "2025-07-10 14:12:45,071 Epoch 1992:\n",
            "\t\t\tTotal Training Recognition Loss 3.068702 || Total Training Translation Loss 0.005125\n",
            "2025-07-10 14:12:45,288 Epoch 1993:\n",
            "\t\t\tTotal Training Recognition Loss 1.197685 || Total Training Translation Loss 0.006375\n",
            "2025-07-10 14:12:45,504 Epoch 1994:\n",
            "\t\t\tTotal Training Recognition Loss 0.478532 || Total Training Translation Loss 0.005740\n",
            "2025-07-10 14:12:45,722 Epoch 1995:\n",
            "\t\t\tTotal Training Recognition Loss 10.789024 || Total Training Translation Loss 0.007728\n",
            "2025-07-10 14:12:45,911 Epoch 1996:\n",
            "\t\t\tTotal Training Recognition Loss 0.998792 || Total Training Translation Loss 0.005788\n",
            "2025-07-10 14:12:46,084 Epoch 1997:\n",
            "\t\t\tTotal Training Recognition Loss 12.623976 || Total Training Translation Loss 0.006969\n",
            "2025-07-10 14:12:46,281 Epoch 1998:\n",
            "\t\t\tTotal Training Recognition Loss 44.273296 || Total Training Translation Loss 0.004848\n",
            "2025-07-10 14:12:46,502 Epoch 1999:\n",
            "\t\t\tTotal Training Recognition Loss 0.465007 || Total Training Translation Loss 0.006311\n",
            "2025-07-10 14:12:46,729 [Epoch: 2000 Step: 00002000] Batch Recognition Loss:   0.819310 => Gls Tokens per Sec:      165 || Batch Translation Loss:   0.007502 => Txt Tokens per Sec:      445 || Lr: 0.000700\n",
            "2025-07-10 14:12:47,036 Validation result at epoch 2000, step     2000: duration: 0.3062s\n",
            "\tRecognition Beam Size: 1\tTranslation Beam Size: 1\tTranslation Beam Alpha: -1\n",
            "\tRecognition Loss: 7420.06787\tTranslation Loss: 237.84140\tPPL: 16.41399\n",
            "\tEval Metric: BLEU\n",
            "\tWER 88.57\t(DEL: 31.43,\tINS: 2.86,\tSUB: 54.29)\n",
            "\tBLEU-4 2.76\t(BLEU-1: 6.67,\tBLEU-2: 4.29,\tBLEU-3: 3.40,\tBLEU-4: 2.76)\n",
            "\tCHRF 23.24\tROUGE 10.44\tFID 0.00\n",
            "2025-07-10 14:12:47,037 Logging Recognition and Translation Outputs\n",
            "2025-07-10 14:12:47,037 ========================================================================================\n",
            "2025-07-10 14:12:47,037 Logging Sequence: 11August_2010_Wednesday_tagesschau-2\n",
            "2025-07-10 14:12:47,038 \tGloss Reference :\tDRUCK TIEF KOMMEN  \n",
            "2025-07-10 14:12:47,038 \tGloss Hypothesis:\t***** **** NORDWEST\n",
            "2025-07-10 14:12:47,039 \tGloss Alignment :\tD     D    S       \n",
            "2025-07-10 14:12:47,039 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:12:47,042 \tText Reference  :\t********** ****** ** ***** ***** ***** ******* ***** **** *** ********* ********* ******** ********* ****** ******* *** ***** *** ****** ***** tiefer luftdruck bestimmt in    den   nächsten tagen unser wetter\n",
            "2025-07-10 14:12:47,042 \tText Hypothesis :\tnordwesten bleibt es heute nacht meist trocken sonst muss mit teilweise kräftigen schauern gerechnet werden örtlich mit blitz und donner <unk> <unk>  <unk>     <unk>    <unk> <unk> <unk>    <unk> <unk> <unk> \n",
            "2025-07-10 14:12:47,043 \tText Alignment  :\tI          I      I  I     I     I     I       I     I    I   I         I         I        I         I      I       I   I     I   I      I     S      S         S        S     S     S        S     S     S     \n",
            "2025-07-10 14:12:47,043 ========================================================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-07-10 14:12:47,043 Logging Sequence: 11August_2010_Wednesday_tagesschau-3\n",
            "2025-07-10 14:12:47,044 \tGloss Reference :\tES-BEDEUTET VIEL WOLKE UND KOENNEN  REGEN      GEWITTER KOENNEN\n",
            "2025-07-10 14:12:47,044 \tGloss Hypothesis:\t*********** **** ***** *** NORDWEST DONNERSTAG HEUTE    KOENNEN\n",
            "2025-07-10 14:12:47,044 \tGloss Alignment :\tD           D    D     D   S        S          S               \n",
            "2025-07-10 14:12:47,044 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:12:47,049 \tText Reference  :\tdas bedeutet viele wolken und ******** **** ****** **************** immer wieder ******* ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** zum   teil  kräftige schauer und   gewitter\n",
            "2025-07-10 14:12:47,049 \tText Hypothesis :\t*** es       auch  länger und ergiebig auch lokale überschwemmungen sind  wieder möglich <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>    <unk>   <unk> <unk>   \n",
            "2025-07-10 14:12:47,049 \tText Alignment  :\tD   S        S     S          I        I    I      I                S            I       I     I     I     I     I     I     I     I     I     I     I     I     I     S     S     S        S       S     S       \n",
            "2025-07-10 14:12:47,049 ========================================================================================\n",
            "2025-07-10 14:12:47,049 Logging Sequence: 11August_2010_Wednesday_tagesschau-8\n",
            "2025-07-10 14:12:47,050 \tGloss Reference :\tWIND MAESSIG SCHWACH REGION  WENN  GEWITTER WIND     KOENNEN\n",
            "2025-07-10 14:12:47,050 \tGloss Hypothesis:\t**** ******* LOCH    BLEIBEN REGEN LOCH     SUEDWEST KOENNEN\n",
            "2025-07-10 14:12:47,051 \tGloss Alignment :\tD    D       S       S       S     S        S               \n",
            "2025-07-10 14:12:47,051 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:12:47,056 \tText Reference  :\t****** ** **** ****** *** ******** **** ****** **************** **** ****** ******* meist weht  nur   ein   schwacher wind  aus   unterschiedlichen richtungen der   bei   schauern und   gewittern stark böig  sein  kann \n",
            "2025-07-10 14:12:47,056 \tText Hypothesis :\tregnet es auch länger und ergiebig auch lokale überschwemmungen sind wieder möglich <unk> <unk> <unk> <unk> <unk>     <unk> <unk> <unk>             <unk>      <unk> <unk> <unk>    <unk> <unk>     <unk> <unk> <unk> <unk>\n",
            "2025-07-10 14:12:47,056 \tText Alignment  :\tI      I  I    I      I   I        I    I      I                I    I      I       S     S     S     S     S         S     S     S                 S          S     S     S        S     S         S     S     S     S    \n",
            "2025-07-10 14:12:47,057 ========================================================================================\n",
            "2025-07-10 14:12:47,057 Logging Sequence: 25October_2010_Monday_tagesschau-22\n",
            "2025-07-10 14:12:47,057 \tGloss Reference :\tMITTWOCH REGEN   KOENNEN    NORDWEST        WAHRSCHEINLICH NORD       STARK WIND   \n",
            "2025-07-10 14:12:47,058 \tGloss Hypothesis:\tOFT      FEBRUAR DONNERSTAG UEBERSCHWEMMUNG KOENNEN        DONNERSTAG JETZT KOENNEN\n",
            "2025-07-10 14:12:47,058 \tGloss Alignment :\tS        S       S          S               S              S          S     S      \n",
            "2025-07-10 14:12:47,058 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:12:47,060 \tText Reference  :\tam mittwoch hier   und ******** **** ****** **************** **** ****** ******* ***** ***** ***** ***** ***** ***** ***** ***** ***** da    nieselregen in    der   nordwesthälfte an    den   küsten kräftiger wind \n",
            "2025-07-10 14:12:47,060 \tText Hypothesis :\tes auch     länger und ergiebig auch lokale überschwemmungen sind wieder möglich <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>       <unk> <unk> <unk>          <unk> <unk> <unk>  <unk>     <unk>\n",
            "2025-07-10 14:12:47,060 \tText Alignment  :\tS  S        S          I        I    I      I                I    I      I       I     I     I     I     I     I     I     I     I     S     S           S     S     S              S     S     S      S         S    \n",
            "2025-07-10 14:12:47,061 ========================================================================================\n",
            "2025-07-10 14:12:47,061 Logging Sequence: 05May_2011_Thursday_tagesschau-25\n",
            "2025-07-10 14:12:47,061 \tGloss Reference :\t****** JETZT WETTER WIE-AUSSEHEN MORGEN FREITAG SECHSTE MAI        ZEIGEN-BILDSCHIRM\n",
            "2025-07-10 14:12:47,061 \tGloss Hypothesis:\tZWOELF JETZT WETTER ************ MORGEN ******* ******* DONNERSTAG ZWOELF           \n",
            "2025-07-10 14:12:47,061 \tGloss Alignment :\tI                   D                   D       D       S          S                \n",
            "2025-07-10 14:12:47,062 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:12:47,063 \tText Reference  :\tund nun die wettervorhersage für morgen freitag    den ******** ****** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** sechsten mai  \n",
            "2025-07-10 14:12:47,063 \tText Hypothesis :\t*** nun die wettervorhersage für morgen donnerstag den zwölften august <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>    <unk>\n",
            "2025-07-10 14:12:47,064 \tText Alignment  :\tD                                       S              I        I      I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     S        S    \n",
            "2025-07-10 14:12:47,064 ========================================================================================\n",
            "2025-07-10 14:12:47,064 Epoch 2000:\n",
            "\t\t\tTotal Training Recognition Loss 0.819310 || Total Training Translation Loss 0.007502\n",
            "2025-07-10 14:12:47,280 Epoch 2001:\n",
            "\t\t\tTotal Training Recognition Loss 2.395389 || Total Training Translation Loss 0.007046\n",
            "2025-07-10 14:12:47,503 Epoch 2002:\n",
            "\t\t\tTotal Training Recognition Loss 68.239876 || Total Training Translation Loss 0.007935\n",
            "2025-07-10 14:12:47,721 Epoch 2003:\n",
            "\t\t\tTotal Training Recognition Loss 0.773573 || Total Training Translation Loss 0.006499\n",
            "2025-07-10 14:12:47,939 Epoch 2004:\n",
            "\t\t\tTotal Training Recognition Loss 6.285077 || Total Training Translation Loss 0.009116\n",
            "2025-07-10 14:12:48,158 Epoch 2005:\n",
            "\t\t\tTotal Training Recognition Loss 5.843941 || Total Training Translation Loss 0.007717\n",
            "2025-07-10 14:12:48,375 Epoch 2006:\n",
            "\t\t\tTotal Training Recognition Loss 9.731336 || Total Training Translation Loss 0.008590\n",
            "2025-07-10 14:12:48,592 Epoch 2007:\n",
            "\t\t\tTotal Training Recognition Loss 7.851405 || Total Training Translation Loss 0.006938\n",
            "2025-07-10 14:12:48,809 Epoch 2008:\n",
            "\t\t\tTotal Training Recognition Loss 6.354004 || Total Training Translation Loss 0.006997\n",
            "2025-07-10 14:12:49,026 Epoch 2009:\n",
            "\t\t\tTotal Training Recognition Loss 5.545115 || Total Training Translation Loss 0.006716\n",
            "2025-07-10 14:12:49,243 Epoch 2010:\n",
            "\t\t\tTotal Training Recognition Loss 6.762060 || Total Training Translation Loss 0.007098\n",
            "2025-07-10 14:12:49,459 Epoch 2011:\n",
            "\t\t\tTotal Training Recognition Loss 3.629724 || Total Training Translation Loss 0.007647\n",
            "2025-07-10 14:12:49,678 Epoch 2012:\n",
            "\t\t\tTotal Training Recognition Loss 2.635857 || Total Training Translation Loss 0.007146\n",
            "2025-07-10 14:12:49,892 Epoch 2013:\n",
            "\t\t\tTotal Training Recognition Loss 11.951270 || Total Training Translation Loss 0.006979\n",
            "2025-07-10 14:12:50,111 Epoch 2014:\n",
            "\t\t\tTotal Training Recognition Loss 0.770146 || Total Training Translation Loss 0.006436\n",
            "2025-07-10 14:12:50,289 Epoch 2015:\n",
            "\t\t\tTotal Training Recognition Loss 0.982572 || Total Training Translation Loss 0.007165\n",
            "2025-07-10 14:12:50,465 Epoch 2016:\n",
            "\t\t\tTotal Training Recognition Loss 0.444616 || Total Training Translation Loss 0.009743\n",
            "2025-07-10 14:12:50,640 Epoch 2017:\n",
            "\t\t\tTotal Training Recognition Loss 0.760271 || Total Training Translation Loss 0.008254\n",
            "2025-07-10 14:12:50,813 Epoch 2018:\n",
            "\t\t\tTotal Training Recognition Loss 18.093781 || Total Training Translation Loss 0.008211\n",
            "2025-07-10 14:12:50,985 Epoch 2019:\n",
            "\t\t\tTotal Training Recognition Loss 36.515667 || Total Training Translation Loss 0.005935\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-07-10 14:12:51,157 Epoch 2020:\n",
            "\t\t\tTotal Training Recognition Loss 1.251723 || Total Training Translation Loss 0.007476\n",
            "2025-07-10 14:12:51,330 Epoch 2021:\n",
            "\t\t\tTotal Training Recognition Loss 3.616409 || Total Training Translation Loss 0.006094\n",
            "2025-07-10 14:12:51,508 Epoch 2022:\n",
            "\t\t\tTotal Training Recognition Loss 8.104889 || Total Training Translation Loss 0.006326\n",
            "2025-07-10 14:12:51,682 Epoch 2023:\n",
            "\t\t\tTotal Training Recognition Loss 1.847376 || Total Training Translation Loss 0.006380\n",
            "2025-07-10 14:12:51,854 Epoch 2024:\n",
            "\t\t\tTotal Training Recognition Loss 1.196131 || Total Training Translation Loss 0.007006\n",
            "2025-07-10 14:12:52,027 Epoch 2025:\n",
            "\t\t\tTotal Training Recognition Loss 1.180070 || Total Training Translation Loss 0.006808\n",
            "2025-07-10 14:12:52,248 Epoch 2026:\n",
            "\t\t\tTotal Training Recognition Loss 2.450505 || Total Training Translation Loss 0.006788\n",
            "2025-07-10 14:12:52,467 Epoch 2027:\n",
            "\t\t\tTotal Training Recognition Loss 9.601316 || Total Training Translation Loss 0.005871\n",
            "2025-07-10 14:12:52,679 Epoch 2028:\n",
            "\t\t\tTotal Training Recognition Loss 1.190370 || Total Training Translation Loss 0.006657\n",
            "2025-07-10 14:12:52,896 Epoch 2029:\n",
            "\t\t\tTotal Training Recognition Loss 2.104078 || Total Training Translation Loss 0.006759\n",
            "2025-07-10 14:12:53,116 Epoch 2030:\n",
            "\t\t\tTotal Training Recognition Loss 6.336827 || Total Training Translation Loss 0.007398\n",
            "2025-07-10 14:12:53,335 Epoch 2031:\n",
            "\t\t\tTotal Training Recognition Loss 0.643017 || Total Training Translation Loss 0.006900\n",
            "2025-07-10 14:12:53,554 Epoch 2032:\n",
            "\t\t\tTotal Training Recognition Loss 2.682811 || Total Training Translation Loss 0.005830\n",
            "2025-07-10 14:12:53,773 Epoch 2033:\n",
            "\t\t\tTotal Training Recognition Loss 0.797870 || Total Training Translation Loss 0.005580\n",
            "2025-07-10 14:12:53,999 Epoch 2034:\n",
            "\t\t\tTotal Training Recognition Loss 1.963794 || Total Training Translation Loss 0.007376\n",
            "2025-07-10 14:12:54,222 Epoch 2035:\n",
            "\t\t\tTotal Training Recognition Loss 4.640015 || Total Training Translation Loss 0.005558\n",
            "2025-07-10 14:12:54,428 Epoch 2036:\n",
            "\t\t\tTotal Training Recognition Loss 2.919215 || Total Training Translation Loss 0.006850\n",
            "2025-07-10 14:12:54,603 Epoch 2037:\n",
            "\t\t\tTotal Training Recognition Loss 1.553065 || Total Training Translation Loss 0.005676\n",
            "2025-07-10 14:12:54,776 Epoch 2038:\n",
            "\t\t\tTotal Training Recognition Loss 0.489776 || Total Training Translation Loss 0.005640\n",
            "2025-07-10 14:12:54,953 Epoch 2039:\n",
            "\t\t\tTotal Training Recognition Loss 0.730882 || Total Training Translation Loss 0.006024\n",
            "2025-07-10 14:12:55,179 Epoch 2040:\n",
            "\t\t\tTotal Training Recognition Loss 0.473308 || Total Training Translation Loss 0.005006\n",
            "2025-07-10 14:12:55,398 Epoch 2041:\n",
            "\t\t\tTotal Training Recognition Loss 0.859501 || Total Training Translation Loss 0.005509\n",
            "2025-07-10 14:12:55,614 Epoch 2042:\n",
            "\t\t\tTotal Training Recognition Loss 0.439137 || Total Training Translation Loss 0.005725\n",
            "2025-07-10 14:12:55,830 Epoch 2043:\n",
            "\t\t\tTotal Training Recognition Loss 0.460654 || Total Training Translation Loss 0.005877\n",
            "2025-07-10 14:12:56,047 Epoch 2044:\n",
            "\t\t\tTotal Training Recognition Loss 0.442658 || Total Training Translation Loss 0.006196\n",
            "2025-07-10 14:12:56,263 Epoch 2045:\n",
            "\t\t\tTotal Training Recognition Loss 1.028888 || Total Training Translation Loss 0.006267\n",
            "2025-07-10 14:12:56,479 Epoch 2046:\n",
            "\t\t\tTotal Training Recognition Loss 0.583303 || Total Training Translation Loss 0.005421\n",
            "2025-07-10 14:12:56,665 Epoch 2047:\n",
            "\t\t\tTotal Training Recognition Loss 0.623447 || Total Training Translation Loss 0.006813\n",
            "2025-07-10 14:12:56,838 Epoch 2048:\n",
            "\t\t\tTotal Training Recognition Loss 0.395115 || Total Training Translation Loss 0.005514\n",
            "2025-07-10 14:12:57,011 Epoch 2049:\n",
            "\t\t\tTotal Training Recognition Loss 0.385513 || Total Training Translation Loss 0.006114\n",
            "2025-07-10 14:12:57,184 Epoch 2050:\n",
            "\t\t\tTotal Training Recognition Loss 19.543600 || Total Training Translation Loss 0.004761\n",
            "2025-07-10 14:12:57,360 Epoch 2051:\n",
            "\t\t\tTotal Training Recognition Loss 0.813752 || Total Training Translation Loss 0.005074\n",
            "2025-07-10 14:12:57,533 Epoch 2052:\n",
            "\t\t\tTotal Training Recognition Loss 0.590249 || Total Training Translation Loss 0.005493\n",
            "2025-07-10 14:12:57,706 Epoch 2053:\n",
            "\t\t\tTotal Training Recognition Loss 3.563639 || Total Training Translation Loss 0.005892\n",
            "2025-07-10 14:12:57,879 Epoch 2054:\n",
            "\t\t\tTotal Training Recognition Loss 4.123378 || Total Training Translation Loss 0.006315\n",
            "2025-07-10 14:12:58,053 Epoch 2055:\n",
            "\t\t\tTotal Training Recognition Loss 0.603223 || Total Training Translation Loss 0.006565\n",
            "2025-07-10 14:12:58,226 Epoch 2056:\n",
            "\t\t\tTotal Training Recognition Loss 0.408756 || Total Training Translation Loss 0.006015\n",
            "2025-07-10 14:12:58,399 Epoch 2057:\n",
            "\t\t\tTotal Training Recognition Loss 0.820051 || Total Training Translation Loss 0.005467\n",
            "2025-07-10 14:12:58,573 Epoch 2058:\n",
            "\t\t\tTotal Training Recognition Loss 1.081043 || Total Training Translation Loss 0.006223\n",
            "2025-07-10 14:12:58,746 Epoch 2059:\n",
            "\t\t\tTotal Training Recognition Loss 4.848233 || Total Training Translation Loss 0.006233\n",
            "2025-07-10 14:12:58,919 Epoch 2060:\n",
            "\t\t\tTotal Training Recognition Loss 3.182528 || Total Training Translation Loss 0.005456\n",
            "2025-07-10 14:12:59,093 Epoch 2061:\n",
            "\t\t\tTotal Training Recognition Loss 5.709800 || Total Training Translation Loss 0.006343\n",
            "2025-07-10 14:12:59,267 Epoch 2062:\n",
            "\t\t\tTotal Training Recognition Loss 1.622462 || Total Training Translation Loss 0.005752\n",
            "2025-07-10 14:12:59,441 Epoch 2063:\n",
            "\t\t\tTotal Training Recognition Loss 1.466561 || Total Training Translation Loss 0.006167\n",
            "2025-07-10 14:12:59,663 Epoch 2064:\n",
            "\t\t\tTotal Training Recognition Loss 2.347826 || Total Training Translation Loss 0.005815\n",
            "2025-07-10 14:12:59,881 Epoch 2065:\n",
            "\t\t\tTotal Training Recognition Loss 0.364157 || Total Training Translation Loss 0.005478\n",
            "2025-07-10 14:13:00,100 Epoch 2066:\n",
            "\t\t\tTotal Training Recognition Loss 0.370677 || Total Training Translation Loss 0.006263\n",
            "2025-07-10 14:13:00,272 Epoch 2067:\n",
            "\t\t\tTotal Training Recognition Loss 0.422855 || Total Training Translation Loss 0.005424\n",
            "2025-07-10 14:13:00,445 Epoch 2068:\n",
            "\t\t\tTotal Training Recognition Loss 0.330476 || Total Training Translation Loss 0.004972\n",
            "2025-07-10 14:13:00,615 Epoch 2069:\n",
            "\t\t\tTotal Training Recognition Loss 0.544727 || Total Training Translation Loss 0.006181\n",
            "2025-07-10 14:13:00,786 Epoch 2070:\n",
            "\t\t\tTotal Training Recognition Loss 1.053518 || Total Training Translation Loss 0.005377\n",
            "2025-07-10 14:13:00,958 Epoch 2071:\n",
            "\t\t\tTotal Training Recognition Loss 0.484872 || Total Training Translation Loss 0.004701\n",
            "2025-07-10 14:13:01,130 Epoch 2072:\n",
            "\t\t\tTotal Training Recognition Loss 0.890889 || Total Training Translation Loss 0.006482\n",
            "2025-07-10 14:13:01,302 Epoch 2073:\n",
            "\t\t\tTotal Training Recognition Loss 0.720495 || Total Training Translation Loss 0.006110\n",
            "2025-07-10 14:13:01,474 Epoch 2074:\n",
            "\t\t\tTotal Training Recognition Loss 0.346201 || Total Training Translation Loss 0.005513\n",
            "2025-07-10 14:13:01,646 Epoch 2075:\n",
            "\t\t\tTotal Training Recognition Loss 0.555070 || Total Training Translation Loss 0.005869\n",
            "2025-07-10 14:13:01,818 Epoch 2076:\n",
            "\t\t\tTotal Training Recognition Loss 0.527718 || Total Training Translation Loss 0.005893\n",
            "2025-07-10 14:13:01,989 Epoch 2077:\n",
            "\t\t\tTotal Training Recognition Loss 0.364241 || Total Training Translation Loss 0.005263\n",
            "2025-07-10 14:13:02,160 Epoch 2078:\n",
            "\t\t\tTotal Training Recognition Loss 0.313891 || Total Training Translation Loss 0.006493\n",
            "2025-07-10 14:13:02,333 Epoch 2079:\n",
            "\t\t\tTotal Training Recognition Loss 0.578420 || Total Training Translation Loss 0.006199\n",
            "2025-07-10 14:13:02,509 Epoch 2080:\n",
            "\t\t\tTotal Training Recognition Loss 0.831746 || Total Training Translation Loss 0.005954\n",
            "2025-07-10 14:13:02,683 Epoch 2081:\n",
            "\t\t\tTotal Training Recognition Loss 1.586848 || Total Training Translation Loss 0.006128\n",
            "2025-07-10 14:13:02,859 Epoch 2082:\n",
            "\t\t\tTotal Training Recognition Loss 0.258219 || Total Training Translation Loss 0.005171\n",
            "2025-07-10 14:13:03,034 Epoch 2083:\n",
            "\t\t\tTotal Training Recognition Loss 0.409047 || Total Training Translation Loss 0.006261\n",
            "2025-07-10 14:13:03,257 Epoch 2084:\n",
            "\t\t\tTotal Training Recognition Loss 0.491004 || Total Training Translation Loss 0.006418\n",
            "2025-07-10 14:13:03,479 Epoch 2085:\n",
            "\t\t\tTotal Training Recognition Loss 0.114841 || Total Training Translation Loss 0.006693\n",
            "2025-07-10 14:13:03,699 Epoch 2086:\n",
            "\t\t\tTotal Training Recognition Loss 0.220059 || Total Training Translation Loss 0.005604\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-07-10 14:13:03,872 Epoch 2087:\n",
            "\t\t\tTotal Training Recognition Loss 0.379939 || Total Training Translation Loss 0.006611\n",
            "2025-07-10 14:13:04,045 Epoch 2088:\n",
            "\t\t\tTotal Training Recognition Loss 0.259462 || Total Training Translation Loss 0.006313\n",
            "2025-07-10 14:13:04,221 Epoch 2089:\n",
            "\t\t\tTotal Training Recognition Loss 0.458758 || Total Training Translation Loss 0.005662\n",
            "2025-07-10 14:13:04,394 Epoch 2090:\n",
            "\t\t\tTotal Training Recognition Loss 0.201686 || Total Training Translation Loss 0.006351\n",
            "2025-07-10 14:13:04,567 Epoch 2091:\n",
            "\t\t\tTotal Training Recognition Loss 0.296030 || Total Training Translation Loss 0.005920\n",
            "2025-07-10 14:13:04,740 Epoch 2092:\n",
            "\t\t\tTotal Training Recognition Loss 0.285684 || Total Training Translation Loss 0.005745\n",
            "2025-07-10 14:13:04,916 Epoch 2093:\n",
            "\t\t\tTotal Training Recognition Loss 0.200035 || Total Training Translation Loss 0.006230\n",
            "2025-07-10 14:13:05,090 Epoch 2094:\n",
            "\t\t\tTotal Training Recognition Loss 0.191862 || Total Training Translation Loss 0.006699\n",
            "2025-07-10 14:13:05,264 Epoch 2095:\n",
            "\t\t\tTotal Training Recognition Loss 0.541365 || Total Training Translation Loss 0.005240\n",
            "2025-07-10 14:13:05,444 Epoch 2096:\n",
            "\t\t\tTotal Training Recognition Loss 0.224422 || Total Training Translation Loss 0.004991\n",
            "2025-07-10 14:13:05,621 Epoch 2097:\n",
            "\t\t\tTotal Training Recognition Loss 0.153788 || Total Training Translation Loss 0.006649\n",
            "2025-07-10 14:13:05,801 Epoch 2098:\n",
            "\t\t\tTotal Training Recognition Loss 0.163965 || Total Training Translation Loss 0.004918\n",
            "2025-07-10 14:13:05,983 Epoch 2099:\n",
            "\t\t\tTotal Training Recognition Loss 0.193407 || Total Training Translation Loss 0.005745\n",
            "2025-07-10 14:13:06,161 [Epoch: 2100 Step: 00002100] Batch Recognition Loss:   0.212743 => Gls Tokens per Sec:      209 || Batch Translation Loss:   0.004875 => Txt Tokens per Sec:      565 || Lr: 0.000700\n",
            "2025-07-10 14:13:06,403 Validation result at epoch 2100, step     2100: duration: 0.2407s\n",
            "\tRecognition Beam Size: 1\tTranslation Beam Size: 1\tTranslation Beam Alpha: -1\n",
            "\tRecognition Loss: 7708.91211\tTranslation Loss: 243.06491\tPPL: 17.45432\n",
            "\tEval Metric: BLEU\n",
            "\tWER 82.86\t(DEL: 28.57,\tINS: 2.86,\tSUB: 51.43)\n",
            "\tBLEU-4 2.76\t(BLEU-1: 6.67,\tBLEU-2: 4.29,\tBLEU-3: 3.40,\tBLEU-4: 2.76)\n",
            "\tCHRF 23.02\tROUGE 10.44\tFID 0.00\n",
            "2025-07-10 14:13:06,404 Logging Recognition and Translation Outputs\n",
            "2025-07-10 14:13:06,404 ========================================================================================\n",
            "2025-07-10 14:13:06,404 Logging Sequence: 11August_2010_Wednesday_tagesschau-2\n",
            "2025-07-10 14:13:06,405 \tGloss Reference :\tDRUCK TIEF     KOMMEN\n",
            "2025-07-10 14:13:06,405 \tGloss Hypothesis:\t***** NORDWEST LOCH  \n",
            "2025-07-10 14:13:06,406 \tGloss Alignment :\tD     S        S     \n",
            "2025-07-10 14:13:06,406 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:13:06,409 \tText Reference  :\t********** ****** ** ***** ***** ***** ******* ***** **** *** ********* ********* ******** ********* ****** ******* *** ***** *** ****** ***** tiefer luftdruck bestimmt in    den   nächsten tagen unser wetter\n",
            "2025-07-10 14:13:06,410 \tText Hypothesis :\tnordwesten bleibt es heute nacht meist trocken sonst muss mit teilweise kräftigen schauern gerechnet werden örtlich mit blitz und donner <unk> <unk>  <unk>     <unk>    <unk> <unk> <unk>    <unk> <unk> <unk> \n",
            "2025-07-10 14:13:06,410 \tText Alignment  :\tI          I      I  I     I     I     I       I     I    I   I         I         I        I         I      I       I   I     I   I      I     S      S         S        S     S     S        S     S     S     \n",
            "2025-07-10 14:13:06,410 ========================================================================================\n",
            "2025-07-10 14:13:06,410 Logging Sequence: 11August_2010_Wednesday_tagesschau-3\n",
            "2025-07-10 14:13:06,411 \tGloss Reference :\tES-BEDEUTET VIEL WOLKE UND   KOENNEN REGEN      GEWITTER KOENNEN\n",
            "2025-07-10 14:13:06,412 \tGloss Hypothesis:\t*********** **** WOLKE JETZT WOLKE   DONNERSTAG HEUTE    KOENNEN\n",
            "2025-07-10 14:13:06,412 \tGloss Alignment :\tD           D          S     S       S          S               \n",
            "2025-07-10 14:13:06,412 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:13:06,416 \tText Reference  :\tdas bedeutet viele wolken und ******** **** ****** **************** immer wieder ******* ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** zum   teil  kräftige schauer und   gewitter\n",
            "2025-07-10 14:13:06,417 \tText Hypothesis :\t*** es       auch  länger und ergiebig auch lokale überschwemmungen sind  wieder möglich <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>    <unk>   <unk> <unk>   \n",
            "2025-07-10 14:13:06,417 \tText Alignment  :\tD   S        S     S          I        I    I      I                S            I       I     I     I     I     I     I     I     I     I     I     I     I     I     S     S     S        S       S     S       \n",
            "2025-07-10 14:13:06,417 ========================================================================================\n",
            "2025-07-10 14:13:06,417 Logging Sequence: 11August_2010_Wednesday_tagesschau-8\n",
            "2025-07-10 14:13:06,418 \tGloss Reference :\tWIND MAESSIG SCHWACH REGION  WENN     GEWITTER WIND     KOENNEN\n",
            "2025-07-10 14:13:06,418 \tGloss Hypothesis:\t**** LOCH    TROCKEN BLEIBEN SPEZIELL KOENNEN  SPEZIELL KOENNEN\n",
            "2025-07-10 14:13:06,419 \tGloss Alignment :\tD    S       S       S       S        S        S               \n",
            "2025-07-10 14:13:06,419 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:13:06,424 \tText Reference  :\t** **** ****** *** ******** **** ****** **************** **** ****** ******* ***** meist weht  nur   ein   schwacher wind  aus   unterschiedlichen richtungen der   bei   schauern und   gewittern stark böig  sein  kann \n",
            "2025-07-10 14:13:06,424 \tText Hypothesis :\tes auch länger und ergiebig auch lokale überschwemmungen sind wieder möglich <unk> <unk> <unk> <unk> <unk> <unk>     <unk> <unk> <unk>             <unk>      <unk> <unk> <unk>    <unk> <unk>     <unk> <unk> <unk> <unk>\n",
            "2025-07-10 14:13:06,424 \tText Alignment  :\tI  I    I      I   I        I    I      I                I    I      I       I     S     S     S     S     S         S     S     S                 S          S     S     S        S     S         S     S     S     S    \n",
            "2025-07-10 14:13:06,425 ========================================================================================\n",
            "2025-07-10 14:13:06,425 Logging Sequence: 25October_2010_Monday_tagesschau-22\n",
            "2025-07-10 14:13:06,425 \tGloss Reference :\tMITTWOCH REGEN    KOENNEN NORDWEST WAHRSCHEINLICH NORD STARK      WIND   \n",
            "2025-07-10 14:13:06,426 \tGloss Hypothesis:\t******** GEWITTER KOENNEN ******** ************** **** DONNERSTAG KOENNEN\n",
            "2025-07-10 14:13:06,426 \tGloss Alignment :\tD        S                D        D              D    S          S      \n",
            "2025-07-10 14:13:06,426 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:13:06,430 \tText Reference  :\tam mittwoch hier   und ******** **** ****** **************** **** ****** ******* ***** ***** ***** ***** ***** ***** ***** ***** ***** da    nieselregen in    der   nordwesthälfte an    den   küsten kräftiger wind \n",
            "2025-07-10 14:13:06,431 \tText Hypothesis :\tes auch     länger und ergiebig auch lokale überschwemmungen sind wieder möglich <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>       <unk> <unk> <unk>          <unk> <unk> <unk>  <unk>     <unk>\n",
            "2025-07-10 14:13:06,431 \tText Alignment  :\tS  S        S          I        I    I      I                I    I      I       I     I     I     I     I     I     I     I     I     S     S           S     S     S              S     S     S      S         S    \n",
            "2025-07-10 14:13:06,431 ========================================================================================\n",
            "2025-07-10 14:13:06,431 Logging Sequence: 05May_2011_Thursday_tagesschau-25\n",
            "2025-07-10 14:13:06,432 \tGloss Reference :\t****** JETZT WETTER WIE-AUSSEHEN MORGEN FREITAG SECHSTE    MAI    ZEIGEN-BILDSCHIRM\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-07-10 14:13:06,432 \tGloss Hypothesis:\tZWOELF JETZT WETTER ************ MORGEN ******* DONNERSTAG ZWOELF FEBRUAR          \n",
            "2025-07-10 14:13:06,432 \tGloss Alignment :\tI                   D                   D       S          S      S                \n",
            "2025-07-10 14:13:06,433 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:13:06,436 \tText Reference  :\tund nun die wettervorhersage für morgen freitag    den ******** ****** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** sechsten mai  \n",
            "2025-07-10 14:13:06,436 \tText Hypothesis :\t*** nun die wettervorhersage für morgen donnerstag den zwölften august <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>    <unk>\n",
            "2025-07-10 14:13:06,436 \tText Alignment  :\tD                                       S              I        I      I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     S        S    \n",
            "2025-07-10 14:13:06,436 ========================================================================================\n",
            "2025-07-10 14:13:06,437 Epoch 2100:\n",
            "\t\t\tTotal Training Recognition Loss 0.212743 || Total Training Translation Loss 0.004875\n",
            "2025-07-10 14:13:06,652 Epoch 2101:\n",
            "\t\t\tTotal Training Recognition Loss 0.159822 || Total Training Translation Loss 0.005672\n",
            "2025-07-10 14:13:06,871 Epoch 2102:\n",
            "\t\t\tTotal Training Recognition Loss 0.349453 || Total Training Translation Loss 0.005222\n",
            "2025-07-10 14:13:07,086 Epoch 2103:\n",
            "\t\t\tTotal Training Recognition Loss 0.298574 || Total Training Translation Loss 0.006351\n",
            "2025-07-10 14:13:07,303 Epoch 2104:\n",
            "\t\t\tTotal Training Recognition Loss 0.168812 || Total Training Translation Loss 0.006659\n",
            "2025-07-10 14:13:07,520 Epoch 2105:\n",
            "\t\t\tTotal Training Recognition Loss 0.143672 || Total Training Translation Loss 0.005774\n",
            "2025-07-10 14:13:07,736 Epoch 2106:\n",
            "\t\t\tTotal Training Recognition Loss 0.228738 || Total Training Translation Loss 0.005275\n",
            "2025-07-10 14:13:07,953 Epoch 2107:\n",
            "\t\t\tTotal Training Recognition Loss 0.166739 || Total Training Translation Loss 0.006062\n",
            "2025-07-10 14:13:08,171 Epoch 2108:\n",
            "\t\t\tTotal Training Recognition Loss 0.828250 || Total Training Translation Loss 0.005448\n",
            "2025-07-10 14:13:08,390 Epoch 2109:\n",
            "\t\t\tTotal Training Recognition Loss 0.144636 || Total Training Translation Loss 0.005550\n",
            "2025-07-10 14:13:08,610 Epoch 2110:\n",
            "\t\t\tTotal Training Recognition Loss 0.163927 || Total Training Translation Loss 0.005644\n",
            "2025-07-10 14:13:08,828 Epoch 2111:\n",
            "\t\t\tTotal Training Recognition Loss 0.126962 || Total Training Translation Loss 0.005933\n",
            "2025-07-10 14:13:09,046 Epoch 2112:\n",
            "\t\t\tTotal Training Recognition Loss 0.107344 || Total Training Translation Loss 0.006823\n",
            "2025-07-10 14:13:09,257 Epoch 2113:\n",
            "\t\t\tTotal Training Recognition Loss 0.264346 || Total Training Translation Loss 0.006924\n",
            "2025-07-10 14:13:09,430 Epoch 2114:\n",
            "\t\t\tTotal Training Recognition Loss 0.179550 || Total Training Translation Loss 0.005184\n",
            "2025-07-10 14:13:09,604 Epoch 2115:\n",
            "\t\t\tTotal Training Recognition Loss 0.272811 || Total Training Translation Loss 0.007496\n",
            "2025-07-10 14:13:09,778 Epoch 2116:\n",
            "\t\t\tTotal Training Recognition Loss 0.321165 || Total Training Translation Loss 0.005749\n",
            "2025-07-10 14:13:09,952 Epoch 2117:\n",
            "\t\t\tTotal Training Recognition Loss 0.828243 || Total Training Translation Loss 0.005326\n",
            "2025-07-10 14:13:10,125 Epoch 2118:\n",
            "\t\t\tTotal Training Recognition Loss 0.373207 || Total Training Translation Loss 0.006497\n",
            "2025-07-10 14:13:10,300 Epoch 2119:\n",
            "\t\t\tTotal Training Recognition Loss 25.175217 || Total Training Translation Loss 0.006677\n",
            "2025-07-10 14:13:10,474 Epoch 2120:\n",
            "\t\t\tTotal Training Recognition Loss 0.220501 || Total Training Translation Loss 0.005935\n",
            "2025-07-10 14:13:10,647 Epoch 2121:\n",
            "\t\t\tTotal Training Recognition Loss 0.127337 || Total Training Translation Loss 0.005753\n",
            "2025-07-10 14:13:10,821 Epoch 2122:\n",
            "\t\t\tTotal Training Recognition Loss 0.124001 || Total Training Translation Loss 0.006651\n",
            "2025-07-10 14:13:10,999 Epoch 2123:\n",
            "\t\t\tTotal Training Recognition Loss 0.296383 || Total Training Translation Loss 0.006636\n",
            "2025-07-10 14:13:11,173 Epoch 2124:\n",
            "\t\t\tTotal Training Recognition Loss 0.377644 || Total Training Translation Loss 0.006881\n",
            "2025-07-10 14:13:11,348 Epoch 2125:\n",
            "\t\t\tTotal Training Recognition Loss 1.015099 || Total Training Translation Loss 0.008750\n",
            "2025-07-10 14:13:11,522 Epoch 2126:\n",
            "\t\t\tTotal Training Recognition Loss 0.169397 || Total Training Translation Loss 0.008336\n",
            "2025-07-10 14:13:11,695 Epoch 2127:\n",
            "\t\t\tTotal Training Recognition Loss 48.207214 || Total Training Translation Loss 0.007195\n",
            "2025-07-10 14:13:11,915 Epoch 2128:\n",
            "\t\t\tTotal Training Recognition Loss 0.749967 || Total Training Translation Loss 0.006169\n",
            "2025-07-10 14:13:12,092 Epoch 2129:\n",
            "\t\t\tTotal Training Recognition Loss 1.719875 || Total Training Translation Loss 0.005577\n",
            "2025-07-10 14:13:12,266 Epoch 2130:\n",
            "\t\t\tTotal Training Recognition Loss 0.302233 || Total Training Translation Loss 0.006459\n",
            "2025-07-10 14:13:12,440 Epoch 2131:\n",
            "\t\t\tTotal Training Recognition Loss 0.620971 || Total Training Translation Loss 0.009162\n",
            "2025-07-10 14:13:12,614 Epoch 2132:\n",
            "\t\t\tTotal Training Recognition Loss 4.221326 || Total Training Translation Loss 0.006780\n",
            "2025-07-10 14:13:12,788 Epoch 2133:\n",
            "\t\t\tTotal Training Recognition Loss 39.908878 || Total Training Translation Loss 0.005933\n",
            "2025-07-10 14:13:12,961 Epoch 2134:\n",
            "\t\t\tTotal Training Recognition Loss 1.777827 || Total Training Translation Loss 0.006356\n",
            "2025-07-10 14:13:13,135 Epoch 2135:\n",
            "\t\t\tTotal Training Recognition Loss 0.194200 || Total Training Translation Loss 0.006463\n",
            "2025-07-10 14:13:13,309 Epoch 2136:\n",
            "\t\t\tTotal Training Recognition Loss 0.609304 || Total Training Translation Loss 0.008645\n",
            "2025-07-10 14:13:13,483 Epoch 2137:\n",
            "\t\t\tTotal Training Recognition Loss 0.296835 || Total Training Translation Loss 0.006212\n",
            "2025-07-10 14:13:13,662 Epoch 2138:\n",
            "\t\t\tTotal Training Recognition Loss 0.324887 || Total Training Translation Loss 0.006206\n",
            "2025-07-10 14:13:13,837 Epoch 2139:\n",
            "\t\t\tTotal Training Recognition Loss 32.176434 || Total Training Translation Loss 0.005481\n",
            "2025-07-10 14:13:14,012 Epoch 2140:\n",
            "\t\t\tTotal Training Recognition Loss 10.597897 || Total Training Translation Loss 0.006211\n",
            "2025-07-10 14:13:14,187 Epoch 2141:\n",
            "\t\t\tTotal Training Recognition Loss 131.829590 || Total Training Translation Loss 0.005970\n",
            "2025-07-10 14:13:14,361 Epoch 2142:\n",
            "\t\t\tTotal Training Recognition Loss 1.227796 || Total Training Translation Loss 0.006706\n",
            "2025-07-10 14:13:14,535 Epoch 2143:\n",
            "\t\t\tTotal Training Recognition Loss 4.121333 || Total Training Translation Loss 0.007144\n",
            "2025-07-10 14:13:14,709 Epoch 2144:\n",
            "\t\t\tTotal Training Recognition Loss 0.350960 || Total Training Translation Loss 0.007141\n",
            "2025-07-10 14:13:14,883 Epoch 2145:\n",
            "\t\t\tTotal Training Recognition Loss 2.479894 || Total Training Translation Loss 0.007447\n",
            "2025-07-10 14:13:15,059 Epoch 2146:\n",
            "\t\t\tTotal Training Recognition Loss 0.517070 || Total Training Translation Loss 0.006434\n",
            "2025-07-10 14:13:15,236 Epoch 2147:\n",
            "\t\t\tTotal Training Recognition Loss 1.736039 || Total Training Translation Loss 0.006797\n",
            "2025-07-10 14:13:15,410 Epoch 2148:\n",
            "\t\t\tTotal Training Recognition Loss 0.288951 || Total Training Translation Loss 0.006951\n",
            "2025-07-10 14:13:15,583 Epoch 2149:\n",
            "\t\t\tTotal Training Recognition Loss 0.360656 || Total Training Translation Loss 0.006107\n",
            "2025-07-10 14:13:15,757 Epoch 2150:\n",
            "\t\t\tTotal Training Recognition Loss 0.570444 || Total Training Translation Loss 0.005361\n",
            "2025-07-10 14:13:15,932 Epoch 2151:\n",
            "\t\t\tTotal Training Recognition Loss 3.907803 || Total Training Translation Loss 0.007663\n",
            "2025-07-10 14:13:16,107 Epoch 2152:\n",
            "\t\t\tTotal Training Recognition Loss 0.594406 || Total Training Translation Loss 0.006204\n",
            "2025-07-10 14:13:16,283 Epoch 2153:\n",
            "\t\t\tTotal Training Recognition Loss 0.266670 || Total Training Translation Loss 0.007875\n",
            "2025-07-10 14:13:16,457 Epoch 2154:\n",
            "\t\t\tTotal Training Recognition Loss 0.441925 || Total Training Translation Loss 0.007424\n",
            "2025-07-10 14:13:16,632 Epoch 2155:\n",
            "\t\t\tTotal Training Recognition Loss 0.612318 || Total Training Translation Loss 0.006019\n",
            "2025-07-10 14:13:16,807 Epoch 2156:\n",
            "\t\t\tTotal Training Recognition Loss 0.275037 || Total Training Translation Loss 0.007265\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-07-10 14:13:16,982 Epoch 2157:\n",
            "\t\t\tTotal Training Recognition Loss 1.323315 || Total Training Translation Loss 0.006882\n",
            "2025-07-10 14:13:17,157 Epoch 2158:\n",
            "\t\t\tTotal Training Recognition Loss 0.867732 || Total Training Translation Loss 0.006827\n",
            "2025-07-10 14:13:17,332 Epoch 2159:\n",
            "\t\t\tTotal Training Recognition Loss 5.102300 || Total Training Translation Loss 0.005266\n",
            "2025-07-10 14:13:17,506 Epoch 2160:\n",
            "\t\t\tTotal Training Recognition Loss 34.886917 || Total Training Translation Loss 0.006229\n",
            "2025-07-10 14:13:17,680 Epoch 2161:\n",
            "\t\t\tTotal Training Recognition Loss 6.522470 || Total Training Translation Loss 0.005016\n",
            "2025-07-10 14:13:17,855 Epoch 2162:\n",
            "\t\t\tTotal Training Recognition Loss 0.828610 || Total Training Translation Loss 0.005506\n",
            "2025-07-10 14:13:18,029 Epoch 2163:\n",
            "\t\t\tTotal Training Recognition Loss 6.819108 || Total Training Translation Loss 0.008624\n",
            "2025-07-10 14:13:18,204 Epoch 2164:\n",
            "\t\t\tTotal Training Recognition Loss 1.736476 || Total Training Translation Loss 0.006629\n",
            "2025-07-10 14:13:18,380 Epoch 2165:\n",
            "\t\t\tTotal Training Recognition Loss 2.539132 || Total Training Translation Loss 0.007988\n",
            "2025-07-10 14:13:18,554 Epoch 2166:\n",
            "\t\t\tTotal Training Recognition Loss 2.015950 || Total Training Translation Loss 0.006349\n",
            "2025-07-10 14:13:18,727 Epoch 2167:\n",
            "\t\t\tTotal Training Recognition Loss 2.530694 || Total Training Translation Loss 0.006692\n",
            "2025-07-10 14:13:18,901 Epoch 2168:\n",
            "\t\t\tTotal Training Recognition Loss 1.398271 || Total Training Translation Loss 0.006619\n",
            "2025-07-10 14:13:19,073 Epoch 2169:\n",
            "\t\t\tTotal Training Recognition Loss 1.315997 || Total Training Translation Loss 0.007707\n",
            "2025-07-10 14:13:19,247 Epoch 2170:\n",
            "\t\t\tTotal Training Recognition Loss 0.903050 || Total Training Translation Loss 0.006964\n",
            "2025-07-10 14:13:19,420 Epoch 2171:\n",
            "\t\t\tTotal Training Recognition Loss 0.624257 || Total Training Translation Loss 0.007115\n",
            "2025-07-10 14:13:19,593 Epoch 2172:\n",
            "\t\t\tTotal Training Recognition Loss 24.549713 || Total Training Translation Loss 0.005980\n",
            "2025-07-10 14:13:19,768 Epoch 2173:\n",
            "\t\t\tTotal Training Recognition Loss 0.405172 || Total Training Translation Loss 0.006353\n",
            "2025-07-10 14:13:19,943 Epoch 2174:\n",
            "\t\t\tTotal Training Recognition Loss 0.710404 || Total Training Translation Loss 0.007679\n",
            "2025-07-10 14:13:20,118 Epoch 2175:\n",
            "\t\t\tTotal Training Recognition Loss 23.500029 || Total Training Translation Loss 0.005864\n",
            "2025-07-10 14:13:20,292 Epoch 2176:\n",
            "\t\t\tTotal Training Recognition Loss 0.831960 || Total Training Translation Loss 0.007893\n",
            "2025-07-10 14:13:20,466 Epoch 2177:\n",
            "\t\t\tTotal Training Recognition Loss 10.833758 || Total Training Translation Loss 0.005360\n",
            "2025-07-10 14:13:20,642 Epoch 2178:\n",
            "\t\t\tTotal Training Recognition Loss 0.369690 || Total Training Translation Loss 0.007120\n",
            "2025-07-10 14:13:20,864 Epoch 2179:\n",
            "\t\t\tTotal Training Recognition Loss 0.443702 || Total Training Translation Loss 0.006926\n",
            "2025-07-10 14:13:21,042 Epoch 2180:\n",
            "\t\t\tTotal Training Recognition Loss 0.333166 || Total Training Translation Loss 0.008039\n",
            "2025-07-10 14:13:21,215 Epoch 2181:\n",
            "\t\t\tTotal Training Recognition Loss 0.859925 || Total Training Translation Loss 0.007939\n",
            "2025-07-10 14:13:21,388 Epoch 2182:\n",
            "\t\t\tTotal Training Recognition Loss 27.581989 || Total Training Translation Loss 0.006335\n",
            "2025-07-10 14:13:21,562 Epoch 2183:\n",
            "\t\t\tTotal Training Recognition Loss 1.318430 || Total Training Translation Loss 0.006844\n",
            "2025-07-10 14:13:21,747 Epoch 2184:\n",
            "\t\t\tTotal Training Recognition Loss 0.456604 || Total Training Translation Loss 0.007687\n",
            "2025-07-10 14:13:21,949 Epoch 2185:\n",
            "\t\t\tTotal Training Recognition Loss 8.794875 || Total Training Translation Loss 0.006714\n",
            "2025-07-10 14:13:22,124 Epoch 2186:\n",
            "\t\t\tTotal Training Recognition Loss 2.612268 || Total Training Translation Loss 0.006947\n",
            "2025-07-10 14:13:22,298 Epoch 2187:\n",
            "\t\t\tTotal Training Recognition Loss 46.516987 || Total Training Translation Loss 0.007842\n",
            "2025-07-10 14:13:22,471 Epoch 2188:\n",
            "\t\t\tTotal Training Recognition Loss 27.949953 || Total Training Translation Loss 0.007010\n",
            "2025-07-10 14:13:22,648 Epoch 2189:\n",
            "\t\t\tTotal Training Recognition Loss 81.138603 || Total Training Translation Loss 0.006915\n",
            "2025-07-10 14:13:22,827 Epoch 2190:\n",
            "\t\t\tTotal Training Recognition Loss 4.430677 || Total Training Translation Loss 0.006827\n",
            "2025-07-10 14:13:23,003 Epoch 2191:\n",
            "\t\t\tTotal Training Recognition Loss 774.902222 || Total Training Translation Loss 0.006731\n",
            "2025-07-10 14:13:23,176 Epoch 2192:\n",
            "\t\t\tTotal Training Recognition Loss 2.593397 || Total Training Translation Loss 0.006365\n",
            "2025-07-10 14:13:23,350 Epoch 2193:\n",
            "\t\t\tTotal Training Recognition Loss 3.034657 || Total Training Translation Loss 0.007178\n",
            "2025-07-10 14:13:23,529 Epoch 2194:\n",
            "\t\t\tTotal Training Recognition Loss 10.543020 || Total Training Translation Loss 0.008037\n",
            "2025-07-10 14:13:23,702 Epoch 2195:\n",
            "\t\t\tTotal Training Recognition Loss 15.978891 || Total Training Translation Loss 0.008494\n",
            "2025-07-10 14:13:23,876 Epoch 2196:\n",
            "\t\t\tTotal Training Recognition Loss 72.884270 || Total Training Translation Loss 0.008405\n",
            "2025-07-10 14:13:24,049 Epoch 2197:\n",
            "\t\t\tTotal Training Recognition Loss 84.570595 || Total Training Translation Loss 0.006411\n",
            "2025-07-10 14:13:24,222 Epoch 2198:\n",
            "\t\t\tTotal Training Recognition Loss 12.122961 || Total Training Translation Loss 0.008505\n",
            "2025-07-10 14:13:24,395 Epoch 2199:\n",
            "\t\t\tTotal Training Recognition Loss 12.880184 || Total Training Translation Loss 0.008340\n",
            "2025-07-10 14:13:24,569 [Epoch: 2200 Step: 00002200] Batch Recognition Loss:   4.815424 => Gls Tokens per Sec:      215 || Batch Translation Loss:   0.008178 => Txt Tokens per Sec:      581 || Lr: 0.000700\n",
            "2025-07-10 14:13:24,805 Validation result at epoch 2200, step     2200: duration: 0.2351s\n",
            "\tRecognition Beam Size: 1\tTranslation Beam Size: 1\tTranslation Beam Alpha: -1\n",
            "\tRecognition Loss: 6235.49365\tTranslation Loss: 246.73759\tPPL: 18.22502\n",
            "\tEval Metric: BLEU\n",
            "\tWER 88.57\t(DEL: 48.57,\tINS: 5.71,\tSUB: 34.29)\n",
            "\tBLEU-4 2.76\t(BLEU-1: 6.67,\tBLEU-2: 4.29,\tBLEU-3: 3.40,\tBLEU-4: 2.76)\n",
            "\tCHRF 23.02\tROUGE 10.44\tFID 0.00\n",
            "2025-07-10 14:13:24,805 Logging Recognition and Translation Outputs\n",
            "2025-07-10 14:13:24,805 ========================================================================================\n",
            "2025-07-10 14:13:24,806 Logging Sequence: 11August_2010_Wednesday_tagesschau-2\n",
            "2025-07-10 14:13:24,806 \tGloss Reference :\tDRUCK TIEF KOMMEN  \n",
            "2025-07-10 14:13:24,806 \tGloss Hypothesis:\t***** **** NORDWEST\n",
            "2025-07-10 14:13:24,807 \tGloss Alignment :\tD     D    S       \n",
            "2025-07-10 14:13:24,807 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:13:24,808 \tText Reference  :\t********** ****** ** ***** ***** ***** ******* ***** **** *** ********* ********* ******** ********* ****** ******* *** ***** *** ****** ***** tiefer luftdruck bestimmt in    den   nächsten tagen unser wetter\n",
            "2025-07-10 14:13:24,809 \tText Hypothesis :\tnordwesten bleibt es heute nacht meist trocken sonst muss mit teilweise kräftigen schauern gerechnet werden örtlich mit blitz und donner <unk> <unk>  <unk>     <unk>    <unk> <unk> <unk>    <unk> <unk> <unk> \n",
            "2025-07-10 14:13:24,809 \tText Alignment  :\tI          I      I  I     I     I     I       I     I    I   I         I         I        I         I      I       I   I     I   I      I     S      S         S        S     S     S        S     S     S     \n",
            "2025-07-10 14:13:24,809 ========================================================================================\n",
            "2025-07-10 14:13:24,809 Logging Sequence: 11August_2010_Wednesday_tagesschau-3\n",
            "2025-07-10 14:13:24,809 \tGloss Reference :\tES-BEDEUTET VIEL WOLKE UND KOENNEN REGEN      GEWITTER KOENNEN\n",
            "2025-07-10 14:13:24,810 \tGloss Hypothesis:\t*********** **** ***** *** ******* DONNERSTAG LOCH     KOENNEN\n",
            "2025-07-10 14:13:24,810 \tGloss Alignment :\tD           D    D     D   D       S          S               \n",
            "2025-07-10 14:13:24,810 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:13:24,812 \tText Reference  :\tdas bedeutet viele wolken und ******** **** ****** **************** immer wieder ******* ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** zum   teil  kräftige schauer und   gewitter\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-07-10 14:13:24,813 \tText Hypothesis :\t*** es       auch  länger und ergiebig auch lokale überschwemmungen sind  wieder möglich <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>    <unk>   <unk> <unk>   \n",
            "2025-07-10 14:13:24,813 \tText Alignment  :\tD   S        S     S          I        I    I      I                S            I       I     I     I     I     I     I     I     I     I     I     I     I     I     S     S     S        S       S     S       \n",
            "2025-07-10 14:13:24,813 ========================================================================================\n",
            "2025-07-10 14:13:24,813 Logging Sequence: 11August_2010_Wednesday_tagesschau-8\n",
            "2025-07-10 14:13:24,814 \tGloss Reference :\tWIND MAESSIG SCHWACH REGION  WENN    GEWITTER WIND    KOENNEN\n",
            "2025-07-10 14:13:24,814 \tGloss Hypothesis:\t**** ******* DURCH   TROCKEN BLEIBEN REGEN    BLEIBEN KOENNEN\n",
            "2025-07-10 14:13:24,814 \tGloss Alignment :\tD    D       S       S       S       S        S              \n",
            "2025-07-10 14:13:24,814 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:13:24,817 \tText Reference  :\t** **** ****** *** ******** **** ****** **************** **** ****** ******* ***** meist weht  nur   ein   schwacher wind  aus   unterschiedlichen richtungen der   bei   schauern und   gewittern stark böig  sein  kann \n",
            "2025-07-10 14:13:24,817 \tText Hypothesis :\tes auch länger und ergiebig auch lokale überschwemmungen sind wieder möglich <unk> <unk> <unk> <unk> <unk> <unk>     <unk> <unk> <unk>             <unk>      <unk> <unk> <unk>    <unk> <unk>     <unk> <unk> <unk> <unk>\n",
            "2025-07-10 14:13:24,817 \tText Alignment  :\tI  I    I      I   I        I    I      I                I    I      I       I     S     S     S     S     S         S     S     S                 S          S     S     S        S     S         S     S     S     S    \n",
            "2025-07-10 14:13:24,817 ========================================================================================\n",
            "2025-07-10 14:13:24,817 Logging Sequence: 25October_2010_Monday_tagesschau-22\n",
            "2025-07-10 14:13:24,818 \tGloss Reference :\t***** MITTWOCH REGEN    KOENNEN NORDWEST WAHRSCHEINLICH NORD STARK WIND\n",
            "2025-07-10 14:13:24,818 \tGloss Hypothesis:\tJETZT SUEDWEST MANCHMAL KOENNEN ******** ************** **** ***** ****\n",
            "2025-07-10 14:13:24,818 \tGloss Alignment :\tI     S        S                D        D              D    D     D   \n",
            "2025-07-10 14:13:24,818 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:13:24,820 \tText Reference  :\tam mittwoch hier   und ******** **** ****** **************** **** ****** ******* ***** ***** ***** ***** ***** ***** ***** ***** ***** da    nieselregen in    der   nordwesthälfte an    den   küsten kräftiger wind \n",
            "2025-07-10 14:13:24,820 \tText Hypothesis :\tes auch     länger und ergiebig auch lokale überschwemmungen sind wieder möglich <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>       <unk> <unk> <unk>          <unk> <unk> <unk>  <unk>     <unk>\n",
            "2025-07-10 14:13:24,820 \tText Alignment  :\tS  S        S          I        I    I      I                I    I      I       I     I     I     I     I     I     I     I     I     S     S           S     S     S              S     S     S      S         S    \n",
            "2025-07-10 14:13:24,821 ========================================================================================\n",
            "2025-07-10 14:13:24,821 Logging Sequence: 05May_2011_Thursday_tagesschau-25\n",
            "2025-07-10 14:13:24,822 \tGloss Reference :\t****** JETZT WETTER WIE-AUSSEHEN MORGEN FREITAG SECHSTE MAI        ZEIGEN-BILDSCHIRM\n",
            "2025-07-10 14:13:24,822 \tGloss Hypothesis:\tWETTER JETZT WETTER ************ MORGEN ******* ******* DONNERSTAG ZWOELF           \n",
            "2025-07-10 14:13:24,822 \tGloss Alignment :\tI                   D                   D       D       S          S                \n",
            "2025-07-10 14:13:24,822 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:13:24,824 \tText Reference  :\tund nun die wettervorhersage für morgen freitag    den ******** ****** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** sechsten mai  \n",
            "2025-07-10 14:13:24,824 \tText Hypothesis :\t*** nun die wettervorhersage für morgen donnerstag den zwölften august <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>    <unk>\n",
            "2025-07-10 14:13:24,824 \tText Alignment  :\tD                                       S              I        I      I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     S        S    \n",
            "2025-07-10 14:13:24,824 ========================================================================================\n",
            "2025-07-10 14:13:24,825 Epoch 2200:\n",
            "\t\t\tTotal Training Recognition Loss 4.815424 || Total Training Translation Loss 0.008178\n",
            "2025-07-10 14:13:25,000 Epoch 2201:\n",
            "\t\t\tTotal Training Recognition Loss 2.505922 || Total Training Translation Loss 0.008353\n",
            "2025-07-10 14:13:25,183 Epoch 2202:\n",
            "\t\t\tTotal Training Recognition Loss 6.198590 || Total Training Translation Loss 0.009091\n",
            "2025-07-10 14:13:25,401 Epoch 2203:\n",
            "\t\t\tTotal Training Recognition Loss 11.625579 || Total Training Translation Loss 0.008333\n",
            "2025-07-10 14:13:25,618 Epoch 2204:\n",
            "\t\t\tTotal Training Recognition Loss 4.681159 || Total Training Translation Loss 0.009781\n",
            "2025-07-10 14:13:25,836 Epoch 2205:\n",
            "\t\t\tTotal Training Recognition Loss 14.811176 || Total Training Translation Loss 0.007305\n",
            "2025-07-10 14:13:26,011 Epoch 2206:\n",
            "\t\t\tTotal Training Recognition Loss 13.945984 || Total Training Translation Loss 0.007416\n",
            "2025-07-10 14:13:26,230 Epoch 2207:\n",
            "\t\t\tTotal Training Recognition Loss 5.987014 || Total Training Translation Loss 0.007055\n",
            "2025-07-10 14:13:26,448 Epoch 2208:\n",
            "\t\t\tTotal Training Recognition Loss 7.041040 || Total Training Translation Loss 0.008536\n",
            "2025-07-10 14:13:26,666 Epoch 2209:\n",
            "\t\t\tTotal Training Recognition Loss 11.705114 || Total Training Translation Loss 0.008740\n",
            "2025-07-10 14:13:26,885 Epoch 2210:\n",
            "\t\t\tTotal Training Recognition Loss 2.177935 || Total Training Translation Loss 0.009244\n",
            "2025-07-10 14:13:27,104 Epoch 2211:\n",
            "\t\t\tTotal Training Recognition Loss 2.084700 || Total Training Translation Loss 0.007733\n",
            "2025-07-10 14:13:27,327 Epoch 2212:\n",
            "\t\t\tTotal Training Recognition Loss 7.387922 || Total Training Translation Loss 0.008846\n",
            "2025-07-10 14:13:27,504 Epoch 2213:\n",
            "\t\t\tTotal Training Recognition Loss 3.891563 || Total Training Translation Loss 0.010344\n",
            "2025-07-10 14:13:27,678 Epoch 2214:\n",
            "\t\t\tTotal Training Recognition Loss 5.250693 || Total Training Translation Loss 0.009490\n",
            "2025-07-10 14:13:27,851 Epoch 2215:\n",
            "\t\t\tTotal Training Recognition Loss 3.020041 || Total Training Translation Loss 0.008383\n",
            "2025-07-10 14:13:28,025 Epoch 2216:\n",
            "\t\t\tTotal Training Recognition Loss 8.088944 || Total Training Translation Loss 0.008763\n",
            "2025-07-10 14:13:28,198 Epoch 2217:\n",
            "\t\t\tTotal Training Recognition Loss 2.528560 || Total Training Translation Loss 0.007631\n",
            "2025-07-10 14:13:28,372 Epoch 2218:\n",
            "\t\t\tTotal Training Recognition Loss 1.484637 || Total Training Translation Loss 0.007490\n",
            "2025-07-10 14:13:28,546 Epoch 2219:\n",
            "\t\t\tTotal Training Recognition Loss 1.599816 || Total Training Translation Loss 0.006814\n",
            "2025-07-10 14:13:28,719 Epoch 2220:\n",
            "\t\t\tTotal Training Recognition Loss 9.467622 || Total Training Translation Loss 0.007088\n",
            "2025-07-10 14:13:28,894 Epoch 2221:\n",
            "\t\t\tTotal Training Recognition Loss 1.418223 || Total Training Translation Loss 0.006927\n",
            "2025-07-10 14:13:29,069 Epoch 2222:\n",
            "\t\t\tTotal Training Recognition Loss 3.180135 || Total Training Translation Loss 0.007143\n",
            "2025-07-10 14:13:29,242 Epoch 2223:\n",
            "\t\t\tTotal Training Recognition Loss 1.162217 || Total Training Translation Loss 0.009631\n",
            "2025-07-10 14:13:29,417 Epoch 2224:\n",
            "\t\t\tTotal Training Recognition Loss 0.620036 || Total Training Translation Loss 0.005868\n",
            "2025-07-10 14:13:29,590 Epoch 2225:\n",
            "\t\t\tTotal Training Recognition Loss 2.126132 || Total Training Translation Loss 0.008089\n",
            "2025-07-10 14:13:29,764 Epoch 2226:\n",
            "\t\t\tTotal Training Recognition Loss 10.002213 || Total Training Translation Loss 0.007775\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-07-10 14:13:29,938 Epoch 2227:\n",
            "\t\t\tTotal Training Recognition Loss 0.984961 || Total Training Translation Loss 0.006035\n",
            "2025-07-10 14:13:30,112 Epoch 2228:\n",
            "\t\t\tTotal Training Recognition Loss 1.294346 || Total Training Translation Loss 0.006583\n",
            "2025-07-10 14:13:30,286 Epoch 2229:\n",
            "\t\t\tTotal Training Recognition Loss 1.418872 || Total Training Translation Loss 0.006622\n",
            "2025-07-10 14:13:30,459 Epoch 2230:\n",
            "\t\t\tTotal Training Recognition Loss 10.578930 || Total Training Translation Loss 0.006884\n",
            "2025-07-10 14:13:30,633 Epoch 2231:\n",
            "\t\t\tTotal Training Recognition Loss 2.212111 || Total Training Translation Loss 0.006652\n",
            "2025-07-10 14:13:30,806 Epoch 2232:\n",
            "\t\t\tTotal Training Recognition Loss 1.111530 || Total Training Translation Loss 0.006786\n",
            "2025-07-10 14:13:30,982 Epoch 2233:\n",
            "\t\t\tTotal Training Recognition Loss 0.879325 || Total Training Translation Loss 0.007782\n",
            "2025-07-10 14:13:31,155 Epoch 2234:\n",
            "\t\t\tTotal Training Recognition Loss 0.791918 || Total Training Translation Loss 0.007633\n",
            "2025-07-10 14:13:31,329 Epoch 2235:\n",
            "\t\t\tTotal Training Recognition Loss 3.191039 || Total Training Translation Loss 0.007042\n",
            "2025-07-10 14:13:31,506 Epoch 2236:\n",
            "\t\t\tTotal Training Recognition Loss 1.996539 || Total Training Translation Loss 0.005880\n",
            "2025-07-10 14:13:31,727 Epoch 2237:\n",
            "\t\t\tTotal Training Recognition Loss 24.166307 || Total Training Translation Loss 0.005178\n",
            "2025-07-10 14:13:31,906 Epoch 2238:\n",
            "\t\t\tTotal Training Recognition Loss 1.615363 || Total Training Translation Loss 0.005480\n",
            "2025-07-10 14:13:32,080 Epoch 2239:\n",
            "\t\t\tTotal Training Recognition Loss 1.396454 || Total Training Translation Loss 0.006719\n",
            "2025-07-10 14:13:32,254 Epoch 2240:\n",
            "\t\t\tTotal Training Recognition Loss 0.557930 || Total Training Translation Loss 0.007381\n",
            "2025-07-10 14:13:32,427 Epoch 2241:\n",
            "\t\t\tTotal Training Recognition Loss 1.078302 || Total Training Translation Loss 0.008055\n",
            "2025-07-10 14:13:32,600 Epoch 2242:\n",
            "\t\t\tTotal Training Recognition Loss 0.898529 || Total Training Translation Loss 0.006219\n",
            "2025-07-10 14:13:32,773 Epoch 2243:\n",
            "\t\t\tTotal Training Recognition Loss 2.530452 || Total Training Translation Loss 0.006502\n",
            "2025-07-10 14:13:32,947 Epoch 2244:\n",
            "\t\t\tTotal Training Recognition Loss 3.925309 || Total Training Translation Loss 0.006069\n",
            "2025-07-10 14:13:33,120 Epoch 2245:\n",
            "\t\t\tTotal Training Recognition Loss 7.240391 || Total Training Translation Loss 0.005324\n",
            "2025-07-10 14:13:33,311 Epoch 2246:\n",
            "\t\t\tTotal Training Recognition Loss 0.970752 || Total Training Translation Loss 0.005755\n",
            "2025-07-10 14:13:33,484 Epoch 2247:\n",
            "\t\t\tTotal Training Recognition Loss 0.327353 || Total Training Translation Loss 0.005801\n",
            "2025-07-10 14:13:33,659 Epoch 2248:\n",
            "\t\t\tTotal Training Recognition Loss 0.360918 || Total Training Translation Loss 0.007245\n",
            "2025-07-10 14:13:33,878 Epoch 2249:\n",
            "\t\t\tTotal Training Recognition Loss 0.605712 || Total Training Translation Loss 0.006936\n",
            "2025-07-10 14:13:34,096 Epoch 2250:\n",
            "\t\t\tTotal Training Recognition Loss 0.384314 || Total Training Translation Loss 0.005713\n",
            "2025-07-10 14:13:34,314 Epoch 2251:\n",
            "\t\t\tTotal Training Recognition Loss 1.999296 || Total Training Translation Loss 0.007174\n",
            "2025-07-10 14:13:34,532 Epoch 2252:\n",
            "\t\t\tTotal Training Recognition Loss 0.418138 || Total Training Translation Loss 0.005852\n",
            "2025-07-10 14:13:34,749 Epoch 2253:\n",
            "\t\t\tTotal Training Recognition Loss 0.420228 || Total Training Translation Loss 0.005502\n",
            "2025-07-10 14:13:34,966 Epoch 2254:\n",
            "\t\t\tTotal Training Recognition Loss 1.540836 || Total Training Translation Loss 0.006826\n",
            "2025-07-10 14:13:35,183 Epoch 2255:\n",
            "\t\t\tTotal Training Recognition Loss 0.478690 || Total Training Translation Loss 0.006467\n",
            "2025-07-10 14:13:35,359 Epoch 2256:\n",
            "\t\t\tTotal Training Recognition Loss 0.538932 || Total Training Translation Loss 0.006660\n",
            "2025-07-10 14:13:35,529 Epoch 2257:\n",
            "\t\t\tTotal Training Recognition Loss 0.377472 || Total Training Translation Loss 0.006609\n",
            "2025-07-10 14:13:35,700 Epoch 2258:\n",
            "\t\t\tTotal Training Recognition Loss 0.455103 || Total Training Translation Loss 0.006168\n",
            "2025-07-10 14:13:35,871 Epoch 2259:\n",
            "\t\t\tTotal Training Recognition Loss 0.234187 || Total Training Translation Loss 0.006406\n",
            "2025-07-10 14:13:36,043 Epoch 2260:\n",
            "\t\t\tTotal Training Recognition Loss 0.278376 || Total Training Translation Loss 0.007121\n",
            "2025-07-10 14:13:36,216 Epoch 2261:\n",
            "\t\t\tTotal Training Recognition Loss 0.286623 || Total Training Translation Loss 0.005268\n",
            "2025-07-10 14:13:36,387 Epoch 2262:\n",
            "\t\t\tTotal Training Recognition Loss 0.334328 || Total Training Translation Loss 0.006836\n",
            "2025-07-10 14:13:36,558 Epoch 2263:\n",
            "\t\t\tTotal Training Recognition Loss 0.271441 || Total Training Translation Loss 0.005171\n",
            "2025-07-10 14:13:36,730 Epoch 2264:\n",
            "\t\t\tTotal Training Recognition Loss 0.224494 || Total Training Translation Loss 0.006527\n",
            "2025-07-10 14:13:36,905 Epoch 2265:\n",
            "\t\t\tTotal Training Recognition Loss 0.392663 || Total Training Translation Loss 0.006375\n",
            "2025-07-10 14:13:37,077 Epoch 2266:\n",
            "\t\t\tTotal Training Recognition Loss 0.173619 || Total Training Translation Loss 0.006291\n",
            "2025-07-10 14:13:37,248 Epoch 2267:\n",
            "\t\t\tTotal Training Recognition Loss 0.347048 || Total Training Translation Loss 0.006272\n",
            "2025-07-10 14:13:37,419 Epoch 2268:\n",
            "\t\t\tTotal Training Recognition Loss 1.123418 || Total Training Translation Loss 0.006254\n",
            "2025-07-10 14:13:37,590 Epoch 2269:\n",
            "\t\t\tTotal Training Recognition Loss 0.154724 || Total Training Translation Loss 0.005437\n",
            "2025-07-10 14:13:37,761 Epoch 2270:\n",
            "\t\t\tTotal Training Recognition Loss 11.233242 || Total Training Translation Loss 0.006382\n",
            "2025-07-10 14:13:37,932 Epoch 2271:\n",
            "\t\t\tTotal Training Recognition Loss 1.441487 || Total Training Translation Loss 0.006010\n",
            "2025-07-10 14:13:38,106 Epoch 2272:\n",
            "\t\t\tTotal Training Recognition Loss 0.375052 || Total Training Translation Loss 0.006343\n",
            "2025-07-10 14:13:38,279 Epoch 2273:\n",
            "\t\t\tTotal Training Recognition Loss 0.305322 || Total Training Translation Loss 0.005162\n",
            "2025-07-10 14:13:38,453 Epoch 2274:\n",
            "\t\t\tTotal Training Recognition Loss 0.477855 || Total Training Translation Loss 0.006760\n",
            "2025-07-10 14:13:38,626 Epoch 2275:\n",
            "\t\t\tTotal Training Recognition Loss 1.112476 || Total Training Translation Loss 0.006181\n",
            "2025-07-10 14:13:38,800 Epoch 2276:\n",
            "\t\t\tTotal Training Recognition Loss 0.366448 || Total Training Translation Loss 0.005428\n",
            "2025-07-10 14:13:38,974 Epoch 2277:\n",
            "\t\t\tTotal Training Recognition Loss 6.736106 || Total Training Translation Loss 0.005551\n",
            "2025-07-10 14:13:39,147 Epoch 2278:\n",
            "\t\t\tTotal Training Recognition Loss 0.893102 || Total Training Translation Loss 0.008358\n",
            "2025-07-10 14:13:39,321 Epoch 2279:\n",
            "\t\t\tTotal Training Recognition Loss 0.280510 || Total Training Translation Loss 0.006028\n",
            "2025-07-10 14:13:39,494 Epoch 2280:\n",
            "\t\t\tTotal Training Recognition Loss 36.290115 || Total Training Translation Loss 0.006390\n",
            "2025-07-10 14:13:39,668 Epoch 2281:\n",
            "\t\t\tTotal Training Recognition Loss 2.006357 || Total Training Translation Loss 0.005831\n",
            "2025-07-10 14:13:39,843 Epoch 2282:\n",
            "\t\t\tTotal Training Recognition Loss 39.943668 || Total Training Translation Loss 0.008007\n",
            "2025-07-10 14:13:40,017 Epoch 2283:\n",
            "\t\t\tTotal Training Recognition Loss 58.940777 || Total Training Translation Loss 0.006888\n",
            "2025-07-10 14:13:40,197 Epoch 2284:\n",
            "\t\t\tTotal Training Recognition Loss 5.004540 || Total Training Translation Loss 0.006460\n",
            "2025-07-10 14:13:40,370 Epoch 2285:\n",
            "\t\t\tTotal Training Recognition Loss 56.004452 || Total Training Translation Loss 0.011104\n",
            "2025-07-10 14:13:40,544 Epoch 2286:\n",
            "\t\t\tTotal Training Recognition Loss 0.798461 || Total Training Translation Loss 0.009067\n",
            "2025-07-10 14:13:40,718 Epoch 2287:\n",
            "\t\t\tTotal Training Recognition Loss 1.122781 || Total Training Translation Loss 0.009838\n",
            "2025-07-10 14:13:40,891 Epoch 2288:\n",
            "\t\t\tTotal Training Recognition Loss 4.055507 || Total Training Translation Loss 0.009870\n",
            "2025-07-10 14:13:41,063 Epoch 2289:\n",
            "\t\t\tTotal Training Recognition Loss 20.465664 || Total Training Translation Loss 0.009119\n",
            "2025-07-10 14:13:41,236 Epoch 2290:\n",
            "\t\t\tTotal Training Recognition Loss 7.737139 || Total Training Translation Loss 0.009107\n",
            "2025-07-10 14:13:41,409 Epoch 2291:\n",
            "\t\t\tTotal Training Recognition Loss 25.273129 || Total Training Translation Loss 0.007381\n",
            "2025-07-10 14:13:41,582 Epoch 2292:\n",
            "\t\t\tTotal Training Recognition Loss 1.921637 || Total Training Translation Loss 0.008846\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-07-10 14:13:41,756 Epoch 2293:\n",
            "\t\t\tTotal Training Recognition Loss 1.234266 || Total Training Translation Loss 0.006747\n",
            "2025-07-10 14:13:41,933 Epoch 2294:\n",
            "\t\t\tTotal Training Recognition Loss 5.125451 || Total Training Translation Loss 0.007693\n",
            "2025-07-10 14:13:42,109 Epoch 2295:\n",
            "\t\t\tTotal Training Recognition Loss 4.512069 || Total Training Translation Loss 0.006817\n",
            "2025-07-10 14:13:42,285 Epoch 2296:\n",
            "\t\t\tTotal Training Recognition Loss 1.821085 || Total Training Translation Loss 0.009229\n",
            "2025-07-10 14:13:42,461 Epoch 2297:\n",
            "\t\t\tTotal Training Recognition Loss 18.491383 || Total Training Translation Loss 0.009236\n",
            "2025-07-10 14:13:42,635 Epoch 2298:\n",
            "\t\t\tTotal Training Recognition Loss 5.426117 || Total Training Translation Loss 0.008202\n",
            "2025-07-10 14:13:42,813 Epoch 2299:\n",
            "\t\t\tTotal Training Recognition Loss 11.928181 || Total Training Translation Loss 0.007709\n",
            "2025-07-10 14:13:42,987 [Epoch: 2300 Step: 00002300] Batch Recognition Loss:   4.276446 => Gls Tokens per Sec:      214 || Batch Translation Loss:   0.007133 => Txt Tokens per Sec:      579 || Lr: 0.000700\n",
            "2025-07-10 14:13:43,228 Validation result at epoch 2300, step     2300: duration: 0.2397s\n",
            "\tRecognition Beam Size: 1\tTranslation Beam Size: 1\tTranslation Beam Alpha: -1\n",
            "\tRecognition Loss: 5822.81641\tTranslation Loss: 260.41122\tPPL: 21.40579\n",
            "\tEval Metric: BLEU\n",
            "\tWER 85.71\t(DEL: 45.71,\tINS: 0.00,\tSUB: 40.00)\n",
            "\tBLEU-4 0.00\t(BLEU-1: 3.33,\tBLEU-2: 0.00,\tBLEU-3: 0.00,\tBLEU-4: 0.00)\n",
            "\tCHRF 18.79\tROUGE 4.95\tFID 0.00\n",
            "2025-07-10 14:13:43,229 Logging Recognition and Translation Outputs\n",
            "2025-07-10 14:13:43,229 ========================================================================================\n",
            "2025-07-10 14:13:43,229 Logging Sequence: 11August_2010_Wednesday_tagesschau-2\n",
            "2025-07-10 14:13:43,230 \tGloss Reference :\tDRUCK TIEF KOMMEN\n",
            "2025-07-10 14:13:43,230 \tGloss Hypothesis:\t***** **** ******\n",
            "2025-07-10 14:13:43,230 \tGloss Alignment :\tD     D    D     \n",
            "2025-07-10 14:13:43,230 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:13:43,232 \tText Reference  :\t********** ****** ** ***** ***** ***** ******* ***** **** *** ********* ********* ******** ********* ****** ******* *** ***** *** ****** ***** tiefer luftdruck bestimmt in    den   nächsten tagen unser wetter\n",
            "2025-07-10 14:13:43,232 \tText Hypothesis :\tnordwesten bleibt es heute nacht meist trocken sonst muss mit teilweise kräftigen schauern gerechnet werden örtlich mit blitz und donner <unk> <unk>  <unk>     <unk>    <unk> <unk> <unk>    <unk> <unk> <unk> \n",
            "2025-07-10 14:13:43,232 \tText Alignment  :\tI          I      I  I     I     I     I       I     I    I   I         I         I        I         I      I       I   I     I   I      I     S      S         S        S     S     S        S     S     S     \n",
            "2025-07-10 14:13:43,232 ========================================================================================\n",
            "2025-07-10 14:13:43,232 Logging Sequence: 11August_2010_Wednesday_tagesschau-3\n",
            "2025-07-10 14:13:43,233 \tGloss Reference :\tES-BEDEUTET VIEL WOLKE UND KOENNEN REGEN  GEWITTER KOENNEN\n",
            "2025-07-10 14:13:43,233 \tGloss Hypothesis:\t*********** **** ***** *** JETZT   MORGEN LOCH     KOENNEN\n",
            "2025-07-10 14:13:43,233 \tGloss Alignment :\tD           D    D     D   S       S      S               \n",
            "2025-07-10 14:13:43,233 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:13:43,235 \tText Reference  :\tdas bedeutet viele wolken und ******** **** ****** **************** immer wieder ******* ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** zum   teil  kräftige schauer und   gewitter\n",
            "2025-07-10 14:13:43,235 \tText Hypothesis :\t*** es       auch  länger und ergiebig auch lokale überschwemmungen sind  wieder möglich <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>    <unk>   <unk> <unk>   \n",
            "2025-07-10 14:13:43,235 \tText Alignment  :\tD   S        S     S          I        I    I      I                S            I       I     I     I     I     I     I     I     I     I     I     I     I     I     S     S     S        S       S     S       \n",
            "2025-07-10 14:13:43,236 ========================================================================================\n",
            "2025-07-10 14:13:43,236 Logging Sequence: 11August_2010_Wednesday_tagesschau-8\n",
            "2025-07-10 14:13:43,236 \tGloss Reference :\tWIND MAESSIG SCHWACH REGION WENN GEWITTER WIND    KOENNEN\n",
            "2025-07-10 14:13:43,236 \tGloss Hypothesis:\t**** ******* ******* ****** **** TROCKEN  BLEIBEN KOENNEN\n",
            "2025-07-10 14:13:43,236 \tGloss Alignment :\tD    D       D       D      D    S        S              \n",
            "2025-07-10 14:13:43,237 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:13:43,239 \tText Reference  :\t****** ** **** ****** *** ******** **** ****** **************** **** ****** ******* meist weht  nur   ein   schwacher wind  aus   unterschiedlichen richtungen der   bei   schauern und   gewittern stark böig  sein  kann \n",
            "2025-07-10 14:13:43,239 \tText Hypothesis :\tregnet es auch länger und ergiebig auch lokale überschwemmungen sind wieder möglich <unk> <unk> <unk> <unk> <unk>     <unk> <unk> <unk>             <unk>      <unk> <unk> <unk>    <unk> <unk>     <unk> <unk> <unk> <unk>\n",
            "2025-07-10 14:13:43,239 \tText Alignment  :\tI      I  I    I      I   I        I    I      I                I    I      I       S     S     S     S     S         S     S     S                 S          S     S     S        S     S         S     S     S     S    \n",
            "2025-07-10 14:13:43,239 ========================================================================================\n",
            "2025-07-10 14:13:43,240 Logging Sequence: 25October_2010_Monday_tagesschau-22\n",
            "2025-07-10 14:13:43,240 \tGloss Reference :\tMITTWOCH REGEN    KOENNEN    NORDWEST WAHRSCHEINLICH NORD  STARK    WIND    \n",
            "2025-07-10 14:13:43,240 \tGloss Hypothesis:\tFEBRUAR  SUEDWEST DONNERSTAG SUEDWEST MANCHMAL       JETZT MANCHMAL GEWITTER\n",
            "2025-07-10 14:13:43,242 \tGloss Alignment :\tS        S        S          S        S              S     S        S       \n",
            "2025-07-10 14:13:43,242 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:13:43,244 \tText Reference  :\tam mittwoch hier   und ******** **** ****** **************** **** ****** ******* ***** ***** ***** ***** ***** ***** ***** ***** ***** da    nieselregen in    der   nordwesthälfte an    den   küsten kräftiger wind \n",
            "2025-07-10 14:13:43,244 \tText Hypothesis :\tes auch     länger und ergiebig auch lokale überschwemmungen sind wieder möglich <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>       <unk> <unk> <unk>          <unk> <unk> <unk>  <unk>     <unk>\n",
            "2025-07-10 14:13:43,244 \tText Alignment  :\tS  S        S          I        I    I      I                I    I      I       I     I     I     I     I     I     I     I     I     S     S           S     S     S              S     S     S      S         S    \n",
            "2025-07-10 14:13:43,245 ========================================================================================\n",
            "2025-07-10 14:13:43,245 Logging Sequence: 05May_2011_Thursday_tagesschau-25\n",
            "2025-07-10 14:13:43,245 \tGloss Reference :\tJETZT WETTER WIE-AUSSEHEN MORGEN FREITAG SECHSTE MAI ZEIGEN-BILDSCHIRM\n",
            "2025-07-10 14:13:43,246 \tGloss Hypothesis:\tJETZT WETTER ************ MORGEN ******* ******* *** DONNERSTAG       \n",
            "2025-07-10 14:13:43,246 \tGloss Alignment :\t             D                   D       D       D   S                \n",
            "2025-07-10 14:13:43,246 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:13:43,247 \tText Reference  :\t** *** **** ** ********** *** **** ******** ******* **** ******** und ** ******* ******** ****** ********* *********** ***** ***** ***** nun   die   wettervorhersage für   morgen freitag den   sechsten mai  \n",
            "2025-07-10 14:13:43,247 \tText Hypothesis :\tam tag gibt es verbreitet zum teil kräftige schauer oder gewitter und in manchen regionen fallen ergiebige regenmengen <unk> <unk> <unk> <unk> <unk> <unk>            <unk> <unk>  <unk>   <unk> <unk>    <unk>\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-07-10 14:13:43,247 \tText Alignment  :\tI  I   I    I  I          I   I    I        I       I    I            I  I       I        I      I         I           I     I     I     S     S     S                S     S      S       S     S        S    \n",
            "2025-07-10 14:13:43,248 ========================================================================================\n",
            "2025-07-10 14:13:43,248 Epoch 2300:\n",
            "\t\t\tTotal Training Recognition Loss 4.276446 || Total Training Translation Loss 0.007133\n",
            "2025-07-10 14:13:43,422 Epoch 2301:\n",
            "\t\t\tTotal Training Recognition Loss 5.170111 || Total Training Translation Loss 0.007194\n",
            "2025-07-10 14:13:43,595 Epoch 2302:\n",
            "\t\t\tTotal Training Recognition Loss 6.235167 || Total Training Translation Loss 0.009955\n",
            "2025-07-10 14:13:43,768 Epoch 2303:\n",
            "\t\t\tTotal Training Recognition Loss 3.142912 || Total Training Translation Loss 0.007151\n",
            "2025-07-10 14:13:43,944 Epoch 2304:\n",
            "\t\t\tTotal Training Recognition Loss 4.378785 || Total Training Translation Loss 0.006157\n",
            "2025-07-10 14:13:44,117 Epoch 2305:\n",
            "\t\t\tTotal Training Recognition Loss 6.759021 || Total Training Translation Loss 0.007217\n",
            "2025-07-10 14:13:44,291 Epoch 2306:\n",
            "\t\t\tTotal Training Recognition Loss 2.220912 || Total Training Translation Loss 0.006214\n",
            "2025-07-10 14:13:44,463 Epoch 2307:\n",
            "\t\t\tTotal Training Recognition Loss 3.014982 || Total Training Translation Loss 0.009408\n",
            "2025-07-10 14:13:44,634 Epoch 2308:\n",
            "\t\t\tTotal Training Recognition Loss 9.837183 || Total Training Translation Loss 0.007796\n",
            "2025-07-10 14:13:44,805 Epoch 2309:\n",
            "\t\t\tTotal Training Recognition Loss 3.204356 || Total Training Translation Loss 0.007969\n",
            "2025-07-10 14:13:44,975 Epoch 2310:\n",
            "\t\t\tTotal Training Recognition Loss 1.870482 || Total Training Translation Loss 0.007162\n",
            "2025-07-10 14:13:45,147 Epoch 2311:\n",
            "\t\t\tTotal Training Recognition Loss 2.856126 || Total Training Translation Loss 0.006896\n",
            "2025-07-10 14:13:45,318 Epoch 2312:\n",
            "\t\t\tTotal Training Recognition Loss 3.783433 || Total Training Translation Loss 0.005885\n",
            "2025-07-10 14:13:45,488 Epoch 2313:\n",
            "\t\t\tTotal Training Recognition Loss 1.550503 || Total Training Translation Loss 0.006425\n",
            "2025-07-10 14:13:45,660 Epoch 2314:\n",
            "\t\t\tTotal Training Recognition Loss 1.279046 || Total Training Translation Loss 0.007493\n",
            "2025-07-10 14:13:45,838 Epoch 2315:\n",
            "\t\t\tTotal Training Recognition Loss 9.599627 || Total Training Translation Loss 0.007261\n",
            "2025-07-10 14:13:46,008 Epoch 2316:\n",
            "\t\t\tTotal Training Recognition Loss 5.462001 || Total Training Translation Loss 0.007297\n",
            "2025-07-10 14:13:46,179 Epoch 2317:\n",
            "\t\t\tTotal Training Recognition Loss 1.139153 || Total Training Translation Loss 0.006181\n",
            "2025-07-10 14:13:46,349 Epoch 2318:\n",
            "\t\t\tTotal Training Recognition Loss 1.067008 || Total Training Translation Loss 0.007115\n",
            "2025-07-10 14:13:46,519 Epoch 2319:\n",
            "\t\t\tTotal Training Recognition Loss 0.918157 || Total Training Translation Loss 0.007162\n",
            "2025-07-10 14:13:46,693 Epoch 2320:\n",
            "\t\t\tTotal Training Recognition Loss 0.733364 || Total Training Translation Loss 0.006522\n",
            "2025-07-10 14:13:46,864 Epoch 2321:\n",
            "\t\t\tTotal Training Recognition Loss 1.630077 || Total Training Translation Loss 0.006134\n",
            "2025-07-10 14:13:47,035 Epoch 2322:\n",
            "\t\t\tTotal Training Recognition Loss 1.336275 || Total Training Translation Loss 0.005631\n",
            "2025-07-10 14:13:47,206 Epoch 2323:\n",
            "\t\t\tTotal Training Recognition Loss 0.612843 || Total Training Translation Loss 0.006436\n",
            "2025-07-10 14:13:47,379 Epoch 2324:\n",
            "\t\t\tTotal Training Recognition Loss 0.546933 || Total Training Translation Loss 0.005506\n",
            "2025-07-10 14:13:47,549 Epoch 2325:\n",
            "\t\t\tTotal Training Recognition Loss 1.861687 || Total Training Translation Loss 0.007622\n",
            "2025-07-10 14:13:47,720 Epoch 2326:\n",
            "\t\t\tTotal Training Recognition Loss 1.622034 || Total Training Translation Loss 0.006834\n",
            "2025-07-10 14:13:47,950 Epoch 2327:\n",
            "\t\t\tTotal Training Recognition Loss 0.735592 || Total Training Translation Loss 0.006631\n",
            "2025-07-10 14:13:48,196 Epoch 2328:\n",
            "\t\t\tTotal Training Recognition Loss 0.784507 || Total Training Translation Loss 0.007211\n",
            "2025-07-10 14:13:48,395 Epoch 2329:\n",
            "\t\t\tTotal Training Recognition Loss 0.470230 || Total Training Translation Loss 0.006572\n",
            "2025-07-10 14:13:48,569 Epoch 2330:\n",
            "\t\t\tTotal Training Recognition Loss 0.599284 || Total Training Translation Loss 0.005872\n",
            "2025-07-10 14:13:48,790 Epoch 2331:\n",
            "\t\t\tTotal Training Recognition Loss 11.590231 || Total Training Translation Loss 0.005638\n",
            "2025-07-10 14:13:49,007 Epoch 2332:\n",
            "\t\t\tTotal Training Recognition Loss 1.033467 || Total Training Translation Loss 0.005843\n",
            "2025-07-10 14:13:49,228 Epoch 2333:\n",
            "\t\t\tTotal Training Recognition Loss 4.559883 || Total Training Translation Loss 0.006692\n",
            "2025-07-10 14:13:49,455 Epoch 2334:\n",
            "\t\t\tTotal Training Recognition Loss 59.389809 || Total Training Translation Loss 0.005785\n",
            "2025-07-10 14:13:49,677 Epoch 2335:\n",
            "\t\t\tTotal Training Recognition Loss 0.422625 || Total Training Translation Loss 0.005630\n",
            "2025-07-10 14:13:49,902 Epoch 2336:\n",
            "\t\t\tTotal Training Recognition Loss 0.602524 || Total Training Translation Loss 0.005793\n",
            "2025-07-10 14:13:50,077 Epoch 2337:\n",
            "\t\t\tTotal Training Recognition Loss 0.615248 || Total Training Translation Loss 0.007086\n",
            "2025-07-10 14:13:50,249 Epoch 2338:\n",
            "\t\t\tTotal Training Recognition Loss 0.820892 || Total Training Translation Loss 0.006529\n",
            "2025-07-10 14:13:50,477 Epoch 2339:\n",
            "\t\t\tTotal Training Recognition Loss 1.785839 || Total Training Translation Loss 0.007156\n",
            "2025-07-10 14:13:50,695 Epoch 2340:\n",
            "\t\t\tTotal Training Recognition Loss 0.435456 || Total Training Translation Loss 0.006087\n",
            "2025-07-10 14:13:50,911 Epoch 2341:\n",
            "\t\t\tTotal Training Recognition Loss 0.684692 || Total Training Translation Loss 0.007575\n",
            "2025-07-10 14:13:51,129 Epoch 2342:\n",
            "\t\t\tTotal Training Recognition Loss 0.476105 || Total Training Translation Loss 0.007831\n",
            "2025-07-10 14:13:51,346 Epoch 2343:\n",
            "\t\t\tTotal Training Recognition Loss 0.989124 || Total Training Translation Loss 0.006932\n",
            "2025-07-10 14:13:51,524 Epoch 2344:\n",
            "\t\t\tTotal Training Recognition Loss 1.011834 || Total Training Translation Loss 0.006908\n",
            "2025-07-10 14:13:51,697 Epoch 2345:\n",
            "\t\t\tTotal Training Recognition Loss 1.471227 || Total Training Translation Loss 0.006487\n",
            "2025-07-10 14:13:51,869 Epoch 2346:\n",
            "\t\t\tTotal Training Recognition Loss 0.861119 || Total Training Translation Loss 0.005920\n",
            "2025-07-10 14:13:52,041 Epoch 2347:\n",
            "\t\t\tTotal Training Recognition Loss 1.225495 || Total Training Translation Loss 0.006469\n",
            "2025-07-10 14:13:52,215 Epoch 2348:\n",
            "\t\t\tTotal Training Recognition Loss 0.598441 || Total Training Translation Loss 0.005835\n",
            "2025-07-10 14:13:52,394 Epoch 2349:\n",
            "\t\t\tTotal Training Recognition Loss 0.587514 || Total Training Translation Loss 0.006842\n",
            "2025-07-10 14:13:52,617 Epoch 2350:\n",
            "\t\t\tTotal Training Recognition Loss 1.262977 || Total Training Translation Loss 0.005697\n",
            "2025-07-10 14:13:52,837 Epoch 2351:\n",
            "\t\t\tTotal Training Recognition Loss 0.554962 || Total Training Translation Loss 0.005434\n",
            "2025-07-10 14:13:53,055 Epoch 2352:\n",
            "\t\t\tTotal Training Recognition Loss 0.793913 || Total Training Translation Loss 0.006415\n",
            "2025-07-10 14:13:53,274 Epoch 2353:\n",
            "\t\t\tTotal Training Recognition Loss 2.048361 || Total Training Translation Loss 0.006849\n",
            "2025-07-10 14:13:53,492 Epoch 2354:\n",
            "\t\t\tTotal Training Recognition Loss 0.379649 || Total Training Translation Loss 0.006079\n",
            "2025-07-10 14:13:53,710 Epoch 2355:\n",
            "\t\t\tTotal Training Recognition Loss 0.441816 || Total Training Translation Loss 0.007714\n",
            "2025-07-10 14:13:53,929 Epoch 2356:\n",
            "\t\t\tTotal Training Recognition Loss 1.190617 || Total Training Translation Loss 0.005770\n",
            "2025-07-10 14:13:54,110 Epoch 2357:\n",
            "\t\t\tTotal Training Recognition Loss 0.785007 || Total Training Translation Loss 0.006556\n",
            "2025-07-10 14:13:54,284 Epoch 2358:\n",
            "\t\t\tTotal Training Recognition Loss 0.300728 || Total Training Translation Loss 0.005939\n",
            "2025-07-10 14:13:54,460 Epoch 2359:\n",
            "\t\t\tTotal Training Recognition Loss 0.306306 || Total Training Translation Loss 0.006629\n",
            "2025-07-10 14:13:54,633 Epoch 2360:\n",
            "\t\t\tTotal Training Recognition Loss 0.456349 || Total Training Translation Loss 0.006836\n",
            "2025-07-10 14:13:54,805 Epoch 2361:\n",
            "\t\t\tTotal Training Recognition Loss 0.982074 || Total Training Translation Loss 0.006099\n",
            "2025-07-10 14:13:54,978 Epoch 2362:\n",
            "\t\t\tTotal Training Recognition Loss 0.291022 || Total Training Translation Loss 0.006191\n",
            "2025-07-10 14:13:55,151 Epoch 2363:\n",
            "\t\t\tTotal Training Recognition Loss 7.678982 || Total Training Translation Loss 0.005712\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-07-10 14:13:55,326 Epoch 2364:\n",
            "\t\t\tTotal Training Recognition Loss 0.398064 || Total Training Translation Loss 0.006382\n",
            "2025-07-10 14:13:55,503 Epoch 2365:\n",
            "\t\t\tTotal Training Recognition Loss 3.755419 || Total Training Translation Loss 0.007656\n",
            "2025-07-10 14:13:55,679 Epoch 2366:\n",
            "\t\t\tTotal Training Recognition Loss 0.448852 || Total Training Translation Loss 0.006615\n",
            "2025-07-10 14:13:55,857 Epoch 2367:\n",
            "\t\t\tTotal Training Recognition Loss 1.049067 || Total Training Translation Loss 0.006399\n",
            "2025-07-10 14:13:56,033 Epoch 2368:\n",
            "\t\t\tTotal Training Recognition Loss 0.336668 || Total Training Translation Loss 0.008511\n",
            "2025-07-10 14:13:56,210 Epoch 2369:\n",
            "\t\t\tTotal Training Recognition Loss 0.565153 || Total Training Translation Loss 0.006785\n",
            "2025-07-10 14:13:56,414 Epoch 2370:\n",
            "\t\t\tTotal Training Recognition Loss 0.449823 || Total Training Translation Loss 0.006375\n",
            "2025-07-10 14:13:56,602 Epoch 2371:\n",
            "\t\t\tTotal Training Recognition Loss 0.359736 || Total Training Translation Loss 0.006130\n",
            "2025-07-10 14:13:56,833 Epoch 2372:\n",
            "\t\t\tTotal Training Recognition Loss 4.135942 || Total Training Translation Loss 0.007420\n",
            "2025-07-10 14:13:57,055 Epoch 2373:\n",
            "\t\t\tTotal Training Recognition Loss 0.366498 || Total Training Translation Loss 0.007288\n",
            "2025-07-10 14:13:57,273 Epoch 2374:\n",
            "\t\t\tTotal Training Recognition Loss 1.169034 || Total Training Translation Loss 0.005568\n",
            "2025-07-10 14:13:57,492 Epoch 2375:\n",
            "\t\t\tTotal Training Recognition Loss 0.421232 || Total Training Translation Loss 0.005717\n",
            "2025-07-10 14:13:57,715 Epoch 2376:\n",
            "\t\t\tTotal Training Recognition Loss 11.894704 || Total Training Translation Loss 0.007887\n",
            "2025-07-10 14:13:57,933 Epoch 2377:\n",
            "\t\t\tTotal Training Recognition Loss 0.159756 || Total Training Translation Loss 0.006201\n",
            "2025-07-10 14:13:58,151 Epoch 2378:\n",
            "\t\t\tTotal Training Recognition Loss 15.862858 || Total Training Translation Loss 0.006255\n",
            "2025-07-10 14:13:58,368 Epoch 2379:\n",
            "\t\t\tTotal Training Recognition Loss 2.828565 || Total Training Translation Loss 0.006430\n",
            "2025-07-10 14:13:58,586 Epoch 2380:\n",
            "\t\t\tTotal Training Recognition Loss 22.758005 || Total Training Translation Loss 0.006529\n",
            "2025-07-10 14:13:58,804 Epoch 2381:\n",
            "\t\t\tTotal Training Recognition Loss 78.617439 || Total Training Translation Loss 0.006090\n",
            "2025-07-10 14:13:59,026 Epoch 2382:\n",
            "\t\t\tTotal Training Recognition Loss 0.248959 || Total Training Translation Loss 0.006964\n",
            "2025-07-10 14:13:59,237 Epoch 2383:\n",
            "\t\t\tTotal Training Recognition Loss 0.461049 || Total Training Translation Loss 0.008005\n",
            "2025-07-10 14:13:59,409 Epoch 2384:\n",
            "\t\t\tTotal Training Recognition Loss 0.657682 || Total Training Translation Loss 0.007744\n",
            "2025-07-10 14:13:59,583 Epoch 2385:\n",
            "\t\t\tTotal Training Recognition Loss 6.908232 || Total Training Translation Loss 0.007235\n",
            "2025-07-10 14:13:59,801 Epoch 2386:\n",
            "\t\t\tTotal Training Recognition Loss 0.610784 || Total Training Translation Loss 0.007827\n",
            "2025-07-10 14:14:00,017 Epoch 2387:\n",
            "\t\t\tTotal Training Recognition Loss 1.196321 || Total Training Translation Loss 0.007505\n",
            "2025-07-10 14:14:00,234 Epoch 2388:\n",
            "\t\t\tTotal Training Recognition Loss 8.805563 || Total Training Translation Loss 0.006370\n",
            "2025-07-10 14:14:00,451 Epoch 2389:\n",
            "\t\t\tTotal Training Recognition Loss 2.113209 || Total Training Translation Loss 0.006651\n",
            "2025-07-10 14:14:00,667 Epoch 2390:\n",
            "\t\t\tTotal Training Recognition Loss 1.603014 || Total Training Translation Loss 0.007123\n",
            "2025-07-10 14:14:00,884 Epoch 2391:\n",
            "\t\t\tTotal Training Recognition Loss 2.930857 || Total Training Translation Loss 0.007817\n",
            "2025-07-10 14:14:01,100 Epoch 2392:\n",
            "\t\t\tTotal Training Recognition Loss 11.086124 || Total Training Translation Loss 0.007348\n",
            "2025-07-10 14:14:01,317 Epoch 2393:\n",
            "\t\t\tTotal Training Recognition Loss 1.944865 || Total Training Translation Loss 0.006649\n",
            "2025-07-10 14:14:01,533 Epoch 2394:\n",
            "\t\t\tTotal Training Recognition Loss 3.795867 || Total Training Translation Loss 0.007825\n",
            "2025-07-10 14:14:01,749 Epoch 2395:\n",
            "\t\t\tTotal Training Recognition Loss 0.757293 || Total Training Translation Loss 0.006518\n",
            "2025-07-10 14:14:01,965 Epoch 2396:\n",
            "\t\t\tTotal Training Recognition Loss 0.662905 || Total Training Translation Loss 0.007388\n",
            "2025-07-10 14:14:02,182 Epoch 2397:\n",
            "\t\t\tTotal Training Recognition Loss 4.130558 || Total Training Translation Loss 0.005693\n",
            "2025-07-10 14:14:02,398 Epoch 2398:\n",
            "\t\t\tTotal Training Recognition Loss 0.707122 || Total Training Translation Loss 0.007211\n",
            "2025-07-10 14:14:02,614 Epoch 2399:\n",
            "\t\t\tTotal Training Recognition Loss 0.427936 || Total Training Translation Loss 0.007514\n",
            "2025-07-10 14:14:02,830 [Epoch: 2400 Step: 00002400] Batch Recognition Loss:   0.595629 => Gls Tokens per Sec:      173 || Batch Translation Loss:   0.007433 => Txt Tokens per Sec:      466 || Lr: 0.000700\n",
            "2025-07-10 14:14:03,136 Validation result at epoch 2400, step     2400: duration: 0.3047s\n",
            "\tRecognition Beam Size: 1\tTranslation Beam Size: 1\tTranslation Beam Alpha: -1\n",
            "\tRecognition Loss: 7065.73633\tTranslation Loss: 256.63297\tPPL: 20.47514\n",
            "\tEval Metric: BLEU\n",
            "\tWER 91.43\t(DEL: 40.00,\tINS: 5.71,\tSUB: 45.71)\n",
            "\tBLEU-4 2.76\t(BLEU-1: 6.67,\tBLEU-2: 4.29,\tBLEU-3: 3.40,\tBLEU-4: 2.76)\n",
            "\tCHRF 23.02\tROUGE 10.44\tFID 0.00\n",
            "2025-07-10 14:14:03,139 Logging Recognition and Translation Outputs\n",
            "2025-07-10 14:14:03,139 ========================================================================================\n",
            "2025-07-10 14:14:03,140 Logging Sequence: 11August_2010_Wednesday_tagesschau-2\n",
            "2025-07-10 14:14:03,140 \tGloss Reference :\tDRUCK TIEF KOMMEN\n",
            "2025-07-10 14:14:03,141 \tGloss Hypothesis:\t***** **** LOCH  \n",
            "2025-07-10 14:14:03,141 \tGloss Alignment :\tD     D    S     \n",
            "2025-07-10 14:14:03,141 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:14:03,144 \tText Reference  :\t********** ****** ** ***** ***** ***** ******* ***** **** *** ********* ********* ******** ********* ****** ******* *** ***** *** ****** ***** tiefer luftdruck bestimmt in    den   nächsten tagen unser wetter\n",
            "2025-07-10 14:14:03,144 \tText Hypothesis :\tnordwesten bleibt es heute nacht meist trocken sonst muss mit teilweise kräftigen schauern gerechnet werden örtlich mit blitz und donner <unk> <unk>  <unk>     <unk>    <unk> <unk> <unk>    <unk> <unk> <unk> \n",
            "2025-07-10 14:14:03,144 \tText Alignment  :\tI          I      I  I     I     I     I       I     I    I   I         I         I        I         I      I       I   I     I   I      I     S      S         S        S     S     S        S     S     S     \n",
            "2025-07-10 14:14:03,145 ========================================================================================\n",
            "2025-07-10 14:14:03,145 Logging Sequence: 11August_2010_Wednesday_tagesschau-3\n",
            "2025-07-10 14:14:03,145 \tGloss Reference :\tES-BEDEUTET VIEL WOLKE UND KOENNEN REGEN GEWITTER KOENNEN\n",
            "2025-07-10 14:14:03,145 \tGloss Hypothesis:\t*********** **** ***** *** ******* ORT   LOCH     KOENNEN\n",
            "2025-07-10 14:14:03,146 \tGloss Alignment :\tD           D    D     D   D       S     S               \n",
            "2025-07-10 14:14:03,146 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:14:03,148 \tText Reference  :\tdas bedeutet viele wolken und ******** **** ****** **************** immer wieder ******* ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** zum   teil  kräftige schauer und   gewitter\n",
            "2025-07-10 14:14:03,148 \tText Hypothesis :\t*** es       auch  länger und ergiebig auch lokale überschwemmungen sind  wieder möglich <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>    <unk>   <unk> <unk>   \n",
            "2025-07-10 14:14:03,149 \tText Alignment  :\tD   S        S     S          I        I    I      I                S            I       I     I     I     I     I     I     I     I     I     I     I     I     I     S     S     S        S       S     S       \n",
            "2025-07-10 14:14:03,149 ========================================================================================\n",
            "2025-07-10 14:14:03,149 Logging Sequence: 11August_2010_Wednesday_tagesschau-8\n",
            "2025-07-10 14:14:03,150 \tGloss Reference :\tWIND    MAESSIG SCHWACH REGION  WENN    GEWITTER WIND    KOENNEN ********\n",
            "2025-07-10 14:14:03,150 \tGloss Hypothesis:\tKOENNEN LOCH    KOENNEN TROCKEN BLEIBEN REGEN    BLEIBEN KOENNEN NORDWEST\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-07-10 14:14:03,150 \tGloss Alignment :\tS       S       S       S       S       S        S               I       \n",
            "2025-07-10 14:14:03,151 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:14:03,153 \tText Reference  :\t** **** ****** *** ******** **** ****** **************** **** ****** ******* ***** meist weht  nur   ein   schwacher wind  aus   unterschiedlichen richtungen der   bei   schauern und   gewittern stark böig  sein  kann \n",
            "2025-07-10 14:14:03,153 \tText Hypothesis :\tes auch länger und ergiebig auch lokale überschwemmungen sind wieder möglich <unk> <unk> <unk> <unk> <unk> <unk>     <unk> <unk> <unk>             <unk>      <unk> <unk> <unk>    <unk> <unk>     <unk> <unk> <unk> <unk>\n",
            "2025-07-10 14:14:03,153 \tText Alignment  :\tI  I    I      I   I        I    I      I                I    I      I       I     S     S     S     S     S         S     S     S                 S          S     S     S        S     S         S     S     S     S    \n",
            "2025-07-10 14:14:03,153 ========================================================================================\n",
            "2025-07-10 14:14:03,154 Logging Sequence: 25October_2010_Monday_tagesschau-22\n",
            "2025-07-10 14:14:03,154 \tGloss Reference :\tMITTWOCH REGEN KOENNEN NORDWEST WAHRSCHEINLICH NORD     STARK WIND\n",
            "2025-07-10 14:14:03,154 \tGloss Hypothesis:\t******** ***** ******* JETZT    FEBRUAR        SUEDWEST JETZT VIEL\n",
            "2025-07-10 14:14:03,154 \tGloss Alignment :\tD        D     D       S        S              S        S     S   \n",
            "2025-07-10 14:14:03,154 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:14:03,156 \tText Reference  :\tam mittwoch hier   und ******** **** ****** **************** **** ****** ******* ***** ***** ***** ***** ***** ***** ***** ***** ***** da    nieselregen in    der   nordwesthälfte an    den   küsten kräftiger wind \n",
            "2025-07-10 14:14:03,156 \tText Hypothesis :\tes auch     länger und ergiebig auch lokale überschwemmungen sind wieder möglich <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>       <unk> <unk> <unk>          <unk> <unk> <unk>  <unk>     <unk>\n",
            "2025-07-10 14:14:03,156 \tText Alignment  :\tS  S        S          I        I    I      I                I    I      I       I     I     I     I     I     I     I     I     I     S     S           S     S     S              S     S     S      S         S    \n",
            "2025-07-10 14:14:03,157 ========================================================================================\n",
            "2025-07-10 14:14:03,157 Logging Sequence: 05May_2011_Thursday_tagesschau-25\n",
            "2025-07-10 14:14:03,157 \tGloss Reference :\t**** JETZT WETTER WIE-AUSSEHEN MORGEN FREITAG SECHSTE MAI ZEIGEN-BILDSCHIRM\n",
            "2025-07-10 14:14:03,157 \tGloss Hypothesis:\tVIEL JETZT WETTER ************ MORGEN ******* ******* *** DONNERSTAG       \n",
            "2025-07-10 14:14:03,158 \tGloss Alignment :\tI                 D                   D       D       D   S                \n",
            "2025-07-10 14:14:03,158 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:14:03,159 \tText Reference  :\tund nun die wettervorhersage für morgen freitag    den ******** ****** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** sechsten mai  \n",
            "2025-07-10 14:14:03,159 \tText Hypothesis :\t*** nun die wettervorhersage für morgen donnerstag den zwölften august <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>    <unk>\n",
            "2025-07-10 14:14:03,160 \tText Alignment  :\tD                                       S              I        I      I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     S        S    \n",
            "2025-07-10 14:14:03,160 ========================================================================================\n",
            "2025-07-10 14:14:03,160 Epoch 2400:\n",
            "\t\t\tTotal Training Recognition Loss 0.595629 || Total Training Translation Loss 0.007433\n",
            "2025-07-10 14:14:03,328 Epoch 2401:\n",
            "\t\t\tTotal Training Recognition Loss 2.256047 || Total Training Translation Loss 0.007881\n",
            "2025-07-10 14:14:03,498 Epoch 2402:\n",
            "\t\t\tTotal Training Recognition Loss 0.336432 || Total Training Translation Loss 0.006494\n",
            "2025-07-10 14:14:03,668 Epoch 2403:\n",
            "\t\t\tTotal Training Recognition Loss 0.479141 || Total Training Translation Loss 0.007338\n",
            "2025-07-10 14:14:03,839 Epoch 2404:\n",
            "\t\t\tTotal Training Recognition Loss 0.177660 || Total Training Translation Loss 0.006654\n",
            "2025-07-10 14:14:04,008 Epoch 2405:\n",
            "\t\t\tTotal Training Recognition Loss 20.061153 || Total Training Translation Loss 0.005798\n",
            "2025-07-10 14:14:04,178 Epoch 2406:\n",
            "\t\t\tTotal Training Recognition Loss 1.300939 || Total Training Translation Loss 0.006546\n",
            "2025-07-10 14:14:04,352 Epoch 2407:\n",
            "\t\t\tTotal Training Recognition Loss 0.361473 || Total Training Translation Loss 0.007517\n",
            "2025-07-10 14:14:04,525 Epoch 2408:\n",
            "\t\t\tTotal Training Recognition Loss 0.599981 || Total Training Translation Loss 0.007513\n",
            "2025-07-10 14:14:04,700 Epoch 2409:\n",
            "\t\t\tTotal Training Recognition Loss 2.747223 || Total Training Translation Loss 0.007309\n",
            "2025-07-10 14:14:04,871 Epoch 2410:\n",
            "\t\t\tTotal Training Recognition Loss 6.118016 || Total Training Translation Loss 0.008027\n",
            "2025-07-10 14:14:05,042 Epoch 2411:\n",
            "\t\t\tTotal Training Recognition Loss 2.054816 || Total Training Translation Loss 0.006361\n",
            "2025-07-10 14:14:05,220 Epoch 2412:\n",
            "\t\t\tTotal Training Recognition Loss 3.500943 || Total Training Translation Loss 0.007330\n",
            "2025-07-10 14:14:05,394 Epoch 2413:\n",
            "\t\t\tTotal Training Recognition Loss 0.204444 || Total Training Translation Loss 0.006482\n",
            "2025-07-10 14:14:05,568 Epoch 2414:\n",
            "\t\t\tTotal Training Recognition Loss 0.311650 || Total Training Translation Loss 0.006683\n",
            "2025-07-10 14:14:05,742 Epoch 2415:\n",
            "\t\t\tTotal Training Recognition Loss 0.390087 || Total Training Translation Loss 0.007193\n",
            "2025-07-10 14:14:05,959 Epoch 2416:\n",
            "\t\t\tTotal Training Recognition Loss 0.179654 || Total Training Translation Loss 0.007204\n",
            "2025-07-10 14:14:06,177 Epoch 2417:\n",
            "\t\t\tTotal Training Recognition Loss 1.671401 || Total Training Translation Loss 0.006066\n",
            "2025-07-10 14:14:06,395 Epoch 2418:\n",
            "\t\t\tTotal Training Recognition Loss 0.139722 || Total Training Translation Loss 0.007149\n",
            "2025-07-10 14:14:06,613 Epoch 2419:\n",
            "\t\t\tTotal Training Recognition Loss 0.158686 || Total Training Translation Loss 0.007338\n",
            "2025-07-10 14:14:06,831 Epoch 2420:\n",
            "\t\t\tTotal Training Recognition Loss 0.388930 || Total Training Translation Loss 0.006247\n",
            "2025-07-10 14:14:07,006 Epoch 2421:\n",
            "\t\t\tTotal Training Recognition Loss 0.210583 || Total Training Translation Loss 0.006783\n",
            "2025-07-10 14:14:07,180 Epoch 2422:\n",
            "\t\t\tTotal Training Recognition Loss 19.279827 || Total Training Translation Loss 0.007919\n",
            "2025-07-10 14:14:07,353 Epoch 2423:\n",
            "\t\t\tTotal Training Recognition Loss 0.412810 || Total Training Translation Loss 0.007537\n",
            "2025-07-10 14:14:07,527 Epoch 2424:\n",
            "\t\t\tTotal Training Recognition Loss 0.421175 || Total Training Translation Loss 0.006229\n",
            "2025-07-10 14:14:07,702 Epoch 2425:\n",
            "\t\t\tTotal Training Recognition Loss 0.314577 || Total Training Translation Loss 0.007378\n",
            "2025-07-10 14:14:07,877 Epoch 2426:\n",
            "\t\t\tTotal Training Recognition Loss 0.315506 || Total Training Translation Loss 0.007161\n",
            "2025-07-10 14:14:08,056 Epoch 2427:\n",
            "\t\t\tTotal Training Recognition Loss 0.292122 || Total Training Translation Loss 0.006638\n",
            "2025-07-10 14:14:08,232 Epoch 2428:\n",
            "\t\t\tTotal Training Recognition Loss 0.336685 || Total Training Translation Loss 0.006880\n",
            "2025-07-10 14:14:08,413 Epoch 2429:\n",
            "\t\t\tTotal Training Recognition Loss 0.454531 || Total Training Translation Loss 0.008740\n",
            "2025-07-10 14:14:08,588 Epoch 2430:\n",
            "\t\t\tTotal Training Recognition Loss 0.283613 || Total Training Translation Loss 0.007213\n",
            "2025-07-10 14:14:08,765 Epoch 2431:\n",
            "\t\t\tTotal Training Recognition Loss 0.309367 || Total Training Translation Loss 0.006769\n",
            "2025-07-10 14:14:08,940 Epoch 2432:\n",
            "\t\t\tTotal Training Recognition Loss 0.972318 || Total Training Translation Loss 0.006950\n",
            "2025-07-10 14:14:09,114 Epoch 2433:\n",
            "\t\t\tTotal Training Recognition Loss 0.843070 || Total Training Translation Loss 0.007713\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-07-10 14:14:09,289 Epoch 2434:\n",
            "\t\t\tTotal Training Recognition Loss 0.721422 || Total Training Translation Loss 0.007996\n",
            "2025-07-10 14:14:09,463 Epoch 2435:\n",
            "\t\t\tTotal Training Recognition Loss 0.489935 || Total Training Translation Loss 0.006753\n",
            "2025-07-10 14:14:09,639 Epoch 2436:\n",
            "\t\t\tTotal Training Recognition Loss 0.835093 || Total Training Translation Loss 0.008072\n",
            "2025-07-10 14:14:09,865 Epoch 2437:\n",
            "\t\t\tTotal Training Recognition Loss 0.822531 || Total Training Translation Loss 0.007135\n",
            "2025-07-10 14:14:10,083 Epoch 2438:\n",
            "\t\t\tTotal Training Recognition Loss 0.887233 || Total Training Translation Loss 0.007916\n",
            "2025-07-10 14:14:10,302 Epoch 2439:\n",
            "\t\t\tTotal Training Recognition Loss 1.027442 || Total Training Translation Loss 0.008242\n",
            "2025-07-10 14:14:10,519 Epoch 2440:\n",
            "\t\t\tTotal Training Recognition Loss 19.140627 || Total Training Translation Loss 0.007198\n",
            "2025-07-10 14:14:10,737 Epoch 2441:\n",
            "\t\t\tTotal Training Recognition Loss 0.320243 || Total Training Translation Loss 0.006465\n",
            "2025-07-10 14:14:10,954 Epoch 2442:\n",
            "\t\t\tTotal Training Recognition Loss 0.844744 || Total Training Translation Loss 0.006628\n",
            "2025-07-10 14:14:11,172 Epoch 2443:\n",
            "\t\t\tTotal Training Recognition Loss 0.352703 || Total Training Translation Loss 0.007171\n",
            "2025-07-10 14:14:11,391 Epoch 2444:\n",
            "\t\t\tTotal Training Recognition Loss 1.246734 || Total Training Translation Loss 0.008342\n",
            "2025-07-10 14:14:11,610 Epoch 2445:\n",
            "\t\t\tTotal Training Recognition Loss 0.713056 || Total Training Translation Loss 0.007662\n",
            "2025-07-10 14:14:11,828 Epoch 2446:\n",
            "\t\t\tTotal Training Recognition Loss 0.635855 || Total Training Translation Loss 0.006794\n",
            "2025-07-10 14:14:12,047 Epoch 2447:\n",
            "\t\t\tTotal Training Recognition Loss 1.464116 || Total Training Translation Loss 0.007295\n",
            "2025-07-10 14:14:12,269 Epoch 2448:\n",
            "\t\t\tTotal Training Recognition Loss 3.741738 || Total Training Translation Loss 0.007304\n",
            "2025-07-10 14:14:12,497 Epoch 2449:\n",
            "\t\t\tTotal Training Recognition Loss 1.141456 || Total Training Translation Loss 0.008411\n",
            "2025-07-10 14:14:12,718 Epoch 2450:\n",
            "\t\t\tTotal Training Recognition Loss 0.440459 || Total Training Translation Loss 0.006426\n",
            "2025-07-10 14:14:12,939 Epoch 2451:\n",
            "\t\t\tTotal Training Recognition Loss 0.811992 || Total Training Translation Loss 0.007428\n",
            "2025-07-10 14:14:13,158 Epoch 2452:\n",
            "\t\t\tTotal Training Recognition Loss 0.781785 || Total Training Translation Loss 0.006244\n",
            "2025-07-10 14:14:13,381 Epoch 2453:\n",
            "\t\t\tTotal Training Recognition Loss 1.472390 || Total Training Translation Loss 0.007991\n",
            "2025-07-10 14:14:13,604 Epoch 2454:\n",
            "\t\t\tTotal Training Recognition Loss 2.365967 || Total Training Translation Loss 0.007238\n",
            "2025-07-10 14:14:13,822 Epoch 2455:\n",
            "\t\t\tTotal Training Recognition Loss 0.430505 || Total Training Translation Loss 0.007805\n",
            "2025-07-10 14:14:14,042 Epoch 2456:\n",
            "\t\t\tTotal Training Recognition Loss 0.210707 || Total Training Translation Loss 0.008006\n",
            "2025-07-10 14:14:14,236 Epoch 2457:\n",
            "\t\t\tTotal Training Recognition Loss 0.151098 || Total Training Translation Loss 0.006361\n",
            "2025-07-10 14:14:14,454 Epoch 2458:\n",
            "\t\t\tTotal Training Recognition Loss 0.312865 || Total Training Translation Loss 0.007160\n",
            "2025-07-10 14:14:14,676 Epoch 2459:\n",
            "\t\t\tTotal Training Recognition Loss 0.542685 || Total Training Translation Loss 0.008544\n",
            "2025-07-10 14:14:14,898 Epoch 2460:\n",
            "\t\t\tTotal Training Recognition Loss 0.171662 || Total Training Translation Loss 0.007520\n",
            "2025-07-10 14:14:15,121 Epoch 2461:\n",
            "\t\t\tTotal Training Recognition Loss 0.227238 || Total Training Translation Loss 0.007523\n",
            "2025-07-10 14:14:15,344 Epoch 2462:\n",
            "\t\t\tTotal Training Recognition Loss 0.149724 || Total Training Translation Loss 0.008058\n",
            "2025-07-10 14:14:15,562 Epoch 2463:\n",
            "\t\t\tTotal Training Recognition Loss 0.288041 || Total Training Translation Loss 0.006932\n",
            "2025-07-10 14:14:15,781 Epoch 2464:\n",
            "\t\t\tTotal Training Recognition Loss 0.147931 || Total Training Translation Loss 0.007117\n",
            "2025-07-10 14:14:16,001 Epoch 2465:\n",
            "\t\t\tTotal Training Recognition Loss 0.178991 || Total Training Translation Loss 0.007544\n",
            "2025-07-10 14:14:16,228 Epoch 2466:\n",
            "\t\t\tTotal Training Recognition Loss 0.316235 || Total Training Translation Loss 0.008005\n",
            "2025-07-10 14:14:16,445 Epoch 2467:\n",
            "\t\t\tTotal Training Recognition Loss 0.207366 || Total Training Translation Loss 0.007066\n",
            "2025-07-10 14:14:16,667 Epoch 2468:\n",
            "\t\t\tTotal Training Recognition Loss 0.193968 || Total Training Translation Loss 0.006058\n",
            "2025-07-10 14:14:16,890 Epoch 2469:\n",
            "\t\t\tTotal Training Recognition Loss 0.189791 || Total Training Translation Loss 0.006620\n",
            "2025-07-10 14:14:17,080 Epoch 2470:\n",
            "\t\t\tTotal Training Recognition Loss 0.383989 || Total Training Translation Loss 0.007785\n",
            "2025-07-10 14:14:17,251 Epoch 2471:\n",
            "\t\t\tTotal Training Recognition Loss 0.154642 || Total Training Translation Loss 0.008220\n",
            "2025-07-10 14:14:17,423 Epoch 2472:\n",
            "\t\t\tTotal Training Recognition Loss 0.155856 || Total Training Translation Loss 0.007226\n",
            "2025-07-10 14:14:17,594 Epoch 2473:\n",
            "\t\t\tTotal Training Recognition Loss 0.259098 || Total Training Translation Loss 0.005963\n",
            "2025-07-10 14:14:17,773 Epoch 2474:\n",
            "\t\t\tTotal Training Recognition Loss 0.154425 || Total Training Translation Loss 0.006736\n",
            "2025-07-10 14:14:17,945 Epoch 2475:\n",
            "\t\t\tTotal Training Recognition Loss 7.893110 || Total Training Translation Loss 0.006851\n",
            "2025-07-10 14:14:18,118 Epoch 2476:\n",
            "\t\t\tTotal Training Recognition Loss 0.160852 || Total Training Translation Loss 0.008445\n",
            "2025-07-10 14:14:18,292 Epoch 2477:\n",
            "\t\t\tTotal Training Recognition Loss 0.418164 || Total Training Translation Loss 0.007349\n",
            "2025-07-10 14:14:18,463 Epoch 2478:\n",
            "\t\t\tTotal Training Recognition Loss 0.272745 || Total Training Translation Loss 0.007027\n",
            "2025-07-10 14:14:18,634 Epoch 2479:\n",
            "\t\t\tTotal Training Recognition Loss 0.177493 || Total Training Translation Loss 0.008049\n",
            "2025-07-10 14:14:18,806 Epoch 2480:\n",
            "\t\t\tTotal Training Recognition Loss 0.288158 || Total Training Translation Loss 0.008471\n",
            "2025-07-10 14:14:18,979 Epoch 2481:\n",
            "\t\t\tTotal Training Recognition Loss 0.850866 || Total Training Translation Loss 0.007565\n",
            "2025-07-10 14:14:19,153 Epoch 2482:\n",
            "\t\t\tTotal Training Recognition Loss 1.256384 || Total Training Translation Loss 0.006519\n",
            "2025-07-10 14:14:19,375 Epoch 2483:\n",
            "\t\t\tTotal Training Recognition Loss 5.026300 || Total Training Translation Loss 0.007107\n",
            "2025-07-10 14:14:19,593 Epoch 2484:\n",
            "\t\t\tTotal Training Recognition Loss 1.317365 || Total Training Translation Loss 0.007354\n",
            "2025-07-10 14:14:19,770 Epoch 2485:\n",
            "\t\t\tTotal Training Recognition Loss 16.310347 || Total Training Translation Loss 0.008351\n",
            "2025-07-10 14:14:19,939 Epoch 2486:\n",
            "\t\t\tTotal Training Recognition Loss 0.394939 || Total Training Translation Loss 0.008559\n",
            "2025-07-10 14:14:20,110 Epoch 2487:\n",
            "\t\t\tTotal Training Recognition Loss 1.324273 || Total Training Translation Loss 0.008200\n",
            "2025-07-10 14:14:20,281 Epoch 2488:\n",
            "\t\t\tTotal Training Recognition Loss 0.722109 || Total Training Translation Loss 0.006742\n",
            "2025-07-10 14:14:20,452 Epoch 2489:\n",
            "\t\t\tTotal Training Recognition Loss 4.211255 || Total Training Translation Loss 0.007631\n",
            "2025-07-10 14:14:20,626 Epoch 2490:\n",
            "\t\t\tTotal Training Recognition Loss 1.593423 || Total Training Translation Loss 0.007130\n",
            "2025-07-10 14:14:20,797 Epoch 2491:\n",
            "\t\t\tTotal Training Recognition Loss 4.743412 || Total Training Translation Loss 0.006778\n",
            "2025-07-10 14:14:20,969 Epoch 2492:\n",
            "\t\t\tTotal Training Recognition Loss 6.652857 || Total Training Translation Loss 0.007547\n",
            "2025-07-10 14:14:21,140 Epoch 2493:\n",
            "\t\t\tTotal Training Recognition Loss 0.959718 || Total Training Translation Loss 0.008473\n",
            "2025-07-10 14:14:21,317 Epoch 2494:\n",
            "\t\t\tTotal Training Recognition Loss 10.865755 || Total Training Translation Loss 0.007031\n",
            "2025-07-10 14:14:21,487 Epoch 2495:\n",
            "\t\t\tTotal Training Recognition Loss 0.616756 || Total Training Translation Loss 0.007847\n",
            "2025-07-10 14:14:21,659 Epoch 2496:\n",
            "\t\t\tTotal Training Recognition Loss 1.552743 || Total Training Translation Loss 0.008968\n",
            "2025-07-10 14:14:21,832 Epoch 2497:\n",
            "\t\t\tTotal Training Recognition Loss 1.742114 || Total Training Translation Loss 0.008538\n",
            "2025-07-10 14:14:22,006 Epoch 2498:\n",
            "\t\t\tTotal Training Recognition Loss 3.945531 || Total Training Translation Loss 0.008360\n",
            "2025-07-10 14:14:22,180 Epoch 2499:\n",
            "\t\t\tTotal Training Recognition Loss 2.494830 || Total Training Translation Loss 0.008220\n",
            "2025-07-10 14:14:22,363 [Epoch: 2500 Step: 00002500] Batch Recognition Loss:   1.102554 => Gls Tokens per Sec:      204 || Batch Translation Loss:   0.008262 => Txt Tokens per Sec:      551 || Lr: 0.000700\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-07-10 14:14:22,645 Hooray! New best validation result [eval_metric]!\n",
            "2025-07-10 14:14:22,647 Saving new checkpoint.\n",
            "2025-07-10 14:14:22,737 Validation result at epoch 2500, step     2500: duration: 0.3727s\n",
            "\tRecognition Beam Size: 1\tTranslation Beam Size: 1\tTranslation Beam Alpha: -1\n",
            "\tRecognition Loss: 8280.37109\tTranslation Loss: 269.55957\tPPL: 23.83819\n",
            "\tEval Metric: BLEU\n",
            "\tWER 88.57\t(DEL: 20.00,\tINS: 2.86,\tSUB: 65.71)\n",
            "\tBLEU-4 2.89\t(BLEU-1: 8.00,\tBLEU-2: 4.70,\tBLEU-3: 3.62,\tBLEU-4: 2.89)\n",
            "\tCHRF 24.78\tROUGE 12.39\tFID 0.00\n",
            "2025-07-10 14:14:22,737 Logging Recognition and Translation Outputs\n",
            "2025-07-10 14:14:22,738 ========================================================================================\n",
            "2025-07-10 14:14:22,738 Logging Sequence: 11August_2010_Wednesday_tagesschau-2\n",
            "2025-07-10 14:14:22,739 \tGloss Reference :\tDRUCK TIEF       KOMMEN\n",
            "2025-07-10 14:14:22,739 \tGloss Hypothesis:\t***** DONNERSTAG LOCH  \n",
            "2025-07-10 14:14:22,739 \tGloss Alignment :\tD     S          S     \n",
            "2025-07-10 14:14:22,739 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:14:22,740 \tText Reference  :\t********** ****** ** ***** ***** ***** ******* ***** **** *** ********* ********* ******** ********* ****** ******* *** ***** *** ****** ***** tiefer luftdruck bestimmt in    den   nächsten tagen unser wetter\n",
            "2025-07-10 14:14:22,741 \tText Hypothesis :\tnordwesten bleibt es heute nacht meist trocken sonst muss mit teilweise kräftigen schauern gerechnet werden örtlich mit blitz und donner <unk> <unk>  <unk>     <unk>    <unk> <unk> <unk>    <unk> <unk> <unk> \n",
            "2025-07-10 14:14:22,741 \tText Alignment  :\tI          I      I  I     I     I     I       I     I    I   I         I         I        I         I      I       I   I     I   I      I     S      S         S        S     S     S        S     S     S     \n",
            "2025-07-10 14:14:22,741 ========================================================================================\n",
            "2025-07-10 14:14:22,741 Logging Sequence: 11August_2010_Wednesday_tagesschau-3\n",
            "2025-07-10 14:14:22,742 \tGloss Reference :\tES-BEDEUTET VIEL WOLKE UND KOENNEN REGEN GEWITTER KOENNEN\n",
            "2025-07-10 14:14:22,742 \tGloss Hypothesis:\t*********** **** ***** ORT OFT     HEUTE ZWOELF   KOENNEN\n",
            "2025-07-10 14:14:22,742 \tGloss Alignment :\tD           D    D     S   S       S     S               \n",
            "2025-07-10 14:14:22,743 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:14:22,744 \tText Reference  :\tdas bedeutet viele wolken und ******** **** ****** **************** immer wieder ******* ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** zum   teil  kräftige schauer und   gewitter\n",
            "2025-07-10 14:14:22,744 \tText Hypothesis :\t*** es       auch  länger und ergiebig auch lokale überschwemmungen sind  wieder möglich <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>    <unk>   <unk> <unk>   \n",
            "2025-07-10 14:14:22,744 \tText Alignment  :\tD   S        S     S          I        I    I      I                S            I       I     I     I     I     I     I     I     I     I     I     I     I     I     S     S     S        S       S     S       \n",
            "2025-07-10 14:14:22,745 ========================================================================================\n",
            "2025-07-10 14:14:22,745 Logging Sequence: 11August_2010_Wednesday_tagesschau-8\n",
            "2025-07-10 14:14:22,745 \tGloss Reference :\tWIND MAESSIG SCHWACH REGION WENN GEWITTER WIND  KOENNEN\n",
            "2025-07-10 14:14:22,745 \tGloss Hypothesis:\t**** ******* LOCH    DURCH  LOCH BLEIBEN  REGEN KOENNEN\n",
            "2025-07-10 14:14:22,745 \tGloss Alignment :\tD    D       S       S      S    S        S            \n",
            "2025-07-10 14:14:22,746 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:14:22,748 \tText Reference  :\t****** ** **** ****** *** ******** **** ****** **************** **** ****** ******* meist weht  nur   ein   schwacher wind  aus   unterschiedlichen richtungen der   bei   schauern und   gewittern stark böig  sein  kann \n",
            "2025-07-10 14:14:22,748 \tText Hypothesis :\tregnet es auch länger und ergiebig auch lokale überschwemmungen sind wieder möglich <unk> <unk> <unk> <unk> <unk>     <unk> <unk> <unk>             <unk>      <unk> <unk> <unk>    <unk> <unk>     <unk> <unk> <unk> <unk>\n",
            "2025-07-10 14:14:22,748 \tText Alignment  :\tI      I  I    I      I   I        I    I      I                I    I      I       S     S     S     S     S         S     S     S                 S          S     S     S        S     S         S     S     S     S    \n",
            "2025-07-10 14:14:22,749 ========================================================================================\n",
            "2025-07-10 14:14:22,749 Logging Sequence: 25October_2010_Monday_tagesschau-22\n",
            "2025-07-10 14:14:22,749 \tGloss Reference :\tMITTWOCH REGEN    KOENNEN NORDWEST WAHRSCHEINLICH NORD   STARK WIND   \n",
            "2025-07-10 14:14:22,749 \tGloss Hypothesis:\tFEBRUAR  GEWITTER FEBRUAR MANCHMAL FEBRUAR        ZWOELF VIEL  KOENNEN\n",
            "2025-07-10 14:14:22,750 \tGloss Alignment :\tS        S        S       S        S              S      S     S      \n",
            "2025-07-10 14:14:22,750 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:14:22,751 \tText Reference  :\tam *** **** ** ********** *** **** ******** ******* mittwoch hier     und ** ******* ******** ****** ********* *********** ***** ***** da    nieselregen in    der   nordwesthälfte an    den   küsten kräftiger wind \n",
            "2025-07-10 14:14:22,752 \tText Hypothesis :\tam tag gibt es verbreitet zum teil kräftige schauer oder     gewitter und in manchen regionen fallen ergiebige regenmengen <unk> <unk> <unk> <unk>       <unk> <unk> <unk>          <unk> <unk> <unk>  <unk>     <unk>\n",
            "2025-07-10 14:14:22,752 \tText Alignment  :\t   I   I    I  I          I   I    I        I       S        S            I  I       I        I      I         I           I     I     S     S           S     S     S              S     S     S      S         S    \n",
            "2025-07-10 14:14:22,752 ========================================================================================\n",
            "2025-07-10 14:14:22,752 Logging Sequence: 05May_2011_Thursday_tagesschau-25\n",
            "2025-07-10 14:14:22,752 \tGloss Reference :\t****** JETZT WETTER WIE-AUSSEHEN MORGEN FREITAG    SECHSTE MAI      ZEIGEN-BILDSCHIRM\n",
            "2025-07-10 14:14:22,752 \tGloss Hypothesis:\tZWOELF JETZT WETTER ************ MORGEN DONNERSTAG FEBRUAR NORDWEST FEBRUAR          \n",
            "2025-07-10 14:14:22,753 \tGloss Alignment :\tI                   D                   S          S       S        S                \n",
            "2025-07-10 14:14:22,753 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:14:22,754 \tText Reference  :\tund nun die wettervorhersage für morgen freitag    den ******** ****** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** sechsten mai  \n",
            "2025-07-10 14:14:22,754 \tText Hypothesis :\t*** nun die wettervorhersage für morgen donnerstag den zwölften august <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>    <unk>\n",
            "2025-07-10 14:14:22,755 \tText Alignment  :\tD                                       S              I        I      I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     S        S    \n",
            "2025-07-10 14:14:22,755 ========================================================================================\n",
            "2025-07-10 14:14:22,755 Epoch 2500:\n",
            "\t\t\tTotal Training Recognition Loss 1.102554 || Total Training Translation Loss 0.008262\n",
            "2025-07-10 14:14:22,967 Epoch 2501:\n",
            "\t\t\tTotal Training Recognition Loss 1.574496 || Total Training Translation Loss 0.008447\n",
            "2025-07-10 14:14:23,144 Epoch 2502:\n",
            "\t\t\tTotal Training Recognition Loss 0.709965 || Total Training Translation Loss 0.007890\n",
            "2025-07-10 14:14:23,315 Epoch 2503:\n",
            "\t\t\tTotal Training Recognition Loss 1.193029 || Total Training Translation Loss 0.007061\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-07-10 14:14:23,492 Epoch 2504:\n",
            "\t\t\tTotal Training Recognition Loss 0.560751 || Total Training Translation Loss 0.008610\n",
            "2025-07-10 14:14:23,664 Epoch 2505:\n",
            "\t\t\tTotal Training Recognition Loss 8.399131 || Total Training Translation Loss 0.007416\n",
            "2025-07-10 14:14:23,835 Epoch 2506:\n",
            "\t\t\tTotal Training Recognition Loss 0.346212 || Total Training Translation Loss 0.007793\n",
            "2025-07-10 14:14:24,008 Epoch 2507:\n",
            "\t\t\tTotal Training Recognition Loss 0.316857 || Total Training Translation Loss 0.007286\n",
            "2025-07-10 14:14:24,179 Epoch 2508:\n",
            "\t\t\tTotal Training Recognition Loss 7.214004 || Total Training Translation Loss 0.008189\n",
            "2025-07-10 14:14:24,352 Epoch 2509:\n",
            "\t\t\tTotal Training Recognition Loss 0.663611 || Total Training Translation Loss 0.007441\n",
            "2025-07-10 14:14:24,523 Epoch 2510:\n",
            "\t\t\tTotal Training Recognition Loss 0.762948 || Total Training Translation Loss 0.007476\n",
            "2025-07-10 14:14:24,697 Epoch 2511:\n",
            "\t\t\tTotal Training Recognition Loss 1.455814 || Total Training Translation Loss 0.010711\n",
            "2025-07-10 14:14:24,870 Epoch 2512:\n",
            "\t\t\tTotal Training Recognition Loss 0.791574 || Total Training Translation Loss 0.007173\n",
            "2025-07-10 14:14:25,041 Epoch 2513:\n",
            "\t\t\tTotal Training Recognition Loss 1.506957 || Total Training Translation Loss 0.008429\n",
            "2025-07-10 14:14:25,215 Epoch 2514:\n",
            "\t\t\tTotal Training Recognition Loss 0.576915 || Total Training Translation Loss 0.007253\n",
            "2025-07-10 14:14:25,388 Epoch 2515:\n",
            "\t\t\tTotal Training Recognition Loss 0.339572 || Total Training Translation Loss 0.009260\n",
            "2025-07-10 14:14:25,561 Epoch 2516:\n",
            "\t\t\tTotal Training Recognition Loss 1.111277 || Total Training Translation Loss 0.008105\n",
            "2025-07-10 14:14:25,732 Epoch 2517:\n",
            "\t\t\tTotal Training Recognition Loss 0.277898 || Total Training Translation Loss 0.007063\n",
            "2025-07-10 14:14:25,904 Epoch 2518:\n",
            "\t\t\tTotal Training Recognition Loss 7.034771 || Total Training Translation Loss 0.007600\n",
            "2025-07-10 14:14:26,075 Epoch 2519:\n",
            "\t\t\tTotal Training Recognition Loss 82.118469 || Total Training Translation Loss 0.009138\n",
            "2025-07-10 14:14:26,246 Epoch 2520:\n",
            "\t\t\tTotal Training Recognition Loss 98.980949 || Total Training Translation Loss 0.007014\n",
            "2025-07-10 14:14:26,417 Epoch 2521:\n",
            "\t\t\tTotal Training Recognition Loss 3.719076 || Total Training Translation Loss 0.007978\n",
            "2025-07-10 14:14:26,589 Epoch 2522:\n",
            "\t\t\tTotal Training Recognition Loss 63.139095 || Total Training Translation Loss 0.009267\n",
            "2025-07-10 14:14:26,760 Epoch 2523:\n",
            "\t\t\tTotal Training Recognition Loss 64.013161 || Total Training Translation Loss 0.008043\n",
            "2025-07-10 14:14:26,932 Epoch 2524:\n",
            "\t\t\tTotal Training Recognition Loss 8.080835 || Total Training Translation Loss 0.008276\n",
            "2025-07-10 14:14:27,104 Epoch 2525:\n",
            "\t\t\tTotal Training Recognition Loss 5.515112 || Total Training Translation Loss 0.007161\n",
            "2025-07-10 14:14:27,275 Epoch 2526:\n",
            "\t\t\tTotal Training Recognition Loss 16.621229 || Total Training Translation Loss 0.008686\n",
            "2025-07-10 14:14:27,447 Epoch 2527:\n",
            "\t\t\tTotal Training Recognition Loss 95.593681 || Total Training Translation Loss 0.009784\n",
            "2025-07-10 14:14:27,618 Epoch 2528:\n",
            "\t\t\tTotal Training Recognition Loss 1.398304 || Total Training Translation Loss 0.008582\n",
            "2025-07-10 14:14:27,789 Epoch 2529:\n",
            "\t\t\tTotal Training Recognition Loss 12.650442 || Total Training Translation Loss 0.009385\n",
            "2025-07-10 14:14:27,960 Epoch 2530:\n",
            "\t\t\tTotal Training Recognition Loss 19.389933 || Total Training Translation Loss 0.009729\n",
            "2025-07-10 14:14:28,145 Epoch 2531:\n",
            "\t\t\tTotal Training Recognition Loss 9.037738 || Total Training Translation Loss 0.014006\n",
            "2025-07-10 14:14:28,318 Epoch 2532:\n",
            "\t\t\tTotal Training Recognition Loss 8.967417 || Total Training Translation Loss 0.014988\n",
            "2025-07-10 14:14:28,489 Epoch 2533:\n",
            "\t\t\tTotal Training Recognition Loss 11.120143 || Total Training Translation Loss 0.010985\n",
            "2025-07-10 14:14:28,663 Epoch 2534:\n",
            "\t\t\tTotal Training Recognition Loss 16.460726 || Total Training Translation Loss 0.012085\n",
            "2025-07-10 14:14:28,836 Epoch 2535:\n",
            "\t\t\tTotal Training Recognition Loss 5.015359 || Total Training Translation Loss 0.011142\n",
            "2025-07-10 14:14:29,007 Epoch 2536:\n",
            "\t\t\tTotal Training Recognition Loss 4.517265 || Total Training Translation Loss 0.010229\n",
            "2025-07-10 14:14:29,179 Epoch 2537:\n",
            "\t\t\tTotal Training Recognition Loss 5.419996 || Total Training Translation Loss 0.008799\n",
            "2025-07-10 14:14:29,351 Epoch 2538:\n",
            "\t\t\tTotal Training Recognition Loss 3.272781 || Total Training Translation Loss 0.008376\n",
            "2025-07-10 14:14:29,521 Epoch 2539:\n",
            "\t\t\tTotal Training Recognition Loss 5.625847 || Total Training Translation Loss 0.008210\n",
            "2025-07-10 14:14:29,692 Epoch 2540:\n",
            "\t\t\tTotal Training Recognition Loss 17.723600 || Total Training Translation Loss 0.009542\n",
            "2025-07-10 14:14:29,864 Epoch 2541:\n",
            "\t\t\tTotal Training Recognition Loss 1.580947 || Total Training Translation Loss 0.010252\n",
            "2025-07-10 14:14:30,035 Epoch 2542:\n",
            "\t\t\tTotal Training Recognition Loss 1.975204 || Total Training Translation Loss 0.010366\n",
            "2025-07-10 14:14:30,206 Epoch 2543:\n",
            "\t\t\tTotal Training Recognition Loss 1.244915 || Total Training Translation Loss 0.008969\n",
            "2025-07-10 14:14:30,379 Epoch 2544:\n",
            "\t\t\tTotal Training Recognition Loss 1.982999 || Total Training Translation Loss 0.008072\n",
            "2025-07-10 14:14:30,553 Epoch 2545:\n",
            "\t\t\tTotal Training Recognition Loss 1.597475 || Total Training Translation Loss 0.008645\n",
            "2025-07-10 14:14:30,731 Epoch 2546:\n",
            "\t\t\tTotal Training Recognition Loss 2.732758 || Total Training Translation Loss 0.007749\n",
            "2025-07-10 14:14:30,904 Epoch 2547:\n",
            "\t\t\tTotal Training Recognition Loss 1.453948 || Total Training Translation Loss 0.008771\n",
            "2025-07-10 14:14:31,076 Epoch 2548:\n",
            "\t\t\tTotal Training Recognition Loss 1.513624 || Total Training Translation Loss 0.007729\n",
            "2025-07-10 14:14:31,250 Epoch 2549:\n",
            "\t\t\tTotal Training Recognition Loss 3.157649 || Total Training Translation Loss 0.009444\n",
            "2025-07-10 14:14:31,424 Epoch 2550:\n",
            "\t\t\tTotal Training Recognition Loss 0.777059 || Total Training Translation Loss 0.007797\n",
            "2025-07-10 14:14:31,597 Epoch 2551:\n",
            "\t\t\tTotal Training Recognition Loss 10.322874 || Total Training Translation Loss 0.009796\n",
            "2025-07-10 14:14:31,771 Epoch 2552:\n",
            "\t\t\tTotal Training Recognition Loss 1.349278 || Total Training Translation Loss 0.009693\n",
            "2025-07-10 14:14:31,945 Epoch 2553:\n",
            "\t\t\tTotal Training Recognition Loss 0.773914 || Total Training Translation Loss 0.007186\n",
            "2025-07-10 14:14:32,118 Epoch 2554:\n",
            "\t\t\tTotal Training Recognition Loss 4.877175 || Total Training Translation Loss 0.008217\n",
            "2025-07-10 14:14:32,290 Epoch 2555:\n",
            "\t\t\tTotal Training Recognition Loss 2.438928 || Total Training Translation Loss 0.007786\n",
            "2025-07-10 14:14:32,464 Epoch 2556:\n",
            "\t\t\tTotal Training Recognition Loss 1.469175 || Total Training Translation Loss 0.008330\n",
            "2025-07-10 14:14:32,635 Epoch 2557:\n",
            "\t\t\tTotal Training Recognition Loss 2.290024 || Total Training Translation Loss 0.007500\n",
            "2025-07-10 14:14:32,806 Epoch 2558:\n",
            "\t\t\tTotal Training Recognition Loss 0.955645 || Total Training Translation Loss 0.007229\n",
            "2025-07-10 14:14:32,980 Epoch 2559:\n",
            "\t\t\tTotal Training Recognition Loss 1.014548 || Total Training Translation Loss 0.007769\n",
            "2025-07-10 14:14:33,199 Epoch 2560:\n",
            "\t\t\tTotal Training Recognition Loss 1.445645 || Total Training Translation Loss 0.006502\n",
            "2025-07-10 14:14:33,417 Epoch 2561:\n",
            "\t\t\tTotal Training Recognition Loss 4.996091 || Total Training Translation Loss 0.006727\n",
            "2025-07-10 14:14:33,636 Epoch 2562:\n",
            "\t\t\tTotal Training Recognition Loss 0.539048 || Total Training Translation Loss 0.006369\n",
            "2025-07-10 14:14:33,854 Epoch 2563:\n",
            "\t\t\tTotal Training Recognition Loss 10.077210 || Total Training Translation Loss 0.007393\n",
            "2025-07-10 14:14:34,071 Epoch 2564:\n",
            "\t\t\tTotal Training Recognition Loss 3.462690 || Total Training Translation Loss 0.007667\n",
            "2025-07-10 14:14:34,288 Epoch 2565:\n",
            "\t\t\tTotal Training Recognition Loss 73.286560 || Total Training Translation Loss 0.006704\n",
            "2025-07-10 14:14:34,461 Epoch 2566:\n",
            "\t\t\tTotal Training Recognition Loss 2.927081 || Total Training Translation Loss 0.007602\n",
            "2025-07-10 14:14:34,684 Epoch 2567:\n",
            "\t\t\tTotal Training Recognition Loss 46.126446 || Total Training Translation Loss 0.009178\n",
            "2025-07-10 14:14:34,902 Epoch 2568:\n",
            "\t\t\tTotal Training Recognition Loss 44.993481 || Total Training Translation Loss 0.007188\n",
            "2025-07-10 14:14:35,087 Epoch 2569:\n",
            "\t\t\tTotal Training Recognition Loss 39.169662 || Total Training Translation Loss 0.008979\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-07-10 14:14:35,263 Epoch 2570:\n",
            "\t\t\tTotal Training Recognition Loss 0.573403 || Total Training Translation Loss 0.006586\n",
            "2025-07-10 14:14:35,438 Epoch 2571:\n",
            "\t\t\tTotal Training Recognition Loss 1.175490 || Total Training Translation Loss 0.007843\n",
            "2025-07-10 14:14:35,614 Epoch 2572:\n",
            "\t\t\tTotal Training Recognition Loss 2.480984 || Total Training Translation Loss 0.008192\n",
            "2025-07-10 14:14:35,788 Epoch 2573:\n",
            "\t\t\tTotal Training Recognition Loss 1.481476 || Total Training Translation Loss 0.007615\n",
            "2025-07-10 14:14:35,963 Epoch 2574:\n",
            "\t\t\tTotal Training Recognition Loss 1.414311 || Total Training Translation Loss 0.006800\n",
            "2025-07-10 14:14:36,138 Epoch 2575:\n",
            "\t\t\tTotal Training Recognition Loss 3.212111 || Total Training Translation Loss 0.007678\n",
            "2025-07-10 14:14:36,312 Epoch 2576:\n",
            "\t\t\tTotal Training Recognition Loss 2.947125 || Total Training Translation Loss 0.006696\n",
            "2025-07-10 14:14:36,486 Epoch 2577:\n",
            "\t\t\tTotal Training Recognition Loss 4.668769 || Total Training Translation Loss 0.008672\n",
            "2025-07-10 14:14:36,661 Epoch 2578:\n",
            "\t\t\tTotal Training Recognition Loss 2.771572 || Total Training Translation Loss 0.007462\n",
            "2025-07-10 14:14:36,836 Epoch 2579:\n",
            "\t\t\tTotal Training Recognition Loss 11.632770 || Total Training Translation Loss 0.007759\n",
            "2025-07-10 14:14:37,013 Epoch 2580:\n",
            "\t\t\tTotal Training Recognition Loss 1.684685 || Total Training Translation Loss 0.006723\n",
            "2025-07-10 14:14:37,194 Epoch 2581:\n",
            "\t\t\tTotal Training Recognition Loss 1.388931 || Total Training Translation Loss 0.006650\n",
            "2025-07-10 14:14:37,381 Epoch 2582:\n",
            "\t\t\tTotal Training Recognition Loss 9.845105 || Total Training Translation Loss 0.009478\n",
            "2025-07-10 14:14:37,552 Epoch 2583:\n",
            "\t\t\tTotal Training Recognition Loss 2.126512 || Total Training Translation Loss 0.007053\n",
            "2025-07-10 14:14:37,723 Epoch 2584:\n",
            "\t\t\tTotal Training Recognition Loss 0.971570 || Total Training Translation Loss 0.008200\n",
            "2025-07-10 14:14:37,894 Epoch 2585:\n",
            "\t\t\tTotal Training Recognition Loss 4.729537 || Total Training Translation Loss 0.007873\n",
            "2025-07-10 14:14:38,066 Epoch 2586:\n",
            "\t\t\tTotal Training Recognition Loss 1.120189 || Total Training Translation Loss 0.007913\n",
            "2025-07-10 14:14:38,238 Epoch 2587:\n",
            "\t\t\tTotal Training Recognition Loss 1.096380 || Total Training Translation Loss 0.008109\n",
            "2025-07-10 14:14:38,410 Epoch 2588:\n",
            "\t\t\tTotal Training Recognition Loss 1.113530 || Total Training Translation Loss 0.007584\n",
            "2025-07-10 14:14:38,582 Epoch 2589:\n",
            "\t\t\tTotal Training Recognition Loss 6.707817 || Total Training Translation Loss 0.010526\n",
            "2025-07-10 14:14:38,755 Epoch 2590:\n",
            "\t\t\tTotal Training Recognition Loss 1.827922 || Total Training Translation Loss 0.007666\n",
            "2025-07-10 14:14:38,928 Epoch 2591:\n",
            "\t\t\tTotal Training Recognition Loss 0.698841 || Total Training Translation Loss 0.007067\n",
            "2025-07-10 14:14:39,103 Epoch 2592:\n",
            "\t\t\tTotal Training Recognition Loss 0.864120 || Total Training Translation Loss 0.007174\n",
            "2025-07-10 14:14:39,325 Epoch 2593:\n",
            "\t\t\tTotal Training Recognition Loss 1.535016 || Total Training Translation Loss 0.009129\n",
            "2025-07-10 14:14:39,498 Epoch 2594:\n",
            "\t\t\tTotal Training Recognition Loss 0.575580 || Total Training Translation Loss 0.006944\n",
            "2025-07-10 14:14:39,670 Epoch 2595:\n",
            "\t\t\tTotal Training Recognition Loss 1.263934 || Total Training Translation Loss 0.008549\n",
            "2025-07-10 14:14:39,840 Epoch 2596:\n",
            "\t\t\tTotal Training Recognition Loss 4.973674 || Total Training Translation Loss 0.006840\n",
            "2025-07-10 14:14:40,011 Epoch 2597:\n",
            "\t\t\tTotal Training Recognition Loss 5.161070 || Total Training Translation Loss 0.007577\n",
            "2025-07-10 14:14:40,182 Epoch 2598:\n",
            "\t\t\tTotal Training Recognition Loss 0.390783 || Total Training Translation Loss 0.009956\n",
            "2025-07-10 14:14:40,354 Epoch 2599:\n",
            "\t\t\tTotal Training Recognition Loss 0.630321 || Total Training Translation Loss 0.006769\n",
            "2025-07-10 14:14:40,526 [Epoch: 2600 Step: 00002600] Batch Recognition Loss:   0.426645 => Gls Tokens per Sec:      217 || Batch Translation Loss:   0.007503 => Txt Tokens per Sec:      586 || Lr: 0.000700\n",
            "2025-07-10 14:14:40,767 Validation result at epoch 2600, step     2600: duration: 0.2399s\n",
            "\tRecognition Beam Size: 1\tTranslation Beam Size: 1\tTranslation Beam Alpha: -1\n",
            "\tRecognition Loss: 8329.71387\tTranslation Loss: 278.01349\tPPL: 26.33100\n",
            "\tEval Metric: BLEU\n",
            "\tWER 100.00\t(DEL: 20.00,\tINS: 17.14,\tSUB: 62.86)\n",
            "\tBLEU-4 2.76\t(BLEU-1: 6.67,\tBLEU-2: 4.29,\tBLEU-3: 3.40,\tBLEU-4: 2.76)\n",
            "\tCHRF 22.77\tROUGE 10.44\tFID 0.00\n",
            "2025-07-10 14:14:40,768 Logging Recognition and Translation Outputs\n",
            "2025-07-10 14:14:40,768 ========================================================================================\n",
            "2025-07-10 14:14:40,768 Logging Sequence: 11August_2010_Wednesday_tagesschau-2\n",
            "2025-07-10 14:14:40,769 \tGloss Reference :\t******** ******** DRUCK TIEF     KOMMEN\n",
            "2025-07-10 14:14:40,769 \tGloss Hypothesis:\tSPEZIELL NORDWEST WOLKE SPEZIELL LOCH  \n",
            "2025-07-10 14:14:40,769 \tGloss Alignment :\tI        I        S     S        S     \n",
            "2025-07-10 14:14:40,770 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:14:40,772 \tText Reference  :\t********** ****** ** ***** ***** ***** ******* ***** **** *** ********* ********* ******** ********* ****** ******* *** ***** *** ****** ***** tiefer luftdruck bestimmt in    den   nächsten tagen unser wetter\n",
            "2025-07-10 14:14:40,772 \tText Hypothesis :\tnordwesten bleibt es heute nacht meist trocken sonst muss mit teilweise kräftigen schauern gerechnet werden örtlich mit blitz und donner <unk> <unk>  <unk>     <unk>    <unk> <unk> <unk>    <unk> <unk> <unk> \n",
            "2025-07-10 14:14:40,773 \tText Alignment  :\tI          I      I  I     I     I     I       I     I    I   I         I         I        I         I      I       I   I     I   I      I     S      S         S        S     S     S        S     S     S     \n",
            "2025-07-10 14:14:40,773 ========================================================================================\n",
            "2025-07-10 14:14:40,773 Logging Sequence: 11August_2010_Wednesday_tagesschau-3\n",
            "2025-07-10 14:14:40,774 \tGloss Reference :\tES-BEDEUTET VIEL WOLKE UND KOENNEN  REGEN  GEWITTER KOENNEN\n",
            "2025-07-10 14:14:40,774 \tGloss Hypothesis:\t*********** **** ***** *** NORDWEST MORGEN LOCH     KOENNEN\n",
            "2025-07-10 14:14:40,775 \tGloss Alignment :\tD           D    D     D   S        S      S               \n",
            "2025-07-10 14:14:40,775 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:14:40,778 \tText Reference  :\tdas bedeutet viele wolken und ******** **** ****** **************** immer wieder ******* ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** zum   teil  kräftige schauer und   gewitter\n",
            "2025-07-10 14:14:40,778 \tText Hypothesis :\t*** es       auch  länger und ergiebig auch lokale überschwemmungen sind  wieder möglich <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>    <unk>   <unk> <unk>   \n",
            "2025-07-10 14:14:40,779 \tText Alignment  :\tD   S        S     S          I        I    I      I                S            I       I     I     I     I     I     I     I     I     I     I     I     I     I     S     S     S        S       S     S       \n",
            "2025-07-10 14:14:40,779 ========================================================================================\n",
            "2025-07-10 14:14:40,779 Logging Sequence: 11August_2010_Wednesday_tagesschau-8\n",
            "2025-07-10 14:14:40,780 \tGloss Reference :\t******** **** ******** WIND MAESSIG SCHWACH REGION  WENN    GEWITTER WIND    KOENNEN\n",
            "2025-07-10 14:14:40,780 \tGloss Hypothesis:\tSPEZIELL VIEL SPEZIELL LOCH DURCH   LOCH    TROCKEN BLEIBEN REGEN    BLEIBEN KOENNEN\n",
            "2025-07-10 14:14:40,781 \tGloss Alignment :\tI        I    I        S    S       S       S       S       S        S              \n",
            "2025-07-10 14:14:40,781 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:14:40,783 \tText Reference  :\t** **** ****** *** ******** **** ****** **************** **** ****** ******* ***** meist weht  nur   ein   schwacher wind  aus   unterschiedlichen richtungen der   bei   schauern und   gewittern stark böig  sein  kann \n",
            "2025-07-10 14:14:40,784 \tText Hypothesis :\tes auch länger und ergiebig auch lokale überschwemmungen sind wieder möglich <unk> <unk> <unk> <unk> <unk> <unk>     <unk> <unk> <unk>             <unk>      <unk> <unk> <unk>    <unk> <unk>     <unk> <unk> <unk> <unk>\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-07-10 14:14:40,784 \tText Alignment  :\tI  I    I      I   I        I    I      I                I    I      I       I     S     S     S     S     S         S     S     S                 S          S     S     S        S     S         S     S     S     S    \n",
            "2025-07-10 14:14:40,784 ========================================================================================\n",
            "2025-07-10 14:14:40,784 Logging Sequence: 25October_2010_Monday_tagesschau-22\n",
            "2025-07-10 14:14:40,785 \tGloss Reference :\tMITTWOCH REGEN    KOENNEN NORDWEST WAHRSCHEINLICH NORD     STARK   WIND\n",
            "2025-07-10 14:14:40,785 \tGloss Hypothesis:\tFEBRUAR  GEWITTER KOENNEN FEBRUAR  SUEDWEST       MANCHMAL KOENNEN VIEL\n",
            "2025-07-10 14:14:40,785 \tGloss Alignment :\tS        S                S        S              S        S       S   \n",
            "2025-07-10 14:14:40,786 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:14:40,787 \tText Reference  :\t*** *** **************** *** ****** ********** *** ******** ****** ***** ***** ***** ***** ***** ***** ***** am    mittwoch hier  und   da    nieselregen in    der   nordwesthälfte an    den   küsten kräftiger wind \n",
            "2025-07-10 14:14:40,788 \tText Hypothesis :\tnun die wettervorhersage für morgen donnerstag den zwölften august <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>    <unk> <unk> <unk> <unk>       <unk> <unk> <unk>          <unk> <unk> <unk>  <unk>     <unk>\n",
            "2025-07-10 14:14:40,788 \tText Alignment  :\tI   I   I                I   I      I          I   I        I      I     I     I     I     I     I     I     S     S        S     S     S     S           S     S     S              S     S     S      S         S    \n",
            "2025-07-10 14:14:40,788 ========================================================================================\n",
            "2025-07-10 14:14:40,788 Logging Sequence: 05May_2011_Thursday_tagesschau-25\n",
            "2025-07-10 14:14:40,788 \tGloss Reference :\t****** JETZT WETTER WIE-AUSSEHEN MORGEN FREITAG SECHSTE MAI        ZEIGEN-BILDSCHIRM\n",
            "2025-07-10 14:14:40,789 \tGloss Hypothesis:\tZWOELF JETZT WETTER ************ MORGEN ******* ******* DONNERSTAG FEBRUAR          \n",
            "2025-07-10 14:14:40,789 \tGloss Alignment :\tI                   D                   D       D       S          S                \n",
            "2025-07-10 14:14:40,789 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:14:40,790 \tText Reference  :\tund nun die wettervorhersage für morgen freitag    den ******** ****** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** sechsten mai  \n",
            "2025-07-10 14:14:40,791 \tText Hypothesis :\t*** nun die wettervorhersage für morgen donnerstag den zwölften august <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>    <unk>\n",
            "2025-07-10 14:14:40,791 \tText Alignment  :\tD                                       S              I        I      I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     S        S    \n",
            "2025-07-10 14:14:40,791 ========================================================================================\n",
            "2025-07-10 14:14:40,791 Epoch 2600:\n",
            "\t\t\tTotal Training Recognition Loss 0.426645 || Total Training Translation Loss 0.007503\n",
            "2025-07-10 14:14:40,960 Epoch 2601:\n",
            "\t\t\tTotal Training Recognition Loss 0.592082 || Total Training Translation Loss 0.006600\n",
            "2025-07-10 14:14:41,132 Epoch 2602:\n",
            "\t\t\tTotal Training Recognition Loss 0.502192 || Total Training Translation Loss 0.007604\n",
            "2025-07-10 14:14:41,303 Epoch 2603:\n",
            "\t\t\tTotal Training Recognition Loss 0.369931 || Total Training Translation Loss 0.007776\n",
            "2025-07-10 14:14:41,478 Epoch 2604:\n",
            "\t\t\tTotal Training Recognition Loss 0.649892 || Total Training Translation Loss 0.007638\n",
            "2025-07-10 14:14:41,651 Epoch 2605:\n",
            "\t\t\tTotal Training Recognition Loss 0.559055 || Total Training Translation Loss 0.007063\n",
            "2025-07-10 14:14:41,823 Epoch 2606:\n",
            "\t\t\tTotal Training Recognition Loss 0.322114 || Total Training Translation Loss 0.007939\n",
            "2025-07-10 14:14:41,995 Epoch 2607:\n",
            "\t\t\tTotal Training Recognition Loss 0.474556 || Total Training Translation Loss 0.006559\n",
            "2025-07-10 14:14:42,167 Epoch 2608:\n",
            "\t\t\tTotal Training Recognition Loss 2.131058 || Total Training Translation Loss 0.006802\n",
            "2025-07-10 14:14:42,339 Epoch 2609:\n",
            "\t\t\tTotal Training Recognition Loss 2.752644 || Total Training Translation Loss 0.008366\n",
            "2025-07-10 14:14:42,513 Epoch 2610:\n",
            "\t\t\tTotal Training Recognition Loss 0.635274 || Total Training Translation Loss 0.009390\n",
            "2025-07-10 14:14:42,690 Epoch 2611:\n",
            "\t\t\tTotal Training Recognition Loss 0.491354 || Total Training Translation Loss 0.007405\n",
            "2025-07-10 14:14:42,911 Epoch 2612:\n",
            "\t\t\tTotal Training Recognition Loss 0.634045 || Total Training Translation Loss 0.007391\n",
            "2025-07-10 14:14:43,087 Epoch 2613:\n",
            "\t\t\tTotal Training Recognition Loss 0.274365 || Total Training Translation Loss 0.007086\n",
            "2025-07-10 14:14:43,262 Epoch 2614:\n",
            "\t\t\tTotal Training Recognition Loss 0.301305 || Total Training Translation Loss 0.008343\n",
            "2025-07-10 14:14:43,435 Epoch 2615:\n",
            "\t\t\tTotal Training Recognition Loss 1.737687 || Total Training Translation Loss 0.008757\n",
            "2025-07-10 14:14:43,607 Epoch 2616:\n",
            "\t\t\tTotal Training Recognition Loss 0.180379 || Total Training Translation Loss 0.007779\n",
            "2025-07-10 14:14:43,781 Epoch 2617:\n",
            "\t\t\tTotal Training Recognition Loss 0.348511 || Total Training Translation Loss 0.009098\n",
            "2025-07-10 14:14:43,953 Epoch 2618:\n",
            "\t\t\tTotal Training Recognition Loss 0.226920 || Total Training Translation Loss 0.007910\n",
            "2025-07-10 14:14:44,129 Epoch 2619:\n",
            "\t\t\tTotal Training Recognition Loss 0.362042 || Total Training Translation Loss 0.008012\n",
            "2025-07-10 14:14:44,308 Epoch 2620:\n",
            "\t\t\tTotal Training Recognition Loss 0.172530 || Total Training Translation Loss 0.007361\n",
            "2025-07-10 14:14:44,480 Epoch 2621:\n",
            "\t\t\tTotal Training Recognition Loss 0.270994 || Total Training Translation Loss 0.007253\n",
            "2025-07-10 14:14:44,654 Epoch 2622:\n",
            "\t\t\tTotal Training Recognition Loss 0.302985 || Total Training Translation Loss 0.007887\n",
            "2025-07-10 14:14:44,831 Epoch 2623:\n",
            "\t\t\tTotal Training Recognition Loss 0.238442 || Total Training Translation Loss 0.007104\n",
            "2025-07-10 14:14:45,005 Epoch 2624:\n",
            "\t\t\tTotal Training Recognition Loss 0.278183 || Total Training Translation Loss 0.006715\n",
            "2025-07-10 14:14:45,185 Epoch 2625:\n",
            "\t\t\tTotal Training Recognition Loss 0.261795 || Total Training Translation Loss 0.006716\n",
            "2025-07-10 14:14:45,358 Epoch 2626:\n",
            "\t\t\tTotal Training Recognition Loss 0.131557 || Total Training Translation Loss 0.006855\n",
            "2025-07-10 14:14:45,532 Epoch 2627:\n",
            "\t\t\tTotal Training Recognition Loss 0.216988 || Total Training Translation Loss 0.008023\n",
            "2025-07-10 14:14:45,706 Epoch 2628:\n",
            "\t\t\tTotal Training Recognition Loss 0.262257 || Total Training Translation Loss 0.007607\n",
            "2025-07-10 14:14:45,882 Epoch 2629:\n",
            "\t\t\tTotal Training Recognition Loss 0.317644 || Total Training Translation Loss 0.008167\n",
            "2025-07-10 14:14:46,057 Epoch 2630:\n",
            "\t\t\tTotal Training Recognition Loss 0.165041 || Total Training Translation Loss 0.006729\n",
            "2025-07-10 14:14:46,232 Epoch 2631:\n",
            "\t\t\tTotal Training Recognition Loss 0.145294 || Total Training Translation Loss 0.006810\n",
            "2025-07-10 14:14:46,408 Epoch 2632:\n",
            "\t\t\tTotal Training Recognition Loss 0.150555 || Total Training Translation Loss 0.007816\n",
            "2025-07-10 14:14:46,582 Epoch 2633:\n",
            "\t\t\tTotal Training Recognition Loss 0.383312 || Total Training Translation Loss 0.007482\n",
            "2025-07-10 14:14:46,756 Epoch 2634:\n",
            "\t\t\tTotal Training Recognition Loss 0.532056 || Total Training Translation Loss 0.007889\n",
            "2025-07-10 14:14:46,930 Epoch 2635:\n",
            "\t\t\tTotal Training Recognition Loss 0.176569 || Total Training Translation Loss 0.007225\n",
            "2025-07-10 14:14:47,103 Epoch 2636:\n",
            "\t\t\tTotal Training Recognition Loss 0.203754 || Total Training Translation Loss 0.006900\n",
            "2025-07-10 14:14:47,277 Epoch 2637:\n",
            "\t\t\tTotal Training Recognition Loss 0.154617 || Total Training Translation Loss 0.007772\n",
            "2025-07-10 14:14:47,455 Epoch 2638:\n",
            "\t\t\tTotal Training Recognition Loss 0.446051 || Total Training Translation Loss 0.009182\n",
            "2025-07-10 14:14:47,631 Epoch 2639:\n",
            "\t\t\tTotal Training Recognition Loss 0.116815 || Total Training Translation Loss 0.007830\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-07-10 14:14:47,805 Epoch 2640:\n",
            "\t\t\tTotal Training Recognition Loss 0.231540 || Total Training Translation Loss 0.007380\n",
            "2025-07-10 14:14:47,981 Epoch 2641:\n",
            "\t\t\tTotal Training Recognition Loss 0.252902 || Total Training Translation Loss 0.008181\n",
            "2025-07-10 14:14:48,156 Epoch 2642:\n",
            "\t\t\tTotal Training Recognition Loss 0.233647 || Total Training Translation Loss 0.008445\n",
            "2025-07-10 14:14:48,331 Epoch 2643:\n",
            "\t\t\tTotal Training Recognition Loss 0.162563 || Total Training Translation Loss 0.007130\n",
            "2025-07-10 14:14:48,506 Epoch 2644:\n",
            "\t\t\tTotal Training Recognition Loss 0.115900 || Total Training Translation Loss 0.010686\n",
            "2025-07-10 14:14:48,681 Epoch 2645:\n",
            "\t\t\tTotal Training Recognition Loss 0.433257 || Total Training Translation Loss 0.007278\n",
            "2025-07-10 14:14:48,857 Epoch 2646:\n",
            "\t\t\tTotal Training Recognition Loss 0.196482 || Total Training Translation Loss 0.007482\n",
            "2025-07-10 14:14:49,030 Epoch 2647:\n",
            "\t\t\tTotal Training Recognition Loss 0.432721 || Total Training Translation Loss 0.007890\n",
            "2025-07-10 14:14:49,206 Epoch 2648:\n",
            "\t\t\tTotal Training Recognition Loss 0.172792 || Total Training Translation Loss 0.006808\n",
            "2025-07-10 14:14:49,381 Epoch 2649:\n",
            "\t\t\tTotal Training Recognition Loss 0.117294 || Total Training Translation Loss 0.007671\n",
            "2025-07-10 14:14:49,558 Epoch 2650:\n",
            "\t\t\tTotal Training Recognition Loss 0.211420 || Total Training Translation Loss 0.008449\n",
            "2025-07-10 14:14:49,731 Epoch 2651:\n",
            "\t\t\tTotal Training Recognition Loss 0.318200 || Total Training Translation Loss 0.007786\n",
            "2025-07-10 14:14:49,951 Epoch 2652:\n",
            "\t\t\tTotal Training Recognition Loss 0.134403 || Total Training Translation Loss 0.006904\n",
            "2025-07-10 14:14:50,167 Epoch 2653:\n",
            "\t\t\tTotal Training Recognition Loss 0.106821 || Total Training Translation Loss 0.007646\n",
            "2025-07-10 14:14:50,384 Epoch 2654:\n",
            "\t\t\tTotal Training Recognition Loss 0.132320 || Total Training Translation Loss 0.008567\n",
            "2025-07-10 14:14:50,576 Epoch 2655:\n",
            "\t\t\tTotal Training Recognition Loss 0.145983 || Total Training Translation Loss 0.009482\n",
            "2025-07-10 14:14:50,749 Epoch 2656:\n",
            "\t\t\tTotal Training Recognition Loss 0.142121 || Total Training Translation Loss 0.008431\n",
            "2025-07-10 14:14:50,922 Epoch 2657:\n",
            "\t\t\tTotal Training Recognition Loss 0.169277 || Total Training Translation Loss 0.007391\n",
            "2025-07-10 14:14:51,096 Epoch 2658:\n",
            "\t\t\tTotal Training Recognition Loss 0.127558 || Total Training Translation Loss 0.010260\n",
            "2025-07-10 14:14:51,270 Epoch 2659:\n",
            "\t\t\tTotal Training Recognition Loss 0.294466 || Total Training Translation Loss 0.007910\n",
            "2025-07-10 14:14:51,450 Epoch 2660:\n",
            "\t\t\tTotal Training Recognition Loss 0.106024 || Total Training Translation Loss 0.007215\n",
            "2025-07-10 14:14:51,624 Epoch 2661:\n",
            "\t\t\tTotal Training Recognition Loss 0.373192 || Total Training Translation Loss 0.008044\n",
            "2025-07-10 14:14:51,798 Epoch 2662:\n",
            "\t\t\tTotal Training Recognition Loss 0.158822 || Total Training Translation Loss 0.007253\n",
            "2025-07-10 14:14:51,972 Epoch 2663:\n",
            "\t\t\tTotal Training Recognition Loss 1.107890 || Total Training Translation Loss 0.007662\n",
            "2025-07-10 14:14:52,146 Epoch 2664:\n",
            "\t\t\tTotal Training Recognition Loss 0.116729 || Total Training Translation Loss 0.008852\n",
            "2025-07-10 14:14:52,321 Epoch 2665:\n",
            "\t\t\tTotal Training Recognition Loss 0.116509 || Total Training Translation Loss 0.008246\n",
            "2025-07-10 14:14:52,494 Epoch 2666:\n",
            "\t\t\tTotal Training Recognition Loss 0.361442 || Total Training Translation Loss 0.007086\n",
            "2025-07-10 14:14:52,667 Epoch 2667:\n",
            "\t\t\tTotal Training Recognition Loss 0.148257 || Total Training Translation Loss 0.008038\n",
            "2025-07-10 14:14:52,841 Epoch 2668:\n",
            "\t\t\tTotal Training Recognition Loss 0.452116 || Total Training Translation Loss 0.009067\n",
            "2025-07-10 14:14:53,016 Epoch 2669:\n",
            "\t\t\tTotal Training Recognition Loss 0.531233 || Total Training Translation Loss 0.007311\n",
            "2025-07-10 14:14:53,233 Epoch 2670:\n",
            "\t\t\tTotal Training Recognition Loss 0.175454 || Total Training Translation Loss 0.009361\n",
            "2025-07-10 14:14:53,406 Epoch 2671:\n",
            "\t\t\tTotal Training Recognition Loss 0.151838 || Total Training Translation Loss 0.008304\n",
            "2025-07-10 14:14:53,580 Epoch 2672:\n",
            "\t\t\tTotal Training Recognition Loss 1.390461 || Total Training Translation Loss 0.007698\n",
            "2025-07-10 14:14:53,756 Epoch 2673:\n",
            "\t\t\tTotal Training Recognition Loss 0.173369 || Total Training Translation Loss 0.008152\n",
            "2025-07-10 14:14:53,929 Epoch 2674:\n",
            "\t\t\tTotal Training Recognition Loss 0.181484 || Total Training Translation Loss 0.009240\n",
            "2025-07-10 14:14:54,102 Epoch 2675:\n",
            "\t\t\tTotal Training Recognition Loss 0.149265 || Total Training Translation Loss 0.007401\n",
            "2025-07-10 14:14:54,276 Epoch 2676:\n",
            "\t\t\tTotal Training Recognition Loss 0.085309 || Total Training Translation Loss 0.007971\n",
            "2025-07-10 14:14:54,451 Epoch 2677:\n",
            "\t\t\tTotal Training Recognition Loss 0.142149 || Total Training Translation Loss 0.010901\n",
            "2025-07-10 14:14:54,627 Epoch 2678:\n",
            "\t\t\tTotal Training Recognition Loss 0.162354 || Total Training Translation Loss 0.007262\n",
            "2025-07-10 14:14:54,802 Epoch 2679:\n",
            "\t\t\tTotal Training Recognition Loss 0.232062 || Total Training Translation Loss 0.010902\n",
            "2025-07-10 14:14:54,978 Epoch 2680:\n",
            "\t\t\tTotal Training Recognition Loss 0.598021 || Total Training Translation Loss 0.008087\n",
            "2025-07-10 14:14:55,153 Epoch 2681:\n",
            "\t\t\tTotal Training Recognition Loss 0.139609 || Total Training Translation Loss 0.009173\n",
            "2025-07-10 14:14:55,327 Epoch 2682:\n",
            "\t\t\tTotal Training Recognition Loss 0.097130 || Total Training Translation Loss 0.008330\n",
            "2025-07-10 14:14:55,500 Epoch 2683:\n",
            "\t\t\tTotal Training Recognition Loss 0.256391 || Total Training Translation Loss 0.007306\n",
            "2025-07-10 14:14:55,674 Epoch 2684:\n",
            "\t\t\tTotal Training Recognition Loss 0.294148 || Total Training Translation Loss 0.008943\n",
            "2025-07-10 14:14:55,847 Epoch 2685:\n",
            "\t\t\tTotal Training Recognition Loss 0.128306 || Total Training Translation Loss 0.007157\n",
            "2025-07-10 14:14:56,021 Epoch 2686:\n",
            "\t\t\tTotal Training Recognition Loss 0.131617 || Total Training Translation Loss 0.008142\n",
            "2025-07-10 14:14:56,194 Epoch 2687:\n",
            "\t\t\tTotal Training Recognition Loss 0.166049 || Total Training Translation Loss 0.009282\n",
            "2025-07-10 14:14:56,367 Epoch 2688:\n",
            "\t\t\tTotal Training Recognition Loss 0.125781 || Total Training Translation Loss 0.008207\n",
            "2025-07-10 14:14:56,543 Epoch 2689:\n",
            "\t\t\tTotal Training Recognition Loss 0.190417 || Total Training Translation Loss 0.009362\n",
            "2025-07-10 14:14:56,717 Epoch 2690:\n",
            "\t\t\tTotal Training Recognition Loss 0.121308 || Total Training Translation Loss 0.008626\n",
            "2025-07-10 14:14:56,935 Epoch 2691:\n",
            "\t\t\tTotal Training Recognition Loss 0.142114 || Total Training Translation Loss 0.008691\n",
            "2025-07-10 14:14:57,153 Epoch 2692:\n",
            "\t\t\tTotal Training Recognition Loss 0.452940 || Total Training Translation Loss 0.008693\n",
            "2025-07-10 14:14:57,348 Epoch 2693:\n",
            "\t\t\tTotal Training Recognition Loss 0.234567 || Total Training Translation Loss 0.007967\n",
            "2025-07-10 14:14:57,518 Epoch 2694:\n",
            "\t\t\tTotal Training Recognition Loss 0.067412 || Total Training Translation Loss 0.007843\n",
            "2025-07-10 14:14:57,690 Epoch 2695:\n",
            "\t\t\tTotal Training Recognition Loss 0.181847 || Total Training Translation Loss 0.007192\n",
            "2025-07-10 14:14:57,861 Epoch 2696:\n",
            "\t\t\tTotal Training Recognition Loss 0.209300 || Total Training Translation Loss 0.008971\n",
            "2025-07-10 14:14:58,031 Epoch 2697:\n",
            "\t\t\tTotal Training Recognition Loss 0.102063 || Total Training Translation Loss 0.007996\n",
            "2025-07-10 14:14:58,203 Epoch 2698:\n",
            "\t\t\tTotal Training Recognition Loss 0.111133 || Total Training Translation Loss 0.009844\n",
            "2025-07-10 14:14:58,374 Epoch 2699:\n",
            "\t\t\tTotal Training Recognition Loss 0.086889 || Total Training Translation Loss 0.008097\n",
            "2025-07-10 14:14:58,548 [Epoch: 2700 Step: 00002700] Batch Recognition Loss:   0.152556 => Gls Tokens per Sec:      214 || Batch Translation Loss:   0.007538 => Txt Tokens per Sec:      579 || Lr: 0.000700\n",
            "2025-07-10 14:14:58,785 Validation result at epoch 2700, step     2700: duration: 0.2361s\n",
            "\tRecognition Beam Size: 1\tTranslation Beam Size: 1\tTranslation Beam Alpha: -1\n",
            "\tRecognition Loss: 9841.56152\tTranslation Loss: 279.33072\tPPL: 26.74222\n",
            "\tEval Metric: BLEU\n",
            "\tWER 102.86\t(DEL: 28.57,\tINS: 20.00,\tSUB: 54.29)\n",
            "\tBLEU-4 2.61\t(BLEU-1: 5.33,\tBLEU-2: 3.84,\tBLEU-3: 3.16,\tBLEU-4: 2.61)\n",
            "\tCHRF 22.13\tROUGE 8.44\tFID 0.00\n",
            "2025-07-10 14:14:58,786 Logging Recognition and Translation Outputs\n",
            "2025-07-10 14:14:58,786 ========================================================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-07-10 14:14:58,786 Logging Sequence: 11August_2010_Wednesday_tagesschau-2\n",
            "2025-07-10 14:14:58,787 \tGloss Reference :\t******** ******** ******** DRUCK TIEF     KOMMEN\n",
            "2025-07-10 14:14:58,787 \tGloss Hypothesis:\tSPEZIELL NORDWEST SPEZIELL WOLKE SPEZIELL LOCH  \n",
            "2025-07-10 14:14:58,787 \tGloss Alignment :\tI        I        I        S     S        S     \n",
            "2025-07-10 14:14:58,788 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:14:58,791 \tText Reference  :\t********** ****** ** ***** ***** ***** ******* ***** **** *** ********* ********* ******** ********* ****** ******* *** ***** *** ****** ***** tiefer luftdruck bestimmt in    den   nächsten tagen unser wetter\n",
            "2025-07-10 14:14:58,791 \tText Hypothesis :\tnordwesten bleibt es heute nacht meist trocken sonst muss mit teilweise kräftigen schauern gerechnet werden örtlich mit blitz und donner <unk> <unk>  <unk>     <unk>    <unk> <unk> <unk>    <unk> <unk> <unk> \n",
            "2025-07-10 14:14:58,792 \tText Alignment  :\tI          I      I  I     I     I     I       I     I    I   I         I         I        I         I      I       I   I     I   I      I     S      S         S        S     S     S        S     S     S     \n",
            "2025-07-10 14:14:58,792 ========================================================================================\n",
            "2025-07-10 14:14:58,792 Logging Sequence: 11August_2010_Wednesday_tagesschau-3\n",
            "2025-07-10 14:14:58,793 \tGloss Reference :\tES-BEDEUTET VIEL WOLKE UND KOENNEN REGEN    GEWITTER KOENNEN\n",
            "2025-07-10 14:14:58,793 \tGloss Hypothesis:\t*********** **** ***** *** ******* NORDWEST LOCH     KOENNEN\n",
            "2025-07-10 14:14:58,794 \tGloss Alignment :\tD           D    D     D   D       S        S               \n",
            "2025-07-10 14:14:58,794 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:14:58,796 \tText Reference  :\t*** *** **************** *** ****** ********** *** ******** ****** ***** ***** ***** ***** ***** ***** ***** ***** das   bedeutet viele wolken und   immer wieder zum   teil  kräftige schauer und   gewitter\n",
            "2025-07-10 14:14:58,797 \tText Hypothesis :\tnun die wettervorhersage für morgen donnerstag den zwölften august <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>    <unk> <unk>  <unk> <unk> <unk>  <unk> <unk> <unk>    <unk>   <unk> <unk>   \n",
            "2025-07-10 14:14:58,797 \tText Alignment  :\tI   I   I                I   I      I          I   I        I      I     I     I     I     I     I     I     I     S     S        S     S      S     S     S      S     S     S        S       S     S       \n",
            "2025-07-10 14:14:58,797 ========================================================================================\n",
            "2025-07-10 14:14:58,798 Logging Sequence: 11August_2010_Wednesday_tagesschau-8\n",
            "2025-07-10 14:14:58,799 \tGloss Reference :\t******** **** WIND  MAESSIG SCHWACH REGION   WENN    GEWITTER WIND    KOENNEN\n",
            "2025-07-10 14:14:58,799 \tGloss Hypothesis:\tSPEZIELL LOCH DURCH LOCH    TROCKEN SPEZIELL BLEIBEN REGEN    BLEIBEN KOENNEN\n",
            "2025-07-10 14:14:58,799 \tGloss Alignment :\tI        I    S     S       S       S        S       S        S              \n",
            "2025-07-10 14:14:58,799 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:14:58,803 \tText Reference  :\t** **** ****** *** ******** **** ****** **************** **** ****** ******* ***** meist weht  nur   ein   schwacher wind  aus   unterschiedlichen richtungen der   bei   schauern und   gewittern stark böig  sein  kann \n",
            "2025-07-10 14:14:58,803 \tText Hypothesis :\tes auch länger und ergiebig auch lokale überschwemmungen sind wieder möglich <unk> <unk> <unk> <unk> <unk> <unk>     <unk> <unk> <unk>             <unk>      <unk> <unk> <unk>    <unk> <unk>     <unk> <unk> <unk> <unk>\n",
            "2025-07-10 14:14:58,803 \tText Alignment  :\tI  I    I      I   I        I    I      I                I    I      I       I     S     S     S     S     S         S     S     S                 S          S     S     S        S     S         S     S     S     S    \n",
            "2025-07-10 14:14:58,803 ========================================================================================\n",
            "2025-07-10 14:14:58,803 Logging Sequence: 25October_2010_Monday_tagesschau-22\n",
            "2025-07-10 14:14:58,804 \tGloss Reference :\t**** MITTWOCH REGEN    KOENNEN NORDWEST WAHRSCHEINLICH NORD     STARK    WIND   \n",
            "2025-07-10 14:14:58,804 \tGloss Hypothesis:\tDAZU FEBRUAR  GEWITTER KOENNEN ******** ************** SUEDWEST MANCHMAL KOENNEN\n",
            "2025-07-10 14:14:58,805 \tGloss Alignment :\tI    S        S                D        D              S        S        S      \n",
            "2025-07-10 14:14:58,805 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:14:58,808 \tText Reference  :\t*** *** **************** *** ****** ********** *** ******** ****** ***** ***** ***** ***** ***** ***** ***** am    mittwoch hier  und   da    nieselregen in    der   nordwesthälfte an    den   küsten kräftiger wind \n",
            "2025-07-10 14:14:58,808 \tText Hypothesis :\tnun die wettervorhersage für morgen donnerstag den zwölften august <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>    <unk> <unk> <unk> <unk>       <unk> <unk> <unk>          <unk> <unk> <unk>  <unk>     <unk>\n",
            "2025-07-10 14:14:58,809 \tText Alignment  :\tI   I   I                I   I      I          I   I        I      I     I     I     I     I     I     I     S     S        S     S     S     S           S     S     S              S     S     S      S         S    \n",
            "2025-07-10 14:14:58,809 ========================================================================================\n",
            "2025-07-10 14:14:58,809 Logging Sequence: 05May_2011_Thursday_tagesschau-25\n",
            "2025-07-10 14:14:58,810 \tGloss Reference :\t****** JETZT WETTER WIE-AUSSEHEN MORGEN FREITAG SECHSTE MAI        ZEIGEN-BILDSCHIRM\n",
            "2025-07-10 14:14:58,810 \tGloss Hypothesis:\tZWOELF JETZT WETTER ************ MORGEN ******* ******* DONNERSTAG FEBRUAR          \n",
            "2025-07-10 14:14:58,810 \tGloss Alignment :\tI                   D                   D       D       S          S                \n",
            "2025-07-10 14:14:58,810 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:14:58,813 \tText Reference  :\tund nun die wettervorhersage für morgen freitag    den ******** ****** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** sechsten mai  \n",
            "2025-07-10 14:14:58,813 \tText Hypothesis :\t*** nun die wettervorhersage für morgen donnerstag den zwölften august <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>    <unk>\n",
            "2025-07-10 14:14:58,813 \tText Alignment  :\tD                                       S              I        I      I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     S        S    \n",
            "2025-07-10 14:14:58,813 ========================================================================================\n",
            "2025-07-10 14:14:58,814 Epoch 2700:\n",
            "\t\t\tTotal Training Recognition Loss 0.152556 || Total Training Translation Loss 0.007538\n",
            "2025-07-10 14:14:58,983 Epoch 2701:\n",
            "\t\t\tTotal Training Recognition Loss 0.300627 || Total Training Translation Loss 0.007995\n",
            "2025-07-10 14:14:59,153 Epoch 2702:\n",
            "\t\t\tTotal Training Recognition Loss 0.109788 || Total Training Translation Loss 0.009502\n",
            "2025-07-10 14:14:59,373 Epoch 2703:\n",
            "\t\t\tTotal Training Recognition Loss 0.121279 || Total Training Translation Loss 0.008303\n",
            "2025-07-10 14:14:59,591 Epoch 2704:\n",
            "\t\t\tTotal Training Recognition Loss 0.083195 || Total Training Translation Loss 0.008304\n",
            "2025-07-10 14:14:59,808 Epoch 2705:\n",
            "\t\t\tTotal Training Recognition Loss 0.096177 || Total Training Translation Loss 0.008304\n",
            "2025-07-10 14:15:00,024 Epoch 2706:\n",
            "\t\t\tTotal Training Recognition Loss 7.120527 || Total Training Translation Loss 0.007397\n",
            "2025-07-10 14:15:00,241 Epoch 2707:\n",
            "\t\t\tTotal Training Recognition Loss 0.076341 || Total Training Translation Loss 0.007722\n",
            "2025-07-10 14:15:00,458 Epoch 2708:\n",
            "\t\t\tTotal Training Recognition Loss 0.147125 || Total Training Translation Loss 0.007060\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-07-10 14:15:00,671 Epoch 2709:\n",
            "\t\t\tTotal Training Recognition Loss 0.131183 || Total Training Translation Loss 0.009267\n",
            "2025-07-10 14:15:00,888 Epoch 2710:\n",
            "\t\t\tTotal Training Recognition Loss 0.104929 || Total Training Translation Loss 0.008670\n",
            "2025-07-10 14:15:01,105 Epoch 2711:\n",
            "\t\t\tTotal Training Recognition Loss 0.462857 || Total Training Translation Loss 0.008595\n",
            "2025-07-10 14:15:01,321 Epoch 2712:\n",
            "\t\t\tTotal Training Recognition Loss 0.347123 || Total Training Translation Loss 0.008564\n",
            "2025-07-10 14:15:01,537 Epoch 2713:\n",
            "\t\t\tTotal Training Recognition Loss 1.518232 || Total Training Translation Loss 0.007498\n",
            "2025-07-10 14:15:01,754 Epoch 2714:\n",
            "\t\t\tTotal Training Recognition Loss 0.283639 || Total Training Translation Loss 0.007966\n",
            "2025-07-10 14:15:01,971 Epoch 2715:\n",
            "\t\t\tTotal Training Recognition Loss 1.281284 || Total Training Translation Loss 0.009450\n",
            "2025-07-10 14:15:02,186 Epoch 2716:\n",
            "\t\t\tTotal Training Recognition Loss 0.364767 || Total Training Translation Loss 0.009659\n",
            "2025-07-10 14:15:02,404 Epoch 2717:\n",
            "\t\t\tTotal Training Recognition Loss 0.213721 || Total Training Translation Loss 0.007480\n",
            "2025-07-10 14:15:02,621 Epoch 2718:\n",
            "\t\t\tTotal Training Recognition Loss 1.336230 || Total Training Translation Loss 0.009438\n",
            "2025-07-10 14:15:02,837 Epoch 2719:\n",
            "\t\t\tTotal Training Recognition Loss 0.400034 || Total Training Translation Loss 0.008356\n",
            "2025-07-10 14:15:03,054 Epoch 2720:\n",
            "\t\t\tTotal Training Recognition Loss 0.503874 || Total Training Translation Loss 0.009488\n",
            "2025-07-10 14:15:03,272 Epoch 2721:\n",
            "\t\t\tTotal Training Recognition Loss 0.178358 || Total Training Translation Loss 0.009052\n",
            "2025-07-10 14:15:03,489 Epoch 2722:\n",
            "\t\t\tTotal Training Recognition Loss 0.202519 || Total Training Translation Loss 0.007600\n",
            "2025-07-10 14:15:03,706 Epoch 2723:\n",
            "\t\t\tTotal Training Recognition Loss 0.301526 || Total Training Translation Loss 0.007520\n",
            "2025-07-10 14:15:03,923 Epoch 2724:\n",
            "\t\t\tTotal Training Recognition Loss 0.406304 || Total Training Translation Loss 0.008624\n",
            "2025-07-10 14:15:04,140 Epoch 2725:\n",
            "\t\t\tTotal Training Recognition Loss 0.195624 || Total Training Translation Loss 0.009782\n",
            "2025-07-10 14:15:04,356 Epoch 2726:\n",
            "\t\t\tTotal Training Recognition Loss 0.234153 || Total Training Translation Loss 0.008120\n",
            "2025-07-10 14:15:04,573 Epoch 2727:\n",
            "\t\t\tTotal Training Recognition Loss 0.259098 || Total Training Translation Loss 0.007158\n",
            "2025-07-10 14:15:04,789 Epoch 2728:\n",
            "\t\t\tTotal Training Recognition Loss 0.096301 || Total Training Translation Loss 0.007588\n",
            "2025-07-10 14:15:05,006 Epoch 2729:\n",
            "\t\t\tTotal Training Recognition Loss 1.516996 || Total Training Translation Loss 0.008109\n",
            "2025-07-10 14:15:05,224 Epoch 2730:\n",
            "\t\t\tTotal Training Recognition Loss 0.127800 || Total Training Translation Loss 0.009573\n",
            "2025-07-10 14:15:05,440 Epoch 2731:\n",
            "\t\t\tTotal Training Recognition Loss 0.956957 || Total Training Translation Loss 0.008719\n",
            "2025-07-10 14:15:05,657 Epoch 2732:\n",
            "\t\t\tTotal Training Recognition Loss 0.110816 || Total Training Translation Loss 0.008369\n",
            "2025-07-10 14:15:05,873 Epoch 2733:\n",
            "\t\t\tTotal Training Recognition Loss 0.309343 || Total Training Translation Loss 0.007486\n",
            "2025-07-10 14:15:06,090 Epoch 2734:\n",
            "\t\t\tTotal Training Recognition Loss 0.169742 || Total Training Translation Loss 0.010030\n",
            "2025-07-10 14:15:06,302 Epoch 2735:\n",
            "\t\t\tTotal Training Recognition Loss 0.347702 || Total Training Translation Loss 0.008712\n",
            "2025-07-10 14:15:06,519 Epoch 2736:\n",
            "\t\t\tTotal Training Recognition Loss 0.147198 || Total Training Translation Loss 0.007977\n",
            "2025-07-10 14:15:06,736 Epoch 2737:\n",
            "\t\t\tTotal Training Recognition Loss 0.152605 || Total Training Translation Loss 0.007829\n",
            "2025-07-10 14:15:06,957 Epoch 2738:\n",
            "\t\t\tTotal Training Recognition Loss 0.114993 || Total Training Translation Loss 0.009061\n",
            "2025-07-10 14:15:07,179 Epoch 2739:\n",
            "\t\t\tTotal Training Recognition Loss 0.149982 || Total Training Translation Loss 0.009078\n",
            "2025-07-10 14:15:07,396 Epoch 2740:\n",
            "\t\t\tTotal Training Recognition Loss 0.325755 || Total Training Translation Loss 0.008235\n",
            "2025-07-10 14:15:07,612 Epoch 2741:\n",
            "\t\t\tTotal Training Recognition Loss 1.571634 || Total Training Translation Loss 0.009357\n",
            "2025-07-10 14:15:07,829 Epoch 2742:\n",
            "\t\t\tTotal Training Recognition Loss 0.166773 || Total Training Translation Loss 0.009360\n",
            "2025-07-10 14:15:08,046 Epoch 2743:\n",
            "\t\t\tTotal Training Recognition Loss 0.711493 || Total Training Translation Loss 0.008008\n",
            "2025-07-10 14:15:08,256 Epoch 2744:\n",
            "\t\t\tTotal Training Recognition Loss 0.085274 || Total Training Translation Loss 0.011092\n",
            "2025-07-10 14:15:08,433 Epoch 2745:\n",
            "\t\t\tTotal Training Recognition Loss 0.120689 || Total Training Translation Loss 0.009935\n",
            "2025-07-10 14:15:08,603 Epoch 2746:\n",
            "\t\t\tTotal Training Recognition Loss 1.875568 || Total Training Translation Loss 0.007776\n",
            "2025-07-10 14:15:08,774 Epoch 2747:\n",
            "\t\t\tTotal Training Recognition Loss 0.385841 || Total Training Translation Loss 0.008247\n",
            "2025-07-10 14:15:08,945 Epoch 2748:\n",
            "\t\t\tTotal Training Recognition Loss 0.128837 || Total Training Translation Loss 0.008230\n",
            "2025-07-10 14:15:09,116 Epoch 2749:\n",
            "\t\t\tTotal Training Recognition Loss 0.099274 || Total Training Translation Loss 0.008827\n",
            "2025-07-10 14:15:09,287 Epoch 2750:\n",
            "\t\t\tTotal Training Recognition Loss 0.521466 || Total Training Translation Loss 0.008652\n",
            "2025-07-10 14:15:09,457 Epoch 2751:\n",
            "\t\t\tTotal Training Recognition Loss 0.181283 || Total Training Translation Loss 0.008919\n",
            "2025-07-10 14:15:09,630 Epoch 2752:\n",
            "\t\t\tTotal Training Recognition Loss 0.138557 || Total Training Translation Loss 0.008132\n",
            "2025-07-10 14:15:09,803 Epoch 2753:\n",
            "\t\t\tTotal Training Recognition Loss 0.121090 || Total Training Translation Loss 0.008324\n",
            "2025-07-10 14:15:09,974 Epoch 2754:\n",
            "\t\t\tTotal Training Recognition Loss 0.274838 || Total Training Translation Loss 0.008664\n",
            "2025-07-10 14:15:10,145 Epoch 2755:\n",
            "\t\t\tTotal Training Recognition Loss 0.465553 || Total Training Translation Loss 0.009180\n",
            "2025-07-10 14:15:10,318 Epoch 2756:\n",
            "\t\t\tTotal Training Recognition Loss 0.101587 || Total Training Translation Loss 0.009324\n",
            "2025-07-10 14:15:10,490 Epoch 2757:\n",
            "\t\t\tTotal Training Recognition Loss 0.225059 || Total Training Translation Loss 0.008124\n",
            "2025-07-10 14:15:10,661 Epoch 2758:\n",
            "\t\t\tTotal Training Recognition Loss 0.077903 || Total Training Translation Loss 0.009557\n",
            "2025-07-10 14:15:10,832 Epoch 2759:\n",
            "\t\t\tTotal Training Recognition Loss 0.209522 || Total Training Translation Loss 0.009685\n",
            "2025-07-10 14:15:11,003 Epoch 2760:\n",
            "\t\t\tTotal Training Recognition Loss 0.084703 || Total Training Translation Loss 0.009175\n",
            "2025-07-10 14:15:11,175 Epoch 2761:\n",
            "\t\t\tTotal Training Recognition Loss 0.149975 || Total Training Translation Loss 0.010515\n",
            "2025-07-10 14:15:11,346 Epoch 2762:\n",
            "\t\t\tTotal Training Recognition Loss 0.179635 || Total Training Translation Loss 0.008368\n",
            "2025-07-10 14:15:11,520 Epoch 2763:\n",
            "\t\t\tTotal Training Recognition Loss 0.105825 || Total Training Translation Loss 0.009365\n",
            "2025-07-10 14:15:11,690 Epoch 2764:\n",
            "\t\t\tTotal Training Recognition Loss 0.127542 || Total Training Translation Loss 0.009471\n",
            "2025-07-10 14:15:11,861 Epoch 2765:\n",
            "\t\t\tTotal Training Recognition Loss 0.073118 || Total Training Translation Loss 0.009397\n",
            "2025-07-10 14:15:12,033 Epoch 2766:\n",
            "\t\t\tTotal Training Recognition Loss 0.284512 || Total Training Translation Loss 0.009069\n",
            "2025-07-10 14:15:12,204 Epoch 2767:\n",
            "\t\t\tTotal Training Recognition Loss 0.131898 || Total Training Translation Loss 0.008314\n",
            "2025-07-10 14:15:12,377 Epoch 2768:\n",
            "\t\t\tTotal Training Recognition Loss 0.101122 || Total Training Translation Loss 0.009560\n",
            "2025-07-10 14:15:12,549 Epoch 2769:\n",
            "\t\t\tTotal Training Recognition Loss 0.365175 || Total Training Translation Loss 0.008613\n",
            "2025-07-10 14:15:12,724 Epoch 2770:\n",
            "\t\t\tTotal Training Recognition Loss 0.081050 || Total Training Translation Loss 0.008782\n",
            "2025-07-10 14:15:12,897 Epoch 2771:\n",
            "\t\t\tTotal Training Recognition Loss 0.314497 || Total Training Translation Loss 0.008883\n",
            "2025-07-10 14:15:13,072 Epoch 2772:\n",
            "\t\t\tTotal Training Recognition Loss 0.182034 || Total Training Translation Loss 0.007896\n",
            "2025-07-10 14:15:13,243 Epoch 2773:\n",
            "\t\t\tTotal Training Recognition Loss 0.097121 || Total Training Translation Loss 0.009164\n",
            "2025-07-10 14:15:13,416 Epoch 2774:\n",
            "\t\t\tTotal Training Recognition Loss 0.104362 || Total Training Translation Loss 0.011266\n",
            "2025-07-10 14:15:13,587 Epoch 2775:\n",
            "\t\t\tTotal Training Recognition Loss 0.085570 || Total Training Translation Loss 0.008560\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-07-10 14:15:13,759 Epoch 2776:\n",
            "\t\t\tTotal Training Recognition Loss 0.548919 || Total Training Translation Loss 0.007911\n",
            "2025-07-10 14:15:13,930 Epoch 2777:\n",
            "\t\t\tTotal Training Recognition Loss 0.245509 || Total Training Translation Loss 0.009525\n",
            "2025-07-10 14:15:14,102 Epoch 2778:\n",
            "\t\t\tTotal Training Recognition Loss 0.081588 || Total Training Translation Loss 0.009279\n",
            "2025-07-10 14:15:14,275 Epoch 2779:\n",
            "\t\t\tTotal Training Recognition Loss 0.100175 || Total Training Translation Loss 0.009041\n",
            "2025-07-10 14:15:14,448 Epoch 2780:\n",
            "\t\t\tTotal Training Recognition Loss 0.098753 || Total Training Translation Loss 0.008247\n",
            "2025-07-10 14:15:14,620 Epoch 2781:\n",
            "\t\t\tTotal Training Recognition Loss 0.891843 || Total Training Translation Loss 0.007512\n",
            "2025-07-10 14:15:14,792 Epoch 2782:\n",
            "\t\t\tTotal Training Recognition Loss 0.110805 || Total Training Translation Loss 0.008505\n",
            "2025-07-10 14:15:14,964 Epoch 2783:\n",
            "\t\t\tTotal Training Recognition Loss 0.070664 || Total Training Translation Loss 0.007173\n",
            "2025-07-10 14:15:15,136 Epoch 2784:\n",
            "\t\t\tTotal Training Recognition Loss 0.194840 || Total Training Translation Loss 0.008787\n",
            "2025-07-10 14:15:15,312 Epoch 2785:\n",
            "\t\t\tTotal Training Recognition Loss 13.061340 || Total Training Translation Loss 0.008923\n",
            "2025-07-10 14:15:15,486 Epoch 2786:\n",
            "\t\t\tTotal Training Recognition Loss 1.642383 || Total Training Translation Loss 0.008827\n",
            "2025-07-10 14:15:15,688 Epoch 2787:\n",
            "\t\t\tTotal Training Recognition Loss 0.104727 || Total Training Translation Loss 0.009065\n",
            "2025-07-10 14:15:15,907 Epoch 2788:\n",
            "\t\t\tTotal Training Recognition Loss 0.151414 || Total Training Translation Loss 0.008285\n",
            "2025-07-10 14:15:16,126 Epoch 2789:\n",
            "\t\t\tTotal Training Recognition Loss 2.236632 || Total Training Translation Loss 0.009518\n",
            "2025-07-10 14:15:16,345 Epoch 2790:\n",
            "\t\t\tTotal Training Recognition Loss 0.078582 || Total Training Translation Loss 0.008995\n",
            "2025-07-10 14:15:16,564 Epoch 2791:\n",
            "\t\t\tTotal Training Recognition Loss 0.077122 || Total Training Translation Loss 0.008336\n",
            "2025-07-10 14:15:16,784 Epoch 2792:\n",
            "\t\t\tTotal Training Recognition Loss 0.167203 || Total Training Translation Loss 0.007734\n",
            "2025-07-10 14:15:17,002 Epoch 2793:\n",
            "\t\t\tTotal Training Recognition Loss 0.470268 || Total Training Translation Loss 0.008163\n",
            "2025-07-10 14:15:17,221 Epoch 2794:\n",
            "\t\t\tTotal Training Recognition Loss 0.583399 || Total Training Translation Loss 0.008698\n",
            "2025-07-10 14:15:17,439 Epoch 2795:\n",
            "\t\t\tTotal Training Recognition Loss 0.070609 || Total Training Translation Loss 0.010312\n",
            "2025-07-10 14:15:17,637 Epoch 2796:\n",
            "\t\t\tTotal Training Recognition Loss 0.928872 || Total Training Translation Loss 0.009334\n",
            "2025-07-10 14:15:17,815 Epoch 2797:\n",
            "\t\t\tTotal Training Recognition Loss 0.100993 || Total Training Translation Loss 0.008287\n",
            "2025-07-10 14:15:17,990 Epoch 2798:\n",
            "\t\t\tTotal Training Recognition Loss 0.069632 || Total Training Translation Loss 0.010135\n",
            "2025-07-10 14:15:18,165 Epoch 2799:\n",
            "\t\t\tTotal Training Recognition Loss 1.026150 || Total Training Translation Loss 0.010297\n",
            "2025-07-10 14:15:18,339 [Epoch: 2800 Step: 00002800] Batch Recognition Loss:   0.078231 => Gls Tokens per Sec:      214 || Batch Translation Loss:   0.008246 => Txt Tokens per Sec:      577 || Lr: 0.000700\n",
            "2025-07-10 14:15:18,587 Validation result at epoch 2800, step     2800: duration: 0.2460s\n",
            "\tRecognition Beam Size: 1\tTranslation Beam Size: 1\tTranslation Beam Alpha: -1\n",
            "\tRecognition Loss: 9938.94434\tTranslation Loss: 287.22772\tPPL: 29.34581\n",
            "\tEval Metric: BLEU\n",
            "\tWER 97.14\t(DEL: 20.00,\tINS: 14.29,\tSUB: 62.86)\n",
            "\tBLEU-4 2.61\t(BLEU-1: 5.33,\tBLEU-2: 3.84,\tBLEU-3: 3.16,\tBLEU-4: 2.61)\n",
            "\tCHRF 22.13\tROUGE 8.44\tFID 0.00\n",
            "2025-07-10 14:15:18,587 Logging Recognition and Translation Outputs\n",
            "2025-07-10 14:15:18,587 ========================================================================================\n",
            "2025-07-10 14:15:18,588 Logging Sequence: 11August_2010_Wednesday_tagesschau-2\n",
            "2025-07-10 14:15:18,588 \tGloss Reference :\t******** ******** DRUCK TIEF     KOMMEN\n",
            "2025-07-10 14:15:18,588 \tGloss Hypothesis:\tSPEZIELL NORDWEST WOLKE SPEZIELL LOCH  \n",
            "2025-07-10 14:15:18,588 \tGloss Alignment :\tI        I        S     S        S     \n",
            "2025-07-10 14:15:18,589 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:15:18,590 \tText Reference  :\t********** ****** ** ***** ***** ***** ******* ***** **** *** ********* ********* ******** ********* ****** ******* *** ***** *** ****** ***** tiefer luftdruck bestimmt in    den   nächsten tagen unser wetter\n",
            "2025-07-10 14:15:18,590 \tText Hypothesis :\tnordwesten bleibt es heute nacht meist trocken sonst muss mit teilweise kräftigen schauern gerechnet werden örtlich mit blitz und donner <unk> <unk>  <unk>     <unk>    <unk> <unk> <unk>    <unk> <unk> <unk> \n",
            "2025-07-10 14:15:18,591 \tText Alignment  :\tI          I      I  I     I     I     I       I     I    I   I         I         I        I         I      I       I   I     I   I      I     S      S         S        S     S     S        S     S     S     \n",
            "2025-07-10 14:15:18,591 ========================================================================================\n",
            "2025-07-10 14:15:18,591 Logging Sequence: 11August_2010_Wednesday_tagesschau-3\n",
            "2025-07-10 14:15:18,592 \tGloss Reference :\tES-BEDEUTET VIEL WOLKE    UND             KOENNEN REGEN  GEWITTER KOENNEN\n",
            "2025-07-10 14:15:18,592 \tGloss Hypothesis:\t*********** **** NORDWEST UEBERSCHWEMMUNG LOCH    MORGEN LOCH     KOENNEN\n",
            "2025-07-10 14:15:18,592 \tGloss Alignment :\tD           D    S        S               S       S      S               \n",
            "2025-07-10 14:15:18,592 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:15:18,594 \tText Reference  :\t*** *** **************** *** ****** ********** *** ******** ****** ***** ***** ***** ***** ***** ***** ***** ***** das   bedeutet viele wolken und   immer wieder zum   teil  kräftige schauer und   gewitter\n",
            "2025-07-10 14:15:18,594 \tText Hypothesis :\tnun die wettervorhersage für morgen donnerstag den zwölften august <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>    <unk> <unk>  <unk> <unk> <unk>  <unk> <unk> <unk>    <unk>   <unk> <unk>   \n",
            "2025-07-10 14:15:18,594 \tText Alignment  :\tI   I   I                I   I      I          I   I        I      I     I     I     I     I     I     I     I     S     S        S     S      S     S     S      S     S     S        S       S     S       \n",
            "2025-07-10 14:15:18,594 ========================================================================================\n",
            "2025-07-10 14:15:18,594 Logging Sequence: 11August_2010_Wednesday_tagesschau-8\n",
            "2025-07-10 14:15:18,595 \tGloss Reference :\t******** WIND MAESSIG SCHWACH REGION  WENN    GEWITTER WIND    KOENNEN ***\n",
            "2025-07-10 14:15:18,595 \tGloss Hypothesis:\tSPEZIELL LOCH DURCH   LOCH    TROCKEN BLEIBEN REGEN    BLEIBEN KOENNEN ORT\n",
            "2025-07-10 14:15:18,595 \tGloss Alignment :\tI        S    S       S       S       S       S        S               I  \n",
            "2025-07-10 14:15:18,596 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:15:18,598 \tText Reference  :\t** **** ****** *** ******** **** ****** **************** **** ****** ******* ***** meist weht  nur   ein   schwacher wind  aus   unterschiedlichen richtungen der   bei   schauern und   gewittern stark böig  sein  kann \n",
            "2025-07-10 14:15:18,598 \tText Hypothesis :\tes auch länger und ergiebig auch lokale überschwemmungen sind wieder möglich <unk> <unk> <unk> <unk> <unk> <unk>     <unk> <unk> <unk>             <unk>      <unk> <unk> <unk>    <unk> <unk>     <unk> <unk> <unk> <unk>\n",
            "2025-07-10 14:15:18,598 \tText Alignment  :\tI  I    I      I   I        I    I      I                I    I      I       I     S     S     S     S     S         S     S     S                 S          S     S     S        S     S         S     S     S     S    \n",
            "2025-07-10 14:15:18,599 ========================================================================================\n",
            "2025-07-10 14:15:18,599 Logging Sequence: 25October_2010_Monday_tagesschau-22\n",
            "2025-07-10 14:15:18,599 \tGloss Reference :\tMITTWOCH REGEN    KOENNEN NORDWEST WAHRSCHEINLICH NORD     STARK    WIND\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-07-10 14:15:18,599 \tGloss Hypothesis:\tDAZU     GEWITTER KOENNEN ******** ************** SUEDWEST MANCHMAL VIEL\n",
            "2025-07-10 14:15:18,599 \tGloss Alignment :\tS        S                D        D              S        S        S   \n",
            "2025-07-10 14:15:18,600 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:15:18,601 \tText Reference  :\t*** *** **************** *** ****** ********** *** ******** ****** ***** ***** ***** ***** ***** ***** ***** am    mittwoch hier  und   da    nieselregen in    der   nordwesthälfte an    den   küsten kräftiger wind \n",
            "2025-07-10 14:15:18,601 \tText Hypothesis :\tnun die wettervorhersage für morgen donnerstag den zwölften august <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>    <unk> <unk> <unk> <unk>       <unk> <unk> <unk>          <unk> <unk> <unk>  <unk>     <unk>\n",
            "2025-07-10 14:15:18,602 \tText Alignment  :\tI   I   I                I   I      I          I   I        I      I     I     I     I     I     I     I     S     S        S     S     S     S           S     S     S              S     S     S      S         S    \n",
            "2025-07-10 14:15:18,602 ========================================================================================\n",
            "2025-07-10 14:15:18,602 Logging Sequence: 05May_2011_Thursday_tagesschau-25\n",
            "2025-07-10 14:15:18,602 \tGloss Reference :\t****** JETZT WETTER WIE-AUSSEHEN MORGEN FREITAG SECHSTE MAI        ZEIGEN-BILDSCHIRM\n",
            "2025-07-10 14:15:18,602 \tGloss Hypothesis:\tZWOELF JETZT WETTER ************ MORGEN ******* ******* DONNERSTAG FEBRUAR          \n",
            "2025-07-10 14:15:18,602 \tGloss Alignment :\tI                   D                   D       D       S          S                \n",
            "2025-07-10 14:15:18,603 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:15:18,604 \tText Reference  :\tund nun die wettervorhersage für morgen freitag    den ******** ****** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** sechsten mai  \n",
            "2025-07-10 14:15:18,604 \tText Hypothesis :\t*** nun die wettervorhersage für morgen donnerstag den zwölften august <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>    <unk>\n",
            "2025-07-10 14:15:18,604 \tText Alignment  :\tD                                       S              I        I      I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     S        S    \n",
            "2025-07-10 14:15:18,605 ========================================================================================\n",
            "2025-07-10 14:15:18,605 Epoch 2800:\n",
            "\t\t\tTotal Training Recognition Loss 0.078231 || Total Training Translation Loss 0.008246\n",
            "2025-07-10 14:15:18,777 Epoch 2801:\n",
            "\t\t\tTotal Training Recognition Loss 0.092806 || Total Training Translation Loss 0.008671\n",
            "2025-07-10 14:15:18,949 Epoch 2802:\n",
            "\t\t\tTotal Training Recognition Loss 0.833703 || Total Training Translation Loss 0.008800\n",
            "2025-07-10 14:15:19,121 Epoch 2803:\n",
            "\t\t\tTotal Training Recognition Loss 0.088251 || Total Training Translation Loss 0.010871\n",
            "2025-07-10 14:15:19,296 Epoch 2804:\n",
            "\t\t\tTotal Training Recognition Loss 7.338813 || Total Training Translation Loss 0.008531\n",
            "2025-07-10 14:15:19,469 Epoch 2805:\n",
            "\t\t\tTotal Training Recognition Loss 0.309509 || Total Training Translation Loss 0.008709\n",
            "2025-07-10 14:15:19,641 Epoch 2806:\n",
            "\t\t\tTotal Training Recognition Loss 98.239212 || Total Training Translation Loss 0.008080\n",
            "2025-07-10 14:15:19,816 Epoch 2807:\n",
            "\t\t\tTotal Training Recognition Loss 51.143040 || Total Training Translation Loss 0.009624\n",
            "2025-07-10 14:15:19,990 Epoch 2808:\n",
            "\t\t\tTotal Training Recognition Loss 0.110533 || Total Training Translation Loss 0.009504\n",
            "2025-07-10 14:15:20,164 Epoch 2809:\n",
            "\t\t\tTotal Training Recognition Loss 0.066022 || Total Training Translation Loss 0.007891\n",
            "2025-07-10 14:15:20,340 Epoch 2810:\n",
            "\t\t\tTotal Training Recognition Loss 0.077353 || Total Training Translation Loss 0.009509\n",
            "2025-07-10 14:15:20,516 Epoch 2811:\n",
            "\t\t\tTotal Training Recognition Loss 0.322724 || Total Training Translation Loss 0.011556\n",
            "2025-07-10 14:15:20,694 Epoch 2812:\n",
            "\t\t\tTotal Training Recognition Loss 4.879488 || Total Training Translation Loss 0.009217\n",
            "2025-07-10 14:15:20,874 Epoch 2813:\n",
            "\t\t\tTotal Training Recognition Loss 32.199745 || Total Training Translation Loss 0.010309\n",
            "2025-07-10 14:15:21,054 Epoch 2814:\n",
            "\t\t\tTotal Training Recognition Loss 1.378674 || Total Training Translation Loss 0.009135\n",
            "2025-07-10 14:15:21,232 Epoch 2815:\n",
            "\t\t\tTotal Training Recognition Loss 1.157312 || Total Training Translation Loss 0.008546\n",
            "2025-07-10 14:15:21,408 Epoch 2816:\n",
            "\t\t\tTotal Training Recognition Loss 0.497533 || Total Training Translation Loss 0.012234\n",
            "2025-07-10 14:15:21,592 Epoch 2817:\n",
            "\t\t\tTotal Training Recognition Loss 0.772358 || Total Training Translation Loss 0.009303\n",
            "2025-07-10 14:15:21,773 Epoch 2818:\n",
            "\t\t\tTotal Training Recognition Loss 0.196721 || Total Training Translation Loss 0.009182\n",
            "2025-07-10 14:15:21,949 Epoch 2819:\n",
            "\t\t\tTotal Training Recognition Loss 0.107981 || Total Training Translation Loss 0.010315\n",
            "2025-07-10 14:15:22,123 Epoch 2820:\n",
            "\t\t\tTotal Training Recognition Loss 0.360095 || Total Training Translation Loss 0.011950\n",
            "2025-07-10 14:15:22,349 Epoch 2821:\n",
            "\t\t\tTotal Training Recognition Loss 0.223141 || Total Training Translation Loss 0.009112\n",
            "2025-07-10 14:15:22,571 Epoch 2822:\n",
            "\t\t\tTotal Training Recognition Loss 0.072509 || Total Training Translation Loss 0.009904\n",
            "2025-07-10 14:15:22,793 Epoch 2823:\n",
            "\t\t\tTotal Training Recognition Loss 3.760867 || Total Training Translation Loss 0.009402\n",
            "2025-07-10 14:15:23,013 Epoch 2824:\n",
            "\t\t\tTotal Training Recognition Loss 0.661348 || Total Training Translation Loss 0.009348\n",
            "2025-07-10 14:15:23,194 Epoch 2825:\n",
            "\t\t\tTotal Training Recognition Loss 0.246276 || Total Training Translation Loss 0.011941\n",
            "2025-07-10 14:15:23,372 Epoch 2826:\n",
            "\t\t\tTotal Training Recognition Loss 0.347152 || Total Training Translation Loss 0.009020\n",
            "2025-07-10 14:15:23,547 Epoch 2827:\n",
            "\t\t\tTotal Training Recognition Loss 0.130505 || Total Training Translation Loss 0.011524\n",
            "2025-07-10 14:15:23,721 Epoch 2828:\n",
            "\t\t\tTotal Training Recognition Loss 0.778687 || Total Training Translation Loss 0.011901\n",
            "2025-07-10 14:15:23,895 Epoch 2829:\n",
            "\t\t\tTotal Training Recognition Loss 0.115013 || Total Training Translation Loss 0.009664\n",
            "2025-07-10 14:15:24,070 Epoch 2830:\n",
            "\t\t\tTotal Training Recognition Loss 0.223800 || Total Training Translation Loss 0.011178\n",
            "2025-07-10 14:15:24,252 Epoch 2831:\n",
            "\t\t\tTotal Training Recognition Loss 1.458324 || Total Training Translation Loss 0.008158\n",
            "2025-07-10 14:15:24,474 Epoch 2832:\n",
            "\t\t\tTotal Training Recognition Loss 0.617438 || Total Training Translation Loss 0.009783\n",
            "2025-07-10 14:15:24,666 Epoch 2833:\n",
            "\t\t\tTotal Training Recognition Loss 0.233710 || Total Training Translation Loss 0.009643\n",
            "2025-07-10 14:15:24,841 Epoch 2834:\n",
            "\t\t\tTotal Training Recognition Loss 0.522848 || Total Training Translation Loss 0.008376\n",
            "2025-07-10 14:15:25,035 Epoch 2835:\n",
            "\t\t\tTotal Training Recognition Loss 0.160010 || Total Training Translation Loss 0.009268\n",
            "2025-07-10 14:15:25,272 Epoch 2836:\n",
            "\t\t\tTotal Training Recognition Loss 0.096706 || Total Training Translation Loss 0.009270\n",
            "2025-07-10 14:15:25,494 Epoch 2837:\n",
            "\t\t\tTotal Training Recognition Loss 0.087592 || Total Training Translation Loss 0.008231\n",
            "2025-07-10 14:15:25,714 Epoch 2838:\n",
            "\t\t\tTotal Training Recognition Loss 0.185494 || Total Training Translation Loss 0.008732\n",
            "2025-07-10 14:15:25,934 Epoch 2839:\n",
            "\t\t\tTotal Training Recognition Loss 0.119326 || Total Training Translation Loss 0.008727\n",
            "2025-07-10 14:15:26,155 Epoch 2840:\n",
            "\t\t\tTotal Training Recognition Loss 0.101588 || Total Training Translation Loss 0.009666\n",
            "2025-07-10 14:15:26,334 Epoch 2841:\n",
            "\t\t\tTotal Training Recognition Loss 0.198587 || Total Training Translation Loss 0.008849\n",
            "2025-07-10 14:15:26,553 Epoch 2842:\n",
            "\t\t\tTotal Training Recognition Loss 0.089089 || Total Training Translation Loss 0.009383\n",
            "2025-07-10 14:15:26,771 Epoch 2843:\n",
            "\t\t\tTotal Training Recognition Loss 0.108844 || Total Training Translation Loss 0.009618\n",
            "2025-07-10 14:15:26,988 Epoch 2844:\n",
            "\t\t\tTotal Training Recognition Loss 0.266884 || Total Training Translation Loss 0.008762\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-07-10 14:15:27,206 Epoch 2845:\n",
            "\t\t\tTotal Training Recognition Loss 0.629249 || Total Training Translation Loss 0.007941\n",
            "2025-07-10 14:15:27,425 Epoch 2846:\n",
            "\t\t\tTotal Training Recognition Loss 0.472931 || Total Training Translation Loss 0.011848\n",
            "2025-07-10 14:15:27,645 Epoch 2847:\n",
            "\t\t\tTotal Training Recognition Loss 0.094915 || Total Training Translation Loss 0.008770\n",
            "2025-07-10 14:15:27,822 Epoch 2848:\n",
            "\t\t\tTotal Training Recognition Loss 0.125562 || Total Training Translation Loss 0.009256\n",
            "2025-07-10 14:15:27,995 Epoch 2849:\n",
            "\t\t\tTotal Training Recognition Loss 0.352138 || Total Training Translation Loss 0.010039\n",
            "2025-07-10 14:15:28,169 Epoch 2850:\n",
            "\t\t\tTotal Training Recognition Loss 0.097087 || Total Training Translation Loss 0.009647\n",
            "2025-07-10 14:15:28,349 Epoch 2851:\n",
            "\t\t\tTotal Training Recognition Loss 5.008618 || Total Training Translation Loss 0.009053\n",
            "2025-07-10 14:15:28,535 Epoch 2852:\n",
            "\t\t\tTotal Training Recognition Loss 0.097347 || Total Training Translation Loss 0.012161\n",
            "2025-07-10 14:15:28,713 Epoch 2853:\n",
            "\t\t\tTotal Training Recognition Loss 0.082089 || Total Training Translation Loss 0.009387\n",
            "2025-07-10 14:15:28,886 Epoch 2854:\n",
            "\t\t\tTotal Training Recognition Loss 0.090812 || Total Training Translation Loss 0.009703\n",
            "2025-07-10 14:15:29,061 Epoch 2855:\n",
            "\t\t\tTotal Training Recognition Loss 0.090434 || Total Training Translation Loss 0.010855\n",
            "2025-07-10 14:15:29,244 Epoch 2856:\n",
            "\t\t\tTotal Training Recognition Loss 0.140715 || Total Training Translation Loss 0.010655\n",
            "2025-07-10 14:15:29,429 Epoch 2857:\n",
            "\t\t\tTotal Training Recognition Loss 0.204053 || Total Training Translation Loss 0.010260\n",
            "2025-07-10 14:15:29,605 Epoch 2858:\n",
            "\t\t\tTotal Training Recognition Loss 0.198133 || Total Training Translation Loss 0.011064\n",
            "2025-07-10 14:15:29,779 Epoch 2859:\n",
            "\t\t\tTotal Training Recognition Loss 0.085791 || Total Training Translation Loss 0.010416\n",
            "2025-07-10 14:15:29,952 Epoch 2860:\n",
            "\t\t\tTotal Training Recognition Loss 0.183168 || Total Training Translation Loss 0.011041\n",
            "2025-07-10 14:15:30,125 Epoch 2861:\n",
            "\t\t\tTotal Training Recognition Loss 0.143038 || Total Training Translation Loss 0.010857\n",
            "2025-07-10 14:15:30,297 Epoch 2862:\n",
            "\t\t\tTotal Training Recognition Loss 0.093719 || Total Training Translation Loss 0.009317\n",
            "2025-07-10 14:15:30,469 Epoch 2863:\n",
            "\t\t\tTotal Training Recognition Loss 0.102601 || Total Training Translation Loss 0.010610\n",
            "2025-07-10 14:15:30,642 Epoch 2864:\n",
            "\t\t\tTotal Training Recognition Loss 0.059474 || Total Training Translation Loss 0.010143\n",
            "2025-07-10 14:15:30,815 Epoch 2865:\n",
            "\t\t\tTotal Training Recognition Loss 0.091173 || Total Training Translation Loss 0.007826\n",
            "2025-07-10 14:15:30,987 Epoch 2866:\n",
            "\t\t\tTotal Training Recognition Loss 0.083805 || Total Training Translation Loss 0.008735\n",
            "2025-07-10 14:15:31,158 Epoch 2867:\n",
            "\t\t\tTotal Training Recognition Loss 0.100764 || Total Training Translation Loss 0.010896\n",
            "2025-07-10 14:15:31,333 Epoch 2868:\n",
            "\t\t\tTotal Training Recognition Loss 0.218729 || Total Training Translation Loss 0.011061\n",
            "2025-07-10 14:15:31,505 Epoch 2869:\n",
            "\t\t\tTotal Training Recognition Loss 0.118253 || Total Training Translation Loss 0.008594\n",
            "2025-07-10 14:15:31,685 Epoch 2870:\n",
            "\t\t\tTotal Training Recognition Loss 0.214546 || Total Training Translation Loss 0.009104\n",
            "2025-07-10 14:15:31,891 Epoch 2871:\n",
            "\t\t\tTotal Training Recognition Loss 0.430826 || Total Training Translation Loss 0.009007\n",
            "2025-07-10 14:15:32,066 Epoch 2872:\n",
            "\t\t\tTotal Training Recognition Loss 0.144801 || Total Training Translation Loss 0.009968\n",
            "2025-07-10 14:15:32,254 Epoch 2873:\n",
            "\t\t\tTotal Training Recognition Loss 0.097120 || Total Training Translation Loss 0.008864\n",
            "2025-07-10 14:15:32,427 Epoch 2874:\n",
            "\t\t\tTotal Training Recognition Loss 0.109883 || Total Training Translation Loss 0.011267\n",
            "2025-07-10 14:15:32,602 Epoch 2875:\n",
            "\t\t\tTotal Training Recognition Loss 0.111531 || Total Training Translation Loss 0.010092\n",
            "2025-07-10 14:15:32,776 Epoch 2876:\n",
            "\t\t\tTotal Training Recognition Loss 0.165058 || Total Training Translation Loss 0.009324\n",
            "2025-07-10 14:15:32,951 Epoch 2877:\n",
            "\t\t\tTotal Training Recognition Loss 0.074223 || Total Training Translation Loss 0.010629\n",
            "2025-07-10 14:15:33,159 Epoch 2878:\n",
            "\t\t\tTotal Training Recognition Loss 0.576309 || Total Training Translation Loss 0.007416\n",
            "2025-07-10 14:15:33,379 Epoch 2879:\n",
            "\t\t\tTotal Training Recognition Loss 0.371009 || Total Training Translation Loss 0.011469\n",
            "2025-07-10 14:15:33,600 Epoch 2880:\n",
            "\t\t\tTotal Training Recognition Loss 1.792436 || Total Training Translation Loss 0.010756\n",
            "2025-07-10 14:15:33,821 Epoch 2881:\n",
            "\t\t\tTotal Training Recognition Loss 0.068795 || Total Training Translation Loss 0.010125\n",
            "2025-07-10 14:15:34,049 Epoch 2882:\n",
            "\t\t\tTotal Training Recognition Loss 0.103980 || Total Training Translation Loss 0.013400\n",
            "2025-07-10 14:15:34,272 Epoch 2883:\n",
            "\t\t\tTotal Training Recognition Loss 2.142744 || Total Training Translation Loss 0.011623\n",
            "2025-07-10 14:15:34,500 Epoch 2884:\n",
            "\t\t\tTotal Training Recognition Loss 0.113874 || Total Training Translation Loss 0.008905\n",
            "2025-07-10 14:15:34,676 Epoch 2885:\n",
            "\t\t\tTotal Training Recognition Loss 6.890298 || Total Training Translation Loss 0.009984\n",
            "2025-07-10 14:15:34,865 Epoch 2886:\n",
            "\t\t\tTotal Training Recognition Loss 0.135001 || Total Training Translation Loss 0.009180\n",
            "2025-07-10 14:15:35,039 Epoch 2887:\n",
            "\t\t\tTotal Training Recognition Loss 0.073328 || Total Training Translation Loss 0.012702\n",
            "2025-07-10 14:15:35,212 Epoch 2888:\n",
            "\t\t\tTotal Training Recognition Loss 0.115529 || Total Training Translation Loss 0.009046\n",
            "2025-07-10 14:15:35,385 Epoch 2889:\n",
            "\t\t\tTotal Training Recognition Loss 0.080406 || Total Training Translation Loss 0.009864\n",
            "2025-07-10 14:15:35,561 Epoch 2890:\n",
            "\t\t\tTotal Training Recognition Loss 0.066448 || Total Training Translation Loss 0.010036\n",
            "2025-07-10 14:15:35,745 Epoch 2891:\n",
            "\t\t\tTotal Training Recognition Loss 0.066631 || Total Training Translation Loss 0.010996\n",
            "2025-07-10 14:15:35,955 Epoch 2892:\n",
            "\t\t\tTotal Training Recognition Loss 0.060168 || Total Training Translation Loss 0.011042\n",
            "2025-07-10 14:15:36,176 Epoch 2893:\n",
            "\t\t\tTotal Training Recognition Loss 0.080157 || Total Training Translation Loss 0.011188\n",
            "2025-07-10 14:15:36,394 Epoch 2894:\n",
            "\t\t\tTotal Training Recognition Loss 0.069368 || Total Training Translation Loss 0.010602\n",
            "2025-07-10 14:15:36,613 Epoch 2895:\n",
            "\t\t\tTotal Training Recognition Loss 0.155672 || Total Training Translation Loss 0.008670\n",
            "2025-07-10 14:15:36,838 Epoch 2896:\n",
            "\t\t\tTotal Training Recognition Loss 0.340149 || Total Training Translation Loss 0.010780\n",
            "2025-07-10 14:15:37,059 Epoch 2897:\n",
            "\t\t\tTotal Training Recognition Loss 0.065885 || Total Training Translation Loss 0.011689\n",
            "2025-07-10 14:15:37,280 Epoch 2898:\n",
            "\t\t\tTotal Training Recognition Loss 0.077660 || Total Training Translation Loss 0.010080\n",
            "2025-07-10 14:15:37,498 Epoch 2899:\n",
            "\t\t\tTotal Training Recognition Loss 0.558103 || Total Training Translation Loss 0.010747\n",
            "2025-07-10 14:15:37,720 [Epoch: 2900 Step: 00002900] Batch Recognition Loss:   0.117631 => Gls Tokens per Sec:      168 || Batch Translation Loss:   0.008009 => Txt Tokens per Sec:      455 || Lr: 0.000700\n",
            "2025-07-10 14:15:38,056 Validation result at epoch 2900, step     2900: duration: 0.3354s\n",
            "\tRecognition Beam Size: 1\tTranslation Beam Size: 1\tTranslation Beam Alpha: -1\n",
            "\tRecognition Loss: 12724.95801\tTranslation Loss: 283.40082\tPPL: 28.05389\n",
            "\tEval Metric: BLEU\n",
            "\tWER 102.86\t(DEL: 11.43,\tINS: 20.00,\tSUB: 71.43)\n",
            "\tBLEU-4 2.61\t(BLEU-1: 5.33,\tBLEU-2: 3.84,\tBLEU-3: 3.16,\tBLEU-4: 2.61)\n",
            "\tCHRF 20.45\tROUGE 8.44\tFID 0.00\n",
            "2025-07-10 14:15:38,057 Logging Recognition and Translation Outputs\n",
            "2025-07-10 14:15:38,057 ========================================================================================\n",
            "2025-07-10 14:15:38,057 Logging Sequence: 11August_2010_Wednesday_tagesschau-2\n",
            "2025-07-10 14:15:38,058 \tGloss Reference :\t******** ******** ******** DRUCK TIEF     KOMMEN\n",
            "2025-07-10 14:15:38,058 \tGloss Hypothesis:\tSPEZIELL NORDWEST SPEZIELL WOLKE SPEZIELL LOCH  \n",
            "2025-07-10 14:15:38,059 \tGloss Alignment :\tI        I        I        S     S        S     \n",
            "2025-07-10 14:15:38,059 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:15:38,062 \tText Reference  :\t********** ****** ** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** tiefer luftdruck bestimmt in    den   nächsten tagen unser wetter\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-07-10 14:15:38,062 \tText Hypothesis :\tnordwesten bleibt es <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>  <unk>     <unk>    <unk> <unk> <unk>    <unk> <unk> <unk> \n",
            "2025-07-10 14:15:38,063 \tText Alignment  :\tI          I      I  I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     S      S         S        S     S     S        S     S     S     \n",
            "2025-07-10 14:15:38,063 ========================================================================================\n",
            "2025-07-10 14:15:38,063 Logging Sequence: 11August_2010_Wednesday_tagesschau-3\n",
            "2025-07-10 14:15:38,064 \tGloss Reference :\tES-BEDEUTET VIEL WOLKE UND      KOENNEN REGEN  GEWITTER KOENNEN\n",
            "2025-07-10 14:15:38,064 \tGloss Hypothesis:\t*********** **** ***** NORDWEST LOCH    MORGEN LOCH     KOENNEN\n",
            "2025-07-10 14:15:38,064 \tGloss Alignment :\tD           D    D     S        S       S      S               \n",
            "2025-07-10 14:15:38,064 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:15:38,068 \tText Reference  :\t*** *** **************** *** ****** ********** *** ******** ****** ***** ***** ***** ***** ***** ***** ***** ***** das   bedeutet viele wolken und   immer wieder zum   teil  kräftige schauer und   gewitter\n",
            "2025-07-10 14:15:38,070 \tText Hypothesis :\tnun die wettervorhersage für morgen donnerstag den zwölften august <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>    <unk> <unk>  <unk> <unk> <unk>  <unk> <unk> <unk>    <unk>   <unk> <unk>   \n",
            "2025-07-10 14:15:38,070 \tText Alignment  :\tI   I   I                I   I      I          I   I        I      I     I     I     I     I     I     I     I     S     S        S     S      S     S     S      S     S     S        S       S     S       \n",
            "2025-07-10 14:15:38,070 ========================================================================================\n",
            "2025-07-10 14:15:38,071 Logging Sequence: 11August_2010_Wednesday_tagesschau-8\n",
            "2025-07-10 14:15:38,071 \tGloss Reference :\t******** **** WIND  MAESSIG SCHWACH  REGION  WENN    GEWITTER WIND    KOENNEN ***\n",
            "2025-07-10 14:15:38,072 \tGloss Hypothesis:\tSPEZIELL LOCH DURCH LOCH    SPEZIELL TROCKEN BLEIBEN REGEN    BLEIBEN KOENNEN ORT\n",
            "2025-07-10 14:15:38,073 \tGloss Alignment :\tI        I    S     S       S        S       S       S        S               I  \n",
            "2025-07-10 14:15:38,073 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:15:38,076 \tText Reference  :\t** **** ****** *** ******** **** ****** **************** **** ****** ******* ***** meist weht  nur   ein   schwacher wind  aus   unterschiedlichen richtungen der   bei   schauern und   gewittern stark böig  sein  kann \n",
            "2025-07-10 14:15:38,076 \tText Hypothesis :\tes auch länger und ergiebig auch lokale überschwemmungen sind wieder möglich <unk> <unk> <unk> <unk> <unk> <unk>     <unk> <unk> <unk>             <unk>      <unk> <unk> <unk>    <unk> <unk>     <unk> <unk> <unk> <unk>\n",
            "2025-07-10 14:15:38,077 \tText Alignment  :\tI  I    I      I   I        I    I      I                I    I      I       I     S     S     S     S     S         S     S     S                 S          S     S     S        S     S         S     S     S     S    \n",
            "2025-07-10 14:15:38,077 ========================================================================================\n",
            "2025-07-10 14:15:38,077 Logging Sequence: 25October_2010_Monday_tagesschau-22\n",
            "2025-07-10 14:15:38,078 \tGloss Reference :\tMITTWOCH REGEN KOENNEN NORDWEST WAHRSCHEINLICH NORD     STARK WIND   \n",
            "2025-07-10 14:15:38,078 \tGloss Hypothesis:\tDAZU     REGEN FEBRUAR GEWITTER KOENNEN        SUEDWEST ORT   KOENNEN\n",
            "2025-07-10 14:15:38,078 \tGloss Alignment :\tS              S       S        S              S        S     S      \n",
            "2025-07-10 14:15:38,078 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:15:38,080 \tText Reference  :\t*** *** **************** *** ****** ********** *** ******** ****** ***** ***** ***** ***** ***** ***** ***** am    mittwoch hier  und   da    nieselregen in    der   nordwesthälfte an    den   küsten kräftiger wind \n",
            "2025-07-10 14:15:38,080 \tText Hypothesis :\tnun die wettervorhersage für morgen donnerstag den zwölften august <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>    <unk> <unk> <unk> <unk>       <unk> <unk> <unk>          <unk> <unk> <unk>  <unk>     <unk>\n",
            "2025-07-10 14:15:38,081 \tText Alignment  :\tI   I   I                I   I      I          I   I        I      I     I     I     I     I     I     I     S     S        S     S     S     S           S     S     S              S     S     S      S         S    \n",
            "2025-07-10 14:15:38,081 ========================================================================================\n",
            "2025-07-10 14:15:38,082 Logging Sequence: 05May_2011_Thursday_tagesschau-25\n",
            "2025-07-10 14:15:38,082 \tGloss Reference :\t****** JETZT WETTER WIE-AUSSEHEN MORGEN FREITAG    SECHSTE MAI      ZEIGEN-BILDSCHIRM\n",
            "2025-07-10 14:15:38,082 \tGloss Hypothesis:\tZWOELF JETZT WETTER ************ MORGEN DONNERSTAG FEBRUAR NORDWEST REGEN            \n",
            "2025-07-10 14:15:38,083 \tGloss Alignment :\tI                   D                   S          S       S        S                \n",
            "2025-07-10 14:15:38,083 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:15:38,085 \tText Reference  :\tund nun die wettervorhersage für morgen freitag    den ******** ****** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** sechsten mai  \n",
            "2025-07-10 14:15:38,085 \tText Hypothesis :\t*** nun die wettervorhersage für morgen donnerstag den zwölften august <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>    <unk>\n",
            "2025-07-10 14:15:38,085 \tText Alignment  :\tD                                       S              I        I      I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     S        S    \n",
            "2025-07-10 14:15:38,086 ========================================================================================\n",
            "2025-07-10 14:15:38,086 Epoch 2900:\n",
            "\t\t\tTotal Training Recognition Loss 0.117631 || Total Training Translation Loss 0.008009\n",
            "2025-07-10 14:15:38,307 Epoch 2901:\n",
            "\t\t\tTotal Training Recognition Loss 0.163667 || Total Training Translation Loss 0.008975\n",
            "2025-07-10 14:15:38,529 Epoch 2902:\n",
            "\t\t\tTotal Training Recognition Loss 6.665328 || Total Training Translation Loss 0.008574\n",
            "2025-07-10 14:15:38,750 Epoch 2903:\n",
            "\t\t\tTotal Training Recognition Loss 0.088465 || Total Training Translation Loss 0.009457\n",
            "2025-07-10 14:15:38,929 Epoch 2904:\n",
            "\t\t\tTotal Training Recognition Loss 0.187507 || Total Training Translation Loss 0.008507\n",
            "2025-07-10 14:15:39,111 Epoch 2905:\n",
            "\t\t\tTotal Training Recognition Loss 0.168135 || Total Training Translation Loss 0.010561\n",
            "2025-07-10 14:15:39,294 Epoch 2906:\n",
            "\t\t\tTotal Training Recognition Loss 0.110728 || Total Training Translation Loss 0.010415\n",
            "2025-07-10 14:15:39,481 Epoch 2907:\n",
            "\t\t\tTotal Training Recognition Loss 2.598771 || Total Training Translation Loss 0.009889\n",
            "2025-07-10 14:15:39,657 Epoch 2908:\n",
            "\t\t\tTotal Training Recognition Loss 0.105792 || Total Training Translation Loss 0.009669\n",
            "2025-07-10 14:15:39,835 Epoch 2909:\n",
            "\t\t\tTotal Training Recognition Loss 0.109526 || Total Training Translation Loss 0.008301\n",
            "2025-07-10 14:15:40,014 Epoch 2910:\n",
            "\t\t\tTotal Training Recognition Loss 0.770393 || Total Training Translation Loss 0.010680\n",
            "2025-07-10 14:15:40,191 Epoch 2911:\n",
            "\t\t\tTotal Training Recognition Loss 0.547696 || Total Training Translation Loss 0.010168\n",
            "2025-07-10 14:15:40,367 Epoch 2912:\n",
            "\t\t\tTotal Training Recognition Loss 25.572395 || Total Training Translation Loss 0.011053\n",
            "2025-07-10 14:15:40,542 Epoch 2913:\n",
            "\t\t\tTotal Training Recognition Loss 0.172866 || Total Training Translation Loss 0.009755\n",
            "2025-07-10 14:15:40,724 Epoch 2914:\n",
            "\t\t\tTotal Training Recognition Loss 5.229152 || Total Training Translation Loss 0.008596\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-07-10 14:15:40,903 Epoch 2915:\n",
            "\t\t\tTotal Training Recognition Loss 0.188280 || Total Training Translation Loss 0.008675\n",
            "2025-07-10 14:15:41,085 Epoch 2916:\n",
            "\t\t\tTotal Training Recognition Loss 0.321989 || Total Training Translation Loss 0.010649\n",
            "2025-07-10 14:15:41,263 Epoch 2917:\n",
            "\t\t\tTotal Training Recognition Loss 0.227277 || Total Training Translation Loss 0.009255\n",
            "2025-07-10 14:15:41,439 Epoch 2918:\n",
            "\t\t\tTotal Training Recognition Loss 0.193866 || Total Training Translation Loss 0.009469\n",
            "2025-07-10 14:15:41,661 Epoch 2919:\n",
            "\t\t\tTotal Training Recognition Loss 0.563048 || Total Training Translation Loss 0.011698\n",
            "2025-07-10 14:15:41,880 Epoch 2920:\n",
            "\t\t\tTotal Training Recognition Loss 0.139256 || Total Training Translation Loss 0.008362\n",
            "2025-07-10 14:15:42,054 Epoch 2921:\n",
            "\t\t\tTotal Training Recognition Loss 0.514373 || Total Training Translation Loss 0.008919\n",
            "2025-07-10 14:15:42,227 Epoch 2922:\n",
            "\t\t\tTotal Training Recognition Loss 0.268148 || Total Training Translation Loss 0.007976\n",
            "2025-07-10 14:15:42,400 Epoch 2923:\n",
            "\t\t\tTotal Training Recognition Loss 0.138845 || Total Training Translation Loss 0.009087\n",
            "2025-07-10 14:15:42,572 Epoch 2924:\n",
            "\t\t\tTotal Training Recognition Loss 0.678597 || Total Training Translation Loss 0.009422\n",
            "2025-07-10 14:15:42,743 Epoch 2925:\n",
            "\t\t\tTotal Training Recognition Loss 0.134778 || Total Training Translation Loss 0.009702\n",
            "2025-07-10 14:15:42,919 Epoch 2926:\n",
            "\t\t\tTotal Training Recognition Loss 0.446889 || Total Training Translation Loss 0.010297\n",
            "2025-07-10 14:15:43,091 Epoch 2927:\n",
            "\t\t\tTotal Training Recognition Loss 0.221302 || Total Training Translation Loss 0.007480\n",
            "2025-07-10 14:15:43,263 Epoch 2928:\n",
            "\t\t\tTotal Training Recognition Loss 0.815554 || Total Training Translation Loss 0.008131\n",
            "2025-07-10 14:15:43,435 Epoch 2929:\n",
            "\t\t\tTotal Training Recognition Loss 0.137560 || Total Training Translation Loss 0.009224\n",
            "2025-07-10 14:15:43,609 Epoch 2930:\n",
            "\t\t\tTotal Training Recognition Loss 0.295552 || Total Training Translation Loss 0.008585\n",
            "2025-07-10 14:15:43,783 Epoch 2931:\n",
            "\t\t\tTotal Training Recognition Loss 0.367690 || Total Training Translation Loss 0.009998\n",
            "2025-07-10 14:15:43,956 Epoch 2932:\n",
            "\t\t\tTotal Training Recognition Loss 0.339306 || Total Training Translation Loss 0.009268\n",
            "2025-07-10 14:15:44,129 Epoch 2933:\n",
            "\t\t\tTotal Training Recognition Loss 0.159461 || Total Training Translation Loss 0.009066\n",
            "2025-07-10 14:15:44,303 Epoch 2934:\n",
            "\t\t\tTotal Training Recognition Loss 0.721031 || Total Training Translation Loss 0.009150\n",
            "2025-07-10 14:15:44,476 Epoch 2935:\n",
            "\t\t\tTotal Training Recognition Loss 0.178682 || Total Training Translation Loss 0.010157\n",
            "2025-07-10 14:15:44,658 Epoch 2936:\n",
            "\t\t\tTotal Training Recognition Loss 0.139055 || Total Training Translation Loss 0.008421\n",
            "2025-07-10 14:15:44,852 Epoch 2937:\n",
            "\t\t\tTotal Training Recognition Loss 0.179113 || Total Training Translation Loss 0.009825\n",
            "2025-07-10 14:15:45,033 Epoch 2938:\n",
            "\t\t\tTotal Training Recognition Loss 0.106384 || Total Training Translation Loss 0.009440\n",
            "2025-07-10 14:15:45,206 Epoch 2939:\n",
            "\t\t\tTotal Training Recognition Loss 0.099799 || Total Training Translation Loss 0.008492\n",
            "2025-07-10 14:15:45,415 Epoch 2940:\n",
            "\t\t\tTotal Training Recognition Loss 0.112572 || Total Training Translation Loss 0.010013\n",
            "2025-07-10 14:15:45,636 Epoch 2941:\n",
            "\t\t\tTotal Training Recognition Loss 0.101714 || Total Training Translation Loss 0.009269\n",
            "2025-07-10 14:15:45,858 Epoch 2942:\n",
            "\t\t\tTotal Training Recognition Loss 0.172534 || Total Training Translation Loss 0.008362\n",
            "2025-07-10 14:15:46,042 Epoch 2943:\n",
            "\t\t\tTotal Training Recognition Loss 0.085938 || Total Training Translation Loss 0.008869\n",
            "2025-07-10 14:15:46,233 Epoch 2944:\n",
            "\t\t\tTotal Training Recognition Loss 0.103212 || Total Training Translation Loss 0.008748\n",
            "2025-07-10 14:15:46,408 Epoch 2945:\n",
            "\t\t\tTotal Training Recognition Loss 0.081227 || Total Training Translation Loss 0.007972\n",
            "2025-07-10 14:15:46,581 Epoch 2946:\n",
            "\t\t\tTotal Training Recognition Loss 0.142530 || Total Training Translation Loss 0.007312\n",
            "2025-07-10 14:15:46,755 Epoch 2947:\n",
            "\t\t\tTotal Training Recognition Loss 0.156321 || Total Training Translation Loss 0.010202\n",
            "2025-07-10 14:15:46,926 Epoch 2948:\n",
            "\t\t\tTotal Training Recognition Loss 0.460943 || Total Training Translation Loss 0.007938\n",
            "2025-07-10 14:15:47,100 Epoch 2949:\n",
            "\t\t\tTotal Training Recognition Loss 0.107626 || Total Training Translation Loss 0.009241\n",
            "2025-07-10 14:15:47,273 Epoch 2950:\n",
            "\t\t\tTotal Training Recognition Loss 0.115352 || Total Training Translation Loss 0.008229\n",
            "2025-07-10 14:15:47,449 Epoch 2951:\n",
            "\t\t\tTotal Training Recognition Loss 0.109593 || Total Training Translation Loss 0.008352\n",
            "2025-07-10 14:15:47,624 Epoch 2952:\n",
            "\t\t\tTotal Training Recognition Loss 0.059908 || Total Training Translation Loss 0.009721\n",
            "2025-07-10 14:15:47,803 Epoch 2953:\n",
            "\t\t\tTotal Training Recognition Loss 0.131197 || Total Training Translation Loss 0.011237\n",
            "2025-07-10 14:15:47,986 Epoch 2954:\n",
            "\t\t\tTotal Training Recognition Loss 0.079495 || Total Training Translation Loss 0.009316\n",
            "2025-07-10 14:15:48,212 Epoch 2955:\n",
            "\t\t\tTotal Training Recognition Loss 0.131015 || Total Training Translation Loss 0.008323\n",
            "2025-07-10 14:15:48,432 Epoch 2956:\n",
            "\t\t\tTotal Training Recognition Loss 0.079299 || Total Training Translation Loss 0.009214\n",
            "2025-07-10 14:15:48,653 Epoch 2957:\n",
            "\t\t\tTotal Training Recognition Loss 0.248482 || Total Training Translation Loss 0.008281\n",
            "2025-07-10 14:15:48,871 Epoch 2958:\n",
            "\t\t\tTotal Training Recognition Loss 0.088153 || Total Training Translation Loss 0.008381\n",
            "2025-07-10 14:15:49,094 Epoch 2959:\n",
            "\t\t\tTotal Training Recognition Loss 0.164240 || Total Training Translation Loss 0.008676\n",
            "2025-07-10 14:15:49,317 Epoch 2960:\n",
            "\t\t\tTotal Training Recognition Loss 0.079252 || Total Training Translation Loss 0.009695\n",
            "2025-07-10 14:15:49,516 Epoch 2961:\n",
            "\t\t\tTotal Training Recognition Loss 0.083281 || Total Training Translation Loss 0.009014\n",
            "2025-07-10 14:15:49,698 Epoch 2962:\n",
            "\t\t\tTotal Training Recognition Loss 0.119429 || Total Training Translation Loss 0.008291\n",
            "2025-07-10 14:15:49,873 Epoch 2963:\n",
            "\t\t\tTotal Training Recognition Loss 0.069599 || Total Training Translation Loss 0.009612\n",
            "2025-07-10 14:15:50,050 Epoch 2964:\n",
            "\t\t\tTotal Training Recognition Loss 0.489878 || Total Training Translation Loss 0.009713\n",
            "2025-07-10 14:15:50,224 Epoch 2965:\n",
            "\t\t\tTotal Training Recognition Loss 0.090136 || Total Training Translation Loss 0.008207\n",
            "2025-07-10 14:15:50,398 Epoch 2966:\n",
            "\t\t\tTotal Training Recognition Loss 0.079356 || Total Training Translation Loss 0.009976\n",
            "2025-07-10 14:15:50,577 Epoch 2967:\n",
            "\t\t\tTotal Training Recognition Loss 0.103144 || Total Training Translation Loss 0.010864\n",
            "2025-07-10 14:15:50,755 Epoch 2968:\n",
            "\t\t\tTotal Training Recognition Loss 0.067231 || Total Training Translation Loss 0.009412\n",
            "2025-07-10 14:15:50,933 Epoch 2969:\n",
            "\t\t\tTotal Training Recognition Loss 0.120917 || Total Training Translation Loss 0.008813\n",
            "2025-07-10 14:15:51,126 Epoch 2970:\n",
            "\t\t\tTotal Training Recognition Loss 0.107314 || Total Training Translation Loss 0.009518\n",
            "2025-07-10 14:15:51,299 Epoch 2971:\n",
            "\t\t\tTotal Training Recognition Loss 0.138222 || Total Training Translation Loss 0.011627\n",
            "2025-07-10 14:15:51,474 Epoch 2972:\n",
            "\t\t\tTotal Training Recognition Loss 0.406893 || Total Training Translation Loss 0.007963\n",
            "2025-07-10 14:15:51,649 Epoch 2973:\n",
            "\t\t\tTotal Training Recognition Loss 0.062571 || Total Training Translation Loss 0.009559\n",
            "2025-07-10 14:15:51,825 Epoch 2974:\n",
            "\t\t\tTotal Training Recognition Loss 0.080049 || Total Training Translation Loss 0.009913\n",
            "2025-07-10 14:15:52,010 Epoch 2975:\n",
            "\t\t\tTotal Training Recognition Loss 0.104816 || Total Training Translation Loss 0.008905\n",
            "2025-07-10 14:15:52,191 Epoch 2976:\n",
            "\t\t\tTotal Training Recognition Loss 0.066647 || Total Training Translation Loss 0.008910\n",
            "2025-07-10 14:15:52,369 Epoch 2977:\n",
            "\t\t\tTotal Training Recognition Loss 0.127381 || Total Training Translation Loss 0.008322\n",
            "2025-07-10 14:15:52,545 Epoch 2978:\n",
            "\t\t\tTotal Training Recognition Loss 0.047175 || Total Training Translation Loss 0.008960\n",
            "2025-07-10 14:15:52,721 Epoch 2979:\n",
            "\t\t\tTotal Training Recognition Loss 0.062076 || Total Training Translation Loss 0.010420\n",
            "2025-07-10 14:15:52,895 Epoch 2980:\n",
            "\t\t\tTotal Training Recognition Loss 0.071978 || Total Training Translation Loss 0.012049\n",
            "2025-07-10 14:15:53,070 Epoch 2981:\n",
            "\t\t\tTotal Training Recognition Loss 0.056347 || Total Training Translation Loss 0.008562\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-07-10 14:15:53,245 Epoch 2982:\n",
            "\t\t\tTotal Training Recognition Loss 0.065582 || Total Training Translation Loss 0.010340\n",
            "2025-07-10 14:15:53,420 Epoch 2983:\n",
            "\t\t\tTotal Training Recognition Loss 0.136119 || Total Training Translation Loss 0.008983\n",
            "2025-07-10 14:15:53,594 Epoch 2984:\n",
            "\t\t\tTotal Training Recognition Loss 0.073109 || Total Training Translation Loss 0.010542\n",
            "2025-07-10 14:15:53,771 Epoch 2985:\n",
            "\t\t\tTotal Training Recognition Loss 0.057078 || Total Training Translation Loss 0.009344\n",
            "2025-07-10 14:15:53,966 Epoch 2986:\n",
            "\t\t\tTotal Training Recognition Loss 0.147977 || Total Training Translation Loss 0.010179\n",
            "2025-07-10 14:15:54,144 Epoch 2987:\n",
            "\t\t\tTotal Training Recognition Loss 0.047479 || Total Training Translation Loss 0.010212\n",
            "2025-07-10 14:15:54,318 Epoch 2988:\n",
            "\t\t\tTotal Training Recognition Loss 0.204425 || Total Training Translation Loss 0.009843\n",
            "2025-07-10 14:15:54,493 Epoch 2989:\n",
            "\t\t\tTotal Training Recognition Loss 0.107402 || Total Training Translation Loss 0.007975\n",
            "2025-07-10 14:15:54,676 Epoch 2990:\n",
            "\t\t\tTotal Training Recognition Loss 0.060137 || Total Training Translation Loss 0.009873\n",
            "2025-07-10 14:15:54,853 Epoch 2991:\n",
            "\t\t\tTotal Training Recognition Loss 0.048669 || Total Training Translation Loss 0.007866\n",
            "2025-07-10 14:15:55,028 Epoch 2992:\n",
            "\t\t\tTotal Training Recognition Loss 0.071399 || Total Training Translation Loss 0.008005\n",
            "2025-07-10 14:15:55,204 Epoch 2993:\n",
            "\t\t\tTotal Training Recognition Loss 0.070336 || Total Training Translation Loss 0.010330\n",
            "2025-07-10 14:15:55,379 Epoch 2994:\n",
            "\t\t\tTotal Training Recognition Loss 0.062364 || Total Training Translation Loss 0.009536\n",
            "2025-07-10 14:15:55,553 Epoch 2995:\n",
            "\t\t\tTotal Training Recognition Loss 0.049166 || Total Training Translation Loss 0.008510\n",
            "2025-07-10 14:15:55,728 Epoch 2996:\n",
            "\t\t\tTotal Training Recognition Loss 0.061677 || Total Training Translation Loss 0.008488\n",
            "2025-07-10 14:15:55,903 Epoch 2997:\n",
            "\t\t\tTotal Training Recognition Loss 0.042685 || Total Training Translation Loss 0.010463\n",
            "2025-07-10 14:15:56,078 Epoch 2998:\n",
            "\t\t\tTotal Training Recognition Loss 0.095578 || Total Training Translation Loss 0.009575\n",
            "2025-07-10 14:15:56,252 Epoch 2999:\n",
            "\t\t\tTotal Training Recognition Loss 0.054677 || Total Training Translation Loss 0.009108\n",
            "2025-07-10 14:15:56,425 [Epoch: 3000 Step: 00003000] Batch Recognition Loss:   0.133342 => Gls Tokens per Sec:      215 || Batch Translation Loss:   0.009487 => Txt Tokens per Sec:      582 || Lr: 0.000700\n",
            "2025-07-10 14:15:56,661 Validation result at epoch 3000, step     3000: duration: 0.2349s\n",
            "\tRecognition Beam Size: 1\tTranslation Beam Size: 1\tTranslation Beam Alpha: -1\n",
            "\tRecognition Loss: 10455.54102\tTranslation Loss: 301.88467\tPPL: 34.86852\n",
            "\tEval Metric: BLEU\n",
            "\tWER 100.00\t(DEL: 11.43,\tINS: 17.14,\tSUB: 71.43)\n",
            "\tBLEU-4 2.53\t(BLEU-1: 4.67,\tBLEU-2: 3.59,\tBLEU-3: 3.02,\tBLEU-4: 2.53)\n",
            "\tCHRF 20.82\tROUGE 7.57\tFID 0.00\n",
            "2025-07-10 14:15:56,662 Logging Recognition and Translation Outputs\n",
            "2025-07-10 14:15:56,662 ========================================================================================\n",
            "2025-07-10 14:15:56,662 Logging Sequence: 11August_2010_Wednesday_tagesschau-2\n",
            "2025-07-10 14:15:56,663 \tGloss Reference :\t******** ******** DRUCK TIEF     KOMMEN\n",
            "2025-07-10 14:15:56,663 \tGloss Hypothesis:\tSPEZIELL NORDWEST WOLKE SPEZIELL LOCH  \n",
            "2025-07-10 14:15:56,663 \tGloss Alignment :\tI        I        S     S        S     \n",
            "2025-07-10 14:15:56,663 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:15:56,665 \tText Reference  :\t********** ****** ** **** ****** *** ******** **** ****** **************** **** ****** ******* ***** ***** ***** ***** ***** ***** ***** ***** tiefer luftdruck bestimmt in    den   nächsten tagen unser wetter\n",
            "2025-07-10 14:15:56,665 \tText Hypothesis :\tnordwesten bleibt es auch länger und ergiebig auch lokale überschwemmungen sind wieder möglich <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>  <unk>     <unk>    <unk> <unk> <unk>    <unk> <unk> <unk> \n",
            "2025-07-10 14:15:56,665 \tText Alignment  :\tI          I      I  I    I      I   I        I    I      I                I    I      I       I     I     I     I     I     I     I     I     S      S         S        S     S     S        S     S     S     \n",
            "2025-07-10 14:15:56,665 ========================================================================================\n",
            "2025-07-10 14:15:56,666 Logging Sequence: 11August_2010_Wednesday_tagesschau-3\n",
            "2025-07-10 14:15:56,666 \tGloss Reference :\tES-BEDEUTET VIEL     WOLKE UND    KOENNEN REGEN GEWITTER KOENNEN\n",
            "2025-07-10 14:15:56,666 \tGloss Hypothesis:\t*********** NORDWEST LOCH  MORGEN LOCH    ORT   LOCH     KOENNEN\n",
            "2025-07-10 14:15:56,666 \tGloss Alignment :\tD           S        S     S      S       S     S               \n",
            "2025-07-10 14:15:56,667 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:15:56,668 \tText Reference  :\t*** *** **************** *** ****** ********** *** ******** ****** ***** ***** ***** ***** ***** ***** ***** ***** das   bedeutet viele wolken und   immer wieder zum   teil  kräftige schauer und   gewitter\n",
            "2025-07-10 14:15:56,669 \tText Hypothesis :\tnun die wettervorhersage für morgen donnerstag den zwölften august <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>    <unk> <unk>  <unk> <unk> <unk>  <unk> <unk> <unk>    <unk>   <unk> <unk>   \n",
            "2025-07-10 14:15:56,669 \tText Alignment  :\tI   I   I                I   I      I          I   I        I      I     I     I     I     I     I     I     I     S     S        S     S      S     S     S      S     S     S        S       S     S       \n",
            "2025-07-10 14:15:56,670 ========================================================================================\n",
            "2025-07-10 14:15:56,670 Logging Sequence: 11August_2010_Wednesday_tagesschau-8\n",
            "2025-07-10 14:15:56,671 \tGloss Reference :\t******** **** WIND  MAESSIG SCHWACH REGION  WENN  GEWITTER WIND     KOENNEN\n",
            "2025-07-10 14:15:56,671 \tGloss Hypothesis:\tSPEZIELL LOCH DURCH LOCH    TROCKEN BLEIBEN REGEN BLEIBEN  NORDWEST KOENNEN\n",
            "2025-07-10 14:15:56,671 \tGloss Alignment :\tI        I    S     S       S       S       S     S        S               \n",
            "2025-07-10 14:15:56,672 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:15:56,674 \tText Reference  :\t*** *** **************** *** ****** ********** *** ******** ****** ***** ***** ***** meist weht  nur   ein   schwacher wind  aus   unterschiedlichen richtungen der   bei   schauern und   gewittern stark böig  sein  kann \n",
            "2025-07-10 14:15:56,674 \tText Hypothesis :\tnun die wettervorhersage für morgen donnerstag den zwölften august <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>     <unk> <unk> <unk>             <unk>      <unk> <unk> <unk>    <unk> <unk>     <unk> <unk> <unk> <unk>\n",
            "2025-07-10 14:15:56,674 \tText Alignment  :\tI   I   I                I   I      I          I   I        I      I     I     I     S     S     S     S     S         S     S     S                 S          S     S     S        S     S         S     S     S     S    \n",
            "2025-07-10 14:15:56,674 ========================================================================================\n",
            "2025-07-10 14:15:56,674 Logging Sequence: 25October_2010_Monday_tagesschau-22\n",
            "2025-07-10 14:15:56,675 \tGloss Reference :\t**** MITTWOCH REGEN    KOENNEN NORDWEST WAHRSCHEINLICH NORD     STARK WIND   \n",
            "2025-07-10 14:15:56,675 \tGloss Hypothesis:\tDAZU FEBRUAR  GEWITTER KOENNEN ******** ************** SUEDWEST ORT   KOENNEN\n",
            "2025-07-10 14:15:56,676 \tGloss Alignment :\tI    S        S                D        D              S        S     S      \n",
            "2025-07-10 14:15:56,676 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:15:56,678 \tText Reference  :\t*** *** **************** *** ****** ********** *** ******** ****** ***** ***** ***** ***** ***** ***** ***** am    mittwoch hier  und   da    nieselregen in    der   nordwesthälfte an    den   küsten kräftiger wind \n",
            "2025-07-10 14:15:56,678 \tText Hypothesis :\tnun die wettervorhersage für morgen donnerstag den zwölften august <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>    <unk> <unk> <unk> <unk>       <unk> <unk> <unk>          <unk> <unk> <unk>  <unk>     <unk>\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-07-10 14:15:56,678 \tText Alignment  :\tI   I   I                I   I      I          I   I        I      I     I     I     I     I     I     I     S     S        S     S     S     S           S     S     S              S     S     S      S         S    \n",
            "2025-07-10 14:15:56,678 ========================================================================================\n",
            "2025-07-10 14:15:56,678 Logging Sequence: 05May_2011_Thursday_tagesschau-25\n",
            "2025-07-10 14:15:56,679 \tGloss Reference :\t****** JETZT WETTER WIE-AUSSEHEN MORGEN FREITAG    SECHSTE MAI      ZEIGEN-BILDSCHIRM\n",
            "2025-07-10 14:15:56,679 \tGloss Hypothesis:\tZWOELF JETZT WETTER ************ MORGEN DONNERSTAG FEBRUAR NORDWEST REGEN            \n",
            "2025-07-10 14:15:56,679 \tGloss Alignment :\tI                   D                   S          S       S        S                \n",
            "2025-07-10 14:15:56,680 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:15:56,681 \tText Reference  :\tund nun die wettervorhersage für morgen freitag    den ******** ****** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** sechsten mai  \n",
            "2025-07-10 14:15:56,681 \tText Hypothesis :\t*** nun die wettervorhersage für morgen donnerstag den zwölften august <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>    <unk>\n",
            "2025-07-10 14:15:56,681 \tText Alignment  :\tD                                       S              I        I      I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     S        S    \n",
            "2025-07-10 14:15:56,682 ========================================================================================\n",
            "2025-07-10 14:15:56,682 Epoch 3000:\n",
            "\t\t\tTotal Training Recognition Loss 0.133342 || Total Training Translation Loss 0.009487\n",
            "2025-07-10 14:15:56,853 Epoch 3001:\n",
            "\t\t\tTotal Training Recognition Loss 0.060327 || Total Training Translation Loss 0.009849\n",
            "2025-07-10 14:15:57,028 Epoch 3002:\n",
            "\t\t\tTotal Training Recognition Loss 0.063970 || Total Training Translation Loss 0.009103\n",
            "2025-07-10 14:15:57,201 Epoch 3003:\n",
            "\t\t\tTotal Training Recognition Loss 0.050463 || Total Training Translation Loss 0.010058\n",
            "2025-07-10 14:15:57,374 Epoch 3004:\n",
            "\t\t\tTotal Training Recognition Loss 0.241797 || Total Training Translation Loss 0.009253\n",
            "2025-07-10 14:15:57,547 Epoch 3005:\n",
            "\t\t\tTotal Training Recognition Loss 0.048116 || Total Training Translation Loss 0.008804\n",
            "2025-07-10 14:15:57,722 Epoch 3006:\n",
            "\t\t\tTotal Training Recognition Loss 0.081506 || Total Training Translation Loss 0.008325\n",
            "2025-07-10 14:15:57,898 Epoch 3007:\n",
            "\t\t\tTotal Training Recognition Loss 0.089098 || Total Training Translation Loss 0.009160\n",
            "2025-07-10 14:15:58,074 Epoch 3008:\n",
            "\t\t\tTotal Training Recognition Loss 0.074126 || Total Training Translation Loss 0.012549\n",
            "2025-07-10 14:15:58,297 Epoch 3009:\n",
            "\t\t\tTotal Training Recognition Loss 0.065864 || Total Training Translation Loss 0.009754\n",
            "2025-07-10 14:15:58,517 Epoch 3010:\n",
            "\t\t\tTotal Training Recognition Loss 0.160455 || Total Training Translation Loss 0.009091\n",
            "2025-07-10 14:15:58,712 Epoch 3011:\n",
            "\t\t\tTotal Training Recognition Loss 0.152036 || Total Training Translation Loss 0.013778\n",
            "2025-07-10 14:15:58,886 Epoch 3012:\n",
            "\t\t\tTotal Training Recognition Loss 0.062613 || Total Training Translation Loss 0.008539\n",
            "2025-07-10 14:15:59,061 Epoch 3013:\n",
            "\t\t\tTotal Training Recognition Loss 0.084421 || Total Training Translation Loss 0.008805\n",
            "2025-07-10 14:15:59,237 Epoch 3014:\n",
            "\t\t\tTotal Training Recognition Loss 0.079384 || Total Training Translation Loss 0.010371\n",
            "2025-07-10 14:15:59,416 Epoch 3015:\n",
            "\t\t\tTotal Training Recognition Loss 0.038736 || Total Training Translation Loss 0.009502\n",
            "2025-07-10 14:15:59,593 Epoch 3016:\n",
            "\t\t\tTotal Training Recognition Loss 0.093765 || Total Training Translation Loss 0.011214\n",
            "2025-07-10 14:15:59,768 Epoch 3017:\n",
            "\t\t\tTotal Training Recognition Loss 0.064502 || Total Training Translation Loss 0.008072\n",
            "2025-07-10 14:15:59,942 Epoch 3018:\n",
            "\t\t\tTotal Training Recognition Loss 0.045979 || Total Training Translation Loss 0.007951\n",
            "2025-07-10 14:16:00,118 Epoch 3019:\n",
            "\t\t\tTotal Training Recognition Loss 0.544124 || Total Training Translation Loss 0.007808\n",
            "2025-07-10 14:16:00,297 Epoch 3020:\n",
            "\t\t\tTotal Training Recognition Loss 0.367487 || Total Training Translation Loss 0.010397\n",
            "2025-07-10 14:16:00,472 Epoch 3021:\n",
            "\t\t\tTotal Training Recognition Loss 0.104325 || Total Training Translation Loss 0.010764\n",
            "2025-07-10 14:16:00,694 Epoch 3022:\n",
            "\t\t\tTotal Training Recognition Loss 1.405478 || Total Training Translation Loss 0.009246\n",
            "2025-07-10 14:16:00,870 Epoch 3023:\n",
            "\t\t\tTotal Training Recognition Loss 0.039149 || Total Training Translation Loss 0.011262\n",
            "2025-07-10 14:16:01,046 Epoch 3024:\n",
            "\t\t\tTotal Training Recognition Loss 0.159227 || Total Training Translation Loss 0.009937\n",
            "2025-07-10 14:16:01,265 Epoch 3025:\n",
            "\t\t\tTotal Training Recognition Loss 0.061509 || Total Training Translation Loss 0.010498\n",
            "2025-07-10 14:16:01,483 Epoch 3026:\n",
            "\t\t\tTotal Training Recognition Loss 0.713738 || Total Training Translation Loss 0.009531\n",
            "2025-07-10 14:16:01,701 Epoch 3027:\n",
            "\t\t\tTotal Training Recognition Loss 0.089756 || Total Training Translation Loss 0.011499\n",
            "2025-07-10 14:16:01,919 Epoch 3028:\n",
            "\t\t\tTotal Training Recognition Loss 0.047863 || Total Training Translation Loss 0.010956\n",
            "2025-07-10 14:16:02,138 Epoch 3029:\n",
            "\t\t\tTotal Training Recognition Loss 0.082205 || Total Training Translation Loss 0.009991\n",
            "2025-07-10 14:16:02,359 Epoch 3030:\n",
            "\t\t\tTotal Training Recognition Loss 0.046631 || Total Training Translation Loss 0.009607\n",
            "2025-07-10 14:16:02,582 Epoch 3031:\n",
            "\t\t\tTotal Training Recognition Loss 0.057806 || Total Training Translation Loss 0.009374\n",
            "2025-07-10 14:16:02,799 Epoch 3032:\n",
            "\t\t\tTotal Training Recognition Loss 0.114893 || Total Training Translation Loss 0.009247\n",
            "2025-07-10 14:16:03,020 Epoch 3033:\n",
            "\t\t\tTotal Training Recognition Loss 0.095214 || Total Training Translation Loss 0.009425\n",
            "2025-07-10 14:16:03,239 Epoch 3034:\n",
            "\t\t\tTotal Training Recognition Loss 0.302249 || Total Training Translation Loss 0.010536\n",
            "2025-07-10 14:16:03,419 Epoch 3035:\n",
            "\t\t\tTotal Training Recognition Loss 0.065225 || Total Training Translation Loss 0.008549\n",
            "2025-07-10 14:16:03,597 Epoch 3036:\n",
            "\t\t\tTotal Training Recognition Loss 0.126715 || Total Training Translation Loss 0.009935\n",
            "2025-07-10 14:16:03,777 Epoch 3037:\n",
            "\t\t\tTotal Training Recognition Loss 0.193913 || Total Training Translation Loss 0.010375\n",
            "2025-07-10 14:16:03,953 Epoch 3038:\n",
            "\t\t\tTotal Training Recognition Loss 0.055890 || Total Training Translation Loss 0.008946\n",
            "2025-07-10 14:16:04,132 Epoch 3039:\n",
            "\t\t\tTotal Training Recognition Loss 0.080131 || Total Training Translation Loss 0.010601\n",
            "2025-07-10 14:16:04,352 Epoch 3040:\n",
            "\t\t\tTotal Training Recognition Loss 0.072618 || Total Training Translation Loss 0.010348\n",
            "2025-07-10 14:16:04,535 Epoch 3041:\n",
            "\t\t\tTotal Training Recognition Loss 0.186175 || Total Training Translation Loss 0.009377\n",
            "2025-07-10 14:16:04,710 Epoch 3042:\n",
            "\t\t\tTotal Training Recognition Loss 0.459158 || Total Training Translation Loss 0.009206\n",
            "2025-07-10 14:16:04,885 Epoch 3043:\n",
            "\t\t\tTotal Training Recognition Loss 0.070454 || Total Training Translation Loss 0.008981\n",
            "2025-07-10 14:16:05,060 Epoch 3044:\n",
            "\t\t\tTotal Training Recognition Loss 0.130379 || Total Training Translation Loss 0.009088\n",
            "2025-07-10 14:16:05,235 Epoch 3045:\n",
            "\t\t\tTotal Training Recognition Loss 0.179834 || Total Training Translation Loss 0.009384\n",
            "2025-07-10 14:16:05,452 Epoch 3046:\n",
            "\t\t\tTotal Training Recognition Loss 0.069873 || Total Training Translation Loss 0.009164\n",
            "2025-07-10 14:16:05,629 Epoch 3047:\n",
            "\t\t\tTotal Training Recognition Loss 0.058346 || Total Training Translation Loss 0.009022\n",
            "2025-07-10 14:16:05,803 Epoch 3048:\n",
            "\t\t\tTotal Training Recognition Loss 0.061646 || Total Training Translation Loss 0.011597\n",
            "2025-07-10 14:16:05,979 Epoch 3049:\n",
            "\t\t\tTotal Training Recognition Loss 0.732267 || Total Training Translation Loss 0.010586\n",
            "2025-07-10 14:16:06,155 Epoch 3050:\n",
            "\t\t\tTotal Training Recognition Loss 0.131713 || Total Training Translation Loss 0.007487\n",
            "2025-07-10 14:16:06,334 Epoch 3051:\n",
            "\t\t\tTotal Training Recognition Loss 0.111998 || Total Training Translation Loss 0.008547\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-07-10 14:16:06,530 Epoch 3052:\n",
            "\t\t\tTotal Training Recognition Loss 0.093703 || Total Training Translation Loss 0.011518\n",
            "2025-07-10 14:16:06,705 Epoch 3053:\n",
            "\t\t\tTotal Training Recognition Loss 0.149920 || Total Training Translation Loss 0.010881\n",
            "2025-07-10 14:16:06,880 Epoch 3054:\n",
            "\t\t\tTotal Training Recognition Loss 0.065853 || Total Training Translation Loss 0.008535\n",
            "2025-07-10 14:16:07,055 Epoch 3055:\n",
            "\t\t\tTotal Training Recognition Loss 0.127409 || Total Training Translation Loss 0.014917\n",
            "2025-07-10 14:16:07,233 Epoch 3056:\n",
            "\t\t\tTotal Training Recognition Loss 0.105842 || Total Training Translation Loss 0.009225\n",
            "2025-07-10 14:16:07,406 Epoch 3057:\n",
            "\t\t\tTotal Training Recognition Loss 0.813301 || Total Training Translation Loss 0.009466\n",
            "2025-07-10 14:16:07,581 Epoch 3058:\n",
            "\t\t\tTotal Training Recognition Loss 0.281944 || Total Training Translation Loss 0.010383\n",
            "2025-07-10 14:16:07,757 Epoch 3059:\n",
            "\t\t\tTotal Training Recognition Loss 0.058409 || Total Training Translation Loss 0.010096\n",
            "2025-07-10 14:16:07,932 Epoch 3060:\n",
            "\t\t\tTotal Training Recognition Loss 0.083958 || Total Training Translation Loss 0.011315\n",
            "2025-07-10 14:16:08,108 Epoch 3061:\n",
            "\t\t\tTotal Training Recognition Loss 0.075875 || Total Training Translation Loss 0.010548\n",
            "2025-07-10 14:16:08,283 Epoch 3062:\n",
            "\t\t\tTotal Training Recognition Loss 0.067378 || Total Training Translation Loss 0.011760\n",
            "2025-07-10 14:16:08,457 Epoch 3063:\n",
            "\t\t\tTotal Training Recognition Loss 0.042480 || Total Training Translation Loss 0.009742\n",
            "2025-07-10 14:16:08,633 Epoch 3064:\n",
            "\t\t\tTotal Training Recognition Loss 0.162094 || Total Training Translation Loss 0.011166\n",
            "2025-07-10 14:16:08,809 Epoch 3065:\n",
            "\t\t\tTotal Training Recognition Loss 0.139867 || Total Training Translation Loss 0.011142\n",
            "2025-07-10 14:16:08,984 Epoch 3066:\n",
            "\t\t\tTotal Training Recognition Loss 0.103477 || Total Training Translation Loss 0.009902\n",
            "2025-07-10 14:16:09,159 Epoch 3067:\n",
            "\t\t\tTotal Training Recognition Loss 0.117478 || Total Training Translation Loss 0.009541\n",
            "2025-07-10 14:16:09,333 Epoch 3068:\n",
            "\t\t\tTotal Training Recognition Loss 0.042504 || Total Training Translation Loss 0.008786\n",
            "2025-07-10 14:16:09,507 Epoch 3069:\n",
            "\t\t\tTotal Training Recognition Loss 0.061585 || Total Training Translation Loss 0.009743\n",
            "2025-07-10 14:16:09,681 Epoch 3070:\n",
            "\t\t\tTotal Training Recognition Loss 0.186527 || Total Training Translation Loss 0.010158\n",
            "2025-07-10 14:16:09,856 Epoch 3071:\n",
            "\t\t\tTotal Training Recognition Loss 0.060351 || Total Training Translation Loss 0.009805\n",
            "2025-07-10 14:16:10,030 Epoch 3072:\n",
            "\t\t\tTotal Training Recognition Loss 0.068057 || Total Training Translation Loss 0.009961\n",
            "2025-07-10 14:16:10,206 Epoch 3073:\n",
            "\t\t\tTotal Training Recognition Loss 5.995942 || Total Training Translation Loss 0.008931\n",
            "2025-07-10 14:16:10,381 Epoch 3074:\n",
            "\t\t\tTotal Training Recognition Loss 0.075004 || Total Training Translation Loss 0.008570\n",
            "2025-07-10 14:16:10,554 Epoch 3075:\n",
            "\t\t\tTotal Training Recognition Loss 0.059502 || Total Training Translation Loss 0.009459\n",
            "2025-07-10 14:16:10,728 Epoch 3076:\n",
            "\t\t\tTotal Training Recognition Loss 3.401547 || Total Training Translation Loss 0.011188\n",
            "2025-07-10 14:16:10,908 Epoch 3077:\n",
            "\t\t\tTotal Training Recognition Loss 0.093258 || Total Training Translation Loss 0.009359\n",
            "2025-07-10 14:16:11,082 Epoch 3078:\n",
            "\t\t\tTotal Training Recognition Loss 0.085626 || Total Training Translation Loss 0.008435\n",
            "2025-07-10 14:16:11,256 Epoch 3079:\n",
            "\t\t\tTotal Training Recognition Loss 3.313791 || Total Training Translation Loss 0.010849\n",
            "2025-07-10 14:16:11,431 Epoch 3080:\n",
            "\t\t\tTotal Training Recognition Loss 0.263583 || Total Training Translation Loss 0.009576\n",
            "2025-07-10 14:16:11,612 Epoch 3081:\n",
            "\t\t\tTotal Training Recognition Loss 0.709452 || Total Training Translation Loss 0.008662\n",
            "2025-07-10 14:16:11,787 Epoch 3082:\n",
            "\t\t\tTotal Training Recognition Loss 1.478890 || Total Training Translation Loss 0.012000\n",
            "2025-07-10 14:16:11,961 Epoch 3083:\n",
            "\t\t\tTotal Training Recognition Loss 12.626397 || Total Training Translation Loss 0.010103\n",
            "2025-07-10 14:16:12,138 Epoch 3084:\n",
            "\t\t\tTotal Training Recognition Loss 0.228082 || Total Training Translation Loss 0.008889\n",
            "2025-07-10 14:16:12,314 Epoch 3085:\n",
            "\t\t\tTotal Training Recognition Loss 0.465754 || Total Training Translation Loss 0.010908\n",
            "2025-07-10 14:16:12,489 Epoch 3086:\n",
            "\t\t\tTotal Training Recognition Loss 4.920409 || Total Training Translation Loss 0.009887\n",
            "2025-07-10 14:16:12,663 Epoch 3087:\n",
            "\t\t\tTotal Training Recognition Loss 0.492741 || Total Training Translation Loss 0.009598\n",
            "2025-07-10 14:16:12,837 Epoch 3088:\n",
            "\t\t\tTotal Training Recognition Loss 2.799592 || Total Training Translation Loss 0.010184\n",
            "2025-07-10 14:16:13,012 Epoch 3089:\n",
            "\t\t\tTotal Training Recognition Loss 2.067387 || Total Training Translation Loss 0.011939\n",
            "2025-07-10 14:16:13,187 Epoch 3090:\n",
            "\t\t\tTotal Training Recognition Loss 2.156225 || Total Training Translation Loss 0.011003\n",
            "2025-07-10 14:16:13,362 Epoch 3091:\n",
            "\t\t\tTotal Training Recognition Loss 0.991484 || Total Training Translation Loss 0.010090\n",
            "2025-07-10 14:16:13,537 Epoch 3092:\n",
            "\t\t\tTotal Training Recognition Loss 0.434789 || Total Training Translation Loss 0.009484\n",
            "2025-07-10 14:16:13,713 Epoch 3093:\n",
            "\t\t\tTotal Training Recognition Loss 26.553900 || Total Training Translation Loss 0.009457\n",
            "2025-07-10 14:16:13,886 Epoch 3094:\n",
            "\t\t\tTotal Training Recognition Loss 17.868509 || Total Training Translation Loss 0.011901\n",
            "2025-07-10 14:16:14,066 Epoch 3095:\n",
            "\t\t\tTotal Training Recognition Loss 2.191338 || Total Training Translation Loss 0.011579\n",
            "2025-07-10 14:16:14,283 Epoch 3096:\n",
            "\t\t\tTotal Training Recognition Loss 1.226611 || Total Training Translation Loss 0.010921\n",
            "2025-07-10 14:16:14,498 Epoch 3097:\n",
            "\t\t\tTotal Training Recognition Loss 0.095701 || Total Training Translation Loss 0.010037\n",
            "2025-07-10 14:16:14,714 Epoch 3098:\n",
            "\t\t\tTotal Training Recognition Loss 3.443118 || Total Training Translation Loss 0.010394\n",
            "2025-07-10 14:16:14,930 Epoch 3099:\n",
            "\t\t\tTotal Training Recognition Loss 7.939914 || Total Training Translation Loss 0.008916\n",
            "2025-07-10 14:16:15,146 [Epoch: 3100 Step: 00003100] Batch Recognition Loss:  18.059927 => Gls Tokens per Sec:      172 || Batch Translation Loss:   0.010813 => Txt Tokens per Sec:      466 || Lr: 0.000700\n",
            "2025-07-10 14:16:15,462 Validation result at epoch 3100, step     3100: duration: 0.3144s\n",
            "\tRecognition Beam Size: 1\tTranslation Beam Size: 1\tTranslation Beam Alpha: -1\n",
            "\tRecognition Loss: 9107.97559\tTranslation Loss: 308.99338\tPPL: 37.91005\n",
            "\tEval Metric: BLEU\n",
            "\tWER 131.43\t(DEL: 14.29,\tINS: 51.43,\tSUB: 65.71)\n",
            "\tBLEU-4 2.76\t(BLEU-1: 6.67,\tBLEU-2: 4.29,\tBLEU-3: 3.40,\tBLEU-4: 2.76)\n",
            "\tCHRF 24.27\tROUGE 10.19\tFID 0.00\n",
            "2025-07-10 14:16:15,478 Logging Recognition and Translation Outputs\n",
            "2025-07-10 14:16:15,478 ========================================================================================\n",
            "2025-07-10 14:16:15,479 Logging Sequence: 11August_2010_Wednesday_tagesschau-2\n",
            "2025-07-10 14:16:15,479 \tGloss Reference :\t******** DRUCK    TIEF  KOMMEN  \n",
            "2025-07-10 14:16:15,480 \tGloss Hypothesis:\tNORDWEST SPEZIELL WOLKE SPEZIELL\n",
            "2025-07-10 14:16:15,481 \tGloss Alignment :\tI        S        S     S       \n",
            "2025-07-10 14:16:15,481 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:16:15,483 \tText Reference  :\t********** ****** ** ***** ***** ***** ******* ***** **** *** ********* ********* ******** ********* ****** ******* *** ***** *** ****** ***** tiefer luftdruck bestimmt in    den   nächsten tagen unser wetter\n",
            "2025-07-10 14:16:15,483 \tText Hypothesis :\tnordwesten bleibt es heute nacht meist trocken sonst muss mit teilweise kräftigen schauern gerechnet werden örtlich mit blitz und donner <unk> <unk>  <unk>     <unk>    <unk> <unk> <unk>    <unk> <unk> <unk> \n",
            "2025-07-10 14:16:15,484 \tText Alignment  :\tI          I      I  I     I     I     I       I     I    I   I         I         I        I         I      I       I   I     I   I      I     S      S         S        S     S     S        S     S     S     \n",
            "2025-07-10 14:16:15,484 ========================================================================================\n",
            "2025-07-10 14:16:15,485 Logging Sequence: 11August_2010_Wednesday_tagesschau-3\n",
            "2025-07-10 14:16:15,485 \tGloss Reference :\tES-BEDEUTET VIEL WOLKE UND KOENNEN REGEN  GEWITTER KOENNEN\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-07-10 14:16:15,485 \tGloss Hypothesis:\t*********** **** ***** *** LOCH    MORGEN LOCH     KOENNEN\n",
            "2025-07-10 14:16:15,486 \tGloss Alignment :\tD           D    D     D   S       S      S               \n",
            "2025-07-10 14:16:15,486 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:16:15,488 \tText Reference  :\t*** *** **************** *** ****** ********** *** ******** ****** ***** ***** ***** ***** ***** ***** ***** ***** das   bedeutet viele wolken und   immer wieder zum   teil  kräftige schauer und   gewitter\n",
            "2025-07-10 14:16:15,488 \tText Hypothesis :\tnun die wettervorhersage für morgen donnerstag den zwölften august <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>    <unk> <unk>  <unk> <unk> <unk>  <unk> <unk> <unk>    <unk>   <unk> <unk>   \n",
            "2025-07-10 14:16:15,488 \tText Alignment  :\tI   I   I                I   I      I          I   I        I      I     I     I     I     I     I     I     I     S     S        S     S      S     S     S      S     S     S        S       S     S       \n",
            "2025-07-10 14:16:15,488 ========================================================================================\n",
            "2025-07-10 14:16:15,488 Logging Sequence: 11August_2010_Wednesday_tagesschau-8\n",
            "2025-07-10 14:16:15,489 \tGloss Reference :\t******** **** ***** **** ******** ******* ******* ******** ******* ******** WIND    MAESSIG SCHWACH         REGION WENN     GEWITTER WIND     KOENNEN *** ****\n",
            "2025-07-10 14:16:15,490 \tGloss Hypothesis:\tSPEZIELL LOCH DURCH LOCH SPEZIELL TROCKEN BLEIBEN SPEZIELL BLEIBEN SPEZIELL BLEIBEN REGEN   UEBERSCHWEMMUNG REGEN  SPEZIELL LOCH     SUEDWEST KOENNEN ORT DAZU\n",
            "2025-07-10 14:16:15,490 \tGloss Alignment :\tI        I    I     I    I        I       I       I        I       I        S       S       S               S      S        S        S                I   I   \n",
            "2025-07-10 14:16:15,490 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:16:15,492 \tText Reference  :\t********** ****** ** ***** ***** meist ******* weht  nur  ein schwacher wind      aus      unterschiedlichen richtungen der     bei schauern und ****** ****** ****** ****** ****** ****** gewittern   stark böig  sein  kann \n",
            "2025-07-10 14:16:15,492 \tText Hypothesis :\tnordwesten bleibt es heute nacht meist trocken sonst muss mit teilweise kräftigen schauern gerechnet         werden     örtlich mit blitz    und donner donner donner donner donner donner regenmengen <unk> <unk> <unk> <unk>\n",
            "2025-07-10 14:16:15,493 \tText Alignment  :\tI          I      I  I     I           I       S     S    S   S         S         S        S                 S          S       S   S            I      I      I      I      I      I      S           S     S     S     S    \n",
            "2025-07-10 14:16:15,493 ========================================================================================\n",
            "2025-07-10 14:16:15,493 Logging Sequence: 25October_2010_Monday_tagesschau-22\n",
            "2025-07-10 14:16:15,493 \tGloss Reference :\tMITTWOCH REGEN ******** ******** *************** KOENNEN NORDWEST WAHRSCHEINLICH  NORD STARK    WIND   \n",
            "2025-07-10 14:16:15,494 \tGloss Hypothesis:\tDAZU     REGEN GEWITTER SUEDWEST UEBERSCHWEMMUNG KOENNEN SUEDWEST UEBERSCHWEMMUNG ORT  SUEDWEST KOENNEN\n",
            "2025-07-10 14:16:15,494 \tGloss Alignment :\tS              I        I        I                       S        S               S    S        S      \n",
            "2025-07-10 14:16:15,494 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:16:15,495 \tText Reference  :\t*** *** **************** *** ****** ********** *** ******** ****** ***** ***** ***** ***** ***** ***** ***** am    mittwoch hier  und   da    nieselregen in    der   nordwesthälfte an    den   küsten kräftiger wind \n",
            "2025-07-10 14:16:15,496 \tText Hypothesis :\tnun die wettervorhersage für morgen donnerstag den zwölften august <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>    <unk> <unk> <unk> <unk>       <unk> <unk> <unk>          <unk> <unk> <unk>  <unk>     <unk>\n",
            "2025-07-10 14:16:15,496 \tText Alignment  :\tI   I   I                I   I      I          I   I        I      I     I     I     I     I     I     I     S     S        S     S     S     S           S     S     S              S     S     S      S         S    \n",
            "2025-07-10 14:16:15,496 ========================================================================================\n",
            "2025-07-10 14:16:15,496 Logging Sequence: 05May_2011_Thursday_tagesschau-25\n",
            "2025-07-10 14:16:15,497 \tGloss Reference :\t****** ****** JETZT WETTER WIE-AUSSEHEN MORGEN FREITAG    SECHSTE MAI   ZEIGEN-BILDSCHIRM\n",
            "2025-07-10 14:16:15,497 \tGloss Hypothesis:\tWETTER ZWOELF JETZT WETTER ************ MORGEN DONNERSTAG FEBRUAR REGEN MORGEN           \n",
            "2025-07-10 14:16:15,497 \tGloss Alignment :\tI      I                   D                   S          S       S     S                \n",
            "2025-07-10 14:16:15,497 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:16:15,499 \tText Reference  :\tund nun die wettervorhersage für morgen freitag    den ******** ****** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** sechsten mai  \n",
            "2025-07-10 14:16:15,499 \tText Hypothesis :\t*** nun die wettervorhersage für morgen donnerstag den zwölften august <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>    <unk>\n",
            "2025-07-10 14:16:15,499 \tText Alignment  :\tD                                       S              I        I      I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     S        S    \n",
            "2025-07-10 14:16:15,499 ========================================================================================\n",
            "2025-07-10 14:16:15,500 Epoch 3100:\n",
            "\t\t\tTotal Training Recognition Loss 18.059927 || Total Training Translation Loss 0.010813\n",
            "2025-07-10 14:16:15,692 Epoch 3101:\n",
            "\t\t\tTotal Training Recognition Loss 82.376991 || Total Training Translation Loss 0.013355\n",
            "2025-07-10 14:16:15,872 Epoch 3102:\n",
            "\t\t\tTotal Training Recognition Loss 0.312451 || Total Training Translation Loss 0.011365\n",
            "2025-07-10 14:16:16,047 Epoch 3103:\n",
            "\t\t\tTotal Training Recognition Loss 5.266237 || Total Training Translation Loss 0.012785\n",
            "2025-07-10 14:16:16,220 Epoch 3104:\n",
            "\t\t\tTotal Training Recognition Loss 4.459426 || Total Training Translation Loss 0.011338\n",
            "2025-07-10 14:16:16,395 Epoch 3105:\n",
            "\t\t\tTotal Training Recognition Loss 13.904251 || Total Training Translation Loss 0.010747\n",
            "2025-07-10 14:16:16,569 Epoch 3106:\n",
            "\t\t\tTotal Training Recognition Loss 45.244476 || Total Training Translation Loss 0.010586\n",
            "2025-07-10 14:16:16,742 Epoch 3107:\n",
            "\t\t\tTotal Training Recognition Loss 4.223485 || Total Training Translation Loss 0.012358\n",
            "2025-07-10 14:16:16,916 Epoch 3108:\n",
            "\t\t\tTotal Training Recognition Loss 0.878490 || Total Training Translation Loss 0.012718\n",
            "2025-07-10 14:16:17,090 Epoch 3109:\n",
            "\t\t\tTotal Training Recognition Loss 51.728806 || Total Training Translation Loss 0.011962\n",
            "2025-07-10 14:16:17,267 Epoch 3110:\n",
            "\t\t\tTotal Training Recognition Loss 0.495538 || Total Training Translation Loss 0.012726\n",
            "2025-07-10 14:16:17,445 Epoch 3111:\n",
            "\t\t\tTotal Training Recognition Loss 1.987465 || Total Training Translation Loss 0.010976\n",
            "2025-07-10 14:16:17,621 Epoch 3112:\n",
            "\t\t\tTotal Training Recognition Loss 22.940081 || Total Training Translation Loss 0.011609\n",
            "2025-07-10 14:16:17,794 Epoch 3113:\n",
            "\t\t\tTotal Training Recognition Loss 5.783800 || Total Training Translation Loss 0.012056\n",
            "2025-07-10 14:16:17,969 Epoch 3114:\n",
            "\t\t\tTotal Training Recognition Loss 1.256543 || Total Training Translation Loss 0.013809\n",
            "2025-07-10 14:16:18,143 Epoch 3115:\n",
            "\t\t\tTotal Training Recognition Loss 8.672193 || Total Training Translation Loss 0.011614\n",
            "2025-07-10 14:16:18,317 Epoch 3116:\n",
            "\t\t\tTotal Training Recognition Loss 40.480076 || Total Training Translation Loss 0.010384\n",
            "2025-07-10 14:16:18,490 Epoch 3117:\n",
            "\t\t\tTotal Training Recognition Loss 98.660141 || Total Training Translation Loss 0.010518\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-07-10 14:16:18,665 Epoch 3118:\n",
            "\t\t\tTotal Training Recognition Loss 2.860971 || Total Training Translation Loss 0.009990\n",
            "2025-07-10 14:16:18,839 Epoch 3119:\n",
            "\t\t\tTotal Training Recognition Loss 47.510651 || Total Training Translation Loss 0.012519\n",
            "2025-07-10 14:16:19,014 Epoch 3120:\n",
            "\t\t\tTotal Training Recognition Loss 5.766124 || Total Training Translation Loss 0.013422\n",
            "2025-07-10 14:16:19,189 Epoch 3121:\n",
            "\t\t\tTotal Training Recognition Loss 2.506951 || Total Training Translation Loss 0.014514\n",
            "2025-07-10 14:16:19,364 Epoch 3122:\n",
            "\t\t\tTotal Training Recognition Loss 9.131010 || Total Training Translation Loss 0.013549\n",
            "2025-07-10 14:16:19,542 Epoch 3123:\n",
            "\t\t\tTotal Training Recognition Loss 19.487898 || Total Training Translation Loss 0.012111\n",
            "2025-07-10 14:16:19,715 Epoch 3124:\n",
            "\t\t\tTotal Training Recognition Loss 1.212427 || Total Training Translation Loss 0.010964\n",
            "2025-07-10 14:16:19,889 Epoch 3125:\n",
            "\t\t\tTotal Training Recognition Loss 6.541040 || Total Training Translation Loss 0.015327\n",
            "2025-07-10 14:16:20,063 Epoch 3126:\n",
            "\t\t\tTotal Training Recognition Loss 16.604191 || Total Training Translation Loss 0.011504\n",
            "2025-07-10 14:16:20,239 Epoch 3127:\n",
            "\t\t\tTotal Training Recognition Loss 33.075115 || Total Training Translation Loss 0.010222\n",
            "2025-07-10 14:16:20,419 Epoch 3128:\n",
            "\t\t\tTotal Training Recognition Loss 26.591320 || Total Training Translation Loss 0.011028\n",
            "2025-07-10 14:16:20,594 Epoch 3129:\n",
            "\t\t\tTotal Training Recognition Loss 39.017845 || Total Training Translation Loss 0.016091\n",
            "2025-07-10 14:16:20,767 Epoch 3130:\n",
            "\t\t\tTotal Training Recognition Loss 1.413477 || Total Training Translation Loss 0.011491\n",
            "2025-07-10 14:16:20,944 Epoch 3131:\n",
            "\t\t\tTotal Training Recognition Loss 30.170673 || Total Training Translation Loss 0.012254\n",
            "2025-07-10 14:16:21,118 Epoch 3132:\n",
            "\t\t\tTotal Training Recognition Loss 38.395710 || Total Training Translation Loss 0.022027\n",
            "2025-07-10 14:16:21,291 Epoch 3133:\n",
            "\t\t\tTotal Training Recognition Loss 108.557579 || Total Training Translation Loss 0.012355\n",
            "2025-07-10 14:16:21,465 Epoch 3134:\n",
            "\t\t\tTotal Training Recognition Loss 24.466116 || Total Training Translation Loss 0.013154\n",
            "2025-07-10 14:16:21,679 Epoch 3135:\n",
            "\t\t\tTotal Training Recognition Loss 12.977255 || Total Training Translation Loss 0.012721\n",
            "2025-07-10 14:16:21,898 Epoch 3136:\n",
            "\t\t\tTotal Training Recognition Loss 12.911683 || Total Training Translation Loss 0.024074\n",
            "2025-07-10 14:16:22,115 Epoch 3137:\n",
            "\t\t\tTotal Training Recognition Loss 10.758725 || Total Training Translation Loss 0.013794\n",
            "2025-07-10 14:16:22,333 Epoch 3138:\n",
            "\t\t\tTotal Training Recognition Loss 78.740417 || Total Training Translation Loss 0.012688\n",
            "2025-07-10 14:16:22,551 Epoch 3139:\n",
            "\t\t\tTotal Training Recognition Loss 7.359197 || Total Training Translation Loss 0.019497\n",
            "2025-07-10 14:16:22,771 Epoch 3140:\n",
            "\t\t\tTotal Training Recognition Loss 20.279091 || Total Training Translation Loss 0.018542\n",
            "2025-07-10 14:16:22,990 Epoch 3141:\n",
            "\t\t\tTotal Training Recognition Loss 35.916515 || Total Training Translation Loss 0.015515\n",
            "2025-07-10 14:16:23,209 Epoch 3142:\n",
            "\t\t\tTotal Training Recognition Loss 10.746201 || Total Training Translation Loss 0.019607\n",
            "2025-07-10 14:16:23,433 Epoch 3143:\n",
            "\t\t\tTotal Training Recognition Loss 17.208729 || Total Training Translation Loss 0.014409\n",
            "2025-07-10 14:16:23,658 Epoch 3144:\n",
            "\t\t\tTotal Training Recognition Loss 21.063601 || Total Training Translation Loss 0.016690\n",
            "2025-07-10 14:16:23,880 Epoch 3145:\n",
            "\t\t\tTotal Training Recognition Loss 4.116840 || Total Training Translation Loss 0.016014\n",
            "2025-07-10 14:16:24,106 Epoch 3146:\n",
            "\t\t\tTotal Training Recognition Loss 1.791391 || Total Training Translation Loss 0.013486\n",
            "2025-07-10 14:16:24,328 Epoch 3147:\n",
            "\t\t\tTotal Training Recognition Loss 8.792530 || Total Training Translation Loss 0.015859\n",
            "2025-07-10 14:16:24,551 Epoch 3148:\n",
            "\t\t\tTotal Training Recognition Loss 9.440165 || Total Training Translation Loss 0.018792\n",
            "2025-07-10 14:16:24,773 Epoch 3149:\n",
            "\t\t\tTotal Training Recognition Loss 2.495752 || Total Training Translation Loss 0.017165\n",
            "2025-07-10 14:16:24,998 Epoch 3150:\n",
            "\t\t\tTotal Training Recognition Loss 8.912908 || Total Training Translation Loss 0.016509\n",
            "2025-07-10 14:16:25,224 Epoch 3151:\n",
            "\t\t\tTotal Training Recognition Loss 2.914804 || Total Training Translation Loss 0.010813\n",
            "2025-07-10 14:16:25,448 Epoch 3152:\n",
            "\t\t\tTotal Training Recognition Loss 15.677569 || Total Training Translation Loss 0.016073\n",
            "2025-07-10 14:16:25,676 Epoch 3153:\n",
            "\t\t\tTotal Training Recognition Loss 2.491893 || Total Training Translation Loss 0.011061\n",
            "2025-07-10 14:16:25,899 Epoch 3154:\n",
            "\t\t\tTotal Training Recognition Loss 2.103508 || Total Training Translation Loss 0.012933\n",
            "2025-07-10 14:16:26,125 Epoch 3155:\n",
            "\t\t\tTotal Training Recognition Loss 2.392620 || Total Training Translation Loss 0.012718\n",
            "2025-07-10 14:16:26,345 Epoch 3156:\n",
            "\t\t\tTotal Training Recognition Loss 1.776456 || Total Training Translation Loss 0.011551\n",
            "2025-07-10 14:16:26,570 Epoch 3157:\n",
            "\t\t\tTotal Training Recognition Loss 2.176200 || Total Training Translation Loss 0.013587\n",
            "2025-07-10 14:16:26,792 Epoch 3158:\n",
            "\t\t\tTotal Training Recognition Loss 8.740665 || Total Training Translation Loss 0.012000\n",
            "2025-07-10 14:16:27,012 Epoch 3159:\n",
            "\t\t\tTotal Training Recognition Loss 2.212923 || Total Training Translation Loss 0.010810\n",
            "2025-07-10 14:16:27,234 Epoch 3160:\n",
            "\t\t\tTotal Training Recognition Loss 4.353549 || Total Training Translation Loss 0.012430\n",
            "2025-07-10 14:16:27,454 Epoch 3161:\n",
            "\t\t\tTotal Training Recognition Loss 3.141282 || Total Training Translation Loss 0.011686\n",
            "2025-07-10 14:16:27,638 Epoch 3162:\n",
            "\t\t\tTotal Training Recognition Loss 1.645589 || Total Training Translation Loss 0.008205\n",
            "2025-07-10 14:16:27,818 Epoch 3163:\n",
            "\t\t\tTotal Training Recognition Loss 1.497675 || Total Training Translation Loss 0.010581\n",
            "2025-07-10 14:16:28,036 Epoch 3164:\n",
            "\t\t\tTotal Training Recognition Loss 1.869777 || Total Training Translation Loss 0.014392\n",
            "2025-07-10 14:16:28,255 Epoch 3165:\n",
            "\t\t\tTotal Training Recognition Loss 39.273094 || Total Training Translation Loss 0.008024\n",
            "2025-07-10 14:16:28,474 Epoch 3166:\n",
            "\t\t\tTotal Training Recognition Loss 1.297731 || Total Training Translation Loss 0.010498\n",
            "2025-07-10 14:16:28,665 Epoch 3167:\n",
            "\t\t\tTotal Training Recognition Loss 0.943285 || Total Training Translation Loss 0.008642\n",
            "2025-07-10 14:16:28,836 Epoch 3168:\n",
            "\t\t\tTotal Training Recognition Loss 3.187861 || Total Training Translation Loss 0.010352\n",
            "2025-07-10 14:16:29,007 Epoch 3169:\n",
            "\t\t\tTotal Training Recognition Loss 0.818336 || Total Training Translation Loss 0.011942\n",
            "2025-07-10 14:16:29,181 Epoch 3170:\n",
            "\t\t\tTotal Training Recognition Loss 0.834857 || Total Training Translation Loss 0.010227\n",
            "2025-07-10 14:16:29,353 Epoch 3171:\n",
            "\t\t\tTotal Training Recognition Loss 5.175741 || Total Training Translation Loss 0.010185\n",
            "2025-07-10 14:16:29,527 Epoch 3172:\n",
            "\t\t\tTotal Training Recognition Loss 0.618247 || Total Training Translation Loss 0.010594\n",
            "2025-07-10 14:16:29,702 Epoch 3173:\n",
            "\t\t\tTotal Training Recognition Loss 1.133907 || Total Training Translation Loss 0.010693\n",
            "2025-07-10 14:16:29,874 Epoch 3174:\n",
            "\t\t\tTotal Training Recognition Loss 1.078077 || Total Training Translation Loss 0.008948\n",
            "2025-07-10 14:16:30,045 Epoch 3175:\n",
            "\t\t\tTotal Training Recognition Loss 12.077420 || Total Training Translation Loss 0.010566\n",
            "2025-07-10 14:16:30,219 Epoch 3176:\n",
            "\t\t\tTotal Training Recognition Loss 1.513148 || Total Training Translation Loss 0.008485\n",
            "2025-07-10 14:16:30,390 Epoch 3177:\n",
            "\t\t\tTotal Training Recognition Loss 2.969906 || Total Training Translation Loss 0.009179\n",
            "2025-07-10 14:16:30,560 Epoch 3178:\n",
            "\t\t\tTotal Training Recognition Loss 1.499841 || Total Training Translation Loss 0.010117\n",
            "2025-07-10 14:16:30,731 Epoch 3179:\n",
            "\t\t\tTotal Training Recognition Loss 5.151382 || Total Training Translation Loss 0.009170\n",
            "2025-07-10 14:16:30,901 Epoch 3180:\n",
            "\t\t\tTotal Training Recognition Loss 2.099930 || Total Training Translation Loss 0.008634\n",
            "2025-07-10 14:16:31,073 Epoch 3181:\n",
            "\t\t\tTotal Training Recognition Loss 1.114021 || Total Training Translation Loss 0.010406\n",
            "2025-07-10 14:16:31,244 Epoch 3182:\n",
            "\t\t\tTotal Training Recognition Loss 28.681543 || Total Training Translation Loss 0.008614\n",
            "2025-07-10 14:16:31,416 Epoch 3183:\n",
            "\t\t\tTotal Training Recognition Loss 5.282082 || Total Training Translation Loss 0.008023\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-07-10 14:16:31,587 Epoch 3184:\n",
            "\t\t\tTotal Training Recognition Loss 4.263205 || Total Training Translation Loss 0.008138\n",
            "2025-07-10 14:16:31,759 Epoch 3185:\n",
            "\t\t\tTotal Training Recognition Loss 87.034401 || Total Training Translation Loss 0.009248\n",
            "2025-07-10 14:16:31,932 Epoch 3186:\n",
            "\t\t\tTotal Training Recognition Loss 3.219195 || Total Training Translation Loss 0.008900\n",
            "2025-07-10 14:16:32,105 Epoch 3187:\n",
            "\t\t\tTotal Training Recognition Loss 1.063156 || Total Training Translation Loss 0.009298\n",
            "2025-07-10 14:16:32,277 Epoch 3188:\n",
            "\t\t\tTotal Training Recognition Loss 3.189255 || Total Training Translation Loss 0.009328\n",
            "2025-07-10 14:16:32,451 Epoch 3189:\n",
            "\t\t\tTotal Training Recognition Loss 41.681549 || Total Training Translation Loss 0.007412\n",
            "2025-07-10 14:16:32,623 Epoch 3190:\n",
            "\t\t\tTotal Training Recognition Loss 134.368622 || Total Training Translation Loss 0.009396\n",
            "2025-07-10 14:16:32,794 Epoch 3191:\n",
            "\t\t\tTotal Training Recognition Loss 271.589813 || Total Training Translation Loss 0.009118\n",
            "2025-07-10 14:16:32,965 Epoch 3192:\n",
            "\t\t\tTotal Training Recognition Loss 16.732100 || Total Training Translation Loss 0.009171\n",
            "2025-07-10 14:16:33,136 Epoch 3193:\n",
            "\t\t\tTotal Training Recognition Loss 97.451591 || Total Training Translation Loss 0.008531\n",
            "2025-07-10 14:16:33,308 Epoch 3194:\n",
            "\t\t\tTotal Training Recognition Loss 16.908165 || Total Training Translation Loss 0.008904\n",
            "2025-07-10 14:16:33,479 Epoch 3195:\n",
            "\t\t\tTotal Training Recognition Loss 11.429128 || Total Training Translation Loss 0.011362\n",
            "2025-07-10 14:16:33,652 Epoch 3196:\n",
            "\t\t\tTotal Training Recognition Loss 241.038635 || Total Training Translation Loss 0.011405\n",
            "2025-07-10 14:16:33,826 Epoch 3197:\n",
            "\t\t\tTotal Training Recognition Loss 36.395859 || Total Training Translation Loss 0.014030\n",
            "2025-07-10 14:16:33,999 Epoch 3198:\n",
            "\t\t\tTotal Training Recognition Loss 318.816162 || Total Training Translation Loss 0.010351\n",
            "2025-07-10 14:16:34,171 Epoch 3199:\n",
            "\t\t\tTotal Training Recognition Loss 143.924088 || Total Training Translation Loss 0.027691\n",
            "2025-07-10 14:16:34,342 [Epoch: 3200 Step: 00003200] Batch Recognition Loss: 353.403412 => Gls Tokens per Sec:      217 || Batch Translation Loss:   0.012374 => Txt Tokens per Sec:      587 || Lr: 0.000700\n",
            "2025-07-10 14:16:34,576 Validation result at epoch 3200, step     3200: duration: 0.2328s\n",
            "\tRecognition Beam Size: 1\tTranslation Beam Size: 1\tTranslation Beam Alpha: -1\n",
            "\tRecognition Loss: 6926.45947\tTranslation Loss: 319.80820\tPPL: 43.05375\n",
            "\tEval Metric: BLEU\n",
            "\tWER 100.00\t(DEL: 40.00,\tINS: 8.57,\tSUB: 51.43)\n",
            "\tBLEU-4 2.61\t(BLEU-1: 5.33,\tBLEU-2: 3.84,\tBLEU-3: 3.16,\tBLEU-4: 2.61)\n",
            "\tCHRF 21.21\tROUGE 8.70\tFID 0.00\n",
            "2025-07-10 14:16:34,576 Logging Recognition and Translation Outputs\n",
            "2025-07-10 14:16:34,577 ========================================================================================\n",
            "2025-07-10 14:16:34,577 Logging Sequence: 11August_2010_Wednesday_tagesschau-2\n",
            "2025-07-10 14:16:34,577 \tGloss Reference :\tDRUCK TIEF     KOMMEN\n",
            "2025-07-10 14:16:34,577 \tGloss Hypothesis:\t***** SPEZIELL DURCH \n",
            "2025-07-10 14:16:34,578 \tGloss Alignment :\tD     S        S     \n",
            "2025-07-10 14:16:34,578 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:16:34,579 \tText Reference  :\t*** *** tiefer           luftdruck bestimmt in         den ******** ****** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** nächsten tagen unser wetter\n",
            "2025-07-10 14:16:34,579 \tText Hypothesis :\tnun die wettervorhersage für       morgen   donnerstag den zwölften august <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>    <unk> <unk> <unk> \n",
            "2025-07-10 14:16:34,580 \tText Alignment  :\tI   I   S                S         S        S              I        I      I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     S        S     S     S     \n",
            "2025-07-10 14:16:34,580 ========================================================================================\n",
            "2025-07-10 14:16:34,580 Logging Sequence: 11August_2010_Wednesday_tagesschau-3\n",
            "2025-07-10 14:16:34,580 \tGloss Reference :\tES-BEDEUTET VIEL WOLKE UND   KOENNEN REGEN GEWITTER KOENNEN\n",
            "2025-07-10 14:16:34,581 \tGloss Hypothesis:\t*********** **** ***** HEUTE KOENNEN ***** ******** JETZT  \n",
            "2025-07-10 14:16:34,581 \tGloss Alignment :\tD           D    D     S             D     D        S      \n",
            "2025-07-10 14:16:34,582 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:16:34,584 \tText Reference  :\t*** *** **************** *** ****** ********** *** ******** ****** ***** ***** ***** ***** ***** ***** ***** ***** das   bedeutet viele wolken und   immer wieder zum   teil  kräftige schauer und   gewitter\n",
            "2025-07-10 14:16:34,584 \tText Hypothesis :\tnun die wettervorhersage für morgen donnerstag den zwölften august <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>    <unk> <unk>  <unk> <unk> <unk>  <unk> <unk> <unk>    <unk>   <unk> <unk>   \n",
            "2025-07-10 14:16:34,584 \tText Alignment  :\tI   I   I                I   I      I          I   I        I      I     I     I     I     I     I     I     I     S     S        S     S      S     S     S      S     S     S        S       S     S       \n",
            "2025-07-10 14:16:34,584 ========================================================================================\n",
            "2025-07-10 14:16:34,584 Logging Sequence: 11August_2010_Wednesday_tagesschau-8\n",
            "2025-07-10 14:16:34,585 \tGloss Reference :\t***** ******* WIND    MAESSIG  SCHWACH REGION   WENN    GEWITTER WIND    KOENNEN *****\n",
            "2025-07-10 14:16:34,585 \tGloss Hypothesis:\tDURCH TROCKEN BLEIBEN SPEZIELL REGEN   SPEZIELL BLEIBEN SUEDWEST BLEIBEN KOENNEN REGEN\n",
            "2025-07-10 14:16:34,586 \tGloss Alignment :\tI     I       S       S        S       S        S       S        S               I    \n",
            "2025-07-10 14:16:34,586 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:16:34,588 \tText Reference  :\t*** *** **************** *** ****** ********** *** ******** ****** ***** ***** ***** meist weht  nur   ein   schwacher wind  aus   unterschiedlichen richtungen der   bei   schauern und   gewittern stark böig  sein  kann \n",
            "2025-07-10 14:16:34,588 \tText Hypothesis :\tnun die wettervorhersage für morgen donnerstag den zwölften august <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>     <unk> <unk> <unk>             <unk>      <unk> <unk> <unk>    <unk> <unk>     <unk> <unk> <unk> <unk>\n",
            "2025-07-10 14:16:34,589 \tText Alignment  :\tI   I   I                I   I      I          I   I        I      I     I     I     S     S     S     S     S         S     S     S                 S          S     S     S        S     S         S     S     S     S    \n",
            "2025-07-10 14:16:34,589 ========================================================================================\n",
            "2025-07-10 14:16:34,589 Logging Sequence: 25October_2010_Monday_tagesschau-22\n",
            "2025-07-10 14:16:34,589 \tGloss Reference :\tMITTWOCH REGEN KOENNEN NORDWEST WAHRSCHEINLICH NORD            STARK    WIND \n",
            "2025-07-10 14:16:34,590 \tGloss Hypothesis:\t******** ***** ******* ******** ************** UEBERSCHWEMMUNG MANCHMAL JETZT\n",
            "2025-07-10 14:16:34,590 \tGloss Alignment :\tD        D     D       D        D              S               S        S    \n",
            "2025-07-10 14:16:34,590 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:16:34,592 \tText Reference  :\t*** *** **************** *** ****** ********** *** ******** ****** ***** ***** ***** ***** ***** ***** ***** am    mittwoch hier  und   da    nieselregen in    der   nordwesthälfte an    den   küsten kräftiger wind \n",
            "2025-07-10 14:16:34,592 \tText Hypothesis :\tnun die wettervorhersage für morgen donnerstag den zwölften august <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>    <unk> <unk> <unk> <unk>       <unk> <unk> <unk>          <unk> <unk> <unk>  <unk>     <unk>\n",
            "2025-07-10 14:16:34,592 \tText Alignment  :\tI   I   I                I   I      I          I   I        I      I     I     I     I     I     I     I     S     S        S     S     S     S           S     S     S              S     S     S      S         S    \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-07-10 14:16:34,593 ========================================================================================\n",
            "2025-07-10 14:16:34,593 Logging Sequence: 05May_2011_Thursday_tagesschau-25\n",
            "2025-07-10 14:16:34,593 \tGloss Reference :\tJETZT WETTER WIE-AUSSEHEN MORGEN FREITAG    SECHSTE MAI   ZEIGEN-BILDSCHIRM\n",
            "2025-07-10 14:16:34,593 \tGloss Hypothesis:\t***** ****** ************ MORGEN DONNERSTAG ZWOELF  REGEN ZWOELF           \n",
            "2025-07-10 14:16:34,594 \tGloss Alignment :\tD     D      D                   S          S       S     S                \n",
            "2025-07-10 14:16:34,594 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:16:34,595 \tText Reference  :\tund nun die wettervorhersage für morgen freitag    den ******** ****** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** sechsten mai  \n",
            "2025-07-10 14:16:34,595 \tText Hypothesis :\t*** nun die wettervorhersage für morgen donnerstag den zwölften august <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>    <unk>\n",
            "2025-07-10 14:16:34,596 \tText Alignment  :\tD                                       S              I        I      I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     S        S    \n",
            "2025-07-10 14:16:34,596 ========================================================================================\n",
            "2025-07-10 14:16:34,596 Epoch 3200:\n",
            "\t\t\tTotal Training Recognition Loss 353.403412 || Total Training Translation Loss 0.012374\n",
            "2025-07-10 14:16:34,770 Epoch 3201:\n",
            "\t\t\tTotal Training Recognition Loss 447.921570 || Total Training Translation Loss 0.011039\n",
            "2025-07-10 14:16:34,945 Epoch 3202:\n",
            "\t\t\tTotal Training Recognition Loss 193.765930 || Total Training Translation Loss 0.016645\n",
            "2025-07-10 14:16:35,119 Epoch 3203:\n",
            "\t\t\tTotal Training Recognition Loss 52.875065 || Total Training Translation Loss 0.021153\n",
            "2025-07-10 14:16:35,299 Epoch 3204:\n",
            "\t\t\tTotal Training Recognition Loss 65.760429 || Total Training Translation Loss 0.010279\n",
            "2025-07-10 14:16:35,474 Epoch 3205:\n",
            "\t\t\tTotal Training Recognition Loss 11.246308 || Total Training Translation Loss 0.016821\n",
            "2025-07-10 14:16:35,656 Epoch 3206:\n",
            "\t\t\tTotal Training Recognition Loss 22.393311 || Total Training Translation Loss 0.020458\n",
            "2025-07-10 14:16:35,830 Epoch 3207:\n",
            "\t\t\tTotal Training Recognition Loss 43.073036 || Total Training Translation Loss 0.013820\n",
            "2025-07-10 14:16:36,004 Epoch 3208:\n",
            "\t\t\tTotal Training Recognition Loss 20.341019 || Total Training Translation Loss 0.020402\n",
            "2025-07-10 14:16:36,182 Epoch 3209:\n",
            "\t\t\tTotal Training Recognition Loss 124.763344 || Total Training Translation Loss 0.027781\n",
            "2025-07-10 14:16:36,367 Epoch 3210:\n",
            "\t\t\tTotal Training Recognition Loss 14.129321 || Total Training Translation Loss 0.015284\n",
            "2025-07-10 14:16:36,541 Epoch 3211:\n",
            "\t\t\tTotal Training Recognition Loss 177.008041 || Total Training Translation Loss 0.012165\n",
            "2025-07-10 14:16:36,723 Epoch 3212:\n",
            "\t\t\tTotal Training Recognition Loss 19.684072 || Total Training Translation Loss 0.011899\n",
            "2025-07-10 14:16:36,897 Epoch 3213:\n",
            "\t\t\tTotal Training Recognition Loss 15.915382 || Total Training Translation Loss 0.016914\n",
            "2025-07-10 14:16:37,074 Epoch 3214:\n",
            "\t\t\tTotal Training Recognition Loss 14.422053 || Total Training Translation Loss 0.014979\n",
            "2025-07-10 14:16:37,251 Epoch 3215:\n",
            "\t\t\tTotal Training Recognition Loss 11.181514 || Total Training Translation Loss 0.016603\n",
            "2025-07-10 14:16:37,473 Epoch 3216:\n",
            "\t\t\tTotal Training Recognition Loss 24.949755 || Total Training Translation Loss 0.015845\n",
            "2025-07-10 14:16:37,692 Epoch 3217:\n",
            "\t\t\tTotal Training Recognition Loss 70.167480 || Total Training Translation Loss 0.013523\n",
            "2025-07-10 14:16:37,914 Epoch 3218:\n",
            "\t\t\tTotal Training Recognition Loss 165.883743 || Total Training Translation Loss 0.011232\n",
            "2025-07-10 14:16:38,134 Epoch 3219:\n",
            "\t\t\tTotal Training Recognition Loss 44.970226 || Total Training Translation Loss 0.012546\n",
            "2025-07-10 14:16:38,360 Epoch 3220:\n",
            "\t\t\tTotal Training Recognition Loss 25.696735 || Total Training Translation Loss 0.014417\n",
            "2025-07-10 14:16:38,583 Epoch 3221:\n",
            "\t\t\tTotal Training Recognition Loss 4.611061 || Total Training Translation Loss 0.011150\n",
            "2025-07-10 14:16:38,804 Epoch 3222:\n",
            "\t\t\tTotal Training Recognition Loss 33.350624 || Total Training Translation Loss 0.009757\n",
            "2025-07-10 14:16:39,025 Epoch 3223:\n",
            "\t\t\tTotal Training Recognition Loss 8.994240 || Total Training Translation Loss 0.010665\n",
            "2025-07-10 14:16:39,247 Epoch 3224:\n",
            "\t\t\tTotal Training Recognition Loss 22.272354 || Total Training Translation Loss 0.010838\n",
            "2025-07-10 14:16:39,472 Epoch 3225:\n",
            "\t\t\tTotal Training Recognition Loss 54.232132 || Total Training Translation Loss 0.010150\n",
            "2025-07-10 14:16:39,694 Epoch 3226:\n",
            "\t\t\tTotal Training Recognition Loss 8.570798 || Total Training Translation Loss 0.008338\n",
            "2025-07-10 14:16:39,883 Epoch 3227:\n",
            "\t\t\tTotal Training Recognition Loss 48.776550 || Total Training Translation Loss 0.010331\n",
            "2025-07-10 14:16:40,060 Epoch 3228:\n",
            "\t\t\tTotal Training Recognition Loss 13.145802 || Total Training Translation Loss 0.008344\n",
            "2025-07-10 14:16:40,239 Epoch 3229:\n",
            "\t\t\tTotal Training Recognition Loss 32.859245 || Total Training Translation Loss 0.008246\n",
            "2025-07-10 14:16:40,417 Epoch 3230:\n",
            "\t\t\tTotal Training Recognition Loss 9.172158 || Total Training Translation Loss 0.010240\n",
            "2025-07-10 14:16:40,602 Epoch 3231:\n",
            "\t\t\tTotal Training Recognition Loss 144.132294 || Total Training Translation Loss 0.010440\n",
            "2025-07-10 14:16:40,784 Epoch 3232:\n",
            "\t\t\tTotal Training Recognition Loss 3.474139 || Total Training Translation Loss 0.007679\n",
            "2025-07-10 14:16:40,969 Epoch 3233:\n",
            "\t\t\tTotal Training Recognition Loss 24.473976 || Total Training Translation Loss 0.007810\n",
            "2025-07-10 14:16:41,153 Epoch 3234:\n",
            "\t\t\tTotal Training Recognition Loss 15.755432 || Total Training Translation Loss 0.008507\n",
            "2025-07-10 14:16:41,356 Epoch 3235:\n",
            "\t\t\tTotal Training Recognition Loss 8.491333 || Total Training Translation Loss 0.009178\n",
            "2025-07-10 14:16:41,547 Epoch 3236:\n",
            "\t\t\tTotal Training Recognition Loss 11.440864 || Total Training Translation Loss 0.008385\n",
            "2025-07-10 14:16:41,742 Epoch 3237:\n",
            "\t\t\tTotal Training Recognition Loss 50.179401 || Total Training Translation Loss 0.007368\n",
            "2025-07-10 14:16:41,975 Epoch 3238:\n",
            "\t\t\tTotal Training Recognition Loss 2.843298 || Total Training Translation Loss 0.006802\n",
            "2025-07-10 14:16:42,204 Epoch 3239:\n",
            "\t\t\tTotal Training Recognition Loss 4.512229 || Total Training Translation Loss 0.007675\n",
            "2025-07-10 14:16:42,432 Epoch 3240:\n",
            "\t\t\tTotal Training Recognition Loss 4.240222 || Total Training Translation Loss 0.008900\n",
            "2025-07-10 14:16:42,663 Epoch 3241:\n",
            "\t\t\tTotal Training Recognition Loss 24.507595 || Total Training Translation Loss 0.007497\n",
            "2025-07-10 14:16:42,889 Epoch 3242:\n",
            "\t\t\tTotal Training Recognition Loss 44.678757 || Total Training Translation Loss 0.007911\n",
            "2025-07-10 14:16:43,113 Epoch 3243:\n",
            "\t\t\tTotal Training Recognition Loss 3.096265 || Total Training Translation Loss 0.009391\n",
            "2025-07-10 14:16:43,341 Epoch 3244:\n",
            "\t\t\tTotal Training Recognition Loss 3.008256 || Total Training Translation Loss 0.008390\n",
            "2025-07-10 14:16:43,564 Epoch 3245:\n",
            "\t\t\tTotal Training Recognition Loss 12.130950 || Total Training Translation Loss 0.007492\n",
            "2025-07-10 14:16:43,792 Epoch 3246:\n",
            "\t\t\tTotal Training Recognition Loss 36.860378 || Total Training Translation Loss 0.008630\n",
            "2025-07-10 14:16:44,013 Epoch 3247:\n",
            "\t\t\tTotal Training Recognition Loss 4.924676 || Total Training Translation Loss 0.007876\n",
            "2025-07-10 14:16:44,247 Epoch 3248:\n",
            "\t\t\tTotal Training Recognition Loss 0.961075 || Total Training Translation Loss 0.007844\n",
            "2025-07-10 14:16:44,482 Epoch 3249:\n",
            "\t\t\tTotal Training Recognition Loss 6.653492 || Total Training Translation Loss 0.009018\n",
            "2025-07-10 14:16:44,704 Epoch 3250:\n",
            "\t\t\tTotal Training Recognition Loss 20.915268 || Total Training Translation Loss 0.008355\n",
            "2025-07-10 14:16:44,952 Epoch 3251:\n",
            "\t\t\tTotal Training Recognition Loss 2.348789 || Total Training Translation Loss 0.008312\n",
            "2025-07-10 14:16:45,177 Epoch 3252:\n",
            "\t\t\tTotal Training Recognition Loss 109.298393 || Total Training Translation Loss 0.008505\n",
            "2025-07-10 14:16:45,399 Epoch 3253:\n",
            "\t\t\tTotal Training Recognition Loss 6.265705 || Total Training Translation Loss 0.007832\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-07-10 14:16:45,605 Epoch 3254:\n",
            "\t\t\tTotal Training Recognition Loss 8.482972 || Total Training Translation Loss 0.007534\n",
            "2025-07-10 14:16:45,826 Epoch 3255:\n",
            "\t\t\tTotal Training Recognition Loss 16.186781 || Total Training Translation Loss 0.008010\n",
            "2025-07-10 14:16:46,040 Epoch 3256:\n",
            "\t\t\tTotal Training Recognition Loss 203.073578 || Total Training Translation Loss 0.010045\n",
            "2025-07-10 14:16:46,249 Epoch 3257:\n",
            "\t\t\tTotal Training Recognition Loss 10.612431 || Total Training Translation Loss 0.008159\n",
            "2025-07-10 14:16:46,471 Epoch 3258:\n",
            "\t\t\tTotal Training Recognition Loss 58.843868 || Total Training Translation Loss 0.008808\n",
            "2025-07-10 14:16:46,684 Epoch 3259:\n",
            "\t\t\tTotal Training Recognition Loss 193.255432 || Total Training Translation Loss 0.008602\n",
            "2025-07-10 14:16:46,906 Epoch 3260:\n",
            "\t\t\tTotal Training Recognition Loss 84.154518 || Total Training Translation Loss 0.007846\n",
            "2025-07-10 14:16:47,107 Epoch 3261:\n",
            "\t\t\tTotal Training Recognition Loss 67.039680 || Total Training Translation Loss 0.007804\n",
            "2025-07-10 14:16:47,325 Epoch 3262:\n",
            "\t\t\tTotal Training Recognition Loss 47.488560 || Total Training Translation Loss 0.007288\n",
            "2025-07-10 14:16:47,535 Epoch 3263:\n",
            "\t\t\tTotal Training Recognition Loss 136.809296 || Total Training Translation Loss 0.007503\n",
            "2025-07-10 14:16:47,741 Epoch 3264:\n",
            "\t\t\tTotal Training Recognition Loss 3.449362 || Total Training Translation Loss 0.009012\n",
            "2025-07-10 14:16:47,944 Epoch 3265:\n",
            "\t\t\tTotal Training Recognition Loss 50.641804 || Total Training Translation Loss 0.008771\n",
            "2025-07-10 14:16:48,147 Epoch 3266:\n",
            "\t\t\tTotal Training Recognition Loss 38.578629 || Total Training Translation Loss 0.010487\n",
            "2025-07-10 14:16:48,349 Epoch 3267:\n",
            "\t\t\tTotal Training Recognition Loss 67.705261 || Total Training Translation Loss 0.009188\n",
            "2025-07-10 14:16:48,553 Epoch 3268:\n",
            "\t\t\tTotal Training Recognition Loss 19.602077 || Total Training Translation Loss 0.009216\n",
            "2025-07-10 14:16:48,770 Epoch 3269:\n",
            "\t\t\tTotal Training Recognition Loss 16.499483 || Total Training Translation Loss 0.008233\n",
            "2025-07-10 14:16:48,991 Epoch 3270:\n",
            "\t\t\tTotal Training Recognition Loss 61.688881 || Total Training Translation Loss 0.007696\n",
            "2025-07-10 14:16:49,191 Epoch 3271:\n",
            "\t\t\tTotal Training Recognition Loss 23.780931 || Total Training Translation Loss 0.008435\n",
            "2025-07-10 14:16:49,413 Epoch 3272:\n",
            "\t\t\tTotal Training Recognition Loss 9.293864 || Total Training Translation Loss 0.008559\n",
            "2025-07-10 14:16:49,621 Epoch 3273:\n",
            "\t\t\tTotal Training Recognition Loss 35.126141 || Total Training Translation Loss 0.007766\n",
            "2025-07-10 14:16:49,846 Epoch 3274:\n",
            "\t\t\tTotal Training Recognition Loss 56.291065 || Total Training Translation Loss 0.008672\n",
            "2025-07-10 14:16:50,073 Epoch 3275:\n",
            "\t\t\tTotal Training Recognition Loss 20.401730 || Total Training Translation Loss 0.008593\n",
            "2025-07-10 14:16:50,276 Epoch 3276:\n",
            "\t\t\tTotal Training Recognition Loss 17.531578 || Total Training Translation Loss 0.011349\n",
            "2025-07-10 14:16:50,477 Epoch 3277:\n",
            "\t\t\tTotal Training Recognition Loss 26.959469 || Total Training Translation Loss 0.010011\n",
            "2025-07-10 14:16:50,670 Epoch 3278:\n",
            "\t\t\tTotal Training Recognition Loss 9.908566 || Total Training Translation Loss 0.009642\n",
            "2025-07-10 14:16:50,883 Epoch 3279:\n",
            "\t\t\tTotal Training Recognition Loss 7.009733 || Total Training Translation Loss 0.008813\n",
            "2025-07-10 14:16:51,115 Epoch 3280:\n",
            "\t\t\tTotal Training Recognition Loss 6.660578 || Total Training Translation Loss 0.009680\n",
            "2025-07-10 14:16:51,323 Epoch 3281:\n",
            "\t\t\tTotal Training Recognition Loss 5.479284 || Total Training Translation Loss 0.008337\n",
            "2025-07-10 14:16:51,541 Epoch 3282:\n",
            "\t\t\tTotal Training Recognition Loss 53.570324 || Total Training Translation Loss 0.009327\n",
            "2025-07-10 14:16:51,760 Epoch 3283:\n",
            "\t\t\tTotal Training Recognition Loss 25.822044 || Total Training Translation Loss 0.008389\n",
            "2025-07-10 14:16:51,968 Epoch 3284:\n",
            "\t\t\tTotal Training Recognition Loss 30.918629 || Total Training Translation Loss 0.007233\n",
            "2025-07-10 14:16:52,193 Epoch 3285:\n",
            "\t\t\tTotal Training Recognition Loss 3.580173 || Total Training Translation Loss 0.009097\n",
            "2025-07-10 14:16:52,422 Epoch 3286:\n",
            "\t\t\tTotal Training Recognition Loss 4.571592 || Total Training Translation Loss 0.008078\n",
            "2025-07-10 14:16:52,632 Epoch 3287:\n",
            "\t\t\tTotal Training Recognition Loss 20.832260 || Total Training Translation Loss 0.009187\n",
            "2025-07-10 14:16:52,843 Epoch 3288:\n",
            "\t\t\tTotal Training Recognition Loss 8.379002 || Total Training Translation Loss 0.008833\n",
            "2025-07-10 14:16:53,058 Epoch 3289:\n",
            "\t\t\tTotal Training Recognition Loss 5.155638 || Total Training Translation Loss 0.008790\n",
            "2025-07-10 14:16:53,308 Epoch 3290:\n",
            "\t\t\tTotal Training Recognition Loss 3.477846 || Total Training Translation Loss 0.008914\n",
            "2025-07-10 14:16:53,565 Epoch 3291:\n",
            "\t\t\tTotal Training Recognition Loss 3.577882 || Total Training Translation Loss 0.007939\n",
            "2025-07-10 14:16:53,828 Epoch 3292:\n",
            "\t\t\tTotal Training Recognition Loss 5.287991 || Total Training Translation Loss 0.010256\n",
            "2025-07-10 14:16:54,098 Epoch 3293:\n",
            "\t\t\tTotal Training Recognition Loss 3.880812 || Total Training Translation Loss 0.008206\n",
            "2025-07-10 14:16:54,367 Epoch 3294:\n",
            "\t\t\tTotal Training Recognition Loss 24.483938 || Total Training Translation Loss 0.008071\n",
            "2025-07-10 14:16:54,595 Epoch 3295:\n",
            "\t\t\tTotal Training Recognition Loss 6.562775 || Total Training Translation Loss 0.008497\n",
            "2025-07-10 14:16:54,822 Epoch 3296:\n",
            "\t\t\tTotal Training Recognition Loss 5.592362 || Total Training Translation Loss 0.007705\n",
            "2025-07-10 14:16:55,047 Epoch 3297:\n",
            "\t\t\tTotal Training Recognition Loss 3.720585 || Total Training Translation Loss 0.009663\n",
            "2025-07-10 14:16:55,271 Epoch 3298:\n",
            "\t\t\tTotal Training Recognition Loss 2.294979 || Total Training Translation Loss 0.009648\n",
            "2025-07-10 14:16:55,478 Epoch 3299:\n",
            "\t\t\tTotal Training Recognition Loss 2.700135 || Total Training Translation Loss 0.010249\n",
            "2025-07-10 14:16:55,693 [Epoch: 3300 Step: 00003300] Batch Recognition Loss:   3.840710 => Gls Tokens per Sec:      173 || Batch Translation Loss:   0.008298 => Txt Tokens per Sec:      468 || Lr: 0.000700\n",
            "2025-07-10 14:16:55,941 Validation result at epoch 3300, step     3300: duration: 0.2469s\n",
            "\tRecognition Beam Size: 1\tTranslation Beam Size: 1\tTranslation Beam Alpha: -1\n",
            "\tRecognition Loss: 9599.79102\tTranslation Loss: 284.62277\tPPL: 28.46010\n",
            "\tEval Metric: BLEU\n",
            "\tWER 117.14\t(DEL: 14.29,\tINS: 37.14,\tSUB: 65.71)\n",
            "\tBLEU-4 2.61\t(BLEU-1: 5.33,\tBLEU-2: 3.84,\tBLEU-3: 3.16,\tBLEU-4: 2.61)\n",
            "\tCHRF 21.21\tROUGE 8.70\tFID 0.00\n",
            "2025-07-10 14:16:55,941 Logging Recognition and Translation Outputs\n",
            "2025-07-10 14:16:55,942 ========================================================================================\n",
            "2025-07-10 14:16:55,942 Logging Sequence: 11August_2010_Wednesday_tagesschau-2\n",
            "2025-07-10 14:16:55,943 \tGloss Reference :\t****** ******** ******* ******** DRUCK   TIEF     KOMMEN\n",
            "2025-07-10 14:16:55,943 \tGloss Hypothesis:\tZWOELF NORDWEST FEBRUAR NORDWEST FEBRUAR NORDWEST WOLKE \n",
            "2025-07-10 14:16:55,943 \tGloss Alignment :\tI      I        I       I        S       S        S     \n",
            "2025-07-10 14:16:55,943 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:16:55,945 \tText Reference  :\t*** *** tiefer           luftdruck bestimmt in         den ******** ****** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** nächsten tagen unser wetter\n",
            "2025-07-10 14:16:55,945 \tText Hypothesis :\tnun die wettervorhersage für       morgen   donnerstag den zwölften august <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>    <unk> <unk> <unk> \n",
            "2025-07-10 14:16:55,945 \tText Alignment  :\tI   I   S                S         S        S              I        I      I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     S        S     S     S     \n",
            "2025-07-10 14:16:55,945 ========================================================================================\n",
            "2025-07-10 14:16:55,945 Logging Sequence: 11August_2010_Wednesday_tagesschau-3\n",
            "2025-07-10 14:16:55,946 \tGloss Reference :\tES-BEDEUTET VIEL    WOLKE ********** UND   KOENNEN REGEN GEWITTER KOENNEN\n",
            "2025-07-10 14:16:55,946 \tGloss Hypothesis:\tWOLKE       FEBRUAR WOLKE DONNERSTAG HEUTE LOCH    HEUTE ZWOELF   KOENNEN\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-07-10 14:16:55,946 \tGloss Alignment :\tS           S             I          S     S       S     S               \n",
            "2025-07-10 14:16:55,947 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:16:55,948 \tText Reference  :\t*** *** **************** *** ****** ********** *** ******** ****** ***** ***** ***** ***** ***** ***** ***** ***** das   bedeutet viele wolken und   immer wieder zum   teil  kräftige schauer und   gewitter\n",
            "2025-07-10 14:16:55,948 \tText Hypothesis :\tnun die wettervorhersage für morgen donnerstag den zwölften august <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>    <unk> <unk>  <unk> <unk> <unk>  <unk> <unk> <unk>    <unk>   <unk> <unk>   \n",
            "2025-07-10 14:16:55,949 \tText Alignment  :\tI   I   I                I   I      I          I   I        I      I     I     I     I     I     I     I     I     S     S        S     S      S     S     S      S     S     S        S       S     S       \n",
            "2025-07-10 14:16:55,949 ========================================================================================\n",
            "2025-07-10 14:16:55,949 Logging Sequence: 11August_2010_Wednesday_tagesschau-8\n",
            "2025-07-10 14:16:55,950 \tGloss Reference :\t******** ***** ***** ******** ******* ***** ******* WIND    MAESSIG SCHWACH REGION WENN    GEWITTER WIND  KOENNEN\n",
            "2025-07-10 14:16:55,950 \tGloss Hypothesis:\tNORDWEST WOLKE DURCH NORDWEST TROCKEN WOLKE TROCKEN BLEIBEN WOLKE   FEBRUAR REGEN  FEBRUAR KOENNEN  WOLKE KOENNEN\n",
            "2025-07-10 14:16:55,951 \tGloss Alignment :\tI        I     I     I        I       I     I       S       S       S       S      S       S        S            \n",
            "2025-07-10 14:16:55,951 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:16:55,953 \tText Reference  :\t*** *** **************** *** ****** ********** *** ******** ****** ***** ***** ***** meist weht  nur   ein   schwacher wind  aus   unterschiedlichen richtungen der   bei   schauern und   gewittern stark böig  sein  kann \n",
            "2025-07-10 14:16:55,954 \tText Hypothesis :\tnun die wettervorhersage für morgen donnerstag den zwölften august <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>     <unk> <unk> <unk>             <unk>      <unk> <unk> <unk>    <unk> <unk>     <unk> <unk> <unk> <unk>\n",
            "2025-07-10 14:16:55,954 \tText Alignment  :\tI   I   I                I   I      I          I   I        I      I     I     I     S     S     S     S     S         S     S     S                 S          S     S     S        S     S         S     S     S     S    \n",
            "2025-07-10 14:16:55,954 ========================================================================================\n",
            "2025-07-10 14:16:55,954 Logging Sequence: 25October_2010_Monday_tagesschau-22\n",
            "2025-07-10 14:16:55,955 \tGloss Reference :\tMITTWOCH REGEN KOENNEN NORDWEST WAHRSCHEINLICH NORD STARK WIND   \n",
            "2025-07-10 14:16:55,955 \tGloss Hypothesis:\tJETZT    OFT   NACHT   NORDWEST ************** **** WOLKE KOENNEN\n",
            "2025-07-10 14:16:55,955 \tGloss Alignment :\tS        S     S                D              D    S     S      \n",
            "2025-07-10 14:16:55,955 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:16:55,957 \tText Reference  :\t*** *** **************** *** ****** ********** *** ******** ****** ***** ***** ***** ***** ***** ***** ***** am    mittwoch hier  und   da    nieselregen in    der   nordwesthälfte an    den   küsten kräftiger wind \n",
            "2025-07-10 14:16:55,957 \tText Hypothesis :\tnun die wettervorhersage für morgen donnerstag den zwölften august <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>    <unk> <unk> <unk> <unk>       <unk> <unk> <unk>          <unk> <unk> <unk>  <unk>     <unk>\n",
            "2025-07-10 14:16:55,957 \tText Alignment  :\tI   I   I                I   I      I          I   I        I      I     I     I     I     I     I     I     S     S        S     S     S     S           S     S     S              S     S     S      S         S    \n",
            "2025-07-10 14:16:55,958 ========================================================================================\n",
            "2025-07-10 14:16:55,958 Logging Sequence: 05May_2011_Thursday_tagesschau-25\n",
            "2025-07-10 14:16:55,958 \tGloss Reference :\t****** JETZT WETTER WIE-AUSSEHEN MORGEN FREITAG SECHSTE MAI        ZEIGEN-BILDSCHIRM\n",
            "2025-07-10 14:16:55,958 \tGloss Hypothesis:\tZWOELF JETZT WETTER ************ MORGEN ******* ******* DONNERSTAG ZWOELF           \n",
            "2025-07-10 14:16:55,959 \tGloss Alignment :\tI                   D                   D       D       S          S                \n",
            "2025-07-10 14:16:55,959 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:16:55,960 \tText Reference  :\tund nun die wettervorhersage für morgen freitag    den ******** ****** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** sechsten mai  \n",
            "2025-07-10 14:16:55,960 \tText Hypothesis :\t*** nun die wettervorhersage für morgen donnerstag den zwölften august <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>    <unk>\n",
            "2025-07-10 14:16:55,960 \tText Alignment  :\tD                                       S              I        I      I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     S        S    \n",
            "2025-07-10 14:16:55,961 ========================================================================================\n",
            "2025-07-10 14:16:55,961 Epoch 3300:\n",
            "\t\t\tTotal Training Recognition Loss 3.840710 || Total Training Translation Loss 0.008298\n",
            "2025-07-10 14:16:56,190 Epoch 3301:\n",
            "\t\t\tTotal Training Recognition Loss 3.176438 || Total Training Translation Loss 0.008496\n",
            "2025-07-10 14:16:56,397 Epoch 3302:\n",
            "\t\t\tTotal Training Recognition Loss 2.090068 || Total Training Translation Loss 0.009117\n",
            "2025-07-10 14:16:56,604 Epoch 3303:\n",
            "\t\t\tTotal Training Recognition Loss 2.236323 || Total Training Translation Loss 0.008588\n",
            "2025-07-10 14:16:56,831 Epoch 3304:\n",
            "\t\t\tTotal Training Recognition Loss 2.768748 || Total Training Translation Loss 0.007209\n",
            "2025-07-10 14:16:57,048 Epoch 3305:\n",
            "\t\t\tTotal Training Recognition Loss 1.163790 || Total Training Translation Loss 0.007855\n",
            "2025-07-10 14:16:57,276 Epoch 3306:\n",
            "\t\t\tTotal Training Recognition Loss 4.218162 || Total Training Translation Loss 0.007979\n",
            "2025-07-10 14:16:57,503 Epoch 3307:\n",
            "\t\t\tTotal Training Recognition Loss 16.333717 || Total Training Translation Loss 0.007775\n",
            "2025-07-10 14:16:57,731 Epoch 3308:\n",
            "\t\t\tTotal Training Recognition Loss 2.431162 || Total Training Translation Loss 0.009699\n",
            "2025-07-10 14:16:57,963 Epoch 3309:\n",
            "\t\t\tTotal Training Recognition Loss 1.154880 || Total Training Translation Loss 0.008774\n",
            "2025-07-10 14:16:58,187 Epoch 3310:\n",
            "\t\t\tTotal Training Recognition Loss 91.406311 || Total Training Translation Loss 0.009278\n",
            "2025-07-10 14:16:58,406 Epoch 3311:\n",
            "\t\t\tTotal Training Recognition Loss 19.080595 || Total Training Translation Loss 0.007763\n",
            "2025-07-10 14:16:58,607 Epoch 3312:\n",
            "\t\t\tTotal Training Recognition Loss 1.290644 || Total Training Translation Loss 0.009278\n",
            "2025-07-10 14:16:58,824 Epoch 3313:\n",
            "\t\t\tTotal Training Recognition Loss 3.337194 || Total Training Translation Loss 0.007167\n",
            "2025-07-10 14:16:59,042 Epoch 3314:\n",
            "\t\t\tTotal Training Recognition Loss 7.354169 || Total Training Translation Loss 0.009142\n",
            "2025-07-10 14:16:59,248 Epoch 3315:\n",
            "\t\t\tTotal Training Recognition Loss 13.555231 || Total Training Translation Loss 0.008473\n",
            "2025-07-10 14:16:59,454 Epoch 3316:\n",
            "\t\t\tTotal Training Recognition Loss 0.601246 || Total Training Translation Loss 0.009066\n",
            "2025-07-10 14:16:59,662 Epoch 3317:\n",
            "\t\t\tTotal Training Recognition Loss 0.497908 || Total Training Translation Loss 0.010883\n",
            "2025-07-10 14:16:59,892 Epoch 3318:\n",
            "\t\t\tTotal Training Recognition Loss 0.755893 || Total Training Translation Loss 0.007559\n",
            "2025-07-10 14:17:00,119 Epoch 3319:\n",
            "\t\t\tTotal Training Recognition Loss 3.458868 || Total Training Translation Loss 0.011637\n",
            "2025-07-10 14:17:00,345 Epoch 3320:\n",
            "\t\t\tTotal Training Recognition Loss 2.156915 || Total Training Translation Loss 0.008992\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-07-10 14:17:00,568 Epoch 3321:\n",
            "\t\t\tTotal Training Recognition Loss 2.424711 || Total Training Translation Loss 0.008734\n",
            "2025-07-10 14:17:00,834 Epoch 3322:\n",
            "\t\t\tTotal Training Recognition Loss 5.583314 || Total Training Translation Loss 0.008950\n",
            "2025-07-10 14:17:01,105 Epoch 3323:\n",
            "\t\t\tTotal Training Recognition Loss 0.437879 || Total Training Translation Loss 0.009113\n",
            "2025-07-10 14:17:01,367 Epoch 3324:\n",
            "\t\t\tTotal Training Recognition Loss 0.775763 || Total Training Translation Loss 0.008356\n",
            "2025-07-10 14:17:01,616 Epoch 3325:\n",
            "\t\t\tTotal Training Recognition Loss 3.176614 || Total Training Translation Loss 0.010115\n",
            "2025-07-10 14:17:01,890 Epoch 3326:\n",
            "\t\t\tTotal Training Recognition Loss 2.178511 || Total Training Translation Loss 0.011058\n",
            "2025-07-10 14:17:02,159 Epoch 3327:\n",
            "\t\t\tTotal Training Recognition Loss 1.166616 || Total Training Translation Loss 0.010037\n",
            "2025-07-10 14:17:02,427 Epoch 3328:\n",
            "\t\t\tTotal Training Recognition Loss 1.039403 || Total Training Translation Loss 0.009936\n",
            "2025-07-10 14:17:02,697 Epoch 3329:\n",
            "\t\t\tTotal Training Recognition Loss 1.088003 || Total Training Translation Loss 0.009224\n",
            "2025-07-10 14:17:02,970 Epoch 3330:\n",
            "\t\t\tTotal Training Recognition Loss 0.688424 || Total Training Translation Loss 0.009224\n",
            "2025-07-10 14:17:03,239 Epoch 3331:\n",
            "\t\t\tTotal Training Recognition Loss 3.669370 || Total Training Translation Loss 0.010084\n",
            "2025-07-10 14:17:03,508 Epoch 3332:\n",
            "\t\t\tTotal Training Recognition Loss 6.720870 || Total Training Translation Loss 0.008676\n",
            "2025-07-10 14:17:03,781 Epoch 3333:\n",
            "\t\t\tTotal Training Recognition Loss 7.904405 || Total Training Translation Loss 0.008182\n",
            "2025-07-10 14:17:04,043 Epoch 3334:\n",
            "\t\t\tTotal Training Recognition Loss 2.250502 || Total Training Translation Loss 0.008974\n",
            "2025-07-10 14:17:04,308 Epoch 3335:\n",
            "\t\t\tTotal Training Recognition Loss 10.514109 || Total Training Translation Loss 0.008435\n",
            "2025-07-10 14:17:04,572 Epoch 3336:\n",
            "\t\t\tTotal Training Recognition Loss 5.928612 || Total Training Translation Loss 0.010637\n",
            "2025-07-10 14:17:04,843 Epoch 3337:\n",
            "\t\t\tTotal Training Recognition Loss 12.579845 || Total Training Translation Loss 0.008659\n",
            "2025-07-10 14:17:05,112 Epoch 3338:\n",
            "\t\t\tTotal Training Recognition Loss 30.440826 || Total Training Translation Loss 0.008479\n",
            "2025-07-10 14:17:05,377 Epoch 3339:\n",
            "\t\t\tTotal Training Recognition Loss 3.714894 || Total Training Translation Loss 0.008369\n",
            "2025-07-10 14:17:05,650 Epoch 3340:\n",
            "\t\t\tTotal Training Recognition Loss 6.016860 || Total Training Translation Loss 0.009879\n",
            "2025-07-10 14:17:05,902 Epoch 3341:\n",
            "\t\t\tTotal Training Recognition Loss 0.290840 || Total Training Translation Loss 0.008859\n",
            "2025-07-10 14:17:06,173 Epoch 3342:\n",
            "\t\t\tTotal Training Recognition Loss 15.139730 || Total Training Translation Loss 0.010057\n",
            "2025-07-10 14:17:06,444 Epoch 3343:\n",
            "\t\t\tTotal Training Recognition Loss 0.182018 || Total Training Translation Loss 0.009554\n",
            "2025-07-10 14:17:06,664 Epoch 3344:\n",
            "\t\t\tTotal Training Recognition Loss 1.004771 || Total Training Translation Loss 0.009072\n",
            "2025-07-10 14:17:06,880 Epoch 3345:\n",
            "\t\t\tTotal Training Recognition Loss 3.326361 || Total Training Translation Loss 0.009390\n",
            "2025-07-10 14:17:07,144 Epoch 3346:\n",
            "\t\t\tTotal Training Recognition Loss 37.774750 || Total Training Translation Loss 0.011366\n",
            "2025-07-10 14:17:07,394 Epoch 3347:\n",
            "\t\t\tTotal Training Recognition Loss 0.231146 || Total Training Translation Loss 0.011670\n",
            "2025-07-10 14:17:07,640 Epoch 3348:\n",
            "\t\t\tTotal Training Recognition Loss 0.682860 || Total Training Translation Loss 0.009189\n",
            "2025-07-10 14:17:07,909 Epoch 3349:\n",
            "\t\t\tTotal Training Recognition Loss 0.534990 || Total Training Translation Loss 0.009988\n",
            "2025-07-10 14:17:08,149 Epoch 3350:\n",
            "\t\t\tTotal Training Recognition Loss 3.658030 || Total Training Translation Loss 0.011569\n",
            "2025-07-10 14:17:08,350 Epoch 3351:\n",
            "\t\t\tTotal Training Recognition Loss 1.049435 || Total Training Translation Loss 0.010622\n",
            "2025-07-10 14:17:08,605 Epoch 3352:\n",
            "\t\t\tTotal Training Recognition Loss 5.012020 || Total Training Translation Loss 0.010267\n",
            "2025-07-10 14:17:08,863 Epoch 3353:\n",
            "\t\t\tTotal Training Recognition Loss 0.606376 || Total Training Translation Loss 0.015572\n",
            "2025-07-10 14:17:09,113 Epoch 3354:\n",
            "\t\t\tTotal Training Recognition Loss 1.348113 || Total Training Translation Loss 0.012233\n",
            "2025-07-10 14:17:09,371 Epoch 3355:\n",
            "\t\t\tTotal Training Recognition Loss 86.115471 || Total Training Translation Loss 0.013012\n",
            "2025-07-10 14:17:09,618 Epoch 3356:\n",
            "\t\t\tTotal Training Recognition Loss 0.452211 || Total Training Translation Loss 0.011483\n",
            "2025-07-10 14:17:09,878 Epoch 3357:\n",
            "\t\t\tTotal Training Recognition Loss 0.595347 || Total Training Translation Loss 0.010595\n",
            "2025-07-10 14:17:10,130 Epoch 3358:\n",
            "\t\t\tTotal Training Recognition Loss 0.682008 || Total Training Translation Loss 0.009869\n",
            "2025-07-10 14:17:10,391 Epoch 3359:\n",
            "\t\t\tTotal Training Recognition Loss 1.464696 || Total Training Translation Loss 0.011114\n",
            "2025-07-10 14:17:10,647 Epoch 3360:\n",
            "\t\t\tTotal Training Recognition Loss 0.697109 || Total Training Translation Loss 0.014321\n",
            "2025-07-10 14:17:10,894 Epoch 3361:\n",
            "\t\t\tTotal Training Recognition Loss 1.231756 || Total Training Translation Loss 0.009757\n",
            "2025-07-10 14:17:11,154 Epoch 3362:\n",
            "\t\t\tTotal Training Recognition Loss 0.343369 || Total Training Translation Loss 0.010800\n",
            "2025-07-10 14:17:11,413 Epoch 3363:\n",
            "\t\t\tTotal Training Recognition Loss 35.921669 || Total Training Translation Loss 0.011242\n",
            "2025-07-10 14:17:11,661 Epoch 3364:\n",
            "\t\t\tTotal Training Recognition Loss 1.024848 || Total Training Translation Loss 0.010348\n",
            "2025-07-10 14:17:11,886 Epoch 3365:\n",
            "\t\t\tTotal Training Recognition Loss 3.451649 || Total Training Translation Loss 0.014422\n",
            "2025-07-10 14:17:12,157 Epoch 3366:\n",
            "\t\t\tTotal Training Recognition Loss 8.043887 || Total Training Translation Loss 0.011387\n",
            "2025-07-10 14:17:12,427 Epoch 3367:\n",
            "\t\t\tTotal Training Recognition Loss 5.355448 || Total Training Translation Loss 0.012296\n",
            "2025-07-10 14:17:12,661 Epoch 3368:\n",
            "\t\t\tTotal Training Recognition Loss 0.762937 || Total Training Translation Loss 0.008933\n",
            "2025-07-10 14:17:12,890 Epoch 3369:\n",
            "\t\t\tTotal Training Recognition Loss 2.542745 || Total Training Translation Loss 0.011702\n",
            "2025-07-10 14:17:13,123 Epoch 3370:\n",
            "\t\t\tTotal Training Recognition Loss 1.851249 || Total Training Translation Loss 0.011273\n",
            "2025-07-10 14:17:13,350 Epoch 3371:\n",
            "\t\t\tTotal Training Recognition Loss 2.570746 || Total Training Translation Loss 0.010784\n",
            "2025-07-10 14:17:13,575 Epoch 3372:\n",
            "\t\t\tTotal Training Recognition Loss 11.842063 || Total Training Translation Loss 0.009783\n",
            "2025-07-10 14:17:13,784 Epoch 3373:\n",
            "\t\t\tTotal Training Recognition Loss 12.149347 || Total Training Translation Loss 0.012259\n",
            "2025-07-10 14:17:14,002 Epoch 3374:\n",
            "\t\t\tTotal Training Recognition Loss 3.407523 || Total Training Translation Loss 0.011181\n",
            "2025-07-10 14:17:14,216 Epoch 3375:\n",
            "\t\t\tTotal Training Recognition Loss 1.806751 || Total Training Translation Loss 0.009258\n",
            "2025-07-10 14:17:14,427 Epoch 3376:\n",
            "\t\t\tTotal Training Recognition Loss 2.781001 || Total Training Translation Loss 0.012182\n",
            "2025-07-10 14:17:14,642 Epoch 3377:\n",
            "\t\t\tTotal Training Recognition Loss 1.220929 || Total Training Translation Loss 0.010225\n",
            "2025-07-10 14:17:14,868 Epoch 3378:\n",
            "\t\t\tTotal Training Recognition Loss 2.810253 || Total Training Translation Loss 0.010884\n",
            "2025-07-10 14:17:15,079 Epoch 3379:\n",
            "\t\t\tTotal Training Recognition Loss 0.862508 || Total Training Translation Loss 0.010409\n",
            "2025-07-10 14:17:15,302 Epoch 3380:\n",
            "\t\t\tTotal Training Recognition Loss 3.880960 || Total Training Translation Loss 0.012519\n",
            "2025-07-10 14:17:15,520 Epoch 3381:\n",
            "\t\t\tTotal Training Recognition Loss 0.933195 || Total Training Translation Loss 0.008688\n",
            "2025-07-10 14:17:15,741 Epoch 3382:\n",
            "\t\t\tTotal Training Recognition Loss 1.820608 || Total Training Translation Loss 0.009191\n",
            "2025-07-10 14:17:15,989 Epoch 3383:\n",
            "\t\t\tTotal Training Recognition Loss 1.352244 || Total Training Translation Loss 0.010585\n",
            "2025-07-10 14:17:16,251 Epoch 3384:\n",
            "\t\t\tTotal Training Recognition Loss 0.755723 || Total Training Translation Loss 0.009453\n",
            "2025-07-10 14:17:16,515 Epoch 3385:\n",
            "\t\t\tTotal Training Recognition Loss 1.547740 || Total Training Translation Loss 0.009651\n",
            "2025-07-10 14:17:16,769 Epoch 3386:\n",
            "\t\t\tTotal Training Recognition Loss 2.895130 || Total Training Translation Loss 0.007933\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-07-10 14:17:17,016 Epoch 3387:\n",
            "\t\t\tTotal Training Recognition Loss 5.620266 || Total Training Translation Loss 0.010985\n",
            "2025-07-10 14:17:17,264 Epoch 3388:\n",
            "\t\t\tTotal Training Recognition Loss 0.993051 || Total Training Translation Loss 0.010075\n",
            "2025-07-10 14:17:17,501 Epoch 3389:\n",
            "\t\t\tTotal Training Recognition Loss 0.622465 || Total Training Translation Loss 0.010164\n",
            "2025-07-10 14:17:17,709 Epoch 3390:\n",
            "\t\t\tTotal Training Recognition Loss 0.411171 || Total Training Translation Loss 0.010206\n",
            "2025-07-10 14:17:17,926 Epoch 3391:\n",
            "\t\t\tTotal Training Recognition Loss 0.420582 || Total Training Translation Loss 0.009067\n",
            "2025-07-10 14:17:18,144 Epoch 3392:\n",
            "\t\t\tTotal Training Recognition Loss 1.027564 || Total Training Translation Loss 0.009570\n",
            "2025-07-10 14:17:18,361 Epoch 3393:\n",
            "\t\t\tTotal Training Recognition Loss 0.535341 || Total Training Translation Loss 0.009737\n",
            "2025-07-10 14:17:18,570 Epoch 3394:\n",
            "\t\t\tTotal Training Recognition Loss 3.786997 || Total Training Translation Loss 0.009938\n",
            "2025-07-10 14:17:18,766 Epoch 3395:\n",
            "\t\t\tTotal Training Recognition Loss 1.072909 || Total Training Translation Loss 0.009634\n",
            "2025-07-10 14:17:18,968 Epoch 3396:\n",
            "\t\t\tTotal Training Recognition Loss 0.224693 || Total Training Translation Loss 0.009051\n",
            "2025-07-10 14:17:19,183 Epoch 3397:\n",
            "\t\t\tTotal Training Recognition Loss 0.897353 || Total Training Translation Loss 0.010681\n",
            "2025-07-10 14:17:19,391 Epoch 3398:\n",
            "\t\t\tTotal Training Recognition Loss 0.494064 || Total Training Translation Loss 0.010696\n",
            "2025-07-10 14:17:19,639 Epoch 3399:\n",
            "\t\t\tTotal Training Recognition Loss 9.433989 || Total Training Translation Loss 0.010815\n",
            "2025-07-10 14:17:19,879 [Epoch: 3400 Step: 00003400] Batch Recognition Loss:   1.419954 => Gls Tokens per Sec:      155 || Batch Translation Loss:   0.009227 => Txt Tokens per Sec:      420 || Lr: 0.000700\n",
            "2025-07-10 14:17:20,195 Validation result at epoch 3400, step     3400: duration: 0.3160s\n",
            "\tRecognition Beam Size: 1\tTranslation Beam Size: 1\tTranslation Beam Alpha: -1\n",
            "\tRecognition Loss: 8478.53223\tTranslation Loss: 278.48752\tPPL: 26.47825\n",
            "\tEval Metric: BLEU\n",
            "\tWER 88.57\t(DEL: 31.43,\tINS: 2.86,\tSUB: 54.29)\n",
            "\tBLEU-4 2.61\t(BLEU-1: 5.33,\tBLEU-2: 3.84,\tBLEU-3: 3.16,\tBLEU-4: 2.61)\n",
            "\tCHRF 21.21\tROUGE 8.70\tFID 0.00\n",
            "2025-07-10 14:17:20,196 Logging Recognition and Translation Outputs\n",
            "2025-07-10 14:17:20,197 ========================================================================================\n",
            "2025-07-10 14:17:20,197 Logging Sequence: 11August_2010_Wednesday_tagesschau-2\n",
            "2025-07-10 14:17:20,197 \tGloss Reference :\tDRUCK TIEF KOMMEN  \n",
            "2025-07-10 14:17:20,198 \tGloss Hypothesis:\t***** **** NORDWEST\n",
            "2025-07-10 14:17:20,198 \tGloss Alignment :\tD     D    S       \n",
            "2025-07-10 14:17:20,198 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:17:20,202 \tText Reference  :\t*** *** tiefer           luftdruck bestimmt in         den ******** ****** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** nächsten tagen unser wetter\n",
            "2025-07-10 14:17:20,202 \tText Hypothesis :\tnun die wettervorhersage für       morgen   donnerstag den zwölften august <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>    <unk> <unk> <unk> \n",
            "2025-07-10 14:17:20,202 \tText Alignment  :\tI   I   S                S         S        S              I        I      I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     S        S     S     S     \n",
            "2025-07-10 14:17:20,203 ========================================================================================\n",
            "2025-07-10 14:17:20,203 Logging Sequence: 11August_2010_Wednesday_tagesschau-3\n",
            "2025-07-10 14:17:20,204 \tGloss Reference :\tES-BEDEUTET VIEL       WOLKE UND   KOENNEN REGEN GEWITTER KOENNEN\n",
            "2025-07-10 14:17:20,204 \tGloss Hypothesis:\tWOLKE       DONNERSTAG WOLKE HEUTE LOCH    HEUTE WOLKE    KOENNEN\n",
            "2025-07-10 14:17:20,204 \tGloss Alignment :\tS           S                S     S       S     S               \n",
            "2025-07-10 14:17:20,205 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:17:20,209 \tText Reference  :\t*** *** **************** *** ****** ********** *** ******** ****** ***** ***** ***** ***** ***** ***** ***** ***** das   bedeutet viele wolken und   immer wieder zum   teil  kräftige schauer und   gewitter\n",
            "2025-07-10 14:17:20,209 \tText Hypothesis :\tnun die wettervorhersage für morgen donnerstag den zwölften august <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>    <unk> <unk>  <unk> <unk> <unk>  <unk> <unk> <unk>    <unk>   <unk> <unk>   \n",
            "2025-07-10 14:17:20,210 \tText Alignment  :\tI   I   I                I   I      I          I   I        I      I     I     I     I     I     I     I     I     S     S        S     S      S     S     S      S     S     S        S       S     S       \n",
            "2025-07-10 14:17:20,210 ========================================================================================\n",
            "2025-07-10 14:17:20,210 Logging Sequence: 11August_2010_Wednesday_tagesschau-8\n",
            "2025-07-10 14:17:20,211 \tGloss Reference :\tWIND  MAESSIG SCHWACH REGION  WENN    GEWITTER WIND  KOENNEN\n",
            "2025-07-10 14:17:20,211 \tGloss Hypothesis:\tWOLKE DURCH   TROCKEN BLEIBEN TROCKEN WOLKE    REGEN WOLKE  \n",
            "2025-07-10 14:17:20,212 \tGloss Alignment :\tS     S       S       S       S       S        S     S      \n",
            "2025-07-10 14:17:20,212 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:17:20,217 \tText Reference  :\t*** *** **************** *** ****** ********** *** ******** ****** ***** ***** ***** meist weht  nur   ein   schwacher wind  aus   unterschiedlichen richtungen der   bei   schauern und   gewittern stark böig  sein  kann \n",
            "2025-07-10 14:17:20,217 \tText Hypothesis :\tnun die wettervorhersage für morgen donnerstag den zwölften august <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>     <unk> <unk> <unk>             <unk>      <unk> <unk> <unk>    <unk> <unk>     <unk> <unk> <unk> <unk>\n",
            "2025-07-10 14:17:20,217 \tText Alignment  :\tI   I   I                I   I      I          I   I        I      I     I     I     S     S     S     S     S         S     S     S                 S          S     S     S        S     S         S     S     S     S    \n",
            "2025-07-10 14:17:20,218 ========================================================================================\n",
            "2025-07-10 14:17:20,218 Logging Sequence: 25October_2010_Monday_tagesschau-22\n",
            "2025-07-10 14:17:20,218 \tGloss Reference :\tMITTWOCH REGEN KOENNEN NORDWEST WAHRSCHEINLICH NORD STARK WIND    \n",
            "2025-07-10 14:17:20,219 \tGloss Hypothesis:\t******** ***** ******* ******** ************** **** JETZT GEWITTER\n",
            "2025-07-10 14:17:20,219 \tGloss Alignment :\tD        D     D       D        D              D    S     S       \n",
            "2025-07-10 14:17:20,219 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:17:20,223 \tText Reference  :\t*** *** **************** *** ****** ********** *** ******** ****** ***** ***** ***** ***** ***** ***** ***** am    mittwoch hier  und   da    nieselregen in    der   nordwesthälfte an    den   küsten kräftiger wind \n",
            "2025-07-10 14:17:20,223 \tText Hypothesis :\tnun die wettervorhersage für morgen donnerstag den zwölften august <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>    <unk> <unk> <unk> <unk>       <unk> <unk> <unk>          <unk> <unk> <unk>  <unk>     <unk>\n",
            "2025-07-10 14:17:20,223 \tText Alignment  :\tI   I   I                I   I      I          I   I        I      I     I     I     I     I     I     I     S     S        S     S     S     S           S     S     S              S     S     S      S         S    \n",
            "2025-07-10 14:17:20,224 ========================================================================================\n",
            "2025-07-10 14:17:20,224 Logging Sequence: 05May_2011_Thursday_tagesschau-25\n",
            "2025-07-10 14:17:20,225 \tGloss Reference :\t****** JETZT WETTER WIE-AUSSEHEN MORGEN FREITAG SECHSTE MAI        ZEIGEN-BILDSCHIRM\n",
            "2025-07-10 14:17:20,225 \tGloss Hypothesis:\tZWOELF JETZT WETTER ************ MORGEN ******* ******* DONNERSTAG ZWOELF           \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-07-10 14:17:20,225 \tGloss Alignment :\tI                   D                   D       D       S          S                \n",
            "2025-07-10 14:17:20,225 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:17:20,228 \tText Reference  :\tund nun die wettervorhersage für morgen freitag    den ******** ****** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** sechsten mai  \n",
            "2025-07-10 14:17:20,228 \tText Hypothesis :\t*** nun die wettervorhersage für morgen donnerstag den zwölften august <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>    <unk>\n",
            "2025-07-10 14:17:20,229 \tText Alignment  :\tD                                       S              I        I      I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     S        S    \n",
            "2025-07-10 14:17:20,229 ========================================================================================\n",
            "2025-07-10 14:17:20,229 Epoch 3400:\n",
            "\t\t\tTotal Training Recognition Loss 1.419954 || Total Training Translation Loss 0.009227\n",
            "2025-07-10 14:17:20,434 Epoch 3401:\n",
            "\t\t\tTotal Training Recognition Loss 1.004057 || Total Training Translation Loss 0.009890\n",
            "2025-07-10 14:17:20,627 Epoch 3402:\n",
            "\t\t\tTotal Training Recognition Loss 2.592867 || Total Training Translation Loss 0.011591\n",
            "2025-07-10 14:17:20,842 Epoch 3403:\n",
            "\t\t\tTotal Training Recognition Loss 13.670525 || Total Training Translation Loss 0.008348\n",
            "2025-07-10 14:17:21,038 Epoch 3404:\n",
            "\t\t\tTotal Training Recognition Loss 3.423228 || Total Training Translation Loss 0.010483\n",
            "2025-07-10 14:17:21,250 Epoch 3405:\n",
            "\t\t\tTotal Training Recognition Loss 0.274301 || Total Training Translation Loss 0.010457\n",
            "2025-07-10 14:17:21,467 Epoch 3406:\n",
            "\t\t\tTotal Training Recognition Loss 0.791590 || Total Training Translation Loss 0.008477\n",
            "2025-07-10 14:17:21,731 Epoch 3407:\n",
            "\t\t\tTotal Training Recognition Loss 23.277485 || Total Training Translation Loss 0.009681\n",
            "2025-07-10 14:17:21,991 Epoch 3408:\n",
            "\t\t\tTotal Training Recognition Loss 0.211638 || Total Training Translation Loss 0.009581\n",
            "2025-07-10 14:17:22,210 Epoch 3409:\n",
            "\t\t\tTotal Training Recognition Loss 3.628824 || Total Training Translation Loss 0.010097\n",
            "2025-07-10 14:17:22,414 Epoch 3410:\n",
            "\t\t\tTotal Training Recognition Loss 0.377067 || Total Training Translation Loss 0.008680\n",
            "2025-07-10 14:17:22,631 Epoch 3411:\n",
            "\t\t\tTotal Training Recognition Loss 0.245946 || Total Training Translation Loss 0.012253\n",
            "2025-07-10 14:17:22,856 Epoch 3412:\n",
            "\t\t\tTotal Training Recognition Loss 0.235110 || Total Training Translation Loss 0.008466\n",
            "2025-07-10 14:17:23,068 Epoch 3413:\n",
            "\t\t\tTotal Training Recognition Loss 0.385645 || Total Training Translation Loss 0.010883\n",
            "2025-07-10 14:17:23,276 Epoch 3414:\n",
            "\t\t\tTotal Training Recognition Loss 0.231429 || Total Training Translation Loss 0.012179\n",
            "2025-07-10 14:17:23,498 Epoch 3415:\n",
            "\t\t\tTotal Training Recognition Loss 3.151968 || Total Training Translation Loss 0.009072\n",
            "2025-07-10 14:17:23,703 Epoch 3416:\n",
            "\t\t\tTotal Training Recognition Loss 0.239500 || Total Training Translation Loss 0.009184\n",
            "2025-07-10 14:17:23,921 Epoch 3417:\n",
            "\t\t\tTotal Training Recognition Loss 0.249192 || Total Training Translation Loss 0.008501\n",
            "2025-07-10 14:17:24,125 Epoch 3418:\n",
            "\t\t\tTotal Training Recognition Loss 0.929316 || Total Training Translation Loss 0.009539\n",
            "2025-07-10 14:17:24,329 Epoch 3419:\n",
            "\t\t\tTotal Training Recognition Loss 36.593166 || Total Training Translation Loss 0.010809\n",
            "2025-07-10 14:17:24,538 Epoch 3420:\n",
            "\t\t\tTotal Training Recognition Loss 1.198773 || Total Training Translation Loss 0.009705\n",
            "2025-07-10 14:17:24,731 Epoch 3421:\n",
            "\t\t\tTotal Training Recognition Loss 0.542097 || Total Training Translation Loss 0.009146\n",
            "2025-07-10 14:17:24,932 Epoch 3422:\n",
            "\t\t\tTotal Training Recognition Loss 11.382300 || Total Training Translation Loss 0.008288\n",
            "2025-07-10 14:17:25,138 Epoch 3423:\n",
            "\t\t\tTotal Training Recognition Loss 0.435529 || Total Training Translation Loss 0.010881\n",
            "2025-07-10 14:17:25,331 Epoch 3424:\n",
            "\t\t\tTotal Training Recognition Loss 0.605152 || Total Training Translation Loss 0.009179\n",
            "2025-07-10 14:17:25,557 Epoch 3425:\n",
            "\t\t\tTotal Training Recognition Loss 2.132282 || Total Training Translation Loss 0.008982\n",
            "2025-07-10 14:17:25,752 Epoch 3426:\n",
            "\t\t\tTotal Training Recognition Loss 1.262395 || Total Training Translation Loss 0.010146\n",
            "2025-07-10 14:17:25,957 Epoch 3427:\n",
            "\t\t\tTotal Training Recognition Loss 0.924538 || Total Training Translation Loss 0.008779\n",
            "2025-07-10 14:17:26,183 Epoch 3428:\n",
            "\t\t\tTotal Training Recognition Loss 1.420099 || Total Training Translation Loss 0.012221\n",
            "2025-07-10 14:17:26,377 Epoch 3429:\n",
            "\t\t\tTotal Training Recognition Loss 2.960804 || Total Training Translation Loss 0.010442\n",
            "2025-07-10 14:17:26,583 Epoch 3430:\n",
            "\t\t\tTotal Training Recognition Loss 0.814023 || Total Training Translation Loss 0.010630\n",
            "2025-07-10 14:17:26,793 Epoch 3431:\n",
            "\t\t\tTotal Training Recognition Loss 0.493632 || Total Training Translation Loss 0.010977\n",
            "2025-07-10 14:17:27,001 Epoch 3432:\n",
            "\t\t\tTotal Training Recognition Loss 0.357538 || Total Training Translation Loss 0.009469\n",
            "2025-07-10 14:17:27,205 Epoch 3433:\n",
            "\t\t\tTotal Training Recognition Loss 0.316352 || Total Training Translation Loss 0.008833\n",
            "2025-07-10 14:17:27,394 Epoch 3434:\n",
            "\t\t\tTotal Training Recognition Loss 0.531217 || Total Training Translation Loss 0.011815\n",
            "2025-07-10 14:17:27,620 Epoch 3435:\n",
            "\t\t\tTotal Training Recognition Loss 0.971254 || Total Training Translation Loss 0.010425\n",
            "2025-07-10 14:17:27,811 Epoch 3436:\n",
            "\t\t\tTotal Training Recognition Loss 0.635654 || Total Training Translation Loss 0.008735\n",
            "2025-07-10 14:17:27,999 Epoch 3437:\n",
            "\t\t\tTotal Training Recognition Loss 0.461654 || Total Training Translation Loss 0.008928\n",
            "2025-07-10 14:17:28,232 Epoch 3438:\n",
            "\t\t\tTotal Training Recognition Loss 0.809191 || Total Training Translation Loss 0.009615\n",
            "2025-07-10 14:17:28,461 Epoch 3439:\n",
            "\t\t\tTotal Training Recognition Loss 0.419625 || Total Training Translation Loss 0.011304\n",
            "2025-07-10 14:17:28,687 Epoch 3440:\n",
            "\t\t\tTotal Training Recognition Loss 25.738081 || Total Training Translation Loss 0.008331\n",
            "2025-07-10 14:17:28,881 Epoch 3441:\n",
            "\t\t\tTotal Training Recognition Loss 1.072264 || Total Training Translation Loss 0.011294\n",
            "2025-07-10 14:17:29,082 Epoch 3442:\n",
            "\t\t\tTotal Training Recognition Loss 0.499101 || Total Training Translation Loss 0.010177\n",
            "2025-07-10 14:17:29,287 Epoch 3443:\n",
            "\t\t\tTotal Training Recognition Loss 0.388062 || Total Training Translation Loss 0.009835\n",
            "2025-07-10 14:17:29,512 Epoch 3444:\n",
            "\t\t\tTotal Training Recognition Loss 0.940741 || Total Training Translation Loss 0.009145\n",
            "2025-07-10 14:17:29,725 Epoch 3445:\n",
            "\t\t\tTotal Training Recognition Loss 0.317615 || Total Training Translation Loss 0.010469\n",
            "2025-07-10 14:17:29,942 Epoch 3446:\n",
            "\t\t\tTotal Training Recognition Loss 0.438981 || Total Training Translation Loss 0.010877\n",
            "2025-07-10 14:17:30,170 Epoch 3447:\n",
            "\t\t\tTotal Training Recognition Loss 0.554192 || Total Training Translation Loss 0.007974\n",
            "2025-07-10 14:17:30,375 Epoch 3448:\n",
            "\t\t\tTotal Training Recognition Loss 0.334368 || Total Training Translation Loss 0.010477\n",
            "2025-07-10 14:17:30,560 Epoch 3449:\n",
            "\t\t\tTotal Training Recognition Loss 43.783909 || Total Training Translation Loss 0.009938\n",
            "2025-07-10 14:17:30,778 Epoch 3450:\n",
            "\t\t\tTotal Training Recognition Loss 0.196739 || Total Training Translation Loss 0.012678\n",
            "2025-07-10 14:17:30,988 Epoch 3451:\n",
            "\t\t\tTotal Training Recognition Loss 35.869553 || Total Training Translation Loss 0.008625\n",
            "2025-07-10 14:17:31,186 Epoch 3452:\n",
            "\t\t\tTotal Training Recognition Loss 0.723690 || Total Training Translation Loss 0.010830\n",
            "2025-07-10 14:17:31,392 Epoch 3453:\n",
            "\t\t\tTotal Training Recognition Loss 0.545594 || Total Training Translation Loss 0.009652\n",
            "2025-07-10 14:17:31,612 Epoch 3454:\n",
            "\t\t\tTotal Training Recognition Loss 1.885989 || Total Training Translation Loss 0.011350\n",
            "2025-07-10 14:17:31,826 Epoch 3455:\n",
            "\t\t\tTotal Training Recognition Loss 7.719888 || Total Training Translation Loss 0.010176\n",
            "2025-07-10 14:17:32,057 Epoch 3456:\n",
            "\t\t\tTotal Training Recognition Loss 15.718572 || Total Training Translation Loss 0.008188\n",
            "2025-07-10 14:17:32,257 Epoch 3457:\n",
            "\t\t\tTotal Training Recognition Loss 1.374938 || Total Training Translation Loss 0.009749\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-07-10 14:17:32,457 Epoch 3458:\n",
            "\t\t\tTotal Training Recognition Loss 24.713737 || Total Training Translation Loss 0.010129\n",
            "2025-07-10 14:17:32,666 Epoch 3459:\n",
            "\t\t\tTotal Training Recognition Loss 1.248018 || Total Training Translation Loss 0.009719\n",
            "2025-07-10 14:17:32,904 Epoch 3460:\n",
            "\t\t\tTotal Training Recognition Loss 0.611713 || Total Training Translation Loss 0.008048\n",
            "2025-07-10 14:17:33,104 Epoch 3461:\n",
            "\t\t\tTotal Training Recognition Loss 0.258842 || Total Training Translation Loss 0.010170\n",
            "2025-07-10 14:17:33,308 Epoch 3462:\n",
            "\t\t\tTotal Training Recognition Loss 0.738538 || Total Training Translation Loss 0.010083\n",
            "2025-07-10 14:17:33,546 Epoch 3463:\n",
            "\t\t\tTotal Training Recognition Loss 2.768883 || Total Training Translation Loss 0.009818\n",
            "2025-07-10 14:17:33,762 Epoch 3464:\n",
            "\t\t\tTotal Training Recognition Loss 0.764503 || Total Training Translation Loss 0.008297\n",
            "2025-07-10 14:17:33,975 Epoch 3465:\n",
            "\t\t\tTotal Training Recognition Loss 0.806668 || Total Training Translation Loss 0.009694\n",
            "2025-07-10 14:17:34,192 Epoch 3466:\n",
            "\t\t\tTotal Training Recognition Loss 1.466772 || Total Training Translation Loss 0.010469\n",
            "2025-07-10 14:17:34,401 Epoch 3467:\n",
            "\t\t\tTotal Training Recognition Loss 13.696852 || Total Training Translation Loss 0.008416\n",
            "2025-07-10 14:17:34,590 Epoch 3468:\n",
            "\t\t\tTotal Training Recognition Loss 0.955504 || Total Training Translation Loss 0.009573\n",
            "2025-07-10 14:17:34,799 Epoch 3469:\n",
            "\t\t\tTotal Training Recognition Loss 2.337474 || Total Training Translation Loss 0.009933\n",
            "2025-07-10 14:17:35,019 Epoch 3470:\n",
            "\t\t\tTotal Training Recognition Loss 1.572186 || Total Training Translation Loss 0.009873\n",
            "2025-07-10 14:17:35,240 Epoch 3471:\n",
            "\t\t\tTotal Training Recognition Loss 2.241720 || Total Training Translation Loss 0.008889\n",
            "2025-07-10 14:17:35,468 Epoch 3472:\n",
            "\t\t\tTotal Training Recognition Loss 0.396110 || Total Training Translation Loss 0.009716\n",
            "2025-07-10 14:17:35,700 Epoch 3473:\n",
            "\t\t\tTotal Training Recognition Loss 0.980208 || Total Training Translation Loss 0.009593\n",
            "2025-07-10 14:17:35,913 Epoch 3474:\n",
            "\t\t\tTotal Training Recognition Loss 2.441586 || Total Training Translation Loss 0.010915\n",
            "2025-07-10 14:17:36,131 Epoch 3475:\n",
            "\t\t\tTotal Training Recognition Loss 0.340984 || Total Training Translation Loss 0.009160\n",
            "2025-07-10 14:17:36,354 Epoch 3476:\n",
            "\t\t\tTotal Training Recognition Loss 2.214077 || Total Training Translation Loss 0.009840\n",
            "2025-07-10 14:17:36,575 Epoch 3477:\n",
            "\t\t\tTotal Training Recognition Loss 3.734446 || Total Training Translation Loss 0.008660\n",
            "2025-07-10 14:17:36,801 Epoch 3478:\n",
            "\t\t\tTotal Training Recognition Loss 24.910248 || Total Training Translation Loss 0.009953\n",
            "2025-07-10 14:17:37,028 Epoch 3479:\n",
            "\t\t\tTotal Training Recognition Loss 1.244258 || Total Training Translation Loss 0.009997\n",
            "2025-07-10 14:17:37,260 Epoch 3480:\n",
            "\t\t\tTotal Training Recognition Loss 1.590457 || Total Training Translation Loss 0.009393\n",
            "2025-07-10 14:17:37,481 Epoch 3481:\n",
            "\t\t\tTotal Training Recognition Loss 0.611403 || Total Training Translation Loss 0.010715\n",
            "2025-07-10 14:17:37,694 Epoch 3482:\n",
            "\t\t\tTotal Training Recognition Loss 2.339315 || Total Training Translation Loss 0.010492\n",
            "2025-07-10 14:17:37,890 Epoch 3483:\n",
            "\t\t\tTotal Training Recognition Loss 1.863652 || Total Training Translation Loss 0.009682\n",
            "2025-07-10 14:17:38,113 Epoch 3484:\n",
            "\t\t\tTotal Training Recognition Loss 0.960467 || Total Training Translation Loss 0.010287\n",
            "2025-07-10 14:17:38,329 Epoch 3485:\n",
            "\t\t\tTotal Training Recognition Loss 1.036895 || Total Training Translation Loss 0.009759\n",
            "2025-07-10 14:17:38,538 Epoch 3486:\n",
            "\t\t\tTotal Training Recognition Loss 7.969311 || Total Training Translation Loss 0.009419\n",
            "2025-07-10 14:17:38,756 Epoch 3487:\n",
            "\t\t\tTotal Training Recognition Loss 1.326280 || Total Training Translation Loss 0.009273\n",
            "2025-07-10 14:17:38,962 Epoch 3488:\n",
            "\t\t\tTotal Training Recognition Loss 0.871901 || Total Training Translation Loss 0.009590\n",
            "2025-07-10 14:17:39,177 Epoch 3489:\n",
            "\t\t\tTotal Training Recognition Loss 0.614322 || Total Training Translation Loss 0.009219\n",
            "2025-07-10 14:17:39,381 Epoch 3490:\n",
            "\t\t\tTotal Training Recognition Loss 5.919968 || Total Training Translation Loss 0.010894\n",
            "2025-07-10 14:17:39,606 Epoch 3491:\n",
            "\t\t\tTotal Training Recognition Loss 5.639535 || Total Training Translation Loss 0.008264\n",
            "2025-07-10 14:17:39,818 Epoch 3492:\n",
            "\t\t\tTotal Training Recognition Loss 8.116275 || Total Training Translation Loss 0.009422\n",
            "2025-07-10 14:17:40,032 Epoch 3493:\n",
            "\t\t\tTotal Training Recognition Loss 10.550264 || Total Training Translation Loss 0.009460\n",
            "2025-07-10 14:17:40,232 Epoch 3494:\n",
            "\t\t\tTotal Training Recognition Loss 2.629745 || Total Training Translation Loss 0.009983\n",
            "2025-07-10 14:17:40,434 Epoch 3495:\n",
            "\t\t\tTotal Training Recognition Loss 0.400965 || Total Training Translation Loss 0.010236\n",
            "2025-07-10 14:17:40,643 Epoch 3496:\n",
            "\t\t\tTotal Training Recognition Loss 0.272199 || Total Training Translation Loss 0.010653\n",
            "2025-07-10 14:17:40,865 Epoch 3497:\n",
            "\t\t\tTotal Training Recognition Loss 0.705069 || Total Training Translation Loss 0.009651\n",
            "2025-07-10 14:17:41,079 Epoch 3498:\n",
            "\t\t\tTotal Training Recognition Loss 1.048583 || Total Training Translation Loss 0.008733\n",
            "2025-07-10 14:17:41,279 Epoch 3499:\n",
            "\t\t\tTotal Training Recognition Loss 0.837802 || Total Training Translation Loss 0.009061\n",
            "2025-07-10 14:17:41,479 [Epoch: 3500 Step: 00003500] Batch Recognition Loss:   1.059496 => Gls Tokens per Sec:      187 || Batch Translation Loss:   0.009073 => Txt Tokens per Sec:      505 || Lr: 0.000490\n",
            "2025-07-10 14:17:41,729 Validation result at epoch 3500, step     3500: duration: 0.2498s\n",
            "\tRecognition Beam Size: 1\tTranslation Beam Size: 1\tTranslation Beam Alpha: -1\n",
            "\tRecognition Loss: 7599.78955\tTranslation Loss: 275.72781\tPPL: 25.63239\n",
            "\tEval Metric: BLEU\n",
            "\tWER 102.86\t(DEL: 25.71,\tINS: 22.86,\tSUB: 54.29)\n",
            "\tBLEU-4 2.61\t(BLEU-1: 5.33,\tBLEU-2: 3.84,\tBLEU-3: 3.16,\tBLEU-4: 2.61)\n",
            "\tCHRF 21.21\tROUGE 8.70\tFID 0.00\n",
            "2025-07-10 14:17:41,730 Logging Recognition and Translation Outputs\n",
            "2025-07-10 14:17:41,731 ========================================================================================\n",
            "2025-07-10 14:17:41,731 Logging Sequence: 11August_2010_Wednesday_tagesschau-2\n",
            "2025-07-10 14:17:41,731 \tGloss Reference :\tDRUCK TIEF     KOMMEN\n",
            "2025-07-10 14:17:41,732 \tGloss Hypothesis:\t***** NORDWEST REGEN \n",
            "2025-07-10 14:17:41,732 \tGloss Alignment :\tD     S        S     \n",
            "2025-07-10 14:17:41,732 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:17:41,735 \tText Reference  :\t*** *** tiefer           luftdruck bestimmt in         den ******** ****** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** nächsten tagen unser wetter\n",
            "2025-07-10 14:17:41,735 \tText Hypothesis :\tnun die wettervorhersage für       morgen   donnerstag den zwölften august <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>    <unk> <unk> <unk> \n",
            "2025-07-10 14:17:41,736 \tText Alignment  :\tI   I   S                S         S        S              I        I      I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     S        S     S     S     \n",
            "2025-07-10 14:17:41,736 ========================================================================================\n",
            "2025-07-10 14:17:41,736 Logging Sequence: 11August_2010_Wednesday_tagesschau-3\n",
            "2025-07-10 14:17:41,737 \tGloss Reference :\tES-BEDEUTET VIEL       WOLKE UND   KOENNEN REGEN GEWITTER KOENNEN\n",
            "2025-07-10 14:17:41,737 \tGloss Hypothesis:\tWOLKE       DONNERSTAG WOLKE HEUTE LOCH    HEUTE WOLKE    KOENNEN\n",
            "2025-07-10 14:17:41,738 \tGloss Alignment :\tS           S                S     S       S     S               \n",
            "2025-07-10 14:17:41,738 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:17:41,741 \tText Reference  :\t*** *** **************** *** ****** ********** *** ******** ****** ***** ***** ***** ***** ***** ***** ***** ***** das   bedeutet viele wolken und   immer wieder zum   teil  kräftige schauer und   gewitter\n",
            "2025-07-10 14:17:41,741 \tText Hypothesis :\tnun die wettervorhersage für morgen donnerstag den zwölften august <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>    <unk> <unk>  <unk> <unk> <unk>  <unk> <unk> <unk>    <unk>   <unk> <unk>   \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-07-10 14:17:41,742 \tText Alignment  :\tI   I   I                I   I      I          I   I        I      I     I     I     I     I     I     I     I     S     S        S     S      S     S     S      S     S     S        S       S     S       \n",
            "2025-07-10 14:17:41,742 ========================================================================================\n",
            "2025-07-10 14:17:41,742 Logging Sequence: 11August_2010_Wednesday_tagesschau-8\n",
            "2025-07-10 14:17:41,744 \tGloss Reference :\t***** ***** **** ******** ******* ******* ******* WIND    MAESSIG SCHWACH REGION  WENN  GEWITTER WIND  KOENNEN\n",
            "2025-07-10 14:17:41,744 \tGloss Hypothesis:\tWOLKE DURCH LOCH NORDWEST TROCKEN BLEIBEN TROCKEN KOENNEN WOLKE   REGEN   KOENNEN WOLKE KOENNEN  WOLKE KOENNEN\n",
            "2025-07-10 14:17:41,744 \tGloss Alignment :\tI     I     I    I        I       I       I       S       S       S       S       S     S        S            \n",
            "2025-07-10 14:17:41,744 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:17:41,747 \tText Reference  :\t*** *** **************** *** ****** ********** *** ******** ****** ***** ***** ***** meist weht  nur   ein   schwacher wind  aus   unterschiedlichen richtungen der   bei   schauern und   gewittern stark böig  sein  kann \n",
            "2025-07-10 14:17:41,748 \tText Hypothesis :\tnun die wettervorhersage für morgen donnerstag den zwölften august <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>     <unk> <unk> <unk>             <unk>      <unk> <unk> <unk>    <unk> <unk>     <unk> <unk> <unk> <unk>\n",
            "2025-07-10 14:17:41,748 \tText Alignment  :\tI   I   I                I   I      I          I   I        I      I     I     I     S     S     S     S     S         S     S     S                 S          S     S     S        S     S         S     S     S     S    \n",
            "2025-07-10 14:17:41,748 ========================================================================================\n",
            "2025-07-10 14:17:41,748 Logging Sequence: 25October_2010_Monday_tagesschau-22\n",
            "2025-07-10 14:17:41,749 \tGloss Reference :\tMITTWOCH REGEN    KOENNEN NORDWEST WAHRSCHEINLICH NORD STARK WIND\n",
            "2025-07-10 14:17:41,749 \tGloss Hypothesis:\t******** GEWITTER KOENNEN ******** ************** **** ***** ****\n",
            "2025-07-10 14:17:41,749 \tGloss Alignment :\tD        S                D        D              D    D     D   \n",
            "2025-07-10 14:17:41,750 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:17:41,753 \tText Reference  :\t*** *** **************** *** ****** ********** *** ******** ****** ***** ***** ***** ***** ***** ***** ***** am    mittwoch hier  und   da    nieselregen in    der   nordwesthälfte an    den   küsten kräftiger wind \n",
            "2025-07-10 14:17:41,753 \tText Hypothesis :\tnun die wettervorhersage für morgen donnerstag den zwölften august <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>    <unk> <unk> <unk> <unk>       <unk> <unk> <unk>          <unk> <unk> <unk>  <unk>     <unk>\n",
            "2025-07-10 14:17:41,753 \tText Alignment  :\tI   I   I                I   I      I          I   I        I      I     I     I     I     I     I     I     S     S        S     S     S     S           S     S     S              S     S     S      S         S    \n",
            "2025-07-10 14:17:41,753 ========================================================================================\n",
            "2025-07-10 14:17:41,754 Logging Sequence: 05May_2011_Thursday_tagesschau-25\n",
            "2025-07-10 14:17:41,754 \tGloss Reference :\t****** JETZT WETTER WIE-AUSSEHEN MORGEN FREITAG SECHSTE    MAI   ZEIGEN-BILDSCHIRM\n",
            "2025-07-10 14:17:41,755 \tGloss Hypothesis:\tZWOELF JETZT WETTER ************ MORGEN ******* DONNERSTAG REGEN ZWOELF           \n",
            "2025-07-10 14:17:41,755 \tGloss Alignment :\tI                   D                   D       S          S     S                \n",
            "2025-07-10 14:17:41,755 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:17:41,757 \tText Reference  :\tund nun die wettervorhersage für morgen freitag    den ******** ****** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** sechsten mai  \n",
            "2025-07-10 14:17:41,758 \tText Hypothesis :\t*** nun die wettervorhersage für morgen donnerstag den zwölften august <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>    <unk>\n",
            "2025-07-10 14:17:41,758 \tText Alignment  :\tD                                       S              I        I      I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     S        S    \n",
            "2025-07-10 14:17:41,758 ========================================================================================\n",
            "2025-07-10 14:17:41,759 Epoch 3500:\n",
            "\t\t\tTotal Training Recognition Loss 1.059496 || Total Training Translation Loss 0.009073\n",
            "2025-07-10 14:17:41,960 Epoch 3501:\n",
            "\t\t\tTotal Training Recognition Loss 0.231928 || Total Training Translation Loss 0.009805\n",
            "2025-07-10 14:17:42,161 Epoch 3502:\n",
            "\t\t\tTotal Training Recognition Loss 0.224743 || Total Training Translation Loss 0.008507\n",
            "2025-07-10 14:17:42,358 Epoch 3503:\n",
            "\t\t\tTotal Training Recognition Loss 0.352460 || Total Training Translation Loss 0.011326\n",
            "2025-07-10 14:17:42,568 Epoch 3504:\n",
            "\t\t\tTotal Training Recognition Loss 0.241792 || Total Training Translation Loss 0.009431\n",
            "2025-07-10 14:17:42,771 Epoch 3505:\n",
            "\t\t\tTotal Training Recognition Loss 0.211216 || Total Training Translation Loss 0.011050\n",
            "2025-07-10 14:17:42,975 Epoch 3506:\n",
            "\t\t\tTotal Training Recognition Loss 0.381423 || Total Training Translation Loss 0.013730\n",
            "2025-07-10 14:17:43,191 Epoch 3507:\n",
            "\t\t\tTotal Training Recognition Loss 0.718348 || Total Training Translation Loss 0.010525\n",
            "2025-07-10 14:17:43,432 Epoch 3508:\n",
            "\t\t\tTotal Training Recognition Loss 5.353937 || Total Training Translation Loss 0.011263\n",
            "2025-07-10 14:17:43,652 Epoch 3509:\n",
            "\t\t\tTotal Training Recognition Loss 0.469120 || Total Training Translation Loss 0.010807\n",
            "2025-07-10 14:17:43,890 Epoch 3510:\n",
            "\t\t\tTotal Training Recognition Loss 0.205439 || Total Training Translation Loss 0.010366\n",
            "2025-07-10 14:17:44,145 Epoch 3511:\n",
            "\t\t\tTotal Training Recognition Loss 0.265976 || Total Training Translation Loss 0.011994\n",
            "2025-07-10 14:17:44,385 Epoch 3512:\n",
            "\t\t\tTotal Training Recognition Loss 0.986153 || Total Training Translation Loss 0.008989\n",
            "2025-07-10 14:17:44,626 Epoch 3513:\n",
            "\t\t\tTotal Training Recognition Loss 0.244122 || Total Training Translation Loss 0.011101\n",
            "2025-07-10 14:17:44,878 Epoch 3514:\n",
            "\t\t\tTotal Training Recognition Loss 0.358318 || Total Training Translation Loss 0.012034\n",
            "2025-07-10 14:17:45,121 Epoch 3515:\n",
            "\t\t\tTotal Training Recognition Loss 3.147058 || Total Training Translation Loss 0.011602\n",
            "2025-07-10 14:17:45,357 Epoch 3516:\n",
            "\t\t\tTotal Training Recognition Loss 0.797114 || Total Training Translation Loss 0.010743\n",
            "2025-07-10 14:17:45,605 Epoch 3517:\n",
            "\t\t\tTotal Training Recognition Loss 0.259427 || Total Training Translation Loss 0.009587\n",
            "2025-07-10 14:17:45,853 Epoch 3518:\n",
            "\t\t\tTotal Training Recognition Loss 0.195146 || Total Training Translation Loss 0.008981\n",
            "2025-07-10 14:17:46,098 Epoch 3519:\n",
            "\t\t\tTotal Training Recognition Loss 0.258681 || Total Training Translation Loss 0.008858\n",
            "2025-07-10 14:17:46,337 Epoch 3520:\n",
            "\t\t\tTotal Training Recognition Loss 0.227883 || Total Training Translation Loss 0.012590\n",
            "2025-07-10 14:17:46,578 Epoch 3521:\n",
            "\t\t\tTotal Training Recognition Loss 1.144848 || Total Training Translation Loss 0.010393\n",
            "2025-07-10 14:17:46,815 Epoch 3522:\n",
            "\t\t\tTotal Training Recognition Loss 0.443905 || Total Training Translation Loss 0.009037\n",
            "2025-07-10 14:17:47,063 Epoch 3523:\n",
            "\t\t\tTotal Training Recognition Loss 0.369056 || Total Training Translation Loss 0.009203\n",
            "2025-07-10 14:17:47,312 Epoch 3524:\n",
            "\t\t\tTotal Training Recognition Loss 0.666682 || Total Training Translation Loss 0.012946\n",
            "2025-07-10 14:17:47,557 Epoch 3525:\n",
            "\t\t\tTotal Training Recognition Loss 0.222447 || Total Training Translation Loss 0.011529\n",
            "2025-07-10 14:17:47,813 Epoch 3526:\n",
            "\t\t\tTotal Training Recognition Loss 0.959728 || Total Training Translation Loss 0.010973\n",
            "2025-07-10 14:17:48,055 Epoch 3527:\n",
            "\t\t\tTotal Training Recognition Loss 0.350612 || Total Training Translation Loss 0.009886\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-07-10 14:17:48,300 Epoch 3528:\n",
            "\t\t\tTotal Training Recognition Loss 1.204348 || Total Training Translation Loss 0.011971\n",
            "2025-07-10 14:17:48,549 Epoch 3529:\n",
            "\t\t\tTotal Training Recognition Loss 0.316416 || Total Training Translation Loss 0.010375\n",
            "2025-07-10 14:17:48,795 Epoch 3530:\n",
            "\t\t\tTotal Training Recognition Loss 0.384215 || Total Training Translation Loss 0.010418\n",
            "2025-07-10 14:17:49,049 Epoch 3531:\n",
            "\t\t\tTotal Training Recognition Loss 0.697677 || Total Training Translation Loss 0.010991\n",
            "2025-07-10 14:17:49,292 Epoch 3532:\n",
            "\t\t\tTotal Training Recognition Loss 4.115856 || Total Training Translation Loss 0.009318\n",
            "2025-07-10 14:17:49,549 Epoch 3533:\n",
            "\t\t\tTotal Training Recognition Loss 0.438423 || Total Training Translation Loss 0.012238\n",
            "2025-07-10 14:17:49,802 Epoch 3534:\n",
            "\t\t\tTotal Training Recognition Loss 0.692560 || Total Training Translation Loss 0.009702\n",
            "2025-07-10 14:17:50,048 Epoch 3535:\n",
            "\t\t\tTotal Training Recognition Loss 0.343086 || Total Training Translation Loss 0.009294\n",
            "2025-07-10 14:17:50,310 Epoch 3536:\n",
            "\t\t\tTotal Training Recognition Loss 0.336216 || Total Training Translation Loss 0.010199\n",
            "2025-07-10 14:17:50,551 Epoch 3537:\n",
            "\t\t\tTotal Training Recognition Loss 0.396663 || Total Training Translation Loss 0.009088\n",
            "2025-07-10 14:17:50,795 Epoch 3538:\n",
            "\t\t\tTotal Training Recognition Loss 0.953894 || Total Training Translation Loss 0.010679\n",
            "2025-07-10 14:17:51,044 Epoch 3539:\n",
            "\t\t\tTotal Training Recognition Loss 0.280147 || Total Training Translation Loss 0.011142\n",
            "2025-07-10 14:17:51,291 Epoch 3540:\n",
            "\t\t\tTotal Training Recognition Loss 0.412603 || Total Training Translation Loss 0.011663\n",
            "2025-07-10 14:17:51,541 Epoch 3541:\n",
            "\t\t\tTotal Training Recognition Loss 0.503027 || Total Training Translation Loss 0.010135\n",
            "2025-07-10 14:17:51,798 Epoch 3542:\n",
            "\t\t\tTotal Training Recognition Loss 0.507403 || Total Training Translation Loss 0.009777\n",
            "2025-07-10 14:17:52,059 Epoch 3543:\n",
            "\t\t\tTotal Training Recognition Loss 0.346721 || Total Training Translation Loss 0.010451\n",
            "2025-07-10 14:17:52,303 Epoch 3544:\n",
            "\t\t\tTotal Training Recognition Loss 0.557665 || Total Training Translation Loss 0.010079\n",
            "2025-07-10 14:17:52,549 Epoch 3545:\n",
            "\t\t\tTotal Training Recognition Loss 0.293388 || Total Training Translation Loss 0.010228\n",
            "2025-07-10 14:17:52,802 Epoch 3546:\n",
            "\t\t\tTotal Training Recognition Loss 0.131873 || Total Training Translation Loss 0.009669\n",
            "2025-07-10 14:17:53,040 Epoch 3547:\n",
            "\t\t\tTotal Training Recognition Loss 0.299325 || Total Training Translation Loss 0.010240\n",
            "2025-07-10 14:17:53,283 Epoch 3548:\n",
            "\t\t\tTotal Training Recognition Loss 0.217021 || Total Training Translation Loss 0.008345\n",
            "2025-07-10 14:17:53,538 Epoch 3549:\n",
            "\t\t\tTotal Training Recognition Loss 0.372477 || Total Training Translation Loss 0.009547\n",
            "2025-07-10 14:17:53,769 Epoch 3550:\n",
            "\t\t\tTotal Training Recognition Loss 1.156130 || Total Training Translation Loss 0.007772\n",
            "2025-07-10 14:17:54,014 Epoch 3551:\n",
            "\t\t\tTotal Training Recognition Loss 0.172320 || Total Training Translation Loss 0.009155\n",
            "2025-07-10 14:17:54,269 Epoch 3552:\n",
            "\t\t\tTotal Training Recognition Loss 0.223405 || Total Training Translation Loss 0.009792\n",
            "2025-07-10 14:17:54,515 Epoch 3553:\n",
            "\t\t\tTotal Training Recognition Loss 0.584571 || Total Training Translation Loss 0.009287\n",
            "2025-07-10 14:17:54,764 Epoch 3554:\n",
            "\t\t\tTotal Training Recognition Loss 0.336944 || Total Training Translation Loss 0.010363\n",
            "2025-07-10 14:17:55,030 Epoch 3555:\n",
            "\t\t\tTotal Training Recognition Loss 0.916818 || Total Training Translation Loss 0.011203\n",
            "2025-07-10 14:17:55,277 Epoch 3556:\n",
            "\t\t\tTotal Training Recognition Loss 1.364103 || Total Training Translation Loss 0.012046\n",
            "2025-07-10 14:17:55,533 Epoch 3557:\n",
            "\t\t\tTotal Training Recognition Loss 0.172614 || Total Training Translation Loss 0.009616\n",
            "2025-07-10 14:17:55,783 Epoch 3558:\n",
            "\t\t\tTotal Training Recognition Loss 0.396869 || Total Training Translation Loss 0.009031\n",
            "2025-07-10 14:17:56,026 Epoch 3559:\n",
            "\t\t\tTotal Training Recognition Loss 0.609702 || Total Training Translation Loss 0.009895\n",
            "2025-07-10 14:17:56,276 Epoch 3560:\n",
            "\t\t\tTotal Training Recognition Loss 0.119550 || Total Training Translation Loss 0.010417\n",
            "2025-07-10 14:17:56,522 Epoch 3561:\n",
            "\t\t\tTotal Training Recognition Loss 0.454631 || Total Training Translation Loss 0.010446\n",
            "2025-07-10 14:17:56,760 Epoch 3562:\n",
            "\t\t\tTotal Training Recognition Loss 0.205299 || Total Training Translation Loss 0.011331\n",
            "2025-07-10 14:17:57,018 Epoch 3563:\n",
            "\t\t\tTotal Training Recognition Loss 0.188269 || Total Training Translation Loss 0.010251\n",
            "2025-07-10 14:17:57,230 Epoch 3564:\n",
            "\t\t\tTotal Training Recognition Loss 0.280747 || Total Training Translation Loss 0.009127\n",
            "2025-07-10 14:17:57,433 Epoch 3565:\n",
            "\t\t\tTotal Training Recognition Loss 0.217709 || Total Training Translation Loss 0.012227\n",
            "2025-07-10 14:17:57,642 Epoch 3566:\n",
            "\t\t\tTotal Training Recognition Loss 0.190921 || Total Training Translation Loss 0.011949\n",
            "2025-07-10 14:17:57,871 Epoch 3567:\n",
            "\t\t\tTotal Training Recognition Loss 0.204211 || Total Training Translation Loss 0.010515\n",
            "2025-07-10 14:17:58,073 Epoch 3568:\n",
            "\t\t\tTotal Training Recognition Loss 0.232703 || Total Training Translation Loss 0.010292\n",
            "2025-07-10 14:17:58,294 Epoch 3569:\n",
            "\t\t\tTotal Training Recognition Loss 0.238609 || Total Training Translation Loss 0.009934\n",
            "2025-07-10 14:17:58,495 Epoch 3570:\n",
            "\t\t\tTotal Training Recognition Loss 0.354480 || Total Training Translation Loss 0.009607\n",
            "2025-07-10 14:17:58,691 Epoch 3571:\n",
            "\t\t\tTotal Training Recognition Loss 0.132586 || Total Training Translation Loss 0.010402\n",
            "2025-07-10 14:17:58,892 Epoch 3572:\n",
            "\t\t\tTotal Training Recognition Loss 0.141488 || Total Training Translation Loss 0.010867\n",
            "2025-07-10 14:17:59,091 Epoch 3573:\n",
            "\t\t\tTotal Training Recognition Loss 0.472288 || Total Training Translation Loss 0.013138\n",
            "2025-07-10 14:17:59,297 Epoch 3574:\n",
            "\t\t\tTotal Training Recognition Loss 0.198528 || Total Training Translation Loss 0.009947\n",
            "2025-07-10 14:17:59,497 Epoch 3575:\n",
            "\t\t\tTotal Training Recognition Loss 0.123057 || Total Training Translation Loss 0.011689\n",
            "2025-07-10 14:17:59,711 Epoch 3576:\n",
            "\t\t\tTotal Training Recognition Loss 25.206848 || Total Training Translation Loss 0.009313\n",
            "2025-07-10 14:17:59,933 Epoch 3577:\n",
            "\t\t\tTotal Training Recognition Loss 0.387932 || Total Training Translation Loss 0.009216\n",
            "2025-07-10 14:18:00,140 Epoch 3578:\n",
            "\t\t\tTotal Training Recognition Loss 0.136983 || Total Training Translation Loss 0.010933\n",
            "2025-07-10 14:18:00,338 Epoch 3579:\n",
            "\t\t\tTotal Training Recognition Loss 0.247189 || Total Training Translation Loss 0.009616\n",
            "2025-07-10 14:18:00,546 Epoch 3580:\n",
            "\t\t\tTotal Training Recognition Loss 0.319464 || Total Training Translation Loss 0.011944\n",
            "2025-07-10 14:18:00,751 Epoch 3581:\n",
            "\t\t\tTotal Training Recognition Loss 0.141143 || Total Training Translation Loss 0.010733\n",
            "2025-07-10 14:18:00,952 Epoch 3582:\n",
            "\t\t\tTotal Training Recognition Loss 0.370781 || Total Training Translation Loss 0.011158\n",
            "2025-07-10 14:18:01,151 Epoch 3583:\n",
            "\t\t\tTotal Training Recognition Loss 0.235360 || Total Training Translation Loss 0.009879\n",
            "2025-07-10 14:18:01,369 Epoch 3584:\n",
            "\t\t\tTotal Training Recognition Loss 0.197956 || Total Training Translation Loss 0.008795\n",
            "2025-07-10 14:18:01,576 Epoch 3585:\n",
            "\t\t\tTotal Training Recognition Loss 0.208462 || Total Training Translation Loss 0.010315\n",
            "2025-07-10 14:18:01,785 Epoch 3586:\n",
            "\t\t\tTotal Training Recognition Loss 0.471080 || Total Training Translation Loss 0.010709\n",
            "2025-07-10 14:18:01,989 Epoch 3587:\n",
            "\t\t\tTotal Training Recognition Loss 0.201619 || Total Training Translation Loss 0.011394\n",
            "2025-07-10 14:18:02,193 Epoch 3588:\n",
            "\t\t\tTotal Training Recognition Loss 0.198474 || Total Training Translation Loss 0.009740\n",
            "2025-07-10 14:18:02,400 Epoch 3589:\n",
            "\t\t\tTotal Training Recognition Loss 4.427210 || Total Training Translation Loss 0.009546\n",
            "2025-07-10 14:18:02,597 Epoch 3590:\n",
            "\t\t\tTotal Training Recognition Loss 0.277288 || Total Training Translation Loss 0.011275\n",
            "2025-07-10 14:18:02,811 Epoch 3591:\n",
            "\t\t\tTotal Training Recognition Loss 0.255213 || Total Training Translation Loss 0.009417\n",
            "2025-07-10 14:18:03,008 Epoch 3592:\n",
            "\t\t\tTotal Training Recognition Loss 49.067623 || Total Training Translation Loss 0.011392\n",
            "2025-07-10 14:18:03,222 Epoch 3593:\n",
            "\t\t\tTotal Training Recognition Loss 0.205619 || Total Training Translation Loss 0.008930\n",
            "2025-07-10 14:18:03,423 Epoch 3594:\n",
            "\t\t\tTotal Training Recognition Loss 0.170046 || Total Training Translation Loss 0.010389\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-07-10 14:18:03,628 Epoch 3595:\n",
            "\t\t\tTotal Training Recognition Loss 0.184432 || Total Training Translation Loss 0.010228\n",
            "2025-07-10 14:18:03,846 Epoch 3596:\n",
            "\t\t\tTotal Training Recognition Loss 0.139827 || Total Training Translation Loss 0.009981\n",
            "2025-07-10 14:18:04,044 Epoch 3597:\n",
            "\t\t\tTotal Training Recognition Loss 0.359501 || Total Training Translation Loss 0.010680\n",
            "2025-07-10 14:18:04,250 Epoch 3598:\n",
            "\t\t\tTotal Training Recognition Loss 0.163679 || Total Training Translation Loss 0.008398\n",
            "2025-07-10 14:18:04,456 Epoch 3599:\n",
            "\t\t\tTotal Training Recognition Loss 0.691767 || Total Training Translation Loss 0.010296\n",
            "2025-07-10 14:18:04,656 [Epoch: 3600 Step: 00003600] Batch Recognition Loss:   3.217415 => Gls Tokens per Sec:      186 || Batch Translation Loss:   0.011225 => Txt Tokens per Sec:      504 || Lr: 0.000490\n",
            "2025-07-10 14:18:04,916 Validation result at epoch 3600, step     3600: duration: 0.2598s\n",
            "\tRecognition Beam Size: 1\tTranslation Beam Size: 1\tTranslation Beam Alpha: -1\n",
            "\tRecognition Loss: 8310.11621\tTranslation Loss: 270.50397\tPPL: 24.10452\n",
            "\tEval Metric: BLEU\n",
            "\tWER 100.00\t(DEL: 37.14,\tINS: 17.14,\tSUB: 45.71)\n",
            "\tBLEU-4 2.83\t(BLEU-1: 7.33,\tBLEU-2: 4.50,\tBLEU-3: 3.51,\tBLEU-4: 2.83)\n",
            "\tCHRF 24.16\tROUGE 11.52\tFID 0.00\n",
            "2025-07-10 14:18:04,917 Logging Recognition and Translation Outputs\n",
            "2025-07-10 14:18:04,917 ========================================================================================\n",
            "2025-07-10 14:18:04,918 Logging Sequence: 11August_2010_Wednesday_tagesschau-2\n",
            "2025-07-10 14:18:04,918 \tGloss Reference :\tDRUCK TIEF KOMMEN  \n",
            "2025-07-10 14:18:04,918 \tGloss Hypothesis:\t***** **** NORDWEST\n",
            "2025-07-10 14:18:04,919 \tGloss Alignment :\tD     D    S       \n",
            "2025-07-10 14:18:04,919 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:18:04,922 \tText Reference  :\t*** *** tiefer           luftdruck bestimmt in         den ******** ****** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** nächsten tagen unser wetter\n",
            "2025-07-10 14:18:04,922 \tText Hypothesis :\tnun die wettervorhersage für       morgen   donnerstag den zwölften august <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>    <unk> <unk> <unk> \n",
            "2025-07-10 14:18:04,923 \tText Alignment  :\tI   I   S                S         S        S              I        I      I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     S        S     S     S     \n",
            "2025-07-10 14:18:04,923 ========================================================================================\n",
            "2025-07-10 14:18:04,923 Logging Sequence: 11August_2010_Wednesday_tagesschau-3\n",
            "2025-07-10 14:18:04,924 \tGloss Reference :\tES-BEDEUTET VIEL WOLKE UND   KOENNEN REGEN GEWITTER KOENNEN\n",
            "2025-07-10 14:18:04,924 \tGloss Hypothesis:\t*********** **** WOLKE HEUTE LOCH    HEUTE WOLKE    KOENNEN\n",
            "2025-07-10 14:18:04,925 \tGloss Alignment :\tD           D          S     S       S     S               \n",
            "2025-07-10 14:18:04,925 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:18:04,929 \tText Reference  :\t*** *** **************** *** ****** ********** *** ******** ****** ***** ***** ***** ***** ***** ***** ***** ***** das   bedeutet viele wolken und   immer wieder zum   teil  kräftige schauer und   gewitter\n",
            "2025-07-10 14:18:04,929 \tText Hypothesis :\tnun die wettervorhersage für morgen donnerstag den zwölften august <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>    <unk> <unk>  <unk> <unk> <unk>  <unk> <unk> <unk>    <unk>   <unk> <unk>   \n",
            "2025-07-10 14:18:04,929 \tText Alignment  :\tI   I   I                I   I      I          I   I        I      I     I     I     I     I     I     I     I     S     S        S     S      S     S     S      S     S     S        S       S     S       \n",
            "2025-07-10 14:18:04,930 ========================================================================================\n",
            "2025-07-10 14:18:04,930 Logging Sequence: 11August_2010_Wednesday_tagesschau-8\n",
            "2025-07-10 14:18:04,931 \tGloss Reference :\t***** ***** **** ******** ******* WIND    MAESSIG SCHWACH REGION WENN  GEWITTER WIND  KOENNEN\n",
            "2025-07-10 14:18:04,932 \tGloss Hypothesis:\tWOLKE DURCH LOCH NORDWEST TROCKEN BLEIBEN TROCKEN KOENNEN WOLKE  REGEN KOENNEN  WOLKE KOENNEN\n",
            "2025-07-10 14:18:04,932 \tGloss Alignment :\tI     I     I    I        I       S       S       S       S      S     S        S            \n",
            "2025-07-10 14:18:04,932 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:18:04,937 \tText Reference  :\t****** ** **** ****** *** ******** **** ****** **************** **** ****** ******* meist weht  nur   ein   schwacher wind  aus   unterschiedlichen richtungen der   bei   schauern und   gewittern stark böig  sein  kann \n",
            "2025-07-10 14:18:04,939 \tText Hypothesis :\tregnet es auch länger und ergiebig auch lokale überschwemmungen sind wieder möglich <unk> <unk> <unk> <unk> <unk>     <unk> <unk> <unk>             <unk>      <unk> <unk> <unk>    <unk> <unk>     <unk> <unk> <unk> <unk>\n",
            "2025-07-10 14:18:04,940 \tText Alignment  :\tI      I  I    I      I   I        I    I      I                I    I      I       S     S     S     S     S         S     S     S                 S          S     S     S        S     S         S     S     S     S    \n",
            "2025-07-10 14:18:04,940 ========================================================================================\n",
            "2025-07-10 14:18:04,940 Logging Sequence: 25October_2010_Monday_tagesschau-22\n",
            "2025-07-10 14:18:04,941 \tGloss Reference :\tMITTWOCH REGEN KOENNEN NORDWEST WAHRSCHEINLICH NORD STARK WIND    \n",
            "2025-07-10 14:18:04,941 \tGloss Hypothesis:\t******** ***** ******* ******** ************** **** ***** GEWITTER\n",
            "2025-07-10 14:18:04,941 \tGloss Alignment :\tD        D     D       D        D              D    D     S       \n",
            "2025-07-10 14:18:04,941 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:18:04,945 \tText Reference  :\tam *** **** ** ********** *** **** ******** ******* mittwoch hier     und ** ******* ******** ****** ********* *********** ***** ***** da    nieselregen in    der   nordwesthälfte an    den   küsten kräftiger wind \n",
            "2025-07-10 14:18:04,946 \tText Hypothesis :\tam tag gibt es verbreitet zum teil kräftige schauer oder     gewitter und in manchen regionen fallen ergiebige regenmengen <unk> <unk> <unk> <unk>       <unk> <unk> <unk>          <unk> <unk> <unk>  <unk>     <unk>\n",
            "2025-07-10 14:18:04,946 \tText Alignment  :\t   I   I    I  I          I   I    I        I       S        S            I  I       I        I      I         I           I     I     S     S           S     S     S              S     S     S      S         S    \n",
            "2025-07-10 14:18:04,946 ========================================================================================\n",
            "2025-07-10 14:18:04,946 Logging Sequence: 05May_2011_Thursday_tagesschau-25\n",
            "2025-07-10 14:18:04,947 \tGloss Reference :\t****** JETZT WETTER WIE-AUSSEHEN MORGEN FREITAG SECHSTE    MAI ZEIGEN-BILDSCHIRM\n",
            "2025-07-10 14:18:04,947 \tGloss Hypothesis:\tZWOELF JETZT WETTER ************ MORGEN ******* DONNERSTAG OFT ZWOELF           \n",
            "2025-07-10 14:18:04,948 \tGloss Alignment :\tI                   D                   D       S          S   S                \n",
            "2025-07-10 14:18:04,948 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:18:04,951 \tText Reference  :\tund nun die wettervorhersage für morgen freitag    den ******** ****** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** sechsten mai  \n",
            "2025-07-10 14:18:04,951 \tText Hypothesis :\t*** nun die wettervorhersage für morgen donnerstag den zwölften august <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>    <unk>\n",
            "2025-07-10 14:18:04,951 \tText Alignment  :\tD                                       S              I        I      I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     S        S    \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-07-10 14:18:04,951 ========================================================================================\n",
            "2025-07-10 14:18:04,953 Epoch 3600:\n",
            "\t\t\tTotal Training Recognition Loss 3.217415 || Total Training Translation Loss 0.011225\n",
            "2025-07-10 14:18:05,201 Epoch 3601:\n",
            "\t\t\tTotal Training Recognition Loss 0.345638 || Total Training Translation Loss 0.009142\n",
            "2025-07-10 14:18:05,453 Epoch 3602:\n",
            "\t\t\tTotal Training Recognition Loss 5.104355 || Total Training Translation Loss 0.009817\n",
            "2025-07-10 14:18:05,696 Epoch 3603:\n",
            "\t\t\tTotal Training Recognition Loss 0.203595 || Total Training Translation Loss 0.008629\n",
            "2025-07-10 14:18:05,941 Epoch 3604:\n",
            "\t\t\tTotal Training Recognition Loss 1.831303 || Total Training Translation Loss 0.010045\n",
            "2025-07-10 14:18:06,182 Epoch 3605:\n",
            "\t\t\tTotal Training Recognition Loss 0.400328 || Total Training Translation Loss 0.010577\n",
            "2025-07-10 14:18:06,428 Epoch 3606:\n",
            "\t\t\tTotal Training Recognition Loss 0.206997 || Total Training Translation Loss 0.008838\n",
            "2025-07-10 14:18:06,685 Epoch 3607:\n",
            "\t\t\tTotal Training Recognition Loss 0.273924 || Total Training Translation Loss 0.009659\n",
            "2025-07-10 14:18:06,929 Epoch 3608:\n",
            "\t\t\tTotal Training Recognition Loss 0.376836 || Total Training Translation Loss 0.009636\n",
            "2025-07-10 14:18:07,177 Epoch 3609:\n",
            "\t\t\tTotal Training Recognition Loss 2.627692 || Total Training Translation Loss 0.009284\n",
            "2025-07-10 14:18:07,415 Epoch 3610:\n",
            "\t\t\tTotal Training Recognition Loss 0.155871 || Total Training Translation Loss 0.009050\n",
            "2025-07-10 14:18:07,615 Epoch 3611:\n",
            "\t\t\tTotal Training Recognition Loss 0.490272 || Total Training Translation Loss 0.010336\n",
            "2025-07-10 14:18:07,821 Epoch 3612:\n",
            "\t\t\tTotal Training Recognition Loss 0.326571 || Total Training Translation Loss 0.011358\n",
            "2025-07-10 14:18:08,034 Epoch 3613:\n",
            "\t\t\tTotal Training Recognition Loss 0.235074 || Total Training Translation Loss 0.010196\n",
            "2025-07-10 14:18:08,242 Epoch 3614:\n",
            "\t\t\tTotal Training Recognition Loss 0.192573 || Total Training Translation Loss 0.008943\n",
            "2025-07-10 14:18:08,492 Epoch 3615:\n",
            "\t\t\tTotal Training Recognition Loss 0.639607 || Total Training Translation Loss 0.010464\n",
            "2025-07-10 14:18:08,735 Epoch 3616:\n",
            "\t\t\tTotal Training Recognition Loss 0.198108 || Total Training Translation Loss 0.010661\n",
            "2025-07-10 14:18:08,983 Epoch 3617:\n",
            "\t\t\tTotal Training Recognition Loss 0.190201 || Total Training Translation Loss 0.011084\n",
            "2025-07-10 14:18:09,226 Epoch 3618:\n",
            "\t\t\tTotal Training Recognition Loss 27.802517 || Total Training Translation Loss 0.011045\n",
            "2025-07-10 14:18:09,472 Epoch 3619:\n",
            "\t\t\tTotal Training Recognition Loss 0.349274 || Total Training Translation Loss 0.010867\n",
            "2025-07-10 14:18:09,717 Epoch 3620:\n",
            "\t\t\tTotal Training Recognition Loss 0.298532 || Total Training Translation Loss 0.010785\n",
            "2025-07-10 14:18:09,970 Epoch 3621:\n",
            "\t\t\tTotal Training Recognition Loss 0.467123 || Total Training Translation Loss 0.012458\n",
            "2025-07-10 14:18:10,217 Epoch 3622:\n",
            "\t\t\tTotal Training Recognition Loss 0.422537 || Total Training Translation Loss 0.009645\n",
            "2025-07-10 14:18:10,461 Epoch 3623:\n",
            "\t\t\tTotal Training Recognition Loss 0.186085 || Total Training Translation Loss 0.009414\n",
            "2025-07-10 14:18:10,714 Epoch 3624:\n",
            "\t\t\tTotal Training Recognition Loss 0.344895 || Total Training Translation Loss 0.011384\n",
            "2025-07-10 14:18:10,963 Epoch 3625:\n",
            "\t\t\tTotal Training Recognition Loss 0.196624 || Total Training Translation Loss 0.010696\n",
            "2025-07-10 14:18:11,206 Epoch 3626:\n",
            "\t\t\tTotal Training Recognition Loss 0.492845 || Total Training Translation Loss 0.010426\n",
            "2025-07-10 14:18:11,457 Epoch 3627:\n",
            "\t\t\tTotal Training Recognition Loss 2.189518 || Total Training Translation Loss 0.011089\n",
            "2025-07-10 14:18:11,674 Epoch 3628:\n",
            "\t\t\tTotal Training Recognition Loss 0.779483 || Total Training Translation Loss 0.010597\n",
            "2025-07-10 14:18:11,920 Epoch 3629:\n",
            "\t\t\tTotal Training Recognition Loss 0.542481 || Total Training Translation Loss 0.009114\n",
            "2025-07-10 14:18:12,157 Epoch 3630:\n",
            "\t\t\tTotal Training Recognition Loss 2.964143 || Total Training Translation Loss 0.012197\n",
            "2025-07-10 14:18:12,403 Epoch 3631:\n",
            "\t\t\tTotal Training Recognition Loss 0.166640 || Total Training Translation Loss 0.010303\n",
            "2025-07-10 14:18:12,650 Epoch 3632:\n",
            "\t\t\tTotal Training Recognition Loss 0.332871 || Total Training Translation Loss 0.011773\n",
            "2025-07-10 14:18:12,903 Epoch 3633:\n",
            "\t\t\tTotal Training Recognition Loss 0.383506 || Total Training Translation Loss 0.010258\n",
            "2025-07-10 14:18:13,179 Epoch 3634:\n",
            "\t\t\tTotal Training Recognition Loss 1.336660 || Total Training Translation Loss 0.013032\n",
            "2025-07-10 14:18:13,382 Epoch 3635:\n",
            "\t\t\tTotal Training Recognition Loss 1.812455 || Total Training Translation Loss 0.008872\n",
            "2025-07-10 14:18:13,585 Epoch 3636:\n",
            "\t\t\tTotal Training Recognition Loss 2.015596 || Total Training Translation Loss 0.011313\n",
            "2025-07-10 14:18:13,780 Epoch 3637:\n",
            "\t\t\tTotal Training Recognition Loss 68.233200 || Total Training Translation Loss 0.009870\n",
            "2025-07-10 14:18:13,989 Epoch 3638:\n",
            "\t\t\tTotal Training Recognition Loss 0.483360 || Total Training Translation Loss 0.009548\n",
            "2025-07-10 14:18:14,193 Epoch 3639:\n",
            "\t\t\tTotal Training Recognition Loss 0.191778 || Total Training Translation Loss 0.009486\n",
            "2025-07-10 14:18:14,396 Epoch 3640:\n",
            "\t\t\tTotal Training Recognition Loss 0.166932 || Total Training Translation Loss 0.008760\n",
            "2025-07-10 14:18:14,614 Epoch 3641:\n",
            "\t\t\tTotal Training Recognition Loss 1.881323 || Total Training Translation Loss 0.010587\n",
            "2025-07-10 14:18:14,818 Epoch 3642:\n",
            "\t\t\tTotal Training Recognition Loss 0.870769 || Total Training Translation Loss 0.009655\n",
            "2025-07-10 14:18:15,020 Epoch 3643:\n",
            "\t\t\tTotal Training Recognition Loss 0.236725 || Total Training Translation Loss 0.011551\n",
            "2025-07-10 14:18:15,215 Epoch 3644:\n",
            "\t\t\tTotal Training Recognition Loss 0.184597 || Total Training Translation Loss 0.008648\n",
            "2025-07-10 14:18:15,426 Epoch 3645:\n",
            "\t\t\tTotal Training Recognition Loss 0.196115 || Total Training Translation Loss 0.011971\n",
            "2025-07-10 14:18:15,635 Epoch 3646:\n",
            "\t\t\tTotal Training Recognition Loss 0.349266 || Total Training Translation Loss 0.010721\n",
            "2025-07-10 14:18:15,839 Epoch 3647:\n",
            "\t\t\tTotal Training Recognition Loss 0.213038 || Total Training Translation Loss 0.008873\n",
            "2025-07-10 14:18:16,044 Epoch 3648:\n",
            "\t\t\tTotal Training Recognition Loss 0.158005 || Total Training Translation Loss 0.012164\n",
            "2025-07-10 14:18:16,271 Epoch 3649:\n",
            "\t\t\tTotal Training Recognition Loss 0.129841 || Total Training Translation Loss 0.009564\n",
            "2025-07-10 14:18:16,476 Epoch 3650:\n",
            "\t\t\tTotal Training Recognition Loss 0.825635 || Total Training Translation Loss 0.011437\n",
            "2025-07-10 14:18:16,676 Epoch 3651:\n",
            "\t\t\tTotal Training Recognition Loss 0.287584 || Total Training Translation Loss 0.009100\n",
            "2025-07-10 14:18:16,881 Epoch 3652:\n",
            "\t\t\tTotal Training Recognition Loss 0.265004 || Total Training Translation Loss 0.008897\n",
            "2025-07-10 14:18:17,072 Epoch 3653:\n",
            "\t\t\tTotal Training Recognition Loss 0.164761 || Total Training Translation Loss 0.010729\n",
            "2025-07-10 14:18:17,268 Epoch 3654:\n",
            "\t\t\tTotal Training Recognition Loss 0.212637 || Total Training Translation Loss 0.009744\n",
            "2025-07-10 14:18:17,480 Epoch 3655:\n",
            "\t\t\tTotal Training Recognition Loss 0.168802 || Total Training Translation Loss 0.008376\n",
            "2025-07-10 14:18:17,683 Epoch 3656:\n",
            "\t\t\tTotal Training Recognition Loss 1.674834 || Total Training Translation Loss 0.010475\n",
            "2025-07-10 14:18:17,884 Epoch 3657:\n",
            "\t\t\tTotal Training Recognition Loss 1.571749 || Total Training Translation Loss 0.009021\n",
            "2025-07-10 14:18:18,086 Epoch 3658:\n",
            "\t\t\tTotal Training Recognition Loss 0.588774 || Total Training Translation Loss 0.010390\n",
            "2025-07-10 14:18:18,288 Epoch 3659:\n",
            "\t\t\tTotal Training Recognition Loss 0.259708 || Total Training Translation Loss 0.011369\n",
            "2025-07-10 14:18:18,481 Epoch 3660:\n",
            "\t\t\tTotal Training Recognition Loss 0.240816 || Total Training Translation Loss 0.012633\n",
            "2025-07-10 14:18:18,681 Epoch 3661:\n",
            "\t\t\tTotal Training Recognition Loss 0.109478 || Total Training Translation Loss 0.009876\n",
            "2025-07-10 14:18:18,885 Epoch 3662:\n",
            "\t\t\tTotal Training Recognition Loss 0.189908 || Total Training Translation Loss 0.011173\n",
            "2025-07-10 14:18:19,095 Epoch 3663:\n",
            "\t\t\tTotal Training Recognition Loss 12.923164 || Total Training Translation Loss 0.011133\n",
            "2025-07-10 14:18:19,291 Epoch 3664:\n",
            "\t\t\tTotal Training Recognition Loss 0.215116 || Total Training Translation Loss 0.008566\n",
            "2025-07-10 14:18:19,489 Epoch 3665:\n",
            "\t\t\tTotal Training Recognition Loss 0.439239 || Total Training Translation Loss 0.011886\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-07-10 14:18:19,727 Epoch 3666:\n",
            "\t\t\tTotal Training Recognition Loss 0.278281 || Total Training Translation Loss 0.011617\n",
            "2025-07-10 14:18:19,975 Epoch 3667:\n",
            "\t\t\tTotal Training Recognition Loss 20.577009 || Total Training Translation Loss 0.011283\n",
            "2025-07-10 14:18:20,197 Epoch 3668:\n",
            "\t\t\tTotal Training Recognition Loss 0.247498 || Total Training Translation Loss 0.011746\n",
            "2025-07-10 14:18:20,396 Epoch 3669:\n",
            "\t\t\tTotal Training Recognition Loss 0.293406 || Total Training Translation Loss 0.010896\n",
            "2025-07-10 14:18:20,588 Epoch 3670:\n",
            "\t\t\tTotal Training Recognition Loss 0.526654 || Total Training Translation Loss 0.010094\n",
            "2025-07-10 14:18:20,787 Epoch 3671:\n",
            "\t\t\tTotal Training Recognition Loss 0.419435 || Total Training Translation Loss 0.010495\n",
            "2025-07-10 14:18:20,984 Epoch 3672:\n",
            "\t\t\tTotal Training Recognition Loss 0.743495 || Total Training Translation Loss 0.011796\n",
            "2025-07-10 14:18:21,184 Epoch 3673:\n",
            "\t\t\tTotal Training Recognition Loss 0.793221 || Total Training Translation Loss 0.009773\n",
            "2025-07-10 14:18:21,427 Epoch 3674:\n",
            "\t\t\tTotal Training Recognition Loss 1.704942 || Total Training Translation Loss 0.009229\n",
            "2025-07-10 14:18:21,666 Epoch 3675:\n",
            "\t\t\tTotal Training Recognition Loss 2.684649 || Total Training Translation Loss 0.011354\n",
            "2025-07-10 14:18:21,909 Epoch 3676:\n",
            "\t\t\tTotal Training Recognition Loss 0.786729 || Total Training Translation Loss 0.011877\n",
            "2025-07-10 14:18:22,147 Epoch 3677:\n",
            "\t\t\tTotal Training Recognition Loss 1.842195 || Total Training Translation Loss 0.009850\n",
            "2025-07-10 14:18:22,388 Epoch 3678:\n",
            "\t\t\tTotal Training Recognition Loss 6.537276 || Total Training Translation Loss 0.008892\n",
            "2025-07-10 14:18:22,629 Epoch 3679:\n",
            "\t\t\tTotal Training Recognition Loss 0.751067 || Total Training Translation Loss 0.010020\n",
            "2025-07-10 14:18:22,866 Epoch 3680:\n",
            "\t\t\tTotal Training Recognition Loss 0.998802 || Total Training Translation Loss 0.011076\n",
            "2025-07-10 14:18:23,116 Epoch 3681:\n",
            "\t\t\tTotal Training Recognition Loss 6.180758 || Total Training Translation Loss 0.010828\n",
            "2025-07-10 14:18:23,354 Epoch 3682:\n",
            "\t\t\tTotal Training Recognition Loss 0.603632 || Total Training Translation Loss 0.009757\n",
            "2025-07-10 14:18:23,601 Epoch 3683:\n",
            "\t\t\tTotal Training Recognition Loss 0.432071 || Total Training Translation Loss 0.010245\n",
            "2025-07-10 14:18:23,846 Epoch 3684:\n",
            "\t\t\tTotal Training Recognition Loss 1.295707 || Total Training Translation Loss 0.009218\n",
            "2025-07-10 14:18:24,080 Epoch 3685:\n",
            "\t\t\tTotal Training Recognition Loss 1.197335 || Total Training Translation Loss 0.010864\n",
            "2025-07-10 14:18:24,321 Epoch 3686:\n",
            "\t\t\tTotal Training Recognition Loss 2.611349 || Total Training Translation Loss 0.010330\n",
            "2025-07-10 14:18:24,557 Epoch 3687:\n",
            "\t\t\tTotal Training Recognition Loss 0.320002 || Total Training Translation Loss 0.011876\n",
            "2025-07-10 14:18:24,794 Epoch 3688:\n",
            "\t\t\tTotal Training Recognition Loss 0.840624 || Total Training Translation Loss 0.009204\n",
            "2025-07-10 14:18:25,037 Epoch 3689:\n",
            "\t\t\tTotal Training Recognition Loss 10.572026 || Total Training Translation Loss 0.009757\n",
            "2025-07-10 14:18:25,279 Epoch 3690:\n",
            "\t\t\tTotal Training Recognition Loss 0.285569 || Total Training Translation Loss 0.012402\n",
            "2025-07-10 14:18:25,520 Epoch 3691:\n",
            "\t\t\tTotal Training Recognition Loss 0.970407 || Total Training Translation Loss 0.010896\n",
            "2025-07-10 14:18:25,746 Epoch 3692:\n",
            "\t\t\tTotal Training Recognition Loss 0.473812 || Total Training Translation Loss 0.010543\n",
            "2025-07-10 14:18:25,989 Epoch 3693:\n",
            "\t\t\tTotal Training Recognition Loss 2.236531 || Total Training Translation Loss 0.009596\n",
            "2025-07-10 14:18:26,219 Epoch 3694:\n",
            "\t\t\tTotal Training Recognition Loss 0.392503 || Total Training Translation Loss 0.009027\n",
            "2025-07-10 14:18:26,457 Epoch 3695:\n",
            "\t\t\tTotal Training Recognition Loss 0.349309 || Total Training Translation Loss 0.011095\n",
            "2025-07-10 14:18:26,686 Epoch 3696:\n",
            "\t\t\tTotal Training Recognition Loss 1.789551 || Total Training Translation Loss 0.008494\n",
            "2025-07-10 14:18:26,932 Epoch 3697:\n",
            "\t\t\tTotal Training Recognition Loss 0.173360 || Total Training Translation Loss 0.008802\n",
            "2025-07-10 14:18:27,168 Epoch 3698:\n",
            "\t\t\tTotal Training Recognition Loss 0.250623 || Total Training Translation Loss 0.009608\n",
            "2025-07-10 14:18:27,406 Epoch 3699:\n",
            "\t\t\tTotal Training Recognition Loss 0.160916 || Total Training Translation Loss 0.009894\n",
            "2025-07-10 14:18:27,651 [Epoch: 3700 Step: 00003700] Batch Recognition Loss:   0.246305 => Gls Tokens per Sec:      153 || Batch Translation Loss:   0.008710 => Txt Tokens per Sec:      413 || Lr: 0.000490\n",
            "2025-07-10 14:18:27,982 Validation result at epoch 3700, step     3700: duration: 0.3303s\n",
            "\tRecognition Beam Size: 1\tTranslation Beam Size: 1\tTranslation Beam Alpha: -1\n",
            "\tRecognition Loss: 7029.07275\tTranslation Loss: 270.37634\tPPL: 24.06836\n",
            "\tEval Metric: BLEU\n",
            "\tWER 91.43\t(DEL: 34.29,\tINS: 11.43,\tSUB: 45.71)\n",
            "\tBLEU-4 2.61\t(BLEU-1: 5.33,\tBLEU-2: 3.84,\tBLEU-3: 3.16,\tBLEU-4: 2.61)\n",
            "\tCHRF 21.21\tROUGE 8.70\tFID 0.00\n",
            "2025-07-10 14:18:27,982 Logging Recognition and Translation Outputs\n",
            "2025-07-10 14:18:27,983 ========================================================================================\n",
            "2025-07-10 14:18:27,983 Logging Sequence: 11August_2010_Wednesday_tagesschau-2\n",
            "2025-07-10 14:18:27,984 \tGloss Reference :\tDRUCK    TIEF    KOMMEN  \n",
            "2025-07-10 14:18:27,984 \tGloss Hypothesis:\tNORDWEST FEBRUAR NORDWEST\n",
            "2025-07-10 14:18:27,985 \tGloss Alignment :\tS        S       S       \n",
            "2025-07-10 14:18:27,985 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:18:27,988 \tText Reference  :\t*** *** tiefer           luftdruck bestimmt in         den ******** ****** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** nächsten tagen unser wetter\n",
            "2025-07-10 14:18:27,988 \tText Hypothesis :\tnun die wettervorhersage für       morgen   donnerstag den zwölften august <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>    <unk> <unk> <unk> \n",
            "2025-07-10 14:18:27,988 \tText Alignment  :\tI   I   S                S         S        S              I        I      I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     S        S     S     S     \n",
            "2025-07-10 14:18:27,989 ========================================================================================\n",
            "2025-07-10 14:18:27,989 Logging Sequence: 11August_2010_Wednesday_tagesschau-3\n",
            "2025-07-10 14:18:27,990 \tGloss Reference :\tES-BEDEUTET VIEL WOLKE UND        KOENNEN REGEN GEWITTER KOENNEN\n",
            "2025-07-10 14:18:27,990 \tGloss Hypothesis:\t*********** **** WOLKE DONNERSTAG HEUTE   LOCH  HEUTE    KOENNEN\n",
            "2025-07-10 14:18:27,990 \tGloss Alignment :\tD           D          S          S       S     S               \n",
            "2025-07-10 14:18:27,990 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:18:27,994 \tText Reference  :\t*** *** **************** *** ****** ********** *** ******** ****** ***** ***** ***** ***** ***** ***** ***** ***** das   bedeutet viele wolken und   immer wieder zum   teil  kräftige schauer und   gewitter\n",
            "2025-07-10 14:18:27,994 \tText Hypothesis :\tnun die wettervorhersage für morgen donnerstag den zwölften august <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>    <unk> <unk>  <unk> <unk> <unk>  <unk> <unk> <unk>    <unk>   <unk> <unk>   \n",
            "2025-07-10 14:18:27,995 \tText Alignment  :\tI   I   I                I   I      I          I   I        I      I     I     I     I     I     I     I     I     S     S        S     S      S     S     S      S     S     S        S       S     S       \n",
            "2025-07-10 14:18:27,995 ========================================================================================\n",
            "2025-07-10 14:18:27,995 Logging Sequence: 11August_2010_Wednesday_tagesschau-8\n",
            "2025-07-10 14:18:27,996 \tGloss Reference :\t***** **** ***** WIND MAESSIG SCHWACH REGION WENN  GEWITTER WIND  KOENNEN\n",
            "2025-07-10 14:18:27,996 \tGloss Hypothesis:\tWOLKE VIEL DURCH LOCH TROCKEN BLEIBEN WOLKE  REGEN KOENNEN  WOLKE KOENNEN\n",
            "2025-07-10 14:18:27,997 \tGloss Alignment :\tI     I    I     S    S       S       S      S     S        S            \n",
            "2025-07-10 14:18:27,997 \t--------------------------------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-07-10 14:18:28,002 \tText Reference  :\t*** *** **************** *** ****** ********** *** ******** ****** ***** ***** ***** meist weht  nur   ein   schwacher wind  aus   unterschiedlichen richtungen der   bei   schauern und   gewittern stark böig  sein  kann \n",
            "2025-07-10 14:18:28,002 \tText Hypothesis :\tnun die wettervorhersage für morgen donnerstag den zwölften august <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>     <unk> <unk> <unk>             <unk>      <unk> <unk> <unk>    <unk> <unk>     <unk> <unk> <unk> <unk>\n",
            "2025-07-10 14:18:28,002 \tText Alignment  :\tI   I   I                I   I      I          I   I        I      I     I     I     S     S     S     S     S         S     S     S                 S          S     S     S        S     S         S     S     S     S    \n",
            "2025-07-10 14:18:28,003 ========================================================================================\n",
            "2025-07-10 14:18:28,003 Logging Sequence: 25October_2010_Monday_tagesschau-22\n",
            "2025-07-10 14:18:28,003 \tGloss Reference :\tMITTWOCH REGEN KOENNEN NORDWEST WAHRSCHEINLICH NORD STARK WIND\n",
            "2025-07-10 14:18:28,005 \tGloss Hypothesis:\t******** ***** KOENNEN ******** ************** **** ***** ****\n",
            "2025-07-10 14:18:28,006 \tGloss Alignment :\tD        D             D        D              D    D     D   \n",
            "2025-07-10 14:18:28,006 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:18:28,010 \tText Reference  :\t*** *** **************** *** ****** ********** *** ******** ****** ***** ***** ***** ***** ***** ***** ***** am    mittwoch hier  und   da    nieselregen in    der   nordwesthälfte an    den   küsten kräftiger wind \n",
            "2025-07-10 14:18:28,010 \tText Hypothesis :\tnun die wettervorhersage für morgen donnerstag den zwölften august <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>    <unk> <unk> <unk> <unk>       <unk> <unk> <unk>          <unk> <unk> <unk>  <unk>     <unk>\n",
            "2025-07-10 14:18:28,011 \tText Alignment  :\tI   I   I                I   I      I          I   I        I      I     I     I     I     I     I     I     S     S        S     S     S     S           S     S     S              S     S     S      S         S    \n",
            "2025-07-10 14:18:28,011 ========================================================================================\n",
            "2025-07-10 14:18:28,011 Logging Sequence: 05May_2011_Thursday_tagesschau-25\n",
            "2025-07-10 14:18:28,012 \tGloss Reference :\t**** JETZT WETTER WIE-AUSSEHEN MORGEN FREITAG SECHSTE MAI        ZEIGEN-BILDSCHIRM\n",
            "2025-07-10 14:18:28,012 \tGloss Hypothesis:\tVIEL JETZT WETTER ************ MORGEN ******* ******* DONNERSTAG ZWOELF           \n",
            "2025-07-10 14:18:28,012 \tGloss Alignment :\tI                 D                   D       D       S          S                \n",
            "2025-07-10 14:18:28,012 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:18:28,015 \tText Reference  :\tund nun die wettervorhersage für morgen freitag    den ******** ****** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** sechsten mai  \n",
            "2025-07-10 14:18:28,015 \tText Hypothesis :\t*** nun die wettervorhersage für morgen donnerstag den zwölften august <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>    <unk>\n",
            "2025-07-10 14:18:28,016 \tText Alignment  :\tD                                       S              I        I      I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     S        S    \n",
            "2025-07-10 14:18:28,016 ========================================================================================\n",
            "2025-07-10 14:18:28,017 Epoch 3700:\n",
            "\t\t\tTotal Training Recognition Loss 0.246305 || Total Training Translation Loss 0.008710\n",
            "2025-07-10 14:18:28,243 Epoch 3701:\n",
            "\t\t\tTotal Training Recognition Loss 0.201032 || Total Training Translation Loss 0.008945\n",
            "2025-07-10 14:18:28,482 Epoch 3702:\n",
            "\t\t\tTotal Training Recognition Loss 0.961817 || Total Training Translation Loss 0.009859\n",
            "2025-07-10 14:18:28,719 Epoch 3703:\n",
            "\t\t\tTotal Training Recognition Loss 0.205862 || Total Training Translation Loss 0.011948\n",
            "2025-07-10 14:18:28,960 Epoch 3704:\n",
            "\t\t\tTotal Training Recognition Loss 16.806488 || Total Training Translation Loss 0.008823\n",
            "2025-07-10 14:18:29,197 Epoch 3705:\n",
            "\t\t\tTotal Training Recognition Loss 0.608095 || Total Training Translation Loss 0.008928\n",
            "2025-07-10 14:18:29,436 Epoch 3706:\n",
            "\t\t\tTotal Training Recognition Loss 0.380656 || Total Training Translation Loss 0.009438\n",
            "2025-07-10 14:18:29,685 Epoch 3707:\n",
            "\t\t\tTotal Training Recognition Loss 0.411114 || Total Training Translation Loss 0.010248\n",
            "2025-07-10 14:18:29,926 Epoch 3708:\n",
            "\t\t\tTotal Training Recognition Loss 0.252885 || Total Training Translation Loss 0.011641\n",
            "2025-07-10 14:18:30,174 Epoch 3709:\n",
            "\t\t\tTotal Training Recognition Loss 3.823072 || Total Training Translation Loss 0.008444\n",
            "2025-07-10 14:18:30,418 Epoch 3710:\n",
            "\t\t\tTotal Training Recognition Loss 0.332311 || Total Training Translation Loss 0.008321\n",
            "2025-07-10 14:18:30,653 Epoch 3711:\n",
            "\t\t\tTotal Training Recognition Loss 3.443303 || Total Training Translation Loss 0.010050\n",
            "2025-07-10 14:18:30,897 Epoch 3712:\n",
            "\t\t\tTotal Training Recognition Loss 1.163412 || Total Training Translation Loss 0.010858\n",
            "2025-07-10 14:18:31,139 Epoch 3713:\n",
            "\t\t\tTotal Training Recognition Loss 40.726421 || Total Training Translation Loss 0.009300\n",
            "2025-07-10 14:18:31,379 Epoch 3714:\n",
            "\t\t\tTotal Training Recognition Loss 2.464204 || Total Training Translation Loss 0.008881\n",
            "2025-07-10 14:18:31,622 Epoch 3715:\n",
            "\t\t\tTotal Training Recognition Loss 3.205394 || Total Training Translation Loss 0.008888\n",
            "2025-07-10 14:18:31,869 Epoch 3716:\n",
            "\t\t\tTotal Training Recognition Loss 0.206150 || Total Training Translation Loss 0.008699\n",
            "2025-07-10 14:18:32,115 Epoch 3717:\n",
            "\t\t\tTotal Training Recognition Loss 0.260839 || Total Training Translation Loss 0.010699\n",
            "2025-07-10 14:18:32,363 Epoch 3718:\n",
            "\t\t\tTotal Training Recognition Loss 0.516333 || Total Training Translation Loss 0.009811\n",
            "2025-07-10 14:18:32,596 Epoch 3719:\n",
            "\t\t\tTotal Training Recognition Loss 0.283595 || Total Training Translation Loss 0.011623\n",
            "2025-07-10 14:18:32,843 Epoch 3720:\n",
            "\t\t\tTotal Training Recognition Loss 1.022987 || Total Training Translation Loss 0.009824\n",
            "2025-07-10 14:18:33,090 Epoch 3721:\n",
            "\t\t\tTotal Training Recognition Loss 0.233522 || Total Training Translation Loss 0.009609\n",
            "2025-07-10 14:18:33,336 Epoch 3722:\n",
            "\t\t\tTotal Training Recognition Loss 31.799906 || Total Training Translation Loss 0.009654\n",
            "2025-07-10 14:18:33,583 Epoch 3723:\n",
            "\t\t\tTotal Training Recognition Loss 0.507169 || Total Training Translation Loss 0.010383\n",
            "2025-07-10 14:18:33,827 Epoch 3724:\n",
            "\t\t\tTotal Training Recognition Loss 0.344104 || Total Training Translation Loss 0.009787\n",
            "2025-07-10 14:18:34,074 Epoch 3725:\n",
            "\t\t\tTotal Training Recognition Loss 0.173176 || Total Training Translation Loss 0.011485\n",
            "2025-07-10 14:18:34,321 Epoch 3726:\n",
            "\t\t\tTotal Training Recognition Loss 0.313120 || Total Training Translation Loss 0.010223\n",
            "2025-07-10 14:18:34,562 Epoch 3727:\n",
            "\t\t\tTotal Training Recognition Loss 1.300092 || Total Training Translation Loss 0.010446\n",
            "2025-07-10 14:18:34,806 Epoch 3728:\n",
            "\t\t\tTotal Training Recognition Loss 0.315567 || Total Training Translation Loss 0.008923\n",
            "2025-07-10 14:18:35,051 Epoch 3729:\n",
            "\t\t\tTotal Training Recognition Loss 0.320623 || Total Training Translation Loss 0.010245\n",
            "2025-07-10 14:18:35,298 Epoch 3730:\n",
            "\t\t\tTotal Training Recognition Loss 0.441101 || Total Training Translation Loss 0.009070\n",
            "2025-07-10 14:18:35,541 Epoch 3731:\n",
            "\t\t\tTotal Training Recognition Loss 0.765950 || Total Training Translation Loss 0.009580\n",
            "2025-07-10 14:18:35,777 Epoch 3732:\n",
            "\t\t\tTotal Training Recognition Loss 5.130176 || Total Training Translation Loss 0.010684\n",
            "2025-07-10 14:18:36,021 Epoch 3733:\n",
            "\t\t\tTotal Training Recognition Loss 0.428050 || Total Training Translation Loss 0.008579\n",
            "2025-07-10 14:18:36,270 Epoch 3734:\n",
            "\t\t\tTotal Training Recognition Loss 2.231637 || Total Training Translation Loss 0.008540\n",
            "2025-07-10 14:18:36,511 Epoch 3735:\n",
            "\t\t\tTotal Training Recognition Loss 0.586330 || Total Training Translation Loss 0.009756\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-07-10 14:18:36,757 Epoch 3736:\n",
            "\t\t\tTotal Training Recognition Loss 3.113899 || Total Training Translation Loss 0.009565\n",
            "2025-07-10 14:18:37,004 Epoch 3737:\n",
            "\t\t\tTotal Training Recognition Loss 1.322556 || Total Training Translation Loss 0.009592\n",
            "2025-07-10 14:18:37,273 Epoch 3738:\n",
            "\t\t\tTotal Training Recognition Loss 1.320909 || Total Training Translation Loss 0.010593\n",
            "2025-07-10 14:18:37,484 Epoch 3739:\n",
            "\t\t\tTotal Training Recognition Loss 4.113191 || Total Training Translation Loss 0.010879\n",
            "2025-07-10 14:18:37,690 Epoch 3740:\n",
            "\t\t\tTotal Training Recognition Loss 1.359583 || Total Training Translation Loss 0.010392\n",
            "2025-07-10 14:18:37,897 Epoch 3741:\n",
            "\t\t\tTotal Training Recognition Loss 2.066995 || Total Training Translation Loss 0.009835\n",
            "2025-07-10 14:18:38,140 Epoch 3742:\n",
            "\t\t\tTotal Training Recognition Loss 3.487920 || Total Training Translation Loss 0.011029\n",
            "2025-07-10 14:18:38,386 Epoch 3743:\n",
            "\t\t\tTotal Training Recognition Loss 0.441549 || Total Training Translation Loss 0.010518\n",
            "2025-07-10 14:18:38,632 Epoch 3744:\n",
            "\t\t\tTotal Training Recognition Loss 0.677261 || Total Training Translation Loss 0.009914\n",
            "2025-07-10 14:18:38,887 Epoch 3745:\n",
            "\t\t\tTotal Training Recognition Loss 0.352814 || Total Training Translation Loss 0.011996\n",
            "2025-07-10 14:18:39,133 Epoch 3746:\n",
            "\t\t\tTotal Training Recognition Loss 0.532771 || Total Training Translation Loss 0.011587\n",
            "2025-07-10 14:18:39,375 Epoch 3747:\n",
            "\t\t\tTotal Training Recognition Loss 0.611212 || Total Training Translation Loss 0.010992\n",
            "2025-07-10 14:18:39,624 Epoch 3748:\n",
            "\t\t\tTotal Training Recognition Loss 0.466695 || Total Training Translation Loss 0.012169\n",
            "2025-07-10 14:18:39,872 Epoch 3749:\n",
            "\t\t\tTotal Training Recognition Loss 0.232200 || Total Training Translation Loss 0.010347\n",
            "2025-07-10 14:18:40,127 Epoch 3750:\n",
            "\t\t\tTotal Training Recognition Loss 0.238671 || Total Training Translation Loss 0.010891\n",
            "2025-07-10 14:18:40,378 Epoch 3751:\n",
            "\t\t\tTotal Training Recognition Loss 0.382398 || Total Training Translation Loss 0.008989\n",
            "2025-07-10 14:18:40,626 Epoch 3752:\n",
            "\t\t\tTotal Training Recognition Loss 0.320732 || Total Training Translation Loss 0.011362\n",
            "2025-07-10 14:18:40,870 Epoch 3753:\n",
            "\t\t\tTotal Training Recognition Loss 0.171339 || Total Training Translation Loss 0.009640\n",
            "2025-07-10 14:18:41,116 Epoch 3754:\n",
            "\t\t\tTotal Training Recognition Loss 0.204703 || Total Training Translation Loss 0.009026\n",
            "2025-07-10 14:18:41,367 Epoch 3755:\n",
            "\t\t\tTotal Training Recognition Loss 0.787019 || Total Training Translation Loss 0.009575\n",
            "2025-07-10 14:18:41,615 Epoch 3756:\n",
            "\t\t\tTotal Training Recognition Loss 0.248834 || Total Training Translation Loss 0.010946\n",
            "2025-07-10 14:18:41,858 Epoch 3757:\n",
            "\t\t\tTotal Training Recognition Loss 0.741725 || Total Training Translation Loss 0.011834\n",
            "2025-07-10 14:18:42,107 Epoch 3758:\n",
            "\t\t\tTotal Training Recognition Loss 0.542603 || Total Training Translation Loss 0.009575\n",
            "2025-07-10 14:18:42,353 Epoch 3759:\n",
            "\t\t\tTotal Training Recognition Loss 58.983006 || Total Training Translation Loss 0.014507\n",
            "2025-07-10 14:18:42,601 Epoch 3760:\n",
            "\t\t\tTotal Training Recognition Loss 0.613243 || Total Training Translation Loss 0.010830\n",
            "2025-07-10 14:18:42,850 Epoch 3761:\n",
            "\t\t\tTotal Training Recognition Loss 0.271330 || Total Training Translation Loss 0.015301\n",
            "2025-07-10 14:18:43,105 Epoch 3762:\n",
            "\t\t\tTotal Training Recognition Loss 0.760169 || Total Training Translation Loss 0.016633\n",
            "2025-07-10 14:18:43,348 Epoch 3763:\n",
            "\t\t\tTotal Training Recognition Loss 0.080100 || Total Training Translation Loss 0.011551\n",
            "2025-07-10 14:18:43,597 Epoch 3764:\n",
            "\t\t\tTotal Training Recognition Loss 1.144036 || Total Training Translation Loss 0.012914\n",
            "2025-07-10 14:18:43,853 Epoch 3765:\n",
            "\t\t\tTotal Training Recognition Loss 0.080943 || Total Training Translation Loss 0.013072\n",
            "2025-07-10 14:18:44,113 Epoch 3766:\n",
            "\t\t\tTotal Training Recognition Loss 0.144608 || Total Training Translation Loss 0.007948\n",
            "2025-07-10 14:18:44,358 Epoch 3767:\n",
            "\t\t\tTotal Training Recognition Loss 0.481283 || Total Training Translation Loss 0.012518\n",
            "2025-07-10 14:18:44,603 Epoch 3768:\n",
            "\t\t\tTotal Training Recognition Loss 0.184745 || Total Training Translation Loss 0.013633\n",
            "2025-07-10 14:18:44,816 Epoch 3769:\n",
            "\t\t\tTotal Training Recognition Loss 0.132166 || Total Training Translation Loss 0.014407\n",
            "2025-07-10 14:18:45,028 Epoch 3770:\n",
            "\t\t\tTotal Training Recognition Loss 0.089824 || Total Training Translation Loss 0.010043\n",
            "2025-07-10 14:18:45,229 Epoch 3771:\n",
            "\t\t\tTotal Training Recognition Loss 4.908425 || Total Training Translation Loss 0.011043\n",
            "2025-07-10 14:18:45,435 Epoch 3772:\n",
            "\t\t\tTotal Training Recognition Loss 0.153010 || Total Training Translation Loss 0.009212\n",
            "2025-07-10 14:18:45,652 Epoch 3773:\n",
            "\t\t\tTotal Training Recognition Loss 0.140336 || Total Training Translation Loss 0.010577\n",
            "2025-07-10 14:18:45,856 Epoch 3774:\n",
            "\t\t\tTotal Training Recognition Loss 0.130529 || Total Training Translation Loss 0.012836\n",
            "2025-07-10 14:18:46,095 Epoch 3775:\n",
            "\t\t\tTotal Training Recognition Loss 0.154240 || Total Training Translation Loss 0.014826\n",
            "2025-07-10 14:18:46,355 Epoch 3776:\n",
            "\t\t\tTotal Training Recognition Loss 0.879728 || Total Training Translation Loss 0.011984\n",
            "2025-07-10 14:18:46,609 Epoch 3777:\n",
            "\t\t\tTotal Training Recognition Loss 0.093944 || Total Training Translation Loss 0.010341\n",
            "2025-07-10 14:18:46,858 Epoch 3778:\n",
            "\t\t\tTotal Training Recognition Loss 1.221211 || Total Training Translation Loss 0.010542\n",
            "2025-07-10 14:18:47,113 Epoch 3779:\n",
            "\t\t\tTotal Training Recognition Loss 0.100049 || Total Training Translation Loss 0.012668\n",
            "2025-07-10 14:18:47,366 Epoch 3780:\n",
            "\t\t\tTotal Training Recognition Loss 0.123008 || Total Training Translation Loss 0.011104\n",
            "2025-07-10 14:18:47,622 Epoch 3781:\n",
            "\t\t\tTotal Training Recognition Loss 0.217398 || Total Training Translation Loss 0.020015\n",
            "2025-07-10 14:18:47,876 Epoch 3782:\n",
            "\t\t\tTotal Training Recognition Loss 0.137732 || Total Training Translation Loss 0.012784\n",
            "2025-07-10 14:18:48,133 Epoch 3783:\n",
            "\t\t\tTotal Training Recognition Loss 0.085508 || Total Training Translation Loss 0.015615\n",
            "2025-07-10 14:18:48,375 Epoch 3784:\n",
            "\t\t\tTotal Training Recognition Loss 0.104565 || Total Training Translation Loss 0.020168\n",
            "2025-07-10 14:18:48,627 Epoch 3785:\n",
            "\t\t\tTotal Training Recognition Loss 1.540566 || Total Training Translation Loss 0.018406\n",
            "2025-07-10 14:18:48,874 Epoch 3786:\n",
            "\t\t\tTotal Training Recognition Loss 0.101443 || Total Training Translation Loss 0.012023\n",
            "2025-07-10 14:18:49,130 Epoch 3787:\n",
            "\t\t\tTotal Training Recognition Loss 0.066773 || Total Training Translation Loss 0.012240\n",
            "2025-07-10 14:18:49,386 Epoch 3788:\n",
            "\t\t\tTotal Training Recognition Loss 0.472308 || Total Training Translation Loss 0.017556\n",
            "2025-07-10 14:18:49,639 Epoch 3789:\n",
            "\t\t\tTotal Training Recognition Loss 0.086170 || Total Training Translation Loss 0.009747\n",
            "2025-07-10 14:18:49,886 Epoch 3790:\n",
            "\t\t\tTotal Training Recognition Loss 0.405064 || Total Training Translation Loss 0.015390\n",
            "2025-07-10 14:18:50,117 Epoch 3791:\n",
            "\t\t\tTotal Training Recognition Loss 0.086794 || Total Training Translation Loss 0.013240\n",
            "2025-07-10 14:18:50,329 Epoch 3792:\n",
            "\t\t\tTotal Training Recognition Loss 0.157176 || Total Training Translation Loss 0.015790\n",
            "2025-07-10 14:18:50,545 Epoch 3793:\n",
            "\t\t\tTotal Training Recognition Loss 0.258203 || Total Training Translation Loss 0.013833\n",
            "2025-07-10 14:18:50,754 Epoch 3794:\n",
            "\t\t\tTotal Training Recognition Loss 0.079193 || Total Training Translation Loss 0.011675\n",
            "2025-07-10 14:18:50,960 Epoch 3795:\n",
            "\t\t\tTotal Training Recognition Loss 0.734228 || Total Training Translation Loss 0.011459\n",
            "2025-07-10 14:18:51,163 Epoch 3796:\n",
            "\t\t\tTotal Training Recognition Loss 0.156044 || Total Training Translation Loss 0.013379\n",
            "2025-07-10 14:18:51,376 Epoch 3797:\n",
            "\t\t\tTotal Training Recognition Loss 0.359042 || Total Training Translation Loss 0.011971\n",
            "2025-07-10 14:18:51,583 Epoch 3798:\n",
            "\t\t\tTotal Training Recognition Loss 1.016381 || Total Training Translation Loss 0.012537\n",
            "2025-07-10 14:18:51,787 Epoch 3799:\n",
            "\t\t\tTotal Training Recognition Loss 3.115392 || Total Training Translation Loss 0.008868\n",
            "2025-07-10 14:18:51,989 [Epoch: 3800 Step: 00003800] Batch Recognition Loss:   0.139133 => Gls Tokens per Sec:      184 || Batch Translation Loss:   0.011154 => Txt Tokens per Sec:      498 || Lr: 0.000490\n",
            "2025-07-10 14:18:52,289 Validation result at epoch 3800, step     3800: duration: 0.2986s\n",
            "\tRecognition Beam Size: 1\tTranslation Beam Size: 1\tTranslation Beam Alpha: -1\n",
            "\tRecognition Loss: 7532.48828\tTranslation Loss: 279.09616\tPPL: 26.66853\n",
            "\tEval Metric: BLEU\n",
            "\tWER 102.86\t(DEL: 25.71,\tINS: 22.86,\tSUB: 54.29)\n",
            "\tBLEU-4 2.61\t(BLEU-1: 5.33,\tBLEU-2: 3.84,\tBLEU-3: 3.16,\tBLEU-4: 2.61)\n",
            "\tCHRF 21.21\tROUGE 8.70\tFID 0.00\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-07-10 14:18:52,290 Logging Recognition and Translation Outputs\n",
            "2025-07-10 14:18:52,290 ========================================================================================\n",
            "2025-07-10 14:18:52,290 Logging Sequence: 11August_2010_Wednesday_tagesschau-2\n",
            "2025-07-10 14:18:52,291 \tGloss Reference :\t******** ******* DRUCK TIEF     KOMMEN\n",
            "2025-07-10 14:18:52,291 \tGloss Hypothesis:\tNORDWEST FEBRUAR REGEN NORDWEST REGEN \n",
            "2025-07-10 14:18:52,291 \tGloss Alignment :\tI        I       S     S        S     \n",
            "2025-07-10 14:18:52,292 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:18:52,293 \tText Reference  :\t*** *** tiefer           luftdruck bestimmt in         den ******** ****** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** nächsten tagen unser wetter\n",
            "2025-07-10 14:18:52,293 \tText Hypothesis :\tnun die wettervorhersage für       morgen   donnerstag den zwölften august <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>    <unk> <unk> <unk> \n",
            "2025-07-10 14:18:52,293 \tText Alignment  :\tI   I   S                S         S        S              I        I      I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     S        S     S     S     \n",
            "2025-07-10 14:18:52,294 ========================================================================================\n",
            "2025-07-10 14:18:52,294 Logging Sequence: 11August_2010_Wednesday_tagesschau-3\n",
            "2025-07-10 14:18:52,294 \tGloss Reference :\tES-BEDEUTET VIEL WOLKE ********** UND   KOENNEN REGEN GEWITTER KOENNEN\n",
            "2025-07-10 14:18:52,295 \tGloss Hypothesis:\t*********** **** WOLKE DONNERSTAG HEUTE LOCH    HEUTE SPEZIELL KOENNEN\n",
            "2025-07-10 14:18:52,295 \tGloss Alignment :\tD           D          I          S     S       S     S               \n",
            "2025-07-10 14:18:52,295 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:18:52,297 \tText Reference  :\t*** *** **************** *** ****** ********** *** ******** ****** ***** ***** ***** ***** ***** ***** ***** ***** das   bedeutet viele wolken und   immer wieder zum   teil  kräftige schauer und   gewitter\n",
            "2025-07-10 14:18:52,297 \tText Hypothesis :\tnun die wettervorhersage für morgen donnerstag den zwölften august <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>    <unk> <unk>  <unk> <unk> <unk>  <unk> <unk> <unk>    <unk>   <unk> <unk>   \n",
            "2025-07-10 14:18:52,297 \tText Alignment  :\tI   I   I                I   I      I          I   I        I      I     I     I     I     I     I     I     I     S     S        S     S      S     S     S      S     S     S        S       S     S       \n",
            "2025-07-10 14:18:52,297 ========================================================================================\n",
            "2025-07-10 14:18:52,298 Logging Sequence: 11August_2010_Wednesday_tagesschau-8\n",
            "2025-07-10 14:18:52,298 \tGloss Reference :\t***** ***** **** ******** WIND    MAESSIG SCHWACH REGION   WENN  GEWITTER WIND  KOENNEN\n",
            "2025-07-10 14:18:52,298 \tGloss Hypothesis:\tWOLKE DURCH LOCH SPEZIELL TROCKEN BLEIBEN WOLKE   SPEZIELL REGEN KOENNEN  WOLKE KOENNEN\n",
            "2025-07-10 14:18:52,298 \tGloss Alignment :\tI     I     I    I        S       S       S       S        S     S        S            \n",
            "2025-07-10 14:18:52,299 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:18:52,301 \tText Reference  :\t*** *** **************** *** ****** ********** *** ******** ****** ***** ***** ***** meist weht  nur   ein   schwacher wind  aus   unterschiedlichen richtungen der   bei   schauern und   gewittern stark böig  sein  kann \n",
            "2025-07-10 14:18:52,301 \tText Hypothesis :\tnun die wettervorhersage für morgen donnerstag den zwölften august <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>     <unk> <unk> <unk>             <unk>      <unk> <unk> <unk>    <unk> <unk>     <unk> <unk> <unk> <unk>\n",
            "2025-07-10 14:18:52,301 \tText Alignment  :\tI   I   I                I   I      I          I   I        I      I     I     I     S     S     S     S     S         S     S     S                 S          S     S     S        S     S         S     S     S     S    \n",
            "2025-07-10 14:18:52,301 ========================================================================================\n",
            "2025-07-10 14:18:52,301 Logging Sequence: 25October_2010_Monday_tagesschau-22\n",
            "2025-07-10 14:18:52,302 \tGloss Reference :\tMITTWOCH REGEN KOENNEN NORDWEST WAHRSCHEINLICH NORD STARK WIND \n",
            "2025-07-10 14:18:52,302 \tGloss Hypothesis:\t******** ***** ******* NORDWEST ************** **** JETZT REGEN\n",
            "2025-07-10 14:18:52,302 \tGloss Alignment :\tD        D     D                D              D    S     S    \n",
            "2025-07-10 14:18:52,302 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:18:52,304 \tText Reference  :\t*** *** **************** *** ****** ********** *** ******** ****** ***** ***** ***** ***** ***** ***** ***** am    mittwoch hier  und   da    nieselregen in    der   nordwesthälfte an    den   küsten kräftiger wind \n",
            "2025-07-10 14:18:52,304 \tText Hypothesis :\tnun die wettervorhersage für morgen donnerstag den zwölften august <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>    <unk> <unk> <unk> <unk>       <unk> <unk> <unk>          <unk> <unk> <unk>  <unk>     <unk>\n",
            "2025-07-10 14:18:52,304 \tText Alignment  :\tI   I   I                I   I      I          I   I        I      I     I     I     I     I     I     I     S     S        S     S     S     S           S     S     S              S     S     S      S         S    \n",
            "2025-07-10 14:18:52,304 ========================================================================================\n",
            "2025-07-10 14:18:52,305 Logging Sequence: 05May_2011_Thursday_tagesschau-25\n",
            "2025-07-10 14:18:52,305 \tGloss Reference :\t**** JETZT WETTER WIE-AUSSEHEN MORGEN FREITAG SECHSTE    MAI   ZEIGEN-BILDSCHIRM\n",
            "2025-07-10 14:18:52,305 \tGloss Hypothesis:\tVIEL JETZT WETTER ************ MORGEN ******* DONNERSTAG REGEN ZWOELF           \n",
            "2025-07-10 14:18:52,306 \tGloss Alignment :\tI                 D                   D       S          S     S                \n",
            "2025-07-10 14:18:52,306 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:18:52,307 \tText Reference  :\tund nun die wettervorhersage für morgen freitag    den ******** ****** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** sechsten mai  \n",
            "2025-07-10 14:18:52,307 \tText Hypothesis :\t*** nun die wettervorhersage für morgen donnerstag den zwölften august <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>    <unk>\n",
            "2025-07-10 14:18:52,307 \tText Alignment  :\tD                                       S              I        I      I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     S        S    \n",
            "2025-07-10 14:18:52,307 ========================================================================================\n",
            "2025-07-10 14:18:52,308 Epoch 3800:\n",
            "\t\t\tTotal Training Recognition Loss 0.139133 || Total Training Translation Loss 0.011154\n",
            "2025-07-10 14:18:52,522 Epoch 3801:\n",
            "\t\t\tTotal Training Recognition Loss 0.120860 || Total Training Translation Loss 0.010916\n",
            "2025-07-10 14:18:52,784 Epoch 3802:\n",
            "\t\t\tTotal Training Recognition Loss 0.374454 || Total Training Translation Loss 0.011210\n",
            "2025-07-10 14:18:53,035 Epoch 3803:\n",
            "\t\t\tTotal Training Recognition Loss 0.153902 || Total Training Translation Loss 0.011270\n",
            "2025-07-10 14:18:53,296 Epoch 3804:\n",
            "\t\t\tTotal Training Recognition Loss 0.186736 || Total Training Translation Loss 0.010307\n",
            "2025-07-10 14:18:53,542 Epoch 3805:\n",
            "\t\t\tTotal Training Recognition Loss 0.185120 || Total Training Translation Loss 0.010716\n",
            "2025-07-10 14:18:53,794 Epoch 3806:\n",
            "\t\t\tTotal Training Recognition Loss 0.121561 || Total Training Translation Loss 0.011723\n",
            "2025-07-10 14:18:54,042 Epoch 3807:\n",
            "\t\t\tTotal Training Recognition Loss 0.092796 || Total Training Translation Loss 0.011360\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-07-10 14:18:54,303 Epoch 3808:\n",
            "\t\t\tTotal Training Recognition Loss 0.107726 || Total Training Translation Loss 0.008590\n",
            "2025-07-10 14:18:54,562 Epoch 3809:\n",
            "\t\t\tTotal Training Recognition Loss 0.109271 || Total Training Translation Loss 0.009497\n",
            "2025-07-10 14:18:54,814 Epoch 3810:\n",
            "\t\t\tTotal Training Recognition Loss 0.377631 || Total Training Translation Loss 0.011009\n",
            "2025-07-10 14:18:55,072 Epoch 3811:\n",
            "\t\t\tTotal Training Recognition Loss 0.106900 || Total Training Translation Loss 0.009946\n",
            "2025-07-10 14:18:55,323 Epoch 3812:\n",
            "\t\t\tTotal Training Recognition Loss 0.131970 || Total Training Translation Loss 0.010259\n",
            "2025-07-10 14:18:55,582 Epoch 3813:\n",
            "\t\t\tTotal Training Recognition Loss 0.248967 || Total Training Translation Loss 0.011086\n",
            "2025-07-10 14:18:55,835 Epoch 3814:\n",
            "\t\t\tTotal Training Recognition Loss 0.109661 || Total Training Translation Loss 0.009839\n",
            "2025-07-10 14:18:56,084 Epoch 3815:\n",
            "\t\t\tTotal Training Recognition Loss 0.114971 || Total Training Translation Loss 0.009606\n",
            "2025-07-10 14:18:56,337 Epoch 3816:\n",
            "\t\t\tTotal Training Recognition Loss 0.080478 || Total Training Translation Loss 0.010892\n",
            "2025-07-10 14:18:56,592 Epoch 3817:\n",
            "\t\t\tTotal Training Recognition Loss 0.086273 || Total Training Translation Loss 0.010675\n",
            "2025-07-10 14:18:56,846 Epoch 3818:\n",
            "\t\t\tTotal Training Recognition Loss 0.100491 || Total Training Translation Loss 0.010672\n",
            "2025-07-10 14:18:57,105 Epoch 3819:\n",
            "\t\t\tTotal Training Recognition Loss 0.093424 || Total Training Translation Loss 0.009352\n",
            "2025-07-10 14:18:57,351 Epoch 3820:\n",
            "\t\t\tTotal Training Recognition Loss 0.144438 || Total Training Translation Loss 0.009685\n",
            "2025-07-10 14:18:57,612 Epoch 3821:\n",
            "\t\t\tTotal Training Recognition Loss 0.168541 || Total Training Translation Loss 0.009324\n",
            "2025-07-10 14:18:57,874 Epoch 3822:\n",
            "\t\t\tTotal Training Recognition Loss 0.129829 || Total Training Translation Loss 0.008896\n",
            "2025-07-10 14:18:58,124 Epoch 3823:\n",
            "\t\t\tTotal Training Recognition Loss 0.102705 || Total Training Translation Loss 0.008985\n",
            "2025-07-10 14:18:58,364 Epoch 3824:\n",
            "\t\t\tTotal Training Recognition Loss 0.076239 || Total Training Translation Loss 0.010494\n",
            "2025-07-10 14:18:58,590 Epoch 3825:\n",
            "\t\t\tTotal Training Recognition Loss 0.198288 || Total Training Translation Loss 0.009768\n",
            "2025-07-10 14:18:58,821 Epoch 3826:\n",
            "\t\t\tTotal Training Recognition Loss 0.090344 || Total Training Translation Loss 0.009677\n",
            "2025-07-10 14:18:59,040 Epoch 3827:\n",
            "\t\t\tTotal Training Recognition Loss 0.109548 || Total Training Translation Loss 0.007615\n",
            "2025-07-10 14:18:59,266 Epoch 3828:\n",
            "\t\t\tTotal Training Recognition Loss 0.159719 || Total Training Translation Loss 0.008542\n",
            "2025-07-10 14:18:59,475 Epoch 3829:\n",
            "\t\t\tTotal Training Recognition Loss 0.233631 || Total Training Translation Loss 0.009030\n",
            "2025-07-10 14:18:59,691 Epoch 3830:\n",
            "\t\t\tTotal Training Recognition Loss 0.078000 || Total Training Translation Loss 0.008105\n",
            "2025-07-10 14:18:59,957 Epoch 3831:\n",
            "\t\t\tTotal Training Recognition Loss 0.149135 || Total Training Translation Loss 0.009201\n",
            "2025-07-10 14:19:00,213 Epoch 3832:\n",
            "\t\t\tTotal Training Recognition Loss 0.087910 || Total Training Translation Loss 0.007791\n",
            "2025-07-10 14:19:00,470 Epoch 3833:\n",
            "\t\t\tTotal Training Recognition Loss 0.090680 || Total Training Translation Loss 0.008819\n",
            "2025-07-10 14:19:00,724 Epoch 3834:\n",
            "\t\t\tTotal Training Recognition Loss 0.116986 || Total Training Translation Loss 0.010657\n",
            "2025-07-10 14:19:00,992 Epoch 3835:\n",
            "\t\t\tTotal Training Recognition Loss 0.087096 || Total Training Translation Loss 0.009089\n",
            "2025-07-10 14:19:01,234 Epoch 3836:\n",
            "\t\t\tTotal Training Recognition Loss 0.360251 || Total Training Translation Loss 0.008513\n",
            "2025-07-10 14:19:01,457 Epoch 3837:\n",
            "\t\t\tTotal Training Recognition Loss 0.567039 || Total Training Translation Loss 0.010633\n",
            "2025-07-10 14:19:01,683 Epoch 3838:\n",
            "\t\t\tTotal Training Recognition Loss 3.438593 || Total Training Translation Loss 0.010295\n",
            "2025-07-10 14:19:01,912 Epoch 3839:\n",
            "\t\t\tTotal Training Recognition Loss 0.089470 || Total Training Translation Loss 0.008555\n",
            "2025-07-10 14:19:02,137 Epoch 3840:\n",
            "\t\t\tTotal Training Recognition Loss 2.079236 || Total Training Translation Loss 0.007894\n",
            "2025-07-10 14:19:02,360 Epoch 3841:\n",
            "\t\t\tTotal Training Recognition Loss 0.144245 || Total Training Translation Loss 0.011080\n",
            "2025-07-10 14:19:02,581 Epoch 3842:\n",
            "\t\t\tTotal Training Recognition Loss 0.188716 || Total Training Translation Loss 0.009083\n",
            "2025-07-10 14:19:02,811 Epoch 3843:\n",
            "\t\t\tTotal Training Recognition Loss 0.065875 || Total Training Translation Loss 0.010616\n",
            "2025-07-10 14:19:03,036 Epoch 3844:\n",
            "\t\t\tTotal Training Recognition Loss 0.238185 || Total Training Translation Loss 0.010628\n",
            "2025-07-10 14:19:03,267 Epoch 3845:\n",
            "\t\t\tTotal Training Recognition Loss 0.120407 || Total Training Translation Loss 0.009138\n",
            "2025-07-10 14:19:03,498 Epoch 3846:\n",
            "\t\t\tTotal Training Recognition Loss 0.075700 || Total Training Translation Loss 0.009293\n",
            "2025-07-10 14:19:03,723 Epoch 3847:\n",
            "\t\t\tTotal Training Recognition Loss 0.136185 || Total Training Translation Loss 0.009457\n",
            "2025-07-10 14:19:03,958 Epoch 3848:\n",
            "\t\t\tTotal Training Recognition Loss 0.116368 || Total Training Translation Loss 0.009082\n",
            "2025-07-10 14:19:04,182 Epoch 3849:\n",
            "\t\t\tTotal Training Recognition Loss 0.096605 || Total Training Translation Loss 0.009632\n",
            "2025-07-10 14:19:04,414 Epoch 3850:\n",
            "\t\t\tTotal Training Recognition Loss 0.122254 || Total Training Translation Loss 0.009918\n",
            "2025-07-10 14:19:04,638 Epoch 3851:\n",
            "\t\t\tTotal Training Recognition Loss 0.081417 || Total Training Translation Loss 0.009340\n",
            "2025-07-10 14:19:04,868 Epoch 3852:\n",
            "\t\t\tTotal Training Recognition Loss 0.162821 || Total Training Translation Loss 0.009586\n",
            "2025-07-10 14:19:05,092 Epoch 3853:\n",
            "\t\t\tTotal Training Recognition Loss 0.106551 || Total Training Translation Loss 0.009354\n",
            "2025-07-10 14:19:05,315 Epoch 3854:\n",
            "\t\t\tTotal Training Recognition Loss 0.128303 || Total Training Translation Loss 0.010288\n",
            "2025-07-10 14:19:05,536 Epoch 3855:\n",
            "\t\t\tTotal Training Recognition Loss 0.201873 || Total Training Translation Loss 0.009554\n",
            "2025-07-10 14:19:05,771 Epoch 3856:\n",
            "\t\t\tTotal Training Recognition Loss 1.044697 || Total Training Translation Loss 0.008268\n",
            "2025-07-10 14:19:05,998 Epoch 3857:\n",
            "\t\t\tTotal Training Recognition Loss 5.056319 || Total Training Translation Loss 0.009075\n",
            "2025-07-10 14:19:06,226 Epoch 3858:\n",
            "\t\t\tTotal Training Recognition Loss 0.082133 || Total Training Translation Loss 0.009683\n",
            "2025-07-10 14:19:06,456 Epoch 3859:\n",
            "\t\t\tTotal Training Recognition Loss 0.085175 || Total Training Translation Loss 0.009553\n",
            "2025-07-10 14:19:06,680 Epoch 3860:\n",
            "\t\t\tTotal Training Recognition Loss 0.223381 || Total Training Translation Loss 0.010893\n",
            "2025-07-10 14:19:06,916 Epoch 3861:\n",
            "\t\t\tTotal Training Recognition Loss 0.138373 || Total Training Translation Loss 0.009048\n",
            "2025-07-10 14:19:07,150 Epoch 3862:\n",
            "\t\t\tTotal Training Recognition Loss 0.086571 || Total Training Translation Loss 0.009438\n",
            "2025-07-10 14:19:07,382 Epoch 3863:\n",
            "\t\t\tTotal Training Recognition Loss 0.252664 || Total Training Translation Loss 0.008227\n",
            "2025-07-10 14:19:07,621 Epoch 3864:\n",
            "\t\t\tTotal Training Recognition Loss 0.096393 || Total Training Translation Loss 0.009537\n",
            "2025-07-10 14:19:07,845 Epoch 3865:\n",
            "\t\t\tTotal Training Recognition Loss 0.621932 || Total Training Translation Loss 0.010307\n",
            "2025-07-10 14:19:08,072 Epoch 3866:\n",
            "\t\t\tTotal Training Recognition Loss 0.687613 || Total Training Translation Loss 0.009632\n",
            "2025-07-10 14:19:08,303 Epoch 3867:\n",
            "\t\t\tTotal Training Recognition Loss 0.150215 || Total Training Translation Loss 0.010078\n",
            "2025-07-10 14:19:08,533 Epoch 3868:\n",
            "\t\t\tTotal Training Recognition Loss 0.376845 || Total Training Translation Loss 0.009747\n",
            "2025-07-10 14:19:08,770 Epoch 3869:\n",
            "\t\t\tTotal Training Recognition Loss 0.076105 || Total Training Translation Loss 0.009286\n",
            "2025-07-10 14:19:08,999 Epoch 3870:\n",
            "\t\t\tTotal Training Recognition Loss 0.103637 || Total Training Translation Loss 0.009750\n",
            "2025-07-10 14:19:09,236 Epoch 3871:\n",
            "\t\t\tTotal Training Recognition Loss 0.091446 || Total Training Translation Loss 0.008919\n",
            "2025-07-10 14:19:09,474 Epoch 3872:\n",
            "\t\t\tTotal Training Recognition Loss 0.192081 || Total Training Translation Loss 0.010398\n",
            "2025-07-10 14:19:09,711 Epoch 3873:\n",
            "\t\t\tTotal Training Recognition Loss 0.078280 || Total Training Translation Loss 0.008611\n",
            "2025-07-10 14:19:09,943 Epoch 3874:\n",
            "\t\t\tTotal Training Recognition Loss 0.165562 || Total Training Translation Loss 0.009966\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-07-10 14:19:10,179 Epoch 3875:\n",
            "\t\t\tTotal Training Recognition Loss 3.104059 || Total Training Translation Loss 0.009778\n",
            "2025-07-10 14:19:10,407 Epoch 3876:\n",
            "\t\t\tTotal Training Recognition Loss 0.613469 || Total Training Translation Loss 0.008949\n",
            "2025-07-10 14:19:10,645 Epoch 3877:\n",
            "\t\t\tTotal Training Recognition Loss 0.098451 || Total Training Translation Loss 0.009413\n",
            "2025-07-10 14:19:10,883 Epoch 3878:\n",
            "\t\t\tTotal Training Recognition Loss 0.100706 || Total Training Translation Loss 0.008339\n",
            "2025-07-10 14:19:11,123 Epoch 3879:\n",
            "\t\t\tTotal Training Recognition Loss 0.131039 || Total Training Translation Loss 0.009049\n",
            "2025-07-10 14:19:11,365 Epoch 3880:\n",
            "\t\t\tTotal Training Recognition Loss 0.109486 || Total Training Translation Loss 0.009612\n",
            "2025-07-10 14:19:11,604 Epoch 3881:\n",
            "\t\t\tTotal Training Recognition Loss 0.077912 || Total Training Translation Loss 0.010764\n",
            "2025-07-10 14:19:11,873 Epoch 3882:\n",
            "\t\t\tTotal Training Recognition Loss 1.896988 || Total Training Translation Loss 0.011922\n",
            "2025-07-10 14:19:12,102 Epoch 3883:\n",
            "\t\t\tTotal Training Recognition Loss 0.257215 || Total Training Translation Loss 0.010018\n",
            "2025-07-10 14:19:12,347 Epoch 3884:\n",
            "\t\t\tTotal Training Recognition Loss 0.071008 || Total Training Translation Loss 0.010301\n",
            "2025-07-10 14:19:12,590 Epoch 3885:\n",
            "\t\t\tTotal Training Recognition Loss 0.497278 || Total Training Translation Loss 0.008736\n",
            "2025-07-10 14:19:12,874 Epoch 3886:\n",
            "\t\t\tTotal Training Recognition Loss 0.142164 || Total Training Translation Loss 0.009862\n",
            "2025-07-10 14:19:13,158 Epoch 3887:\n",
            "\t\t\tTotal Training Recognition Loss 0.268678 || Total Training Translation Loss 0.010498\n",
            "2025-07-10 14:19:13,432 Epoch 3888:\n",
            "\t\t\tTotal Training Recognition Loss 2.757648 || Total Training Translation Loss 0.010501\n",
            "2025-07-10 14:19:13,713 Epoch 3889:\n",
            "\t\t\tTotal Training Recognition Loss 0.065334 || Total Training Translation Loss 0.008421\n",
            "2025-07-10 14:19:13,987 Epoch 3890:\n",
            "\t\t\tTotal Training Recognition Loss 3.187135 || Total Training Translation Loss 0.009832\n",
            "2025-07-10 14:19:14,272 Epoch 3891:\n",
            "\t\t\tTotal Training Recognition Loss 2.057197 || Total Training Translation Loss 0.007967\n",
            "2025-07-10 14:19:14,550 Epoch 3892:\n",
            "\t\t\tTotal Training Recognition Loss 0.067350 || Total Training Translation Loss 0.011481\n",
            "2025-07-10 14:19:14,832 Epoch 3893:\n",
            "\t\t\tTotal Training Recognition Loss 0.158476 || Total Training Translation Loss 0.008738\n",
            "2025-07-10 14:19:15,108 Epoch 3894:\n",
            "\t\t\tTotal Training Recognition Loss 0.115270 || Total Training Translation Loss 0.009500\n",
            "2025-07-10 14:19:15,357 Epoch 3895:\n",
            "\t\t\tTotal Training Recognition Loss 0.089294 || Total Training Translation Loss 0.009676\n",
            "2025-07-10 14:19:15,587 Epoch 3896:\n",
            "\t\t\tTotal Training Recognition Loss 0.294094 || Total Training Translation Loss 0.010994\n",
            "2025-07-10 14:19:15,818 Epoch 3897:\n",
            "\t\t\tTotal Training Recognition Loss 0.067260 || Total Training Translation Loss 0.008973\n",
            "2025-07-10 14:19:16,096 Epoch 3898:\n",
            "\t\t\tTotal Training Recognition Loss 0.072600 || Total Training Translation Loss 0.011226\n",
            "2025-07-10 14:19:16,360 Epoch 3899:\n",
            "\t\t\tTotal Training Recognition Loss 0.136924 || Total Training Translation Loss 0.008073\n",
            "2025-07-10 14:19:16,622 [Epoch: 3900 Step: 00003900] Batch Recognition Loss:   0.173954 => Gls Tokens per Sec:      142 || Batch Translation Loss:   0.008456 => Txt Tokens per Sec:      384 || Lr: 0.000490\n",
            "2025-07-10 14:19:17,046 Validation result at epoch 3900, step     3900: duration: 0.4223s\n",
            "\tRecognition Beam Size: 1\tTranslation Beam Size: 1\tTranslation Beam Alpha: -1\n",
            "\tRecognition Loss: 8255.51758\tTranslation Loss: 269.88333\tPPL: 23.92916\n",
            "\tEval Metric: BLEU\n",
            "\tWER 97.14\t(DEL: 22.86,\tINS: 17.14,\tSUB: 57.14)\n",
            "\tBLEU-4 2.61\t(BLEU-1: 5.33,\tBLEU-2: 3.84,\tBLEU-3: 3.16,\tBLEU-4: 2.61)\n",
            "\tCHRF 21.21\tROUGE 8.70\tFID 0.00\n",
            "2025-07-10 14:19:17,046 Logging Recognition and Translation Outputs\n",
            "2025-07-10 14:19:17,047 ========================================================================================\n",
            "2025-07-10 14:19:17,047 Logging Sequence: 11August_2010_Wednesday_tagesschau-2\n",
            "2025-07-10 14:19:17,048 \tGloss Reference :\t******** DRUCK   TIEF     KOMMEN\n",
            "2025-07-10 14:19:17,048 \tGloss Hypothesis:\tNORDWEST FEBRUAR NORDWEST REGEN \n",
            "2025-07-10 14:19:17,048 \tGloss Alignment :\tI        S       S        S     \n",
            "2025-07-10 14:19:17,049 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:19:17,052 \tText Reference  :\t*** *** tiefer           luftdruck bestimmt in         den ******** ****** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** nächsten tagen unser wetter\n",
            "2025-07-10 14:19:17,052 \tText Hypothesis :\tnun die wettervorhersage für       morgen   donnerstag den zwölften august <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>    <unk> <unk> <unk> \n",
            "2025-07-10 14:19:17,052 \tText Alignment  :\tI   I   S                S         S        S              I        I      I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     S        S     S     S     \n",
            "2025-07-10 14:19:17,053 ========================================================================================\n",
            "2025-07-10 14:19:17,053 Logging Sequence: 11August_2010_Wednesday_tagesschau-3\n",
            "2025-07-10 14:19:17,054 \tGloss Reference :\tES-BEDEUTET VIEL WOLKE UND        KOENNEN REGEN GEWITTER KOENNEN\n",
            "2025-07-10 14:19:17,054 \tGloss Hypothesis:\t*********** **** WOLKE DONNERSTAG HEUTE   LOCH  HEUTE    KOENNEN\n",
            "2025-07-10 14:19:17,054 \tGloss Alignment :\tD           D          S          S       S     S               \n",
            "2025-07-10 14:19:17,054 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:19:17,057 \tText Reference  :\t*** *** **************** *** ****** ********** *** ******** ****** ***** ***** ***** ***** ***** ***** ***** ***** das   bedeutet viele wolken und   immer wieder zum   teil  kräftige schauer und   gewitter\n",
            "2025-07-10 14:19:17,057 \tText Hypothesis :\tnun die wettervorhersage für morgen donnerstag den zwölften august <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>    <unk> <unk>  <unk> <unk> <unk>  <unk> <unk> <unk>    <unk>   <unk> <unk>   \n",
            "2025-07-10 14:19:17,057 \tText Alignment  :\tI   I   I                I   I      I          I   I        I      I     I     I     I     I     I     I     I     S     S        S     S      S     S     S      S     S     S        S       S     S       \n",
            "2025-07-10 14:19:17,057 ========================================================================================\n",
            "2025-07-10 14:19:17,058 Logging Sequence: 11August_2010_Wednesday_tagesschau-8\n",
            "2025-07-10 14:19:17,058 \tGloss Reference :\t***** **** ***** **** WIND     MAESSIG SCHWACH REGION WENN  GEWITTER WIND  KOENNEN\n",
            "2025-07-10 14:19:17,059 \tGloss Hypothesis:\tWOLKE VIEL DURCH LOCH SPEZIELL TROCKEN BLEIBEN WOLKE  REGEN KOENNEN  WOLKE KOENNEN\n",
            "2025-07-10 14:19:17,059 \tGloss Alignment :\tI     I    I     I    S        S       S       S      S     S        S            \n",
            "2025-07-10 14:19:17,059 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:19:17,061 \tText Reference  :\t*** *** **************** *** ****** ********** *** ******** ****** ***** ***** ***** meist weht  nur   ein   schwacher wind  aus   unterschiedlichen richtungen der   bei   schauern und   gewittern stark böig  sein  kann \n",
            "2025-07-10 14:19:17,062 \tText Hypothesis :\tnun die wettervorhersage für morgen donnerstag den zwölften august <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>     <unk> <unk> <unk>             <unk>      <unk> <unk> <unk>    <unk> <unk>     <unk> <unk> <unk> <unk>\n",
            "2025-07-10 14:19:17,062 \tText Alignment  :\tI   I   I                I   I      I          I   I        I      I     I     I     S     S     S     S     S         S     S     S                 S          S     S     S        S     S         S     S     S     S    \n",
            "2025-07-10 14:19:17,062 ========================================================================================\n",
            "2025-07-10 14:19:17,062 Logging Sequence: 25October_2010_Monday_tagesschau-22\n",
            "2025-07-10 14:19:17,063 \tGloss Reference :\tMITTWOCH REGEN KOENNEN NORDWEST WAHRSCHEINLICH NORD            STARK WIND\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-07-10 14:19:17,063 \tGloss Hypothesis:\t******** ***** ******* NORDWEST ************** UEBERSCHWEMMUNG JETZT VIEL\n",
            "2025-07-10 14:19:17,063 \tGloss Alignment :\tD        D     D                D              S               S     S   \n",
            "2025-07-10 14:19:17,063 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:19:17,065 \tText Reference  :\t*** *** **************** *** ****** ********** *** ******** ****** ***** ***** ***** ***** ***** ***** ***** am    mittwoch hier  und   da    nieselregen in    der   nordwesthälfte an    den   küsten kräftiger wind \n",
            "2025-07-10 14:19:17,066 \tText Hypothesis :\tnun die wettervorhersage für morgen donnerstag den zwölften august <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>    <unk> <unk> <unk> <unk>       <unk> <unk> <unk>          <unk> <unk> <unk>  <unk>     <unk>\n",
            "2025-07-10 14:19:17,066 \tText Alignment  :\tI   I   I                I   I      I          I   I        I      I     I     I     I     I     I     I     S     S        S     S     S     S           S     S     S              S     S     S      S         S    \n",
            "2025-07-10 14:19:17,066 ========================================================================================\n",
            "2025-07-10 14:19:17,066 Logging Sequence: 05May_2011_Thursday_tagesschau-25\n",
            "2025-07-10 14:19:17,066 \tGloss Reference :\t**** JETZT WETTER WIE-AUSSEHEN MORGEN FREITAG SECHSTE    MAI     ZEIGEN-BILDSCHIRM\n",
            "2025-07-10 14:19:17,067 \tGloss Hypothesis:\tVIEL JETZT WETTER ************ MORGEN ******* DONNERSTAG FEBRUAR ZWOELF           \n",
            "2025-07-10 14:19:17,067 \tGloss Alignment :\tI                 D                   D       S          S       S                \n",
            "2025-07-10 14:19:17,067 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:19:17,068 \tText Reference  :\tund nun die wettervorhersage für morgen freitag    den ******** ****** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** sechsten mai  \n",
            "2025-07-10 14:19:17,069 \tText Hypothesis :\t*** nun die wettervorhersage für morgen donnerstag den zwölften august <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>    <unk>\n",
            "2025-07-10 14:19:17,069 \tText Alignment  :\tD                                       S              I        I      I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     S        S    \n",
            "2025-07-10 14:19:17,069 ========================================================================================\n",
            "2025-07-10 14:19:17,070 Epoch 3900:\n",
            "\t\t\tTotal Training Recognition Loss 0.173954 || Total Training Translation Loss 0.008456\n",
            "2025-07-10 14:19:17,297 Epoch 3901:\n",
            "\t\t\tTotal Training Recognition Loss 1.144888 || Total Training Translation Loss 0.009335\n",
            "2025-07-10 14:19:17,523 Epoch 3902:\n",
            "\t\t\tTotal Training Recognition Loss 0.153091 || Total Training Translation Loss 0.009982\n",
            "2025-07-10 14:19:17,788 Epoch 3903:\n",
            "\t\t\tTotal Training Recognition Loss 0.112153 || Total Training Translation Loss 0.010419\n",
            "2025-07-10 14:19:18,053 Epoch 3904:\n",
            "\t\t\tTotal Training Recognition Loss 0.145863 || Total Training Translation Loss 0.009363\n",
            "2025-07-10 14:19:18,327 Epoch 3905:\n",
            "\t\t\tTotal Training Recognition Loss 39.008507 || Total Training Translation Loss 0.008074\n",
            "2025-07-10 14:19:18,598 Epoch 3906:\n",
            "\t\t\tTotal Training Recognition Loss 0.091345 || Total Training Translation Loss 0.007940\n",
            "2025-07-10 14:19:18,871 Epoch 3907:\n",
            "\t\t\tTotal Training Recognition Loss 0.127907 || Total Training Translation Loss 0.009697\n",
            "2025-07-10 14:19:19,145 Epoch 3908:\n",
            "\t\t\tTotal Training Recognition Loss 0.216912 || Total Training Translation Loss 0.010410\n",
            "2025-07-10 14:19:19,413 Epoch 3909:\n",
            "\t\t\tTotal Training Recognition Loss 0.084692 || Total Training Translation Loss 0.010858\n",
            "2025-07-10 14:19:19,697 Epoch 3910:\n",
            "\t\t\tTotal Training Recognition Loss 0.132160 || Total Training Translation Loss 0.009329\n",
            "2025-07-10 14:19:19,942 Epoch 3911:\n",
            "\t\t\tTotal Training Recognition Loss 0.104077 || Total Training Translation Loss 0.009864\n",
            "2025-07-10 14:19:20,169 Epoch 3912:\n",
            "\t\t\tTotal Training Recognition Loss 0.117531 || Total Training Translation Loss 0.009737\n",
            "2025-07-10 14:19:20,433 Epoch 3913:\n",
            "\t\t\tTotal Training Recognition Loss 0.475078 || Total Training Translation Loss 0.011467\n",
            "2025-07-10 14:19:20,702 Epoch 3914:\n",
            "\t\t\tTotal Training Recognition Loss 15.527891 || Total Training Translation Loss 0.009443\n",
            "2025-07-10 14:19:20,938 Epoch 3915:\n",
            "\t\t\tTotal Training Recognition Loss 11.980723 || Total Training Translation Loss 0.010741\n",
            "2025-07-10 14:19:21,167 Epoch 3916:\n",
            "\t\t\tTotal Training Recognition Loss 2.179437 || Total Training Translation Loss 0.009938\n",
            "2025-07-10 14:19:21,393 Epoch 3917:\n",
            "\t\t\tTotal Training Recognition Loss 22.399942 || Total Training Translation Loss 0.010182\n",
            "2025-07-10 14:19:21,629 Epoch 3918:\n",
            "\t\t\tTotal Training Recognition Loss 6.454072 || Total Training Translation Loss 0.009036\n",
            "2025-07-10 14:19:21,861 Epoch 3919:\n",
            "\t\t\tTotal Training Recognition Loss 9.232648 || Total Training Translation Loss 0.009936\n",
            "2025-07-10 14:19:22,083 Epoch 3920:\n",
            "\t\t\tTotal Training Recognition Loss 0.179918 || Total Training Translation Loss 0.011376\n",
            "2025-07-10 14:19:22,320 Epoch 3921:\n",
            "\t\t\tTotal Training Recognition Loss 0.279011 || Total Training Translation Loss 0.009906\n",
            "2025-07-10 14:19:22,550 Epoch 3922:\n",
            "\t\t\tTotal Training Recognition Loss 0.144236 || Total Training Translation Loss 0.011134\n",
            "2025-07-10 14:19:22,780 Epoch 3923:\n",
            "\t\t\tTotal Training Recognition Loss 0.124485 || Total Training Translation Loss 0.010642\n",
            "2025-07-10 14:19:23,004 Epoch 3924:\n",
            "\t\t\tTotal Training Recognition Loss 0.296150 || Total Training Translation Loss 0.011128\n",
            "2025-07-10 14:19:23,238 Epoch 3925:\n",
            "\t\t\tTotal Training Recognition Loss 0.115441 || Total Training Translation Loss 0.011409\n",
            "2025-07-10 14:19:23,470 Epoch 3926:\n",
            "\t\t\tTotal Training Recognition Loss 0.145731 || Total Training Translation Loss 0.009791\n",
            "2025-07-10 14:19:23,704 Epoch 3927:\n",
            "\t\t\tTotal Training Recognition Loss 0.265722 || Total Training Translation Loss 0.012006\n",
            "2025-07-10 14:19:23,941 Epoch 3928:\n",
            "\t\t\tTotal Training Recognition Loss 0.215292 || Total Training Translation Loss 0.009780\n",
            "2025-07-10 14:19:24,171 Epoch 3929:\n",
            "\t\t\tTotal Training Recognition Loss 0.251944 || Total Training Translation Loss 0.013355\n",
            "2025-07-10 14:19:24,396 Epoch 3930:\n",
            "\t\t\tTotal Training Recognition Loss 2.234887 || Total Training Translation Loss 0.009661\n",
            "2025-07-10 14:19:24,625 Epoch 3931:\n",
            "\t\t\tTotal Training Recognition Loss 0.265727 || Total Training Translation Loss 0.009881\n",
            "2025-07-10 14:19:24,854 Epoch 3932:\n",
            "\t\t\tTotal Training Recognition Loss 0.715649 || Total Training Translation Loss 0.010393\n",
            "2025-07-10 14:19:25,091 Epoch 3933:\n",
            "\t\t\tTotal Training Recognition Loss 0.174057 || Total Training Translation Loss 0.010165\n",
            "2025-07-10 14:19:25,327 Epoch 3934:\n",
            "\t\t\tTotal Training Recognition Loss 3.856668 || Total Training Translation Loss 0.010138\n",
            "2025-07-10 14:19:25,568 Epoch 3935:\n",
            "\t\t\tTotal Training Recognition Loss 0.395571 || Total Training Translation Loss 0.010606\n",
            "2025-07-10 14:19:25,804 Epoch 3936:\n",
            "\t\t\tTotal Training Recognition Loss 4.723264 || Total Training Translation Loss 0.011197\n",
            "2025-07-10 14:19:26,051 Epoch 3937:\n",
            "\t\t\tTotal Training Recognition Loss 0.163529 || Total Training Translation Loss 0.010821\n",
            "2025-07-10 14:19:26,289 Epoch 3938:\n",
            "\t\t\tTotal Training Recognition Loss 0.131357 || Total Training Translation Loss 0.012384\n",
            "2025-07-10 14:19:26,526 Epoch 3939:\n",
            "\t\t\tTotal Training Recognition Loss 0.411832 || Total Training Translation Loss 0.010596\n",
            "2025-07-10 14:19:26,765 Epoch 3940:\n",
            "\t\t\tTotal Training Recognition Loss 0.505528 || Total Training Translation Loss 0.009946\n",
            "2025-07-10 14:19:27,000 Epoch 3941:\n",
            "\t\t\tTotal Training Recognition Loss 0.226210 || Total Training Translation Loss 0.011144\n",
            "2025-07-10 14:19:27,247 Epoch 3942:\n",
            "\t\t\tTotal Training Recognition Loss 0.333618 || Total Training Translation Loss 0.009561\n",
            "2025-07-10 14:19:27,493 Epoch 3943:\n",
            "\t\t\tTotal Training Recognition Loss 0.362526 || Total Training Translation Loss 0.009279\n",
            "2025-07-10 14:19:27,735 Epoch 3944:\n",
            "\t\t\tTotal Training Recognition Loss 0.305485 || Total Training Translation Loss 0.010292\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-07-10 14:19:27,978 Epoch 3945:\n",
            "\t\t\tTotal Training Recognition Loss 0.220949 || Total Training Translation Loss 0.009801\n",
            "2025-07-10 14:19:28,224 Epoch 3946:\n",
            "\t\t\tTotal Training Recognition Loss 0.105571 || Total Training Translation Loss 0.011321\n",
            "2025-07-10 14:19:28,468 Epoch 3947:\n",
            "\t\t\tTotal Training Recognition Loss 0.164428 || Total Training Translation Loss 0.009846\n",
            "2025-07-10 14:19:28,712 Epoch 3948:\n",
            "\t\t\tTotal Training Recognition Loss 0.135222 || Total Training Translation Loss 0.011777\n",
            "2025-07-10 14:19:28,954 Epoch 3949:\n",
            "\t\t\tTotal Training Recognition Loss 0.121157 || Total Training Translation Loss 0.009183\n",
            "2025-07-10 14:19:29,196 Epoch 3950:\n",
            "\t\t\tTotal Training Recognition Loss 0.081659 || Total Training Translation Loss 0.009746\n",
            "2025-07-10 14:19:29,441 Epoch 3951:\n",
            "\t\t\tTotal Training Recognition Loss 0.264189 || Total Training Translation Loss 0.010164\n",
            "2025-07-10 14:19:29,690 Epoch 3952:\n",
            "\t\t\tTotal Training Recognition Loss 0.087955 || Total Training Translation Loss 0.011943\n",
            "2025-07-10 14:19:29,933 Epoch 3953:\n",
            "\t\t\tTotal Training Recognition Loss 0.068013 || Total Training Translation Loss 0.007972\n",
            "2025-07-10 14:19:30,170 Epoch 3954:\n",
            "\t\t\tTotal Training Recognition Loss 0.122756 || Total Training Translation Loss 0.017409\n",
            "2025-07-10 14:19:30,411 Epoch 3955:\n",
            "\t\t\tTotal Training Recognition Loss 0.090625 || Total Training Translation Loss 0.011056\n",
            "2025-07-10 14:19:30,643 Epoch 3956:\n",
            "\t\t\tTotal Training Recognition Loss 0.352515 || Total Training Translation Loss 0.019217\n",
            "2025-07-10 14:19:30,881 Epoch 3957:\n",
            "\t\t\tTotal Training Recognition Loss 0.079019 || Total Training Translation Loss 0.016660\n",
            "2025-07-10 14:19:31,112 Epoch 3958:\n",
            "\t\t\tTotal Training Recognition Loss 0.103871 || Total Training Translation Loss 0.016660\n",
            "2025-07-10 14:19:31,355 Epoch 3959:\n",
            "\t\t\tTotal Training Recognition Loss 0.081689 || Total Training Translation Loss 0.014590\n",
            "2025-07-10 14:19:31,593 Epoch 3960:\n",
            "\t\t\tTotal Training Recognition Loss 0.113642 || Total Training Translation Loss 0.011688\n",
            "2025-07-10 14:19:31,833 Epoch 3961:\n",
            "\t\t\tTotal Training Recognition Loss 0.173696 || Total Training Translation Loss 0.011282\n",
            "2025-07-10 14:19:32,072 Epoch 3962:\n",
            "\t\t\tTotal Training Recognition Loss 0.076294 || Total Training Translation Loss 0.012939\n",
            "2025-07-10 14:19:32,317 Epoch 3963:\n",
            "\t\t\tTotal Training Recognition Loss 0.103063 || Total Training Translation Loss 0.013199\n",
            "2025-07-10 14:19:32,564 Epoch 3964:\n",
            "\t\t\tTotal Training Recognition Loss 0.139357 || Total Training Translation Loss 0.011632\n",
            "2025-07-10 14:19:32,811 Epoch 3965:\n",
            "\t\t\tTotal Training Recognition Loss 0.230727 || Total Training Translation Loss 0.010858\n",
            "2025-07-10 14:19:33,061 Epoch 3966:\n",
            "\t\t\tTotal Training Recognition Loss 0.226608 || Total Training Translation Loss 0.011678\n",
            "2025-07-10 14:19:33,306 Epoch 3967:\n",
            "\t\t\tTotal Training Recognition Loss 0.088562 || Total Training Translation Loss 0.012485\n",
            "2025-07-10 14:19:33,555 Epoch 3968:\n",
            "\t\t\tTotal Training Recognition Loss 0.062930 || Total Training Translation Loss 0.013147\n",
            "2025-07-10 14:19:33,797 Epoch 3969:\n",
            "\t\t\tTotal Training Recognition Loss 0.182100 || Total Training Translation Loss 0.012040\n",
            "2025-07-10 14:19:34,042 Epoch 3970:\n",
            "\t\t\tTotal Training Recognition Loss 0.167996 || Total Training Translation Loss 0.011668\n",
            "2025-07-10 14:19:34,294 Epoch 3971:\n",
            "\t\t\tTotal Training Recognition Loss 0.166154 || Total Training Translation Loss 0.011041\n",
            "2025-07-10 14:19:34,543 Epoch 3972:\n",
            "\t\t\tTotal Training Recognition Loss 1.027440 || Total Training Translation Loss 0.011835\n",
            "2025-07-10 14:19:34,805 Epoch 3973:\n",
            "\t\t\tTotal Training Recognition Loss 0.069090 || Total Training Translation Loss 0.010707\n",
            "2025-07-10 14:19:35,096 Epoch 3974:\n",
            "\t\t\tTotal Training Recognition Loss 0.110172 || Total Training Translation Loss 0.009840\n",
            "2025-07-10 14:19:35,381 Epoch 3975:\n",
            "\t\t\tTotal Training Recognition Loss 2.041621 || Total Training Translation Loss 0.011544\n",
            "2025-07-10 14:19:35,677 Epoch 3976:\n",
            "\t\t\tTotal Training Recognition Loss 0.158529 || Total Training Translation Loss 0.010636\n",
            "2025-07-10 14:19:35,968 Epoch 3977:\n",
            "\t\t\tTotal Training Recognition Loss 0.079886 || Total Training Translation Loss 0.008752\n",
            "2025-07-10 14:19:36,262 Epoch 3978:\n",
            "\t\t\tTotal Training Recognition Loss 0.075789 || Total Training Translation Loss 0.010459\n",
            "2025-07-10 14:19:36,519 Epoch 3979:\n",
            "\t\t\tTotal Training Recognition Loss 0.164930 || Total Training Translation Loss 0.010036\n",
            "2025-07-10 14:19:36,772 Epoch 3980:\n",
            "\t\t\tTotal Training Recognition Loss 10.608334 || Total Training Translation Loss 0.010173\n",
            "2025-07-10 14:19:37,026 Epoch 3981:\n",
            "\t\t\tTotal Training Recognition Loss 0.305438 || Total Training Translation Loss 0.011091\n",
            "2025-07-10 14:19:37,272 Epoch 3982:\n",
            "\t\t\tTotal Training Recognition Loss 0.425539 || Total Training Translation Loss 0.009417\n",
            "2025-07-10 14:19:37,528 Epoch 3983:\n",
            "\t\t\tTotal Training Recognition Loss 0.109215 || Total Training Translation Loss 0.012630\n",
            "2025-07-10 14:19:37,782 Epoch 3984:\n",
            "\t\t\tTotal Training Recognition Loss 14.377360 || Total Training Translation Loss 0.011564\n",
            "2025-07-10 14:19:38,049 Epoch 3985:\n",
            "\t\t\tTotal Training Recognition Loss 0.090086 || Total Training Translation Loss 0.011444\n",
            "2025-07-10 14:19:38,322 Epoch 3986:\n",
            "\t\t\tTotal Training Recognition Loss 0.061940 || Total Training Translation Loss 0.010060\n",
            "2025-07-10 14:19:38,588 Epoch 3987:\n",
            "\t\t\tTotal Training Recognition Loss 0.264418 || Total Training Translation Loss 0.008407\n",
            "2025-07-10 14:19:38,853 Epoch 3988:\n",
            "\t\t\tTotal Training Recognition Loss 0.131342 || Total Training Translation Loss 0.010441\n",
            "2025-07-10 14:19:39,119 Epoch 3989:\n",
            "\t\t\tTotal Training Recognition Loss 0.135089 || Total Training Translation Loss 0.008846\n",
            "2025-07-10 14:19:39,387 Epoch 3990:\n",
            "\t\t\tTotal Training Recognition Loss 0.072885 || Total Training Translation Loss 0.009072\n",
            "2025-07-10 14:19:39,660 Epoch 3991:\n",
            "\t\t\tTotal Training Recognition Loss 0.104155 || Total Training Translation Loss 0.009431\n",
            "2025-07-10 14:19:39,917 Epoch 3992:\n",
            "\t\t\tTotal Training Recognition Loss 0.086960 || Total Training Translation Loss 0.008814\n",
            "2025-07-10 14:19:40,188 Epoch 3993:\n",
            "\t\t\tTotal Training Recognition Loss 0.104837 || Total Training Translation Loss 0.009741\n",
            "2025-07-10 14:19:40,454 Epoch 3994:\n",
            "\t\t\tTotal Training Recognition Loss 0.132890 || Total Training Translation Loss 0.009082\n",
            "2025-07-10 14:19:40,716 Epoch 3995:\n",
            "\t\t\tTotal Training Recognition Loss 0.100250 || Total Training Translation Loss 0.011196\n",
            "2025-07-10 14:19:40,985 Epoch 3996:\n",
            "\t\t\tTotal Training Recognition Loss 0.198535 || Total Training Translation Loss 0.009055\n",
            "2025-07-10 14:19:41,263 Epoch 3997:\n",
            "\t\t\tTotal Training Recognition Loss 0.138115 || Total Training Translation Loss 0.010774\n",
            "2025-07-10 14:19:41,536 Epoch 3998:\n",
            "\t\t\tTotal Training Recognition Loss 0.097554 || Total Training Translation Loss 0.010970\n",
            "2025-07-10 14:19:41,797 Epoch 3999:\n",
            "\t\t\tTotal Training Recognition Loss 0.884359 || Total Training Translation Loss 0.008394\n",
            "2025-07-10 14:19:42,059 [Epoch: 4000 Step: 00004000] Batch Recognition Loss:   0.686245 => Gls Tokens per Sec:      142 || Batch Translation Loss:   0.009949 => Txt Tokens per Sec:      383 || Lr: 0.000490\n",
            "2025-07-10 14:19:42,545 Validation result at epoch 4000, step     4000: duration: 0.4847s\n",
            "\tRecognition Beam Size: 1\tTranslation Beam Size: 1\tTranslation Beam Alpha: -1\n",
            "\tRecognition Loss: 7648.19482\tTranslation Loss: 267.95676\tPPL: 23.39290\n",
            "\tEval Metric: BLEU\n",
            "\tWER 97.14\t(DEL: 25.71,\tINS: 17.14,\tSUB: 54.29)\n",
            "\tBLEU-4 2.69\t(BLEU-1: 6.00,\tBLEU-2: 4.07,\tBLEU-3: 3.29,\tBLEU-4: 2.69)\n",
            "\tCHRF 22.36\tROUGE 9.58\tFID 0.00\n",
            "2025-07-10 14:19:42,546 Logging Recognition and Translation Outputs\n",
            "2025-07-10 14:19:42,546 ========================================================================================\n",
            "2025-07-10 14:19:42,546 Logging Sequence: 11August_2010_Wednesday_tagesschau-2\n",
            "2025-07-10 14:19:42,547 \tGloss Reference :\t******** ******* DRUCK TIEF     KOMMEN\n",
            "2025-07-10 14:19:42,547 \tGloss Hypothesis:\tNORDWEST FEBRUAR REGEN NORDWEST REGEN \n",
            "2025-07-10 14:19:42,548 \tGloss Alignment :\tI        I       S     S        S     \n",
            "2025-07-10 14:19:42,548 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:19:42,549 \tText Reference  :\t*** *** tiefer           luftdruck bestimmt in         den ******** ****** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** nächsten tagen unser wetter\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-07-10 14:19:42,550 \tText Hypothesis :\tnun die wettervorhersage für       morgen   donnerstag den zwölften august <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>    <unk> <unk> <unk> \n",
            "2025-07-10 14:19:42,550 \tText Alignment  :\tI   I   S                S         S        S              I        I      I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     S        S     S     S     \n",
            "2025-07-10 14:19:42,550 ========================================================================================\n",
            "2025-07-10 14:19:42,550 Logging Sequence: 11August_2010_Wednesday_tagesschau-3\n",
            "2025-07-10 14:19:42,551 \tGloss Reference :\tES-BEDEUTET VIEL            WOLKE UND KOENNEN REGEN GEWITTER KOENNEN\n",
            "2025-07-10 14:19:42,551 \tGloss Hypothesis:\tWOLKE       UEBERSCHWEMMUNG WOLKE *** HEUTE   LOCH  SPEZIELL KOENNEN\n",
            "2025-07-10 14:19:42,551 \tGloss Alignment :\tS           S                     D   S       S     S               \n",
            "2025-07-10 14:19:42,552 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:19:42,553 \tText Reference  :\t*** *** **************** *** ****** ********** *** ******** ****** ***** ***** ***** ***** ***** ***** ***** ***** das   bedeutet viele wolken und   immer wieder zum   teil  kräftige schauer und   gewitter\n",
            "2025-07-10 14:19:42,553 \tText Hypothesis :\tnun die wettervorhersage für morgen donnerstag den zwölften august <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>    <unk> <unk>  <unk> <unk> <unk>  <unk> <unk> <unk>    <unk>   <unk> <unk>   \n",
            "2025-07-10 14:19:42,554 \tText Alignment  :\tI   I   I                I   I      I          I   I        I      I     I     I     I     I     I     I     I     S     S        S     S      S     S     S      S     S     S        S       S     S       \n",
            "2025-07-10 14:19:42,554 ========================================================================================\n",
            "2025-07-10 14:19:42,554 Logging Sequence: 11August_2010_Wednesday_tagesschau-8\n",
            "2025-07-10 14:19:42,555 \tGloss Reference :\t***** ******* WIND    MAESSIG SCHWACH  REGION WENN  GEWITTER WIND  KOENNEN\n",
            "2025-07-10 14:19:42,555 \tGloss Hypothesis:\tWOLKE TROCKEN BLEIBEN WOLKE   SPEZIELL WOLKE  REGEN KOENNEN  WOLKE KOENNEN\n",
            "2025-07-10 14:19:42,555 \tGloss Alignment :\tI     I       S       S       S        S      S     S        S            \n",
            "2025-07-10 14:19:42,555 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:19:42,557 \tText Reference  :\t****** ** **** ****** *** ******** **** ****** **************** **** ****** ******* meist weht  nur   ein   schwacher wind  aus   unterschiedlichen richtungen der   bei   schauern und   gewittern stark böig  sein  kann \n",
            "2025-07-10 14:19:42,557 \tText Hypothesis :\tregnet es auch länger und ergiebig auch lokale überschwemmungen sind wieder möglich <unk> <unk> <unk> <unk> <unk>     <unk> <unk> <unk>             <unk>      <unk> <unk> <unk>    <unk> <unk>     <unk> <unk> <unk> <unk>\n",
            "2025-07-10 14:19:42,558 \tText Alignment  :\tI      I  I    I      I   I        I    I      I                I    I      I       S     S     S     S     S         S     S     S                 S          S     S     S        S     S         S     S     S     S    \n",
            "2025-07-10 14:19:42,558 ========================================================================================\n",
            "2025-07-10 14:19:42,560 Logging Sequence: 25October_2010_Monday_tagesschau-22\n",
            "2025-07-10 14:19:42,560 \tGloss Reference :\t********* MITTWOCH REGEN KOENNEN NORDWEST WAHRSCHEINLICH NORD STARK WIND\n",
            "2025-07-10 14:19:42,560 \tGloss Hypothesis:\tTAGSUEBER JETZT    REGEN ******* ******** ************** **** ***** ****\n",
            "2025-07-10 14:19:42,561 \tGloss Alignment :\tI         S              D       D        D              D    D     D   \n",
            "2025-07-10 14:19:42,561 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:19:42,562 \tText Reference  :\t*** *** **************** *** ****** ********** *** ******** ****** ***** ***** ***** ***** ***** ***** ***** am    mittwoch hier  und   da    nieselregen in    der   nordwesthälfte an    den   küsten kräftiger wind \n",
            "2025-07-10 14:19:42,563 \tText Hypothesis :\tnun die wettervorhersage für morgen donnerstag den zwölften august <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>    <unk> <unk> <unk> <unk>       <unk> <unk> <unk>          <unk> <unk> <unk>  <unk>     <unk>\n",
            "2025-07-10 14:19:42,563 \tText Alignment  :\tI   I   I                I   I      I          I   I        I      I     I     I     I     I     I     I     S     S        S     S     S     S           S     S     S              S     S     S      S         S    \n",
            "2025-07-10 14:19:42,564 ========================================================================================\n",
            "2025-07-10 14:19:42,564 Logging Sequence: 05May_2011_Thursday_tagesschau-25\n",
            "2025-07-10 14:19:42,564 \tGloss Reference :\t****** JETZT WETTER WIE-AUSSEHEN MORGEN FREITAG SECHSTE    MAI   ZEIGEN-BILDSCHIRM\n",
            "2025-07-10 14:19:42,564 \tGloss Hypothesis:\tZWOELF JETZT WETTER ************ MORGEN ******* DONNERSTAG REGEN ZWOELF           \n",
            "2025-07-10 14:19:42,565 \tGloss Alignment :\tI                   D                   D       S          S     S                \n",
            "2025-07-10 14:19:42,565 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:19:42,566 \tText Reference  :\tund nun die wettervorhersage für morgen freitag    den ******** ****** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** sechsten mai  \n",
            "2025-07-10 14:19:42,566 \tText Hypothesis :\t*** nun die wettervorhersage für morgen donnerstag den zwölften august <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>    <unk>\n",
            "2025-07-10 14:19:42,567 \tText Alignment  :\tD                                       S              I        I      I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     S        S    \n",
            "2025-07-10 14:19:42,567 ========================================================================================\n",
            "2025-07-10 14:19:42,568 Epoch 4000:\n",
            "\t\t\tTotal Training Recognition Loss 0.686245 || Total Training Translation Loss 0.009949\n",
            "2025-07-10 14:19:42,827 Epoch 4001:\n",
            "\t\t\tTotal Training Recognition Loss 0.159779 || Total Training Translation Loss 0.010766\n",
            "2025-07-10 14:19:43,088 Epoch 4002:\n",
            "\t\t\tTotal Training Recognition Loss 0.402922 || Total Training Translation Loss 0.010680\n",
            "2025-07-10 14:19:43,350 Epoch 4003:\n",
            "\t\t\tTotal Training Recognition Loss 8.934750 || Total Training Translation Loss 0.009505\n",
            "2025-07-10 14:19:43,615 Epoch 4004:\n",
            "\t\t\tTotal Training Recognition Loss 0.517546 || Total Training Translation Loss 0.010432\n",
            "2025-07-10 14:19:43,878 Epoch 4005:\n",
            "\t\t\tTotal Training Recognition Loss 0.228133 || Total Training Translation Loss 0.010022\n",
            "2025-07-10 14:19:44,144 Epoch 4006:\n",
            "\t\t\tTotal Training Recognition Loss 0.167924 || Total Training Translation Loss 0.008218\n",
            "2025-07-10 14:19:44,412 Epoch 4007:\n",
            "\t\t\tTotal Training Recognition Loss 0.157899 || Total Training Translation Loss 0.010212\n",
            "2025-07-10 14:19:44,690 Epoch 4008:\n",
            "\t\t\tTotal Training Recognition Loss 0.131141 || Total Training Translation Loss 0.009152\n",
            "2025-07-10 14:19:44,969 Epoch 4009:\n",
            "\t\t\tTotal Training Recognition Loss 0.144493 || Total Training Translation Loss 0.009119\n",
            "2025-07-10 14:19:45,223 Epoch 4010:\n",
            "\t\t\tTotal Training Recognition Loss 0.065741 || Total Training Translation Loss 0.010103\n",
            "2025-07-10 14:19:45,499 Epoch 4011:\n",
            "\t\t\tTotal Training Recognition Loss 0.140566 || Total Training Translation Loss 0.008654\n",
            "2025-07-10 14:19:45,770 Epoch 4012:\n",
            "\t\t\tTotal Training Recognition Loss 0.135592 || Total Training Translation Loss 0.010193\n",
            "2025-07-10 14:19:46,028 Epoch 4013:\n",
            "\t\t\tTotal Training Recognition Loss 0.190276 || Total Training Translation Loss 0.009575\n",
            "2025-07-10 14:19:46,287 Epoch 4014:\n",
            "\t\t\tTotal Training Recognition Loss 0.127559 || Total Training Translation Loss 0.008520\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-07-10 14:19:46,550 Epoch 4015:\n",
            "\t\t\tTotal Training Recognition Loss 0.132126 || Total Training Translation Loss 0.008686\n",
            "2025-07-10 14:19:46,806 Epoch 4016:\n",
            "\t\t\tTotal Training Recognition Loss 0.089930 || Total Training Translation Loss 0.009139\n",
            "2025-07-10 14:19:47,069 Epoch 4017:\n",
            "\t\t\tTotal Training Recognition Loss 0.759159 || Total Training Translation Loss 0.007265\n",
            "2025-07-10 14:19:47,330 Epoch 4018:\n",
            "\t\t\tTotal Training Recognition Loss 0.197643 || Total Training Translation Loss 0.010536\n",
            "2025-07-10 14:19:47,594 Epoch 4019:\n",
            "\t\t\tTotal Training Recognition Loss 0.091746 || Total Training Translation Loss 0.009657\n",
            "2025-07-10 14:19:47,853 Epoch 4020:\n",
            "\t\t\tTotal Training Recognition Loss 0.405366 || Total Training Translation Loss 0.008551\n",
            "2025-07-10 14:19:48,093 Epoch 4021:\n",
            "\t\t\tTotal Training Recognition Loss 8.731907 || Total Training Translation Loss 0.008962\n",
            "2025-07-10 14:19:48,350 Epoch 4022:\n",
            "\t\t\tTotal Training Recognition Loss 1.532697 || Total Training Translation Loss 0.008671\n",
            "2025-07-10 14:19:48,599 Epoch 4023:\n",
            "\t\t\tTotal Training Recognition Loss 0.315813 || Total Training Translation Loss 0.007947\n",
            "2025-07-10 14:19:48,844 Epoch 4024:\n",
            "\t\t\tTotal Training Recognition Loss 0.169940 || Total Training Translation Loss 0.007482\n",
            "2025-07-10 14:19:49,096 Epoch 4025:\n",
            "\t\t\tTotal Training Recognition Loss 0.119641 || Total Training Translation Loss 0.009408\n",
            "2025-07-10 14:19:49,350 Epoch 4026:\n",
            "\t\t\tTotal Training Recognition Loss 0.297617 || Total Training Translation Loss 0.009365\n",
            "2025-07-10 14:19:49,620 Epoch 4027:\n",
            "\t\t\tTotal Training Recognition Loss 0.129017 || Total Training Translation Loss 0.009554\n",
            "2025-07-10 14:19:49,884 Epoch 4028:\n",
            "\t\t\tTotal Training Recognition Loss 0.127714 || Total Training Translation Loss 0.008624\n",
            "2025-07-10 14:19:50,138 Epoch 4029:\n",
            "\t\t\tTotal Training Recognition Loss 0.095569 || Total Training Translation Loss 0.008497\n",
            "2025-07-10 14:19:50,408 Epoch 4030:\n",
            "\t\t\tTotal Training Recognition Loss 0.063234 || Total Training Translation Loss 0.008812\n",
            "2025-07-10 14:19:50,675 Epoch 4031:\n",
            "\t\t\tTotal Training Recognition Loss 0.068459 || Total Training Translation Loss 0.009293\n",
            "2025-07-10 14:19:50,939 Epoch 4032:\n",
            "\t\t\tTotal Training Recognition Loss 0.104818 || Total Training Translation Loss 0.010457\n",
            "2025-07-10 14:19:51,209 Epoch 4033:\n",
            "\t\t\tTotal Training Recognition Loss 0.114601 || Total Training Translation Loss 0.007622\n",
            "2025-07-10 14:19:51,505 Epoch 4034:\n",
            "\t\t\tTotal Training Recognition Loss 0.074818 || Total Training Translation Loss 0.009806\n",
            "2025-07-10 14:19:51,801 Epoch 4035:\n",
            "\t\t\tTotal Training Recognition Loss 0.090674 || Total Training Translation Loss 0.009055\n",
            "2025-07-10 14:19:52,081 Epoch 4036:\n",
            "\t\t\tTotal Training Recognition Loss 0.055274 || Total Training Translation Loss 0.008439\n",
            "2025-07-10 14:19:52,353 Epoch 4037:\n",
            "\t\t\tTotal Training Recognition Loss 0.064147 || Total Training Translation Loss 0.010511\n",
            "2025-07-10 14:19:52,629 Epoch 4038:\n",
            "\t\t\tTotal Training Recognition Loss 0.137250 || Total Training Translation Loss 0.010903\n",
            "2025-07-10 14:19:52,910 Epoch 4039:\n",
            "\t\t\tTotal Training Recognition Loss 0.079387 || Total Training Translation Loss 0.008509\n",
            "2025-07-10 14:19:53,187 Epoch 4040:\n",
            "\t\t\tTotal Training Recognition Loss 0.083762 || Total Training Translation Loss 0.010900\n",
            "2025-07-10 14:19:53,467 Epoch 4041:\n",
            "\t\t\tTotal Training Recognition Loss 0.079971 || Total Training Translation Loss 0.008637\n",
            "2025-07-10 14:19:53,734 Epoch 4042:\n",
            "\t\t\tTotal Training Recognition Loss 0.134804 || Total Training Translation Loss 0.009550\n",
            "2025-07-10 14:19:54,055 Epoch 4043:\n",
            "\t\t\tTotal Training Recognition Loss 0.075217 || Total Training Translation Loss 0.008210\n",
            "2025-07-10 14:19:54,361 Epoch 4044:\n",
            "\t\t\tTotal Training Recognition Loss 0.186127 || Total Training Translation Loss 0.010151\n",
            "2025-07-10 14:19:54,680 Epoch 4045:\n",
            "\t\t\tTotal Training Recognition Loss 0.081241 || Total Training Translation Loss 0.009537\n",
            "2025-07-10 14:19:54,996 Epoch 4046:\n",
            "\t\t\tTotal Training Recognition Loss 0.304753 || Total Training Translation Loss 0.009083\n",
            "2025-07-10 14:19:55,314 Epoch 4047:\n",
            "\t\t\tTotal Training Recognition Loss 1.741288 || Total Training Translation Loss 0.009000\n",
            "2025-07-10 14:19:55,630 Epoch 4048:\n",
            "\t\t\tTotal Training Recognition Loss 0.235898 || Total Training Translation Loss 0.008501\n",
            "2025-07-10 14:19:55,948 Epoch 4049:\n",
            "\t\t\tTotal Training Recognition Loss 0.059081 || Total Training Translation Loss 0.010015\n",
            "2025-07-10 14:19:56,259 Epoch 4050:\n",
            "\t\t\tTotal Training Recognition Loss 0.053094 || Total Training Translation Loss 0.009047\n",
            "2025-07-10 14:19:56,536 Epoch 4051:\n",
            "\t\t\tTotal Training Recognition Loss 0.048656 || Total Training Translation Loss 0.009717\n",
            "2025-07-10 14:19:56,811 Epoch 4052:\n",
            "\t\t\tTotal Training Recognition Loss 0.065483 || Total Training Translation Loss 0.008707\n",
            "2025-07-10 14:19:57,095 Epoch 4053:\n",
            "\t\t\tTotal Training Recognition Loss 0.064446 || Total Training Translation Loss 0.010765\n",
            "2025-07-10 14:19:57,376 Epoch 4054:\n",
            "\t\t\tTotal Training Recognition Loss 0.103446 || Total Training Translation Loss 0.008774\n",
            "2025-07-10 14:19:57,654 Epoch 4055:\n",
            "\t\t\tTotal Training Recognition Loss 0.406332 || Total Training Translation Loss 0.009506\n",
            "2025-07-10 14:19:57,932 Epoch 4056:\n",
            "\t\t\tTotal Training Recognition Loss 0.367165 || Total Training Translation Loss 0.010276\n",
            "2025-07-10 14:19:58,210 Epoch 4057:\n",
            "\t\t\tTotal Training Recognition Loss 0.076333 || Total Training Translation Loss 0.011589\n",
            "2025-07-10 14:19:58,494 Epoch 4058:\n",
            "\t\t\tTotal Training Recognition Loss 0.179849 || Total Training Translation Loss 0.008920\n",
            "2025-07-10 14:19:58,784 Epoch 4059:\n",
            "\t\t\tTotal Training Recognition Loss 0.054086 || Total Training Translation Loss 0.009004\n",
            "2025-07-10 14:19:59,072 Epoch 4060:\n",
            "\t\t\tTotal Training Recognition Loss 0.080023 || Total Training Translation Loss 0.010502\n",
            "2025-07-10 14:19:59,351 Epoch 4061:\n",
            "\t\t\tTotal Training Recognition Loss 0.083596 || Total Training Translation Loss 0.012039\n",
            "2025-07-10 14:19:59,610 Epoch 4062:\n",
            "\t\t\tTotal Training Recognition Loss 0.065029 || Total Training Translation Loss 0.008890\n",
            "2025-07-10 14:19:59,892 Epoch 4063:\n",
            "\t\t\tTotal Training Recognition Loss 0.071921 || Total Training Translation Loss 0.010740\n",
            "2025-07-10 14:20:00,179 Epoch 4064:\n",
            "\t\t\tTotal Training Recognition Loss 0.099874 || Total Training Translation Loss 0.008838\n",
            "2025-07-10 14:20:00,499 Epoch 4065:\n",
            "\t\t\tTotal Training Recognition Loss 0.066145 || Total Training Translation Loss 0.009574\n",
            "2025-07-10 14:20:00,822 Epoch 4066:\n",
            "\t\t\tTotal Training Recognition Loss 0.042583 || Total Training Translation Loss 0.010203\n",
            "2025-07-10 14:20:01,138 Epoch 4067:\n",
            "\t\t\tTotal Training Recognition Loss 0.289039 || Total Training Translation Loss 0.009739\n",
            "2025-07-10 14:20:01,460 Epoch 4068:\n",
            "\t\t\tTotal Training Recognition Loss 0.069509 || Total Training Translation Loss 0.009384\n",
            "2025-07-10 14:20:01,774 Epoch 4069:\n",
            "\t\t\tTotal Training Recognition Loss 0.090324 || Total Training Translation Loss 0.011819\n",
            "2025-07-10 14:20:02,095 Epoch 4070:\n",
            "\t\t\tTotal Training Recognition Loss 0.107601 || Total Training Translation Loss 0.009577\n",
            "2025-07-10 14:20:02,415 Epoch 4071:\n",
            "\t\t\tTotal Training Recognition Loss 0.064227 || Total Training Translation Loss 0.011653\n",
            "2025-07-10 14:20:02,739 Epoch 4072:\n",
            "\t\t\tTotal Training Recognition Loss 0.059555 || Total Training Translation Loss 0.009277\n",
            "2025-07-10 14:20:03,068 Epoch 4073:\n",
            "\t\t\tTotal Training Recognition Loss 0.187077 || Total Training Translation Loss 0.010106\n",
            "2025-07-10 14:20:03,389 Epoch 4074:\n",
            "\t\t\tTotal Training Recognition Loss 0.079054 || Total Training Translation Loss 0.009388\n",
            "2025-07-10 14:20:03,716 Epoch 4075:\n",
            "\t\t\tTotal Training Recognition Loss 0.097319 || Total Training Translation Loss 0.010192\n",
            "2025-07-10 14:20:04,037 Epoch 4076:\n",
            "\t\t\tTotal Training Recognition Loss 0.040929 || Total Training Translation Loss 0.010391\n",
            "2025-07-10 14:20:04,357 Epoch 4077:\n",
            "\t\t\tTotal Training Recognition Loss 0.044132 || Total Training Translation Loss 0.010636\n",
            "2025-07-10 14:20:04,684 Epoch 4078:\n",
            "\t\t\tTotal Training Recognition Loss 0.072332 || Total Training Translation Loss 0.008562\n",
            "2025-07-10 14:20:05,016 Epoch 4079:\n",
            "\t\t\tTotal Training Recognition Loss 0.044671 || Total Training Translation Loss 0.009142\n",
            "2025-07-10 14:20:05,355 Epoch 4080:\n",
            "\t\t\tTotal Training Recognition Loss 0.065816 || Total Training Translation Loss 0.008830\n",
            "2025-07-10 14:20:05,682 Epoch 4081:\n",
            "\t\t\tTotal Training Recognition Loss 0.095933 || Total Training Translation Loss 0.009601\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-07-10 14:20:06,008 Epoch 4082:\n",
            "\t\t\tTotal Training Recognition Loss 0.124408 || Total Training Translation Loss 0.012973\n",
            "2025-07-10 14:20:06,340 Epoch 4083:\n",
            "\t\t\tTotal Training Recognition Loss 0.053745 || Total Training Translation Loss 0.009370\n",
            "2025-07-10 14:20:06,661 Epoch 4084:\n",
            "\t\t\tTotal Training Recognition Loss 0.067612 || Total Training Translation Loss 0.009901\n",
            "2025-07-10 14:20:06,989 Epoch 4085:\n",
            "\t\t\tTotal Training Recognition Loss 0.057957 || Total Training Translation Loss 0.010846\n",
            "2025-07-10 14:20:07,312 Epoch 4086:\n",
            "\t\t\tTotal Training Recognition Loss 0.054906 || Total Training Translation Loss 0.010803\n",
            "2025-07-10 14:20:07,645 Epoch 4087:\n",
            "\t\t\tTotal Training Recognition Loss 0.096986 || Total Training Translation Loss 0.011132\n",
            "2025-07-10 14:20:07,967 Epoch 4088:\n",
            "\t\t\tTotal Training Recognition Loss 0.057698 || Total Training Translation Loss 0.010305\n",
            "2025-07-10 14:20:08,301 Epoch 4089:\n",
            "\t\t\tTotal Training Recognition Loss 0.232567 || Total Training Translation Loss 0.009771\n",
            "2025-07-10 14:20:08,634 Epoch 4090:\n",
            "\t\t\tTotal Training Recognition Loss 0.204845 || Total Training Translation Loss 0.009397\n",
            "2025-07-10 14:20:08,972 Epoch 4091:\n",
            "\t\t\tTotal Training Recognition Loss 0.071133 || Total Training Translation Loss 0.008949\n",
            "2025-07-10 14:20:09,300 Epoch 4092:\n",
            "\t\t\tTotal Training Recognition Loss 0.118346 || Total Training Translation Loss 0.009433\n",
            "2025-07-10 14:20:09,630 Epoch 4093:\n",
            "\t\t\tTotal Training Recognition Loss 0.079993 || Total Training Translation Loss 0.010837\n",
            "2025-07-10 14:20:09,972 Epoch 4094:\n",
            "\t\t\tTotal Training Recognition Loss 0.123246 || Total Training Translation Loss 0.008982\n",
            "2025-07-10 14:20:10,307 Epoch 4095:\n",
            "\t\t\tTotal Training Recognition Loss 0.054548 || Total Training Translation Loss 0.010255\n",
            "2025-07-10 14:20:10,663 Epoch 4096:\n",
            "\t\t\tTotal Training Recognition Loss 0.057171 || Total Training Translation Loss 0.008324\n",
            "2025-07-10 14:20:11,001 Epoch 4097:\n",
            "\t\t\tTotal Training Recognition Loss 4.863966 || Total Training Translation Loss 0.010598\n",
            "2025-07-10 14:20:11,338 Epoch 4098:\n",
            "\t\t\tTotal Training Recognition Loss 0.035404 || Total Training Translation Loss 0.009113\n",
            "2025-07-10 14:20:11,649 Epoch 4099:\n",
            "\t\t\tTotal Training Recognition Loss 0.771623 || Total Training Translation Loss 0.009607\n",
            "2025-07-10 14:20:11,945 [Epoch: 4100 Step: 00004100] Batch Recognition Loss:   0.452805 => Gls Tokens per Sec:      126 || Batch Translation Loss:   0.008224 => Txt Tokens per Sec:      340 || Lr: 0.000490\n",
            "2025-07-10 14:20:12,583 Validation result at epoch 4100, step     4100: duration: 0.6371s\n",
            "\tRecognition Beam Size: 1\tTranslation Beam Size: 1\tTranslation Beam Alpha: -1\n",
            "\tRecognition Loss: 7020.08447\tTranslation Loss: 254.23509\tPPL: 19.90560\n",
            "\tEval Metric: BLEU\n",
            "\tWER 82.86\t(DEL: 40.00,\tINS: 2.86,\tSUB: 40.00)\n",
            "\tBLEU-4 2.69\t(BLEU-1: 6.00,\tBLEU-2: 4.07,\tBLEU-3: 3.29,\tBLEU-4: 2.69)\n",
            "\tCHRF 22.36\tROUGE 9.58\tFID 0.00\n",
            "2025-07-10 14:20:12,583 Logging Recognition and Translation Outputs\n",
            "2025-07-10 14:20:12,584 ========================================================================================\n",
            "2025-07-10 14:20:12,584 Logging Sequence: 11August_2010_Wednesday_tagesschau-2\n",
            "2025-07-10 14:20:12,584 \tGloss Reference :\tDRUCK TIEF KOMMEN  \n",
            "2025-07-10 14:20:12,585 \tGloss Hypothesis:\t***** **** NORDWEST\n",
            "2025-07-10 14:20:12,585 \tGloss Alignment :\tD     D    S       \n",
            "2025-07-10 14:20:12,585 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:20:12,587 \tText Reference  :\t*** *** tiefer           luftdruck bestimmt in         den ******** ****** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** nächsten tagen unser wetter\n",
            "2025-07-10 14:20:12,587 \tText Hypothesis :\tnun die wettervorhersage für       morgen   donnerstag den zwölften august <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>    <unk> <unk> <unk> \n",
            "2025-07-10 14:20:12,587 \tText Alignment  :\tI   I   S                S         S        S              I        I      I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     S        S     S     S     \n",
            "2025-07-10 14:20:12,588 ========================================================================================\n",
            "2025-07-10 14:20:12,588 Logging Sequence: 11August_2010_Wednesday_tagesschau-3\n",
            "2025-07-10 14:20:12,588 \tGloss Reference :\tES-BEDEUTET VIEL WOLKE UND             KOENNEN REGEN GEWITTER KOENNEN\n",
            "2025-07-10 14:20:12,588 \tGloss Hypothesis:\t*********** **** WOLKE UEBERSCHWEMMUNG WOLKE   HEUTE LOCH     KOENNEN\n",
            "2025-07-10 14:20:12,589 \tGloss Alignment :\tD           D          S               S       S     S               \n",
            "2025-07-10 14:20:12,589 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:20:12,590 \tText Reference  :\t*** *** **************** *** ****** ********** *** ******** ****** ***** ***** ***** ***** ***** ***** ***** ***** das   bedeutet viele wolken und   immer wieder zum   teil  kräftige schauer und   gewitter\n",
            "2025-07-10 14:20:12,591 \tText Hypothesis :\tnun die wettervorhersage für morgen donnerstag den zwölften august <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>    <unk> <unk>  <unk> <unk> <unk>  <unk> <unk> <unk>    <unk>   <unk> <unk>   \n",
            "2025-07-10 14:20:12,591 \tText Alignment  :\tI   I   I                I   I      I          I   I        I      I     I     I     I     I     I     I     I     S     S        S     S      S     S     S      S     S     S        S       S     S       \n",
            "2025-07-10 14:20:12,591 ========================================================================================\n",
            "2025-07-10 14:20:12,591 Logging Sequence: 11August_2010_Wednesday_tagesschau-8\n",
            "2025-07-10 14:20:12,592 \tGloss Reference :\tWIND MAESSIG SCHWACH REGION WENN     GEWITTER WIND  KOENNEN\n",
            "2025-07-10 14:20:12,592 \tGloss Hypothesis:\t**** TROCKEN BLEIBEN WOLKE  SPEZIELL KOENNEN  WOLKE KOENNEN\n",
            "2025-07-10 14:20:12,592 \tGloss Alignment :\tD    S       S       S      S        S        S            \n",
            "2025-07-10 14:20:12,592 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:20:12,594 \tText Reference  :\t****** ** **** ****** *** ******** **** ****** **************** **** ****** ******* meist weht  nur   ein   schwacher wind  aus   unterschiedlichen richtungen der   bei   schauern und   gewittern stark böig  sein  kann \n",
            "2025-07-10 14:20:12,595 \tText Hypothesis :\tregnet es auch länger und ergiebig auch lokale überschwemmungen sind wieder möglich <unk> <unk> <unk> <unk> <unk>     <unk> <unk> <unk>             <unk>      <unk> <unk> <unk>    <unk> <unk>     <unk> <unk> <unk> <unk>\n",
            "2025-07-10 14:20:12,595 \tText Alignment  :\tI      I  I    I      I   I        I    I      I                I    I      I       S     S     S     S     S         S     S     S                 S          S     S     S        S     S         S     S     S     S    \n",
            "2025-07-10 14:20:12,595 ========================================================================================\n",
            "2025-07-10 14:20:12,595 Logging Sequence: 25October_2010_Monday_tagesschau-22\n",
            "2025-07-10 14:20:12,595 \tGloss Reference :\tMITTWOCH REGEN KOENNEN NORDWEST WAHRSCHEINLICH NORD STARK WIND\n",
            "2025-07-10 14:20:12,597 \tGloss Hypothesis:\tGEWITTER REGEN ******* ******** ************** **** ***** ****\n",
            "2025-07-10 14:20:12,597 \tGloss Alignment :\tS              D       D        D              D    D     D   \n",
            "2025-07-10 14:20:12,597 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:20:12,599 \tText Reference  :\t*** *** **************** *** ****** ********** *** ******** ****** ***** ***** ***** ***** ***** ***** ***** am    mittwoch hier  und   da    nieselregen in    der   nordwesthälfte an    den   küsten kräftiger wind \n",
            "2025-07-10 14:20:12,599 \tText Hypothesis :\tnun die wettervorhersage für morgen donnerstag den zwölften august <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>    <unk> <unk> <unk> <unk>       <unk> <unk> <unk>          <unk> <unk> <unk>  <unk>     <unk>\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-07-10 14:20:12,600 \tText Alignment  :\tI   I   I                I   I      I          I   I        I      I     I     I     I     I     I     I     S     S        S     S     S     S           S     S     S              S     S     S      S         S    \n",
            "2025-07-10 14:20:12,600 ========================================================================================\n",
            "2025-07-10 14:20:12,600 Logging Sequence: 05May_2011_Thursday_tagesschau-25\n",
            "2025-07-10 14:20:12,600 \tGloss Reference :\t****** JETZT WETTER WIE-AUSSEHEN MORGEN FREITAG SECHSTE MAI        ZEIGEN-BILDSCHIRM\n",
            "2025-07-10 14:20:12,600 \tGloss Hypothesis:\tZWOELF JETZT WETTER ************ MORGEN ******* ******* DONNERSTAG ZWOELF           \n",
            "2025-07-10 14:20:12,601 \tGloss Alignment :\tI                   D                   D       D       S          S                \n",
            "2025-07-10 14:20:12,602 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:20:12,603 \tText Reference  :\tund nun die wettervorhersage für morgen freitag    den ******** ****** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** sechsten mai  \n",
            "2025-07-10 14:20:12,603 \tText Hypothesis :\t*** nun die wettervorhersage für morgen donnerstag den zwölften august <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>    <unk>\n",
            "2025-07-10 14:20:12,603 \tText Alignment  :\tD                                       S              I        I      I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     S        S    \n",
            "2025-07-10 14:20:12,603 ========================================================================================\n",
            "2025-07-10 14:20:12,604 Epoch 4100:\n",
            "\t\t\tTotal Training Recognition Loss 0.452805 || Total Training Translation Loss 0.008224\n",
            "2025-07-10 14:20:12,899 Epoch 4101:\n",
            "\t\t\tTotal Training Recognition Loss 0.054512 || Total Training Translation Loss 0.009516\n",
            "2025-07-10 14:20:13,191 Epoch 4102:\n",
            "\t\t\tTotal Training Recognition Loss 0.053438 || Total Training Translation Loss 0.010923\n",
            "2025-07-10 14:20:13,502 Epoch 4103:\n",
            "\t\t\tTotal Training Recognition Loss 0.739635 || Total Training Translation Loss 0.009084\n",
            "2025-07-10 14:20:13,813 Epoch 4104:\n",
            "\t\t\tTotal Training Recognition Loss 0.197912 || Total Training Translation Loss 0.011204\n",
            "2025-07-10 14:20:14,117 Epoch 4105:\n",
            "\t\t\tTotal Training Recognition Loss 0.064943 || Total Training Translation Loss 0.010450\n",
            "2025-07-10 14:20:14,459 Epoch 4106:\n",
            "\t\t\tTotal Training Recognition Loss 0.084134 || Total Training Translation Loss 0.010450\n",
            "2025-07-10 14:20:14,763 Epoch 4107:\n",
            "\t\t\tTotal Training Recognition Loss 0.074031 || Total Training Translation Loss 0.010309\n",
            "2025-07-10 14:20:15,120 Epoch 4108:\n",
            "\t\t\tTotal Training Recognition Loss 0.081213 || Total Training Translation Loss 0.008670\n",
            "2025-07-10 14:20:15,473 Epoch 4109:\n",
            "\t\t\tTotal Training Recognition Loss 0.936956 || Total Training Translation Loss 0.008964\n",
            "2025-07-10 14:20:15,793 Epoch 4110:\n",
            "\t\t\tTotal Training Recognition Loss 0.051015 || Total Training Translation Loss 0.009316\n",
            "2025-07-10 14:20:16,118 Epoch 4111:\n",
            "\t\t\tTotal Training Recognition Loss 0.396065 || Total Training Translation Loss 0.010680\n",
            "2025-07-10 14:20:16,434 Epoch 4112:\n",
            "\t\t\tTotal Training Recognition Loss 0.119025 || Total Training Translation Loss 0.009647\n",
            "2025-07-10 14:20:16,777 Epoch 4113:\n",
            "\t\t\tTotal Training Recognition Loss 0.105318 || Total Training Translation Loss 0.010274\n",
            "2025-07-10 14:20:17,126 Epoch 4114:\n",
            "\t\t\tTotal Training Recognition Loss 1.265596 || Total Training Translation Loss 0.009464\n",
            "2025-07-10 14:20:17,489 Epoch 4115:\n",
            "\t\t\tTotal Training Recognition Loss 0.135514 || Total Training Translation Loss 0.009655\n",
            "2025-07-10 14:20:17,839 Epoch 4116:\n",
            "\t\t\tTotal Training Recognition Loss 0.196145 || Total Training Translation Loss 0.009532\n",
            "2025-07-10 14:20:18,189 Epoch 4117:\n",
            "\t\t\tTotal Training Recognition Loss 0.044235 || Total Training Translation Loss 0.010203\n",
            "2025-07-10 14:20:18,544 Epoch 4118:\n",
            "\t\t\tTotal Training Recognition Loss 0.215571 || Total Training Translation Loss 0.008254\n",
            "2025-07-10 14:20:18,898 Epoch 4119:\n",
            "\t\t\tTotal Training Recognition Loss 0.073467 || Total Training Translation Loss 0.010287\n",
            "2025-07-10 14:20:19,249 Epoch 4120:\n",
            "\t\t\tTotal Training Recognition Loss 0.585819 || Total Training Translation Loss 0.008382\n",
            "2025-07-10 14:20:19,602 Epoch 4121:\n",
            "\t\t\tTotal Training Recognition Loss 0.068473 || Total Training Translation Loss 0.009729\n",
            "2025-07-10 14:20:19,954 Epoch 4122:\n",
            "\t\t\tTotal Training Recognition Loss 3.302312 || Total Training Translation Loss 0.009487\n",
            "2025-07-10 14:20:20,302 Epoch 4123:\n",
            "\t\t\tTotal Training Recognition Loss 0.096520 || Total Training Translation Loss 0.010002\n",
            "2025-07-10 14:20:20,664 Epoch 4124:\n",
            "\t\t\tTotal Training Recognition Loss 0.099019 || Total Training Translation Loss 0.008706\n",
            "2025-07-10 14:20:21,018 Epoch 4125:\n",
            "\t\t\tTotal Training Recognition Loss 0.065317 || Total Training Translation Loss 0.009044\n",
            "2025-07-10 14:20:21,374 Epoch 4126:\n",
            "\t\t\tTotal Training Recognition Loss 0.060296 || Total Training Translation Loss 0.009918\n",
            "2025-07-10 14:20:21,723 Epoch 4127:\n",
            "\t\t\tTotal Training Recognition Loss 1.078624 || Total Training Translation Loss 0.013641\n",
            "2025-07-10 14:20:22,082 Epoch 4128:\n",
            "\t\t\tTotal Training Recognition Loss 0.034443 || Total Training Translation Loss 0.009098\n",
            "2025-07-10 14:20:22,439 Epoch 4129:\n",
            "\t\t\tTotal Training Recognition Loss 0.075568 || Total Training Translation Loss 0.011511\n",
            "2025-07-10 14:20:22,794 Epoch 4130:\n",
            "\t\t\tTotal Training Recognition Loss 0.879126 || Total Training Translation Loss 0.009597\n",
            "2025-07-10 14:20:23,146 Epoch 4131:\n",
            "\t\t\tTotal Training Recognition Loss 0.082021 || Total Training Translation Loss 0.009061\n",
            "2025-07-10 14:20:23,509 Epoch 4132:\n",
            "\t\t\tTotal Training Recognition Loss 0.155399 || Total Training Translation Loss 0.011630\n",
            "2025-07-10 14:20:23,869 Epoch 4133:\n",
            "\t\t\tTotal Training Recognition Loss 0.131780 || Total Training Translation Loss 0.010672\n",
            "2025-07-10 14:20:24,228 Epoch 4134:\n",
            "\t\t\tTotal Training Recognition Loss 0.179727 || Total Training Translation Loss 0.011113\n",
            "2025-07-10 14:20:24,594 Epoch 4135:\n",
            "\t\t\tTotal Training Recognition Loss 0.087923 || Total Training Translation Loss 0.010101\n",
            "2025-07-10 14:20:24,950 Epoch 4136:\n",
            "\t\t\tTotal Training Recognition Loss 0.046914 || Total Training Translation Loss 0.010231\n",
            "2025-07-10 14:20:25,300 Epoch 4137:\n",
            "\t\t\tTotal Training Recognition Loss 0.096914 || Total Training Translation Loss 0.010412\n",
            "2025-07-10 14:20:25,649 Epoch 4138:\n",
            "\t\t\tTotal Training Recognition Loss 0.083587 || Total Training Translation Loss 0.010812\n",
            "2025-07-10 14:20:26,006 Epoch 4139:\n",
            "\t\t\tTotal Training Recognition Loss 0.052688 || Total Training Translation Loss 0.010105\n",
            "2025-07-10 14:20:26,363 Epoch 4140:\n",
            "\t\t\tTotal Training Recognition Loss 0.044932 || Total Training Translation Loss 0.009996\n",
            "2025-07-10 14:20:26,729 Epoch 4141:\n",
            "\t\t\tTotal Training Recognition Loss 0.143095 || Total Training Translation Loss 0.011457\n",
            "2025-07-10 14:20:27,090 Epoch 4142:\n",
            "\t\t\tTotal Training Recognition Loss 0.118079 || Total Training Translation Loss 0.007646\n",
            "2025-07-10 14:20:27,446 Epoch 4143:\n",
            "\t\t\tTotal Training Recognition Loss 0.050310 || Total Training Translation Loss 0.009678\n",
            "2025-07-10 14:20:27,807 Epoch 4144:\n",
            "\t\t\tTotal Training Recognition Loss 0.060546 || Total Training Translation Loss 0.011051\n",
            "2025-07-10 14:20:28,175 Epoch 4145:\n",
            "\t\t\tTotal Training Recognition Loss 0.039937 || Total Training Translation Loss 0.009082\n",
            "2025-07-10 14:20:28,541 Epoch 4146:\n",
            "\t\t\tTotal Training Recognition Loss 0.120350 || Total Training Translation Loss 0.012303\n",
            "2025-07-10 14:20:28,903 Epoch 4147:\n",
            "\t\t\tTotal Training Recognition Loss 0.043080 || Total Training Translation Loss 0.009054\n",
            "2025-07-10 14:20:29,282 Epoch 4148:\n",
            "\t\t\tTotal Training Recognition Loss 0.225323 || Total Training Translation Loss 0.010347\n",
            "2025-07-10 14:20:29,642 Epoch 4149:\n",
            "\t\t\tTotal Training Recognition Loss 7.822474 || Total Training Translation Loss 0.010466\n",
            "2025-07-10 14:20:30,009 Epoch 4150:\n",
            "\t\t\tTotal Training Recognition Loss 0.066714 || Total Training Translation Loss 0.009001\n",
            "2025-07-10 14:20:30,377 Epoch 4151:\n",
            "\t\t\tTotal Training Recognition Loss 1.207842 || Total Training Translation Loss 0.010696\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-07-10 14:20:30,744 Epoch 4152:\n",
            "\t\t\tTotal Training Recognition Loss 0.047971 || Total Training Translation Loss 0.010713\n",
            "2025-07-10 14:20:31,114 Epoch 4153:\n",
            "\t\t\tTotal Training Recognition Loss 0.242617 || Total Training Translation Loss 0.008322\n",
            "2025-07-10 14:20:31,467 Epoch 4154:\n",
            "\t\t\tTotal Training Recognition Loss 0.061066 || Total Training Translation Loss 0.009773\n",
            "2025-07-10 14:20:31,802 Epoch 4155:\n",
            "\t\t\tTotal Training Recognition Loss 0.066242 || Total Training Translation Loss 0.010762\n",
            "2025-07-10 14:20:32,145 Epoch 4156:\n",
            "\t\t\tTotal Training Recognition Loss 0.045876 || Total Training Translation Loss 0.008827\n",
            "2025-07-10 14:20:32,519 Epoch 4157:\n",
            "\t\t\tTotal Training Recognition Loss 0.057206 || Total Training Translation Loss 0.009884\n",
            "2025-07-10 14:20:32,893 Epoch 4158:\n",
            "\t\t\tTotal Training Recognition Loss 0.221889 || Total Training Translation Loss 0.009774\n",
            "2025-07-10 14:20:33,262 Epoch 4159:\n",
            "\t\t\tTotal Training Recognition Loss 0.189937 || Total Training Translation Loss 0.010159\n",
            "2025-07-10 14:20:33,607 Epoch 4160:\n",
            "\t\t\tTotal Training Recognition Loss 0.046770 || Total Training Translation Loss 0.007735\n",
            "2025-07-10 14:20:33,950 Epoch 4161:\n",
            "\t\t\tTotal Training Recognition Loss 0.042792 || Total Training Translation Loss 0.008358\n",
            "2025-07-10 14:20:34,289 Epoch 4162:\n",
            "\t\t\tTotal Training Recognition Loss 0.058102 || Total Training Translation Loss 0.010065\n",
            "2025-07-10 14:20:34,629 Epoch 4163:\n",
            "\t\t\tTotal Training Recognition Loss 0.110661 || Total Training Translation Loss 0.010710\n",
            "2025-07-10 14:20:34,998 Epoch 4164:\n",
            "\t\t\tTotal Training Recognition Loss 0.053739 || Total Training Translation Loss 0.009900\n",
            "2025-07-10 14:20:35,368 Epoch 4165:\n",
            "\t\t\tTotal Training Recognition Loss 0.065435 || Total Training Translation Loss 0.009645\n",
            "2025-07-10 14:20:35,741 Epoch 4166:\n",
            "\t\t\tTotal Training Recognition Loss 0.032097 || Total Training Translation Loss 0.009271\n",
            "2025-07-10 14:20:36,114 Epoch 4167:\n",
            "\t\t\tTotal Training Recognition Loss 0.075593 || Total Training Translation Loss 0.010536\n",
            "2025-07-10 14:20:36,479 Epoch 4168:\n",
            "\t\t\tTotal Training Recognition Loss 0.049004 || Total Training Translation Loss 0.011486\n",
            "2025-07-10 14:20:36,857 Epoch 4169:\n",
            "\t\t\tTotal Training Recognition Loss 0.040104 || Total Training Translation Loss 0.009571\n",
            "2025-07-10 14:20:37,234 Epoch 4170:\n",
            "\t\t\tTotal Training Recognition Loss 0.043972 || Total Training Translation Loss 0.008872\n",
            "2025-07-10 14:20:37,578 Epoch 4171:\n",
            "\t\t\tTotal Training Recognition Loss 0.055043 || Total Training Translation Loss 0.008947\n",
            "2025-07-10 14:20:37,917 Epoch 4172:\n",
            "\t\t\tTotal Training Recognition Loss 0.058334 || Total Training Translation Loss 0.009095\n",
            "2025-07-10 14:20:38,259 Epoch 4173:\n",
            "\t\t\tTotal Training Recognition Loss 0.073836 || Total Training Translation Loss 0.009316\n",
            "2025-07-10 14:20:38,604 Epoch 4174:\n",
            "\t\t\tTotal Training Recognition Loss 0.104120 || Total Training Translation Loss 0.008381\n",
            "2025-07-10 14:20:38,951 Epoch 4175:\n",
            "\t\t\tTotal Training Recognition Loss 0.260406 || Total Training Translation Loss 0.008970\n",
            "2025-07-10 14:20:39,295 Epoch 4176:\n",
            "\t\t\tTotal Training Recognition Loss 0.033829 || Total Training Translation Loss 0.008520\n",
            "2025-07-10 14:20:39,639 Epoch 4177:\n",
            "\t\t\tTotal Training Recognition Loss 0.049771 || Total Training Translation Loss 0.009363\n",
            "2025-07-10 14:20:39,986 Epoch 4178:\n",
            "\t\t\tTotal Training Recognition Loss 0.267894 || Total Training Translation Loss 0.010601\n",
            "2025-07-10 14:20:40,377 Epoch 4179:\n",
            "\t\t\tTotal Training Recognition Loss 0.036706 || Total Training Translation Loss 0.008895\n",
            "2025-07-10 14:20:40,765 Epoch 4180:\n",
            "\t\t\tTotal Training Recognition Loss 0.074639 || Total Training Translation Loss 0.009260\n",
            "2025-07-10 14:20:41,151 Epoch 4181:\n",
            "\t\t\tTotal Training Recognition Loss 0.033055 || Total Training Translation Loss 0.007937\n",
            "2025-07-10 14:20:41,534 Epoch 4182:\n",
            "\t\t\tTotal Training Recognition Loss 0.161070 || Total Training Translation Loss 0.009396\n",
            "2025-07-10 14:20:41,922 Epoch 4183:\n",
            "\t\t\tTotal Training Recognition Loss 0.385184 || Total Training Translation Loss 0.009081\n",
            "2025-07-10 14:20:42,323 Epoch 4184:\n",
            "\t\t\tTotal Training Recognition Loss 0.064708 || Total Training Translation Loss 0.008835\n",
            "2025-07-10 14:20:42,710 Epoch 4185:\n",
            "\t\t\tTotal Training Recognition Loss 0.050778 || Total Training Translation Loss 0.008738\n",
            "2025-07-10 14:20:43,107 Epoch 4186:\n",
            "\t\t\tTotal Training Recognition Loss 0.100130 || Total Training Translation Loss 0.008131\n",
            "2025-07-10 14:20:43,501 Epoch 4187:\n",
            "\t\t\tTotal Training Recognition Loss 0.042525 || Total Training Translation Loss 0.010644\n",
            "2025-07-10 14:20:43,886 Epoch 4188:\n",
            "\t\t\tTotal Training Recognition Loss 0.096463 || Total Training Translation Loss 0.010493\n",
            "2025-07-10 14:20:44,277 Epoch 4189:\n",
            "\t\t\tTotal Training Recognition Loss 0.043284 || Total Training Translation Loss 0.008661\n",
            "2025-07-10 14:20:44,660 Epoch 4190:\n",
            "\t\t\tTotal Training Recognition Loss 0.081238 || Total Training Translation Loss 0.011413\n",
            "2025-07-10 14:20:45,050 Epoch 4191:\n",
            "\t\t\tTotal Training Recognition Loss 0.085420 || Total Training Translation Loss 0.008803\n",
            "2025-07-10 14:20:45,438 Epoch 4192:\n",
            "\t\t\tTotal Training Recognition Loss 0.048842 || Total Training Translation Loss 0.009793\n",
            "2025-07-10 14:20:45,842 Epoch 4193:\n",
            "\t\t\tTotal Training Recognition Loss 0.069876 || Total Training Translation Loss 0.009816\n",
            "2025-07-10 14:20:46,231 Epoch 4194:\n",
            "\t\t\tTotal Training Recognition Loss 0.045867 || Total Training Translation Loss 0.008932\n",
            "2025-07-10 14:20:46,597 Epoch 4195:\n",
            "\t\t\tTotal Training Recognition Loss 0.108158 || Total Training Translation Loss 0.009574\n",
            "2025-07-10 14:20:46,974 Epoch 4196:\n",
            "\t\t\tTotal Training Recognition Loss 0.045639 || Total Training Translation Loss 0.011005\n",
            "2025-07-10 14:20:47,342 Epoch 4197:\n",
            "\t\t\tTotal Training Recognition Loss 0.128527 || Total Training Translation Loss 0.010732\n",
            "2025-07-10 14:20:47,725 Epoch 4198:\n",
            "\t\t\tTotal Training Recognition Loss 0.038026 || Total Training Translation Loss 0.009407\n",
            "2025-07-10 14:20:48,085 Epoch 4199:\n",
            "\t\t\tTotal Training Recognition Loss 0.035524 || Total Training Translation Loss 0.008722\n",
            "2025-07-10 14:20:48,441 [Epoch: 4200 Step: 00004200] Batch Recognition Loss:   0.031636 => Gls Tokens per Sec:      104 || Batch Translation Loss:   0.009524 => Txt Tokens per Sec:      282 || Lr: 0.000490\n",
            "2025-07-10 14:20:49,734 Validation result at epoch 4200, step     4200: duration: 1.2915s\n",
            "\tRecognition Beam Size: 1\tTranslation Beam Size: 1\tTranslation Beam Alpha: -1\n",
            "\tRecognition Loss: 7309.38428\tTranslation Loss: 249.53453\tPPL: 18.83469\n",
            "\tEval Metric: BLEU\n",
            "\tWER 91.43\t(DEL: 40.00,\tINS: 8.57,\tSUB: 42.86)\n",
            "\tBLEU-4 2.61\t(BLEU-1: 5.33,\tBLEU-2: 3.84,\tBLEU-3: 3.16,\tBLEU-4: 2.61)\n",
            "\tCHRF 19.78\tROUGE 8.44\tFID 0.00\n",
            "2025-07-10 14:20:49,745 Logging Recognition and Translation Outputs\n",
            "2025-07-10 14:20:49,745 ========================================================================================\n",
            "2025-07-10 14:20:49,746 Logging Sequence: 11August_2010_Wednesday_tagesschau-2\n",
            "2025-07-10 14:20:49,746 \tGloss Reference :\tDRUCK    TIEF       KOMMEN  \n",
            "2025-07-10 14:20:49,746 \tGloss Hypothesis:\tNORDWEST DONNERSTAG NORDWEST\n",
            "2025-07-10 14:20:49,747 \tGloss Alignment :\tS        S          S       \n",
            "2025-07-10 14:20:49,747 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:20:49,750 \tText Reference  :\t***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** tiefer luftdruck bestimmt in    den   nächsten tagen unser wetter\n",
            "2025-07-10 14:20:49,750 \tText Hypothesis :\t<unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>  <unk>     <unk>    <unk> <unk> <unk>    <unk> <unk> <unk> \n",
            "2025-07-10 14:20:49,751 \tText Alignment  :\tI     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     S      S         S        S     S     S        S     S     S     \n",
            "2025-07-10 14:20:49,751 ========================================================================================\n",
            "2025-07-10 14:20:49,751 Logging Sequence: 11August_2010_Wednesday_tagesschau-3\n",
            "2025-07-10 14:20:49,752 \tGloss Reference :\tES-BEDEUTET VIEL WOLKE UND KOENNEN REGEN GEWITTER KOENNEN\n",
            "2025-07-10 14:20:49,752 \tGloss Hypothesis:\t*********** **** WOLKE *** ******* HEUTE LOCH     KOENNEN\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-07-10 14:20:49,752 \tGloss Alignment :\tD           D          D   D       S     S               \n",
            "2025-07-10 14:20:49,753 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:20:49,757 \tText Reference  :\t*** *** **************** *** ****** ********** *** ******** ****** ***** ***** ***** ***** ***** ***** ***** ***** das   bedeutet viele wolken und   immer wieder zum   teil  kräftige schauer und   gewitter\n",
            "2025-07-10 14:20:49,757 \tText Hypothesis :\tnun die wettervorhersage für morgen donnerstag den zwölften august <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>    <unk> <unk>  <unk> <unk> <unk>  <unk> <unk> <unk>    <unk>   <unk> <unk>   \n",
            "2025-07-10 14:20:49,757 \tText Alignment  :\tI   I   I                I   I      I          I   I        I      I     I     I     I     I     I     I     I     S     S        S     S      S     S     S      S     S     S        S       S     S       \n",
            "2025-07-10 14:20:49,758 ========================================================================================\n",
            "2025-07-10 14:20:49,758 Logging Sequence: 11August_2010_Wednesday_tagesschau-8\n",
            "2025-07-10 14:20:49,759 \tGloss Reference :\tWIND MAESSIG SCHWACH REGION  WENN  GEWITTER WIND  KOENNEN\n",
            "2025-07-10 14:20:49,759 \tGloss Hypothesis:\t**** VIEL    TROCKEN BLEIBEN WOLKE KOENNEN  WOLKE KOENNEN\n",
            "2025-07-10 14:20:49,759 \tGloss Alignment :\tD    S       S       S       S     S        S            \n",
            "2025-07-10 14:20:49,760 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:20:49,762 \tText Reference  :\t****** ** **** ****** *** ******** **** ****** **************** **** ****** ******* meist weht  nur   ein   schwacher wind  aus   unterschiedlichen richtungen der   bei   schauern und   gewittern stark böig  sein  kann \n",
            "2025-07-10 14:20:49,763 \tText Hypothesis :\tregnet es auch länger und ergiebig auch lokale überschwemmungen sind wieder möglich <unk> <unk> <unk> <unk> <unk>     <unk> <unk> <unk>             <unk>      <unk> <unk> <unk>    <unk> <unk>     <unk> <unk> <unk> <unk>\n",
            "2025-07-10 14:20:49,763 \tText Alignment  :\tI      I  I    I      I   I        I    I      I                I    I      I       S     S     S     S     S         S     S     S                 S          S     S     S        S     S         S     S     S     S    \n",
            "2025-07-10 14:20:49,763 ========================================================================================\n",
            "2025-07-10 14:20:49,763 Logging Sequence: 25October_2010_Monday_tagesschau-22\n",
            "2025-07-10 14:20:49,764 \tGloss Reference :\tMITTWOCH REGEN KOENNEN NORDWEST WAHRSCHEINLICH NORD STARK    WIND  \n",
            "2025-07-10 14:20:49,764 \tGloss Hypothesis:\t******** ***** ******* ******** ************** **** GEWITTER ZWOELF\n",
            "2025-07-10 14:20:49,764 \tGloss Alignment :\tD        D     D       D        D              D    S        S     \n",
            "2025-07-10 14:20:49,764 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:20:49,766 \tText Reference  :\t*** *** **************** *** ****** ********** *** ******** ****** ***** ***** ***** ***** ***** ***** ***** am    mittwoch hier  und   da    nieselregen in    der   nordwesthälfte an    den   küsten kräftiger wind \n",
            "2025-07-10 14:20:49,766 \tText Hypothesis :\tnun die wettervorhersage für morgen donnerstag den zwölften august <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>    <unk> <unk> <unk> <unk>       <unk> <unk> <unk>          <unk> <unk> <unk>  <unk>     <unk>\n",
            "2025-07-10 14:20:49,766 \tText Alignment  :\tI   I   I                I   I      I          I   I        I      I     I     I     I     I     I     I     S     S        S     S     S     S           S     S     S              S     S     S      S         S    \n",
            "2025-07-10 14:20:49,767 ========================================================================================\n",
            "2025-07-10 14:20:49,767 Logging Sequence: 05May_2011_Thursday_tagesschau-25\n",
            "2025-07-10 14:20:49,767 \tGloss Reference :\t****** ***** **** JETZT WETTER WIE-AUSSEHEN MORGEN FREITAG SECHSTE MAI        ZEIGEN-BILDSCHIRM\n",
            "2025-07-10 14:20:49,767 \tGloss Hypothesis:\tZWOELF JETZT VIEL JETZT WETTER ************ MORGEN ******* ******* DONNERSTAG ZWOELF           \n",
            "2025-07-10 14:20:49,767 \tGloss Alignment :\tI      I     I                 D                   D       D       S          S                \n",
            "2025-07-10 14:20:49,768 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:20:49,769 \tText Reference  :\tund nun die wettervorhersage für morgen freitag    den ******** ****** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** sechsten mai  \n",
            "2025-07-10 14:20:49,769 \tText Hypothesis :\t*** nun die wettervorhersage für morgen donnerstag den zwölften august <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>    <unk>\n",
            "2025-07-10 14:20:49,770 \tText Alignment  :\tD                                       S              I        I      I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     S        S    \n",
            "2025-07-10 14:20:49,770 ========================================================================================\n",
            "2025-07-10 14:20:49,770 Epoch 4200:\n",
            "\t\t\tTotal Training Recognition Loss 0.031636 || Total Training Translation Loss 0.009524\n",
            "2025-07-10 14:20:50,156 Epoch 4201:\n",
            "\t\t\tTotal Training Recognition Loss 0.028181 || Total Training Translation Loss 0.009262\n",
            "2025-07-10 14:20:50,537 Epoch 4202:\n",
            "\t\t\tTotal Training Recognition Loss 0.038353 || Total Training Translation Loss 0.011115\n",
            "2025-07-10 14:20:50,895 Epoch 4203:\n",
            "\t\t\tTotal Training Recognition Loss 0.101955 || Total Training Translation Loss 0.009692\n",
            "2025-07-10 14:20:51,282 Epoch 4204:\n",
            "\t\t\tTotal Training Recognition Loss 0.057294 || Total Training Translation Loss 0.009803\n",
            "2025-07-10 14:20:51,670 Epoch 4205:\n",
            "\t\t\tTotal Training Recognition Loss 0.239716 || Total Training Translation Loss 0.008898\n",
            "2025-07-10 14:20:52,039 Epoch 4206:\n",
            "\t\t\tTotal Training Recognition Loss 0.073843 || Total Training Translation Loss 0.009363\n",
            "2025-07-10 14:20:52,404 Epoch 4207:\n",
            "\t\t\tTotal Training Recognition Loss 0.033172 || Total Training Translation Loss 0.010394\n",
            "2025-07-10 14:20:52,777 Epoch 4208:\n",
            "\t\t\tTotal Training Recognition Loss 0.033872 || Total Training Translation Loss 0.010784\n",
            "2025-07-10 14:20:53,150 Epoch 4209:\n",
            "\t\t\tTotal Training Recognition Loss 0.036196 || Total Training Translation Loss 0.010616\n",
            "2025-07-10 14:20:53,525 Epoch 4210:\n",
            "\t\t\tTotal Training Recognition Loss 0.058006 || Total Training Translation Loss 0.008562\n",
            "2025-07-10 14:20:53,905 Epoch 4211:\n",
            "\t\t\tTotal Training Recognition Loss 0.091133 || Total Training Translation Loss 0.010065\n",
            "2025-07-10 14:20:54,280 Epoch 4212:\n",
            "\t\t\tTotal Training Recognition Loss 0.049462 || Total Training Translation Loss 0.011301\n",
            "2025-07-10 14:20:54,640 Epoch 4213:\n",
            "\t\t\tTotal Training Recognition Loss 0.115311 || Total Training Translation Loss 0.010609\n",
            "2025-07-10 14:20:55,012 Epoch 4214:\n",
            "\t\t\tTotal Training Recognition Loss 0.034146 || Total Training Translation Loss 0.009464\n",
            "2025-07-10 14:20:55,376 Epoch 4215:\n",
            "\t\t\tTotal Training Recognition Loss 0.045872 || Total Training Translation Loss 0.009896\n",
            "2025-07-10 14:20:55,762 Epoch 4216:\n",
            "\t\t\tTotal Training Recognition Loss 0.075147 || Total Training Translation Loss 0.008316\n",
            "2025-07-10 14:20:56,139 Epoch 4217:\n",
            "\t\t\tTotal Training Recognition Loss 0.070493 || Total Training Translation Loss 0.010544\n",
            "2025-07-10 14:20:56,506 Epoch 4218:\n",
            "\t\t\tTotal Training Recognition Loss 0.050367 || Total Training Translation Loss 0.009556\n",
            "2025-07-10 14:20:56,917 Epoch 4219:\n",
            "\t\t\tTotal Training Recognition Loss 0.044438 || Total Training Translation Loss 0.010009\n",
            "2025-07-10 14:20:57,323 Epoch 4220:\n",
            "\t\t\tTotal Training Recognition Loss 0.037479 || Total Training Translation Loss 0.010840\n",
            "2025-07-10 14:20:57,739 Epoch 4221:\n",
            "\t\t\tTotal Training Recognition Loss 0.041441 || Total Training Translation Loss 0.009604\n",
            "2025-07-10 14:20:58,160 Epoch 4222:\n",
            "\t\t\tTotal Training Recognition Loss 0.057627 || Total Training Translation Loss 0.009114\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-07-10 14:20:58,564 Epoch 4223:\n",
            "\t\t\tTotal Training Recognition Loss 0.042047 || Total Training Translation Loss 0.009423\n",
            "2025-07-10 14:20:58,969 Epoch 4224:\n",
            "\t\t\tTotal Training Recognition Loss 0.044053 || Total Training Translation Loss 0.009913\n",
            "2025-07-10 14:20:59,343 Epoch 4225:\n",
            "\t\t\tTotal Training Recognition Loss 0.092589 || Total Training Translation Loss 0.008989\n",
            "2025-07-10 14:20:59,714 Epoch 4226:\n",
            "\t\t\tTotal Training Recognition Loss 0.026935 || Total Training Translation Loss 0.009888\n",
            "2025-07-10 14:21:00,134 Epoch 4227:\n",
            "\t\t\tTotal Training Recognition Loss 0.045621 || Total Training Translation Loss 0.011692\n",
            "2025-07-10 14:21:00,550 Epoch 4228:\n",
            "\t\t\tTotal Training Recognition Loss 0.032441 || Total Training Translation Loss 0.009851\n",
            "2025-07-10 14:21:00,974 Epoch 4229:\n",
            "\t\t\tTotal Training Recognition Loss 0.090101 || Total Training Translation Loss 0.009577\n",
            "2025-07-10 14:21:01,396 Epoch 4230:\n",
            "\t\t\tTotal Training Recognition Loss 0.058823 || Total Training Translation Loss 0.010563\n",
            "2025-07-10 14:21:01,819 Epoch 4231:\n",
            "\t\t\tTotal Training Recognition Loss 0.033801 || Total Training Translation Loss 0.009023\n",
            "2025-07-10 14:21:02,223 Epoch 4232:\n",
            "\t\t\tTotal Training Recognition Loss 0.038028 || Total Training Translation Loss 0.010165\n",
            "2025-07-10 14:21:02,608 Epoch 4233:\n",
            "\t\t\tTotal Training Recognition Loss 0.032923 || Total Training Translation Loss 0.009886\n",
            "2025-07-10 14:21:03,003 Epoch 4234:\n",
            "\t\t\tTotal Training Recognition Loss 0.056156 || Total Training Translation Loss 0.008911\n",
            "2025-07-10 14:21:03,427 Epoch 4235:\n",
            "\t\t\tTotal Training Recognition Loss 0.127813 || Total Training Translation Loss 0.009909\n",
            "2025-07-10 14:21:03,826 Epoch 4236:\n",
            "\t\t\tTotal Training Recognition Loss 0.106060 || Total Training Translation Loss 0.012055\n",
            "2025-07-10 14:21:04,224 Epoch 4237:\n",
            "\t\t\tTotal Training Recognition Loss 0.033613 || Total Training Translation Loss 0.009616\n",
            "2025-07-10 14:21:04,619 Epoch 4238:\n",
            "\t\t\tTotal Training Recognition Loss 0.047329 || Total Training Translation Loss 0.010748\n",
            "2025-07-10 14:21:05,017 Epoch 4239:\n",
            "\t\t\tTotal Training Recognition Loss 23.842842 || Total Training Translation Loss 0.008424\n",
            "2025-07-10 14:21:05,444 Epoch 4240:\n",
            "\t\t\tTotal Training Recognition Loss 0.064034 || Total Training Translation Loss 0.010180\n",
            "2025-07-10 14:21:05,853 Epoch 4241:\n",
            "\t\t\tTotal Training Recognition Loss 0.061409 || Total Training Translation Loss 0.010837\n",
            "2025-07-10 14:21:06,241 Epoch 4242:\n",
            "\t\t\tTotal Training Recognition Loss 0.071039 || Total Training Translation Loss 0.008693\n",
            "2025-07-10 14:21:06,622 Epoch 4243:\n",
            "\t\t\tTotal Training Recognition Loss 0.947398 || Total Training Translation Loss 0.009356\n",
            "2025-07-10 14:21:07,025 Epoch 4244:\n",
            "\t\t\tTotal Training Recognition Loss 0.057853 || Total Training Translation Loss 0.009412\n",
            "2025-07-10 14:21:07,429 Epoch 4245:\n",
            "\t\t\tTotal Training Recognition Loss 0.457353 || Total Training Translation Loss 0.008465\n",
            "2025-07-10 14:21:07,832 Epoch 4246:\n",
            "\t\t\tTotal Training Recognition Loss 0.083617 || Total Training Translation Loss 0.009364\n",
            "2025-07-10 14:21:08,223 Epoch 4247:\n",
            "\t\t\tTotal Training Recognition Loss 0.143350 || Total Training Translation Loss 0.010063\n",
            "2025-07-10 14:21:08,614 Epoch 4248:\n",
            "\t\t\tTotal Training Recognition Loss 3.781343 || Total Training Translation Loss 0.012103\n",
            "2025-07-10 14:21:09,010 Epoch 4249:\n",
            "\t\t\tTotal Training Recognition Loss 0.220713 || Total Training Translation Loss 0.011256\n",
            "2025-07-10 14:21:09,401 Epoch 4250:\n",
            "\t\t\tTotal Training Recognition Loss 36.649597 || Total Training Translation Loss 0.010551\n",
            "2025-07-10 14:21:09,796 Epoch 4251:\n",
            "\t\t\tTotal Training Recognition Loss 0.121143 || Total Training Translation Loss 0.008832\n",
            "2025-07-10 14:21:10,192 Epoch 4252:\n",
            "\t\t\tTotal Training Recognition Loss 0.167679 || Total Training Translation Loss 0.009968\n",
            "2025-07-10 14:21:10,592 Epoch 4253:\n",
            "\t\t\tTotal Training Recognition Loss 0.047777 || Total Training Translation Loss 0.009448\n",
            "2025-07-10 14:21:10,994 Epoch 4254:\n",
            "\t\t\tTotal Training Recognition Loss 0.075829 || Total Training Translation Loss 0.011476\n",
            "2025-07-10 14:21:11,404 Epoch 4255:\n",
            "\t\t\tTotal Training Recognition Loss 0.051628 || Total Training Translation Loss 0.010718\n",
            "2025-07-10 14:21:11,805 Epoch 4256:\n",
            "\t\t\tTotal Training Recognition Loss 0.052736 || Total Training Translation Loss 0.010882\n",
            "2025-07-10 14:21:12,200 Epoch 4257:\n",
            "\t\t\tTotal Training Recognition Loss 0.101169 || Total Training Translation Loss 0.008581\n",
            "2025-07-10 14:21:12,589 Epoch 4258:\n",
            "\t\t\tTotal Training Recognition Loss 0.077572 || Total Training Translation Loss 0.009960\n",
            "2025-07-10 14:21:12,985 Epoch 4259:\n",
            "\t\t\tTotal Training Recognition Loss 0.081592 || Total Training Translation Loss 0.009955\n",
            "2025-07-10 14:21:13,370 Epoch 4260:\n",
            "\t\t\tTotal Training Recognition Loss 0.572026 || Total Training Translation Loss 0.010114\n",
            "2025-07-10 14:21:13,758 Epoch 4261:\n",
            "\t\t\tTotal Training Recognition Loss 0.208180 || Total Training Translation Loss 0.008580\n",
            "2025-07-10 14:21:14,141 Epoch 4262:\n",
            "\t\t\tTotal Training Recognition Loss 0.312918 || Total Training Translation Loss 0.009867\n",
            "2025-07-10 14:21:14,538 Epoch 4263:\n",
            "\t\t\tTotal Training Recognition Loss 22.452650 || Total Training Translation Loss 0.010064\n",
            "2025-07-10 14:21:14,977 Epoch 4264:\n",
            "\t\t\tTotal Training Recognition Loss 0.252740 || Total Training Translation Loss 0.008736\n",
            "2025-07-10 14:21:15,412 Epoch 4265:\n",
            "\t\t\tTotal Training Recognition Loss 0.200374 || Total Training Translation Loss 0.009544\n",
            "2025-07-10 14:21:15,838 Epoch 4266:\n",
            "\t\t\tTotal Training Recognition Loss 9.457278 || Total Training Translation Loss 0.010120\n",
            "2025-07-10 14:21:16,249 Epoch 4267:\n",
            "\t\t\tTotal Training Recognition Loss 2.018291 || Total Training Translation Loss 0.008376\n",
            "2025-07-10 14:21:16,645 Epoch 4268:\n",
            "\t\t\tTotal Training Recognition Loss 1.133638 || Total Training Translation Loss 0.010355\n",
            "2025-07-10 14:21:17,055 Epoch 4269:\n",
            "\t\t\tTotal Training Recognition Loss 0.315400 || Total Training Translation Loss 0.010402\n",
            "2025-07-10 14:21:17,451 Epoch 4270:\n",
            "\t\t\tTotal Training Recognition Loss 0.705377 || Total Training Translation Loss 0.011112\n",
            "2025-07-10 14:21:17,853 Epoch 4271:\n",
            "\t\t\tTotal Training Recognition Loss 0.091580 || Total Training Translation Loss 0.009338\n",
            "2025-07-10 14:21:18,266 Epoch 4272:\n",
            "\t\t\tTotal Training Recognition Loss 0.069110 || Total Training Translation Loss 0.010393\n",
            "2025-07-10 14:21:18,683 Epoch 4273:\n",
            "\t\t\tTotal Training Recognition Loss 0.099476 || Total Training Translation Loss 0.010744\n",
            "2025-07-10 14:21:19,075 Epoch 4274:\n",
            "\t\t\tTotal Training Recognition Loss 0.171538 || Total Training Translation Loss 0.009123\n",
            "2025-07-10 14:21:19,480 Epoch 4275:\n",
            "\t\t\tTotal Training Recognition Loss 1.276149 || Total Training Translation Loss 0.010956\n",
            "2025-07-10 14:21:19,876 Epoch 4276:\n",
            "\t\t\tTotal Training Recognition Loss 0.289391 || Total Training Translation Loss 0.009802\n",
            "2025-07-10 14:21:20,276 Epoch 4277:\n",
            "\t\t\tTotal Training Recognition Loss 0.118487 || Total Training Translation Loss 0.009569\n",
            "2025-07-10 14:21:20,693 Epoch 4278:\n",
            "\t\t\tTotal Training Recognition Loss 0.336579 || Total Training Translation Loss 0.009614\n",
            "2025-07-10 14:21:21,107 Epoch 4279:\n",
            "\t\t\tTotal Training Recognition Loss 27.898792 || Total Training Translation Loss 0.010837\n",
            "2025-07-10 14:21:21,513 Epoch 4280:\n",
            "\t\t\tTotal Training Recognition Loss 3.773183 || Total Training Translation Loss 0.010728\n",
            "2025-07-10 14:21:21,968 Epoch 4281:\n",
            "\t\t\tTotal Training Recognition Loss 0.087401 || Total Training Translation Loss 0.008652\n",
            "2025-07-10 14:21:22,421 Epoch 4282:\n",
            "\t\t\tTotal Training Recognition Loss 0.067466 || Total Training Translation Loss 0.009326\n",
            "2025-07-10 14:21:22,873 Epoch 4283:\n",
            "\t\t\tTotal Training Recognition Loss 0.188223 || Total Training Translation Loss 0.008458\n",
            "2025-07-10 14:21:23,326 Epoch 4284:\n",
            "\t\t\tTotal Training Recognition Loss 0.127753 || Total Training Translation Loss 0.008805\n",
            "2025-07-10 14:21:23,749 Epoch 4285:\n",
            "\t\t\tTotal Training Recognition Loss 1.540138 || Total Training Translation Loss 0.010763\n",
            "2025-07-10 14:21:24,161 Epoch 4286:\n",
            "\t\t\tTotal Training Recognition Loss 0.070860 || Total Training Translation Loss 0.009691\n",
            "2025-07-10 14:21:24,562 Epoch 4287:\n",
            "\t\t\tTotal Training Recognition Loss 2.238169 || Total Training Translation Loss 0.008811\n",
            "2025-07-10 14:21:24,965 Epoch 4288:\n",
            "\t\t\tTotal Training Recognition Loss 1.142641 || Total Training Translation Loss 0.011936\n",
            "2025-07-10 14:21:25,399 Epoch 4289:\n",
            "\t\t\tTotal Training Recognition Loss 0.701714 || Total Training Translation Loss 0.009059\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-07-10 14:21:25,834 Epoch 4290:\n",
            "\t\t\tTotal Training Recognition Loss 3.415990 || Total Training Translation Loss 0.009690\n",
            "2025-07-10 14:21:26,290 Epoch 4291:\n",
            "\t\t\tTotal Training Recognition Loss 2.169148 || Total Training Translation Loss 0.007913\n",
            "2025-07-10 14:21:26,749 Epoch 4292:\n",
            "\t\t\tTotal Training Recognition Loss 8.071148 || Total Training Translation Loss 0.008120\n",
            "2025-07-10 14:21:27,209 Epoch 4293:\n",
            "\t\t\tTotal Training Recognition Loss 1.704355 || Total Training Translation Loss 0.009846\n",
            "2025-07-10 14:21:27,667 Epoch 4294:\n",
            "\t\t\tTotal Training Recognition Loss 1.174782 || Total Training Translation Loss 0.008514\n",
            "2025-07-10 14:21:28,140 Epoch 4295:\n",
            "\t\t\tTotal Training Recognition Loss 0.221128 || Total Training Translation Loss 0.010870\n",
            "2025-07-10 14:21:28,611 Epoch 4296:\n",
            "\t\t\tTotal Training Recognition Loss 0.225697 || Total Training Translation Loss 0.009409\n",
            "2025-07-10 14:21:29,080 Epoch 4297:\n",
            "\t\t\tTotal Training Recognition Loss 0.096146 || Total Training Translation Loss 0.010253\n",
            "2025-07-10 14:21:29,538 Epoch 4298:\n",
            "\t\t\tTotal Training Recognition Loss 4.252007 || Total Training Translation Loss 0.010061\n",
            "2025-07-10 14:21:29,993 Epoch 4299:\n",
            "\t\t\tTotal Training Recognition Loss 0.157294 || Total Training Translation Loss 0.009844\n",
            "2025-07-10 14:21:30,453 [Epoch: 4300 Step: 00004300] Batch Recognition Loss:   0.132095 => Gls Tokens per Sec:       81 || Batch Translation Loss:   0.010006 => Txt Tokens per Sec:      218 || Lr: 0.000490\n",
            "2025-07-10 14:21:32,370 Validation result at epoch 4300, step     4300: duration: 1.9153s\n",
            "\tRecognition Beam Size: 1\tTranslation Beam Size: 1\tTranslation Beam Alpha: -1\n",
            "\tRecognition Loss: 8219.25879\tTranslation Loss: 244.62378\tPPL: 17.77738\n",
            "\tEval Metric: BLEU\n",
            "\tWER 88.57\t(DEL: 17.14,\tINS: 5.71,\tSUB: 65.71)\n",
            "\tBLEU-4 2.76\t(BLEU-1: 6.67,\tBLEU-2: 4.29,\tBLEU-3: 3.40,\tBLEU-4: 2.76)\n",
            "\tCHRF 21.46\tROUGE 10.39\tFID 0.00\n",
            "2025-07-10 14:21:32,370 Logging Recognition and Translation Outputs\n",
            "2025-07-10 14:21:32,371 ========================================================================================\n",
            "2025-07-10 14:21:32,371 Logging Sequence: 11August_2010_Wednesday_tagesschau-2\n",
            "2025-07-10 14:21:32,372 \tGloss Reference :\tDRUCK    TIEF    KOMMEN  \n",
            "2025-07-10 14:21:32,372 \tGloss Hypothesis:\tNORDWEST FEBRUAR NORDWEST\n",
            "2025-07-10 14:21:32,372 \tGloss Alignment :\tS        S       S       \n",
            "2025-07-10 14:21:32,372 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:21:32,375 \tText Reference  :\t***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** tiefer luftdruck bestimmt in    den   nächsten tagen unser wetter\n",
            "2025-07-10 14:21:32,375 \tText Hypothesis :\t<unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>  <unk>     <unk>    <unk> <unk> <unk>    <unk> <unk> <unk> \n",
            "2025-07-10 14:21:32,376 \tText Alignment  :\tI     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     S      S         S        S     S     S        S     S     S     \n",
            "2025-07-10 14:21:32,376 ========================================================================================\n",
            "2025-07-10 14:21:32,377 Logging Sequence: 11August_2010_Wednesday_tagesschau-3\n",
            "2025-07-10 14:21:32,377 \tGloss Reference :\tES-BEDEUTET VIEL            WOLKE UND      KOENNEN REGEN GEWITTER KOENNEN\n",
            "2025-07-10 14:21:32,378 \tGloss Hypothesis:\tWOLKE       UEBERSCHWEMMUNG WOLKE NORDWEST LOCH    HEUTE SPEZIELL KOENNEN\n",
            "2025-07-10 14:21:32,378 \tGloss Alignment :\tS           S                     S        S       S     S               \n",
            "2025-07-10 14:21:32,378 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:21:32,382 \tText Reference  :\t*** *** **************** *** ****** ********** *** ******** ****** ***** ***** ***** ***** ***** ***** ***** ***** das   bedeutet viele wolken und   immer wieder zum   teil  kräftige schauer und   gewitter\n",
            "2025-07-10 14:21:32,382 \tText Hypothesis :\tnun die wettervorhersage für morgen donnerstag den zwölften august <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>    <unk> <unk>  <unk> <unk> <unk>  <unk> <unk> <unk>    <unk>   <unk> <unk>   \n",
            "2025-07-10 14:21:32,383 \tText Alignment  :\tI   I   I                I   I      I          I   I        I      I     I     I     I     I     I     I     I     S     S        S     S      S     S     S      S     S     S        S       S     S       \n",
            "2025-07-10 14:21:32,383 ========================================================================================\n",
            "2025-07-10 14:21:32,383 Logging Sequence: 11August_2010_Wednesday_tagesschau-8\n",
            "2025-07-10 14:21:32,384 \tGloss Reference :\t***** WIND MAESSIG SCHWACH REGION WENN  GEWITTER WIND  KOENNEN\n",
            "2025-07-10 14:21:32,384 \tGloss Hypothesis:\tDURCH LOCH TROCKEN BLEIBEN WOLKE  REGEN KOENNEN  WOLKE KOENNEN\n",
            "2025-07-10 14:21:32,384 \tGloss Alignment :\tI     S    S       S       S      S     S        S            \n",
            "2025-07-10 14:21:32,385 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:21:32,389 \tText Reference  :\t****** ** **** ****** *** ******** **** ****** **************** **** ****** ******* meist weht  nur   ein   schwacher wind  aus   unterschiedlichen richtungen der   bei   schauern und   gewittern stark böig  sein  kann \n",
            "2025-07-10 14:21:32,390 \tText Hypothesis :\tregnet es auch länger und ergiebig auch lokale überschwemmungen sind wieder möglich <unk> <unk> <unk> <unk> <unk>     <unk> <unk> <unk>             <unk>      <unk> <unk> <unk>    <unk> <unk>     <unk> <unk> <unk> <unk>\n",
            "2025-07-10 14:21:32,390 \tText Alignment  :\tI      I  I    I      I   I        I    I      I                I    I      I       S     S     S     S     S         S     S     S                 S          S     S     S        S     S         S     S     S     S    \n",
            "2025-07-10 14:21:32,390 ========================================================================================\n",
            "2025-07-10 14:21:32,390 Logging Sequence: 25October_2010_Monday_tagesschau-22\n",
            "2025-07-10 14:21:32,391 \tGloss Reference :\tMITTWOCH REGEN KOENNEN NORDWEST WAHRSCHEINLICH NORD STARK WIND\n",
            "2025-07-10 14:21:32,392 \tGloss Hypothesis:\t******** ***** ******* ******** ************** OFT  JETZT VIEL\n",
            "2025-07-10 14:21:32,392 \tGloss Alignment :\tD        D     D       D        D              S    S     S   \n",
            "2025-07-10 14:21:32,392 \t--------------------------------------------------------------------------------------------------------------------\n",
            "2025-07-10 14:21:32,396 \tText Reference  :\tam *** **** ** ********** *** **** ******** ******* mittwoch hier     und ** ******* ******** ****** ********* *********** ***** ***** da    nieselregen in    der   nordwesthälfte an    den   küsten kräftiger   wind       \n",
            "2025-07-10 14:21:32,396 \tText Hypothesis :\tam tag gibt es verbreitet zum teil kräftige schauer oder     gewitter und in manchen regionen fallen ergiebige regenmengen <unk> <unk> <unk> <unk>       <unk> <unk> <unk>          <unk> <unk> <unk>  regenmengen regenmengen\n",
            "2025-07-10 14:21:32,397 \tText Alignment  :\t   I   I    I  I          I   I    I        I       S        S            I  I       I        I      I         I           I     I     S     S           S     S     S              S     S     S      S           S          \n",
            "2025-07-10 14:21:32,397 ========================================================================================\n",
            "2025-07-10 14:21:32,397 Logging Sequence: 05May_2011_Thursday_tagesschau-25\n",
            "2025-07-10 14:21:32,398 \tGloss Reference :\t****** JETZT WETTER WIE-AUSSEHEN MORGEN FREITAG    SECHSTE MAI ZEIGEN-BILDSCHIRM\n",
            "2025-07-10 14:21:32,398 \tGloss Hypothesis:\tZWOELF JETZT WETTER ************ MORGEN DONNERSTAG REGEN   OFT ZWOELF           \n",
            "2025-07-10 14:21:32,398 \tGloss Alignment :\tI                   D                   S          S       S   S                \n",
            "2025-07-10 14:21:32,398 \t--------------------------------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-07-10 14:21:32,401 \tText Reference  :\tund nun die wettervorhersage für morgen freitag    den ******** ****** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** ***** sechsten mai  \n",
            "2025-07-10 14:21:32,401 \tText Hypothesis :\t*** nun die wettervorhersage für morgen donnerstag den zwölften august <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>    <unk>\n",
            "2025-07-10 14:21:32,402 \tText Alignment  :\tD                                       S              I        I      I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     I     S        S    \n",
            "2025-07-10 14:21:32,402 ========================================================================================\n",
            "2025-07-10 14:21:32,403 Training ended since there were no improvements inthe last learning rate step: 0.000490\n",
            "2025-07-10 14:21:32,403 Best validation result at step 2500: 2.893129 eval_metric.\n",
            "2025-07-10 14:21:32,517 ----------------------------------------------------------------------------------------\n",
            "2025-07-10 14:21:32,518 [DEV] partition [RECOGNITION] experiment [BW]: 10\n",
            "2025-07-10 14:21:37,945 finished in 5.4270s \n",
            "2025-07-10 14:21:37,946 ****************************************************************************************\n",
            "2025-07-10 14:21:37,946 [DEV] partition [RECOGNITION] results:\n",
            "\tNew Best CTC Decode Beam Size: 10\n",
            "\tWER 88.57\t(DEL: 20.00,\tINS: 2.86,\tSUB: 65.71)\n",
            "2025-07-10 14:21:37,946 ****************************************************************************************\n",
            "2025-07-10 14:21:37,946 ========================================================================================\n",
            "2025-07-10 14:21:42,016 [DEV] partition [Translation] results:\n",
            "\tNew Best Translation Beam Size: 10 and Alpha: -1\n",
            "\tBLEU-4 2.89\t(BLEU-1: 8.00,\tBLEU-2: 4.70,\tBLEU-3: 3.62,\tBLEU-4: 2.89)\n",
            "\tCHRF 24.19\tROUGE 12.39\tFID 0.64\tMPJPE 0.59\tMPVPE 0.00\tMPJAE 32.36\t\n",
            "2025-07-10 14:21:42,016 ----------------------------------------------------------------------------------------\n",
            "2025-07-10 14:22:15,564 ****************************************************************************************\n",
            "2025-07-10 14:22:15,565 [DEV] partition [Recognition & Translation] results:\n",
            "\tBest CTC Decode Beam Size: 10\n",
            "\tBest Translation Beam Size: 10 and Alpha: -1\n",
            "\tWER 88.57\t(DEL: 20.00,\tINS: 2.86,\tSUB: 65.71)\n",
            "\tBLEU-4 2.89\t(BLEU-1: 8.00,\tBLEU-2: 4.70,\tBLEU-3: 3.62,\tBLEU-4: 2.89)\n",
            "\tCHRF 24.19\tROUGE 12.39\tFID 0.64\tMPJPE 0.59\tMPVPE 0.00\tMPJAE 32.36\n",
            "2025-07-10 14:22:15,565 ****************************************************************************************\n",
            "2025-07-10 14:22:23,301 [TEST] partition [Recognition & Translation] results:\n",
            "\tBest CTC Decode Beam Size: 10\n",
            "\tBest Translation Beam Size: 10 and Alpha: -1\n",
            "\tWER 139.58\t(DEL: 14.58,\tINS: 45.83,\tSUB: 79.17)\n",
            "\tBLEU-4 0.00\t(BLEU-1: 4.00,\tBLEU-2: 0.00,\tBLEU-3: 0.00,\tBLEU-4: 0.00)\n",
            "\tCHRF 16.29\tROUGE 4.14\tFID 0.35\tMPJPE 0.32\tMPVPE 0.00\tMPJAE 19.02\n",
            "2025-07-10 14:22:23,301 ****************************************************************************************\n"
          ]
        }
      ],
      "source": [
        "train(cfg_file=config_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inference for Sign-IDD\n",
        "## Dataset creation\n"
      ],
      "metadata": {
        "id": "74LOjMD7yeZH"
      },
      "id": "74LOjMD7yeZH"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Predicted dev set\n"
      ],
      "metadata": {
        "id": "-NaBxf1TxtkK"
      },
      "id": "-NaBxf1TxtkK"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "436375a0",
      "metadata": {
        "id": "436375a0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f7f74f5-3604-44f0-b7f9-a3767a318c89"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved 5 samples to /content/drive/MyDrive/Sign-IDD SLT/data/Inference/phoenix14t.infer.skels.dev\n"
          ]
        }
      ],
      "source": [
        "import gzip\n",
        "import joblib\n",
        "import pandas as pd\n",
        "\n",
        "# Config\n",
        "feature_dim = 150\n",
        "input_csv = \"/content/drive/MyDrive/Sign-IDD SLT/data/PHOENIX2014T/Dev/dev.csv\"\n",
        "input_skels = \"/content/drive/MyDrive/Sign-IDD SLT/data/Inference/dev.pred.skels\"\n",
        "output_dir = \"/content/drive/MyDrive/Sign-IDD SLT/data/Inference\"\n",
        "output_file = os.path.join(output_dir, \"phoenix14t.infer.skels.dev\")\n",
        "\n",
        "# Ensure output directory exists\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Load metadata CSV\n",
        "df = pd.read_csv(input_csv, sep=\"|\")\n",
        "assert len(df) == 5, \"Expected 5 samples in dev.csv\"\n",
        "\n",
        "# Read skeleton lines\n",
        "with open(input_skels, \"r\") as f:\n",
        "    skel_lines = [line.strip() for line in f.readlines()]\n",
        "\n",
        "assert len(skel_lines) == len(df), \"Mismatch between CSV and skels line count\"\n",
        "\n",
        "samples = []\n",
        "\n",
        "for idx, (line, meta) in enumerate(zip(skel_lines, df.itertuples())):\n",
        "    raw = list(map(float, line.split()))\n",
        "    assert len(raw) % (feature_dim + 1) == 0, f\"Line {idx} isn't multiple of {feature_dim+1}\"\n",
        "\n",
        "    num_frames = len(raw) // (feature_dim + 1)\n",
        "    frames = []\n",
        "    for i in range(num_frames):\n",
        "        frame_vec = raw[i * (feature_dim + 1) : i * (feature_dim + 1) + feature_dim]\n",
        "        frames.append(torch.tensor(frame_vec, dtype=torch.float32))\n",
        "\n",
        "    sign_tensor = torch.stack(frames, dim=0)  # [T, 150]\n",
        "\n",
        "    sample = {\n",
        "        \"name\": meta.name,\n",
        "        \"signer\": meta.speaker,\n",
        "        \"sign\": sign_tensor + 1e-8,  # small value added for numerical stability\n",
        "        \"gloss\": meta.orth.strip(),\n",
        "        \"text\": meta.translation.strip(),\n",
        "    }\n",
        "    samples.append(sample)\n",
        "\n",
        "# Save as joblib file in gzip\n",
        "with gzip.open(output_file, \"wb\") as f:\n",
        "    joblib.dump(samples, f)\n",
        "\n",
        "print(f\"Saved {len(samples)} samples to {output_file}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Predicted test set"
      ],
      "metadata": {
        "id": "-CLsu8vJzNtF"
      },
      "id": "-CLsu8vJzNtF"
    },
    {
      "cell_type": "code",
      "source": [
        "import gzip\n",
        "import joblib\n",
        "import pandas as pd\n",
        "\n",
        "# Config\n",
        "feature_dim = 150\n",
        "input_csv = \"/content/drive/MyDrive/Sign-IDD SLT/data/PHOENIX2014T/Test/test.csv\"\n",
        "input_skels = \"/content/drive/MyDrive/Sign-IDD SLT/data/Inference/test.pred.skels\"\n",
        "output_dir = \"/content/drive/MyDrive/Sign-IDD SLT/data/Inference\"\n",
        "output_file = os.path.join(output_dir, \"phoenix14t.infer.skels.test\")\n",
        "\n",
        "# Ensure output directory exists\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Load metadata CSV\n",
        "df = pd.read_csv(input_csv, sep=\"|\")\n",
        "assert len(df) == 5, \"Expected 5 samples in dev.csv\"\n",
        "\n",
        "# Read skeleton lines\n",
        "with open(input_skels, \"r\") as f:\n",
        "    skel_lines = [line.strip() for line in f.readlines()]\n",
        "\n",
        "assert len(skel_lines) == len(df), \"Mismatch between CSV and skels line count\"\n",
        "\n",
        "samples = []\n",
        "\n",
        "for idx, (line, meta) in enumerate(zip(skel_lines, df.itertuples())):\n",
        "    raw = list(map(float, line.split()))\n",
        "    assert len(raw) % (feature_dim + 1) == 0, f\"Line {idx} isn't multiple of {feature_dim+1}\"\n",
        "\n",
        "    num_frames = len(raw) // (feature_dim + 1)\n",
        "    frames = []\n",
        "    for i in range(num_frames):\n",
        "        frame_vec = raw[i * (feature_dim + 1) : i * (feature_dim + 1) + feature_dim]\n",
        "        frames.append(torch.tensor(frame_vec, dtype=torch.float32))\n",
        "\n",
        "    sign_tensor = torch.stack(frames, dim=0)  # [T, 150]\n",
        "\n",
        "    sample = {\n",
        "        \"name\": meta.name,\n",
        "        \"signer\": meta.speaker,\n",
        "        \"sign\": sign_tensor + 1e-8,  # small value added for numerical stability\n",
        "        \"gloss\": meta.orth.strip(),\n",
        "        \"text\": meta.translation.strip(),\n",
        "    }\n",
        "    samples.append(sample)\n",
        "\n",
        "# Save as joblib file in gzip\n",
        "with gzip.open(output_file, \"wb\") as f:\n",
        "    joblib.dump(samples, f)\n",
        "\n",
        "print(f\"Saved {len(samples)} samples to {output_file}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x4WaXOVczKHY",
        "outputId": "efb913c4-3394-41f7-b30b-154c93d36a5b"
      },
      "id": "x4WaXOVczKHY",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved 5 samples to /content/drive/MyDrive/Sign-IDD SLT/data/Inference/phoenix14t.infer.skels.test\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gzip\n",
        "import joblib\n",
        "\n",
        "# Load the skels file\n",
        "with gzip.open(\"/content/drive/MyDrive/Sign-IDD SLT/data/Inference/phoenix14t.infer.skels.dev\", \"rb\") as f:\n",
        "    data = joblib.load(f)\n",
        "\n",
        "# View one sample (e.g., the first one)\n",
        "sample = data[0]\n",
        "\n",
        "# Print contents\n",
        "print(\"Name:\", sample[\"name\"])\n",
        "print(\"Signer:\", sample[\"signer\"])\n",
        "print(\"Gloss:\", sample[\"gloss\"])\n",
        "print(\"Text:\", sample[\"text\"])\n",
        "print(\"Sign shape:\", sample[\"sign\"].shape)\n",
        "print(\"Sign (first frame):\", sample[\"sign\"][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QqdSmjZRzqxL",
        "outputId": "276b9908-827f-4650-c4b8-ca7210fe46f4"
      },
      "id": "QqdSmjZRzqxL",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: 11August_2010_Wednesday_tagesschau-2\n",
            "Signer: Signer08\n",
            "Gloss: DRUCK TIEF KOMMEN\n",
            "Text: tiefer luftdruck bestimmt in den nächsten tagen unser wetter\n",
            "Sign shape: torch.Size([147, 150])\n",
            "Sign (first frame): tensor([-0.0316, -0.1996, -0.0297, -0.0043, -0.0012,  0.0011,  0.1654,  0.0018,\n",
            "         0.0159,  0.2371,  0.2798,  0.1127,  0.1562,  0.3521,  0.2921, -0.1664,\n",
            "        -0.0060,  0.0012, -0.2597,  0.2816, -0.0046, -0.1560,  0.2862,  0.1769,\n",
            "        -0.1463,  0.2380,  0.1782, -0.1395,  0.2109,  0.1759, -0.1012,  0.2228,\n",
            "         0.1767, -0.0789,  0.2407,  0.1733, -0.0525,  0.2499,  0.1719, -0.1002,\n",
            "         0.2174,  0.2192, -0.0735,  0.2389,  0.2391, -0.0659,  0.2610,  0.2419,\n",
            "        -0.0731,  0.2666,  0.2605, -0.1245,  0.2290,  0.2265, -0.0934,  0.2569,\n",
            "         0.2264, -0.0819,  0.2803,  0.2277, -0.0912,  0.2791,  0.2510, -0.1415,\n",
            "         0.2519,  0.2220, -0.1133,  0.2724,  0.2234, -0.0995,  0.2918,  0.2217,\n",
            "        -0.1070,  0.2927,  0.2393, -0.1511,  0.2785,  0.1915, -0.1309,  0.2939,\n",
            "         0.1879, -0.1189,  0.3056,  0.1891, -0.1223,  0.3101,  0.2035,  0.1533,\n",
            "         0.2965,  0.2871,  0.1350,  0.2739,  0.2895,  0.1045,  0.3000,  0.2879,\n",
            "         0.0904,  0.3270,  0.2906,  0.0790,  0.3546,  0.2899,  0.0988,  0.3316,\n",
            "         0.2889,  0.0765,  0.3679,  0.2890,  0.0695,  0.3924,  0.2893,  0.0725,\n",
            "         0.3999,  0.3087,  0.1184,  0.3366,  0.2880,  0.0973,  0.3753,  0.2895,\n",
            "         0.0912,  0.3999,  0.2900,  0.0960,  0.3945,  0.3116,  0.1393,  0.3379,\n",
            "         0.2896,  0.1262,  0.3703,  0.2912,  0.1176,  0.3925,  0.2935,  0.1169,\n",
            "         0.3967,  0.3085,  0.1528,  0.3359,  0.2894,  0.1436,  0.3631,  0.2900,\n",
            "         0.1405,  0.3770,  0.2899,  0.1391,  0.3910,  0.2902])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inference_config_path=\"/content/drive/MyDrive/Sign-IDD SLT/configs/sign_inference.yaml\""
      ],
      "metadata": {
        "id": "vT0mmHQyz2vh"
      },
      "id": "vT0mmHQyz2vh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test(cfg_file=inference_config_path,\n",
        "     ckpt='/content/drive/MyDrive/Sign-IDD SLT/signjoey/sign_skels_model/best.ckpt',\n",
        "     output_path='/content/drive/MyDrive/Sign-IDD SLT/signjoey/sign_skels_model/Inference_output')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "H-g0cyhaHXlV",
        "outputId": "74253107-8350-46fc-b495-c5d9f99104cf"
      },
      "id": "H-g0cyhaHXlV",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:prediction:----------------------------------------------------------------------------------------\n",
            "INFO:prediction:[DEV] partition [RECOGNITION] experiment [BW]: 10\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "The size of tensor a (147) must match the size of tensor b (148) at non-singleton dimension 3",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-17-1118412316.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m test(cfg_file=inference_config_path,\n\u001b[0m\u001b[1;32m      2\u001b[0m      \u001b[0mckpt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'/content/drive/MyDrive/Sign-IDD SLT/signjoey/sign_skels_model/best.ckpt'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m      output_path='/content/drive/MyDrive/Sign-IDD SLT/signjoey/sign_skels_model/Inference_output')\n",
            "\u001b[0;32m/content/drive/MyDrive/Sign-IDD SLT/signjoey/prediction.py\u001b[0m in \u001b[0;36mtest\u001b[0;34m(cfg_file, ckpt, output_path, logger)\u001b[0m\n\u001b[1;32m    439\u001b[0m             \u001b[0mvalid_start_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"[DEV] partition [RECOGNITION] experiment [BW]: %d\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrbw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 441\u001b[0;31m             dev_recognition_results[rbw] = validate_on_data(\n\u001b[0m\u001b[1;32m    442\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m                 \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdev_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/MyDrive/Sign-IDD SLT/signjoey/prediction.py\u001b[0m in \u001b[0;36mvalidate_on_data\u001b[0;34m(model, data, ground_data, batch_size, use_cuda, sgn_dim, do_recognition, recognition_loss_function, recognition_loss_weight, do_translation, translation_loss_function, translation_loss_weight, translation_max_output_length, level, txt_pad_index, recognition_beam_size, translation_beam_size, translation_beam_alpha, batch_type, dataset_version, frame_subsampling_ratio, i)\u001b[0m\n\u001b[1;32m    186\u001b[0m                 \u001b[0mbatch_signs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0mbatch_sign_grounds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m             \u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m                 \u001b[0mbatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m                 \u001b[0mbatch_ground\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_ground\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/MyDrive/Sign-IDD SLT/signjoey/model.py\u001b[0m in \u001b[0;36mrun_batch\u001b[0;34m(self, batch, batch_ground, recognition_beam_size, translation_beam_size, translation_beam_alpha, translation_max_output_length)\u001b[0m\n\u001b[1;32m    279\u001b[0m         )\n\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 281\u001b[0;31m         encoder_output1, encoder_hidden = self.encode(\n\u001b[0m\u001b[1;32m    282\u001b[0m             \u001b[0msgn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msgn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msgn_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_ground\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msgn_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msgn_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_ground\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msgn_lengths\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m         )\n",
            "\u001b[0;32m/content/drive/MyDrive/Sign-IDD SLT/signjoey/model.py\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, sgn, sgn_mask, sgn_length)\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0;32mreturn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mencoder\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_concat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m         \"\"\"\n\u001b[0;32m--> 153\u001b[0;31m         return self.encoder(\n\u001b[0m\u001b[1;32m    154\u001b[0m             \u001b[0membed_src\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msgn_embed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msgn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msgn_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0msrc_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msgn_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/MyDrive/Sign-IDD SLT/signjoey/encoders.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, embed_src, src_length, mask)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    240\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/MyDrive/Sign-IDD SLT/signjoey/transformer_layers.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, mask)\u001b[0m\n\u001b[1;32m    204\u001b[0m         \"\"\"\n\u001b[1;32m    205\u001b[0m         \u001b[0mx_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc_src_att\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0mo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/MyDrive/Sign-IDD SLT/signjoey/transformer_layers.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, k, v, q, mask)\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;31m# we add a dimension for the heads to it below: [B, 1, 1, M]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m             \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmasked_fill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"-inf\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;31m# apply attention dropout and compute context vectors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (147) must match the size of tensor b (148) at non-singleton dimension 3"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Path to your file\n",
        "file_path = \"/content/drive/MyDrive/Sign-IDD SLT/data/Inference/dev.pred.skels\"\n",
        "\n",
        "# Read and parse the file\n",
        "with open(file_path, 'r') as f:\n",
        "    lines = f.readlines()\n",
        "\n",
        "# Assume 151 values per frame\n",
        "frame_dim = 151\n",
        "feature_dim = 150\n",
        "\n",
        "# Parse and visualize each line\n",
        "for i, line in enumerate(lines):\n",
        "    values = np.fromstring(line.strip(), sep=' ')\n",
        "    num_frames = len(values) // frame_dim\n",
        "    data = values.reshape(num_frames, frame_dim)\n",
        "\n",
        "    features = data[:, :feature_dim]\n",
        "    counters = data[:, -1]\n",
        "\n",
        "\n",
        "    print(f\"\\nLine {i+1} - {num_frames} frames\")\n",
        "    print(\"Frame\\tCounter\\tSample Features\")\n",
        "    j=0;\n",
        "    for f in range(num_frames):\n",
        "        print(f\"{f+1}\\t{counters[f]}\\t{features[f, :5]} ...\")\n",
        "\n",
        "    # Optional: Plot counter values across frames\n",
        "    plt.figure(figsize=(8, 2))\n",
        "    plt.plot(counters, marker='o')\n",
        "    plt.title(f\"Line {i+1} - Frame Counters\")\n",
        "    plt.xlabel(\"Frame\")\n",
        "    plt.ylabel(\"Counter\")\n",
        "    plt.grid(True)\n",
        "    plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "y08vd6g2OmLw",
        "outputId": "754705a4-5ea8-449d-a47b-c4a28cf3673d"
      },
      "id": "y08vd6g2OmLw",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Line 1 - 147 frames\n",
            "Frame\tCounter\tSample Features\n",
            "1\t0.0\t[-0.031581 -0.199573 -0.029702 -0.004307 -0.00124 ] ...\n",
            "2\t0.007\t[-0.032786 -0.200893 -0.026416 -0.00424  -0.001233] ...\n",
            "3\t0.0141\t[-0.034456 -0.202972 -0.017222 -0.004627 -0.001256] ...\n",
            "4\t0.0211\t[-0.034795 -0.201999 -0.011731 -0.005019 -0.000769] ...\n",
            "5\t0.0282\t[-2.53270e-02 -2.02674e-01 -1.40000e-03 -4.80400e-03 -6.60000e-05] ...\n",
            "6\t0.0352\t[-0.050608 -0.203272 -0.009485 -0.004561 -0.000373] ...\n",
            "7\t0.0423\t[-0.047572 -0.206092 -0.006808 -0.005187 -0.000955] ...\n",
            "8\t0.0493\t[-0.05118  -0.205186 -0.041507 -0.004965 -0.001523] ...\n",
            "9\t0.0563\t[-0.057365 -0.20251  -0.0678   -0.004891 -0.001797] ...\n",
            "10\t0.0634\t[-0.05453  -0.199635 -0.078505 -0.005192 -0.001448] ...\n",
            "11\t0.0704\t[-0.05455  -0.194831 -0.081472 -0.004991 -0.000677] ...\n",
            "12\t0.0775\t[-0.049295 -0.189491 -0.091973 -0.005768 -0.000655] ...\n",
            "13\t0.0845\t[-0.041835 -0.181679 -0.105262 -0.004915 -0.000772] ...\n",
            "14\t0.0915\t[-0.03011  -0.17338  -0.120211 -0.004553 -0.000588] ...\n",
            "15\t0.0986\t[-0.02771  -0.167802 -0.127675 -0.004125 -0.000896] ...\n",
            "16\t0.1056\t[-0.023559 -0.165914 -0.124401 -0.003848 -0.000575] ...\n",
            "17\t0.1127\t[-0.018937 -0.176483 -0.109341 -0.003104  0.00022 ] ...\n",
            "18\t0.1197\t[-1.58090e-02 -1.86394e-01 -9.38420e-02 -3.02800e-03  1.70000e-04] ...\n",
            "19\t0.1268\t[-0.003976 -0.198267 -0.068373 -0.003278  0.000402] ...\n",
            "20\t0.1338\t[-0.000342 -0.208168 -0.027544 -0.003531  0.00037 ] ...\n",
            "21\t0.1408\t[ 8.64700e-03 -2.10316e-01  9.77000e-04 -2.85200e-03 -5.70000e-05] ...\n",
            "22\t0.1479\t[ 0.019077 -0.205468 -0.012057 -0.002691 -0.000327] ...\n",
            "23\t0.1549\t[ 0.020204 -0.208123 -0.003772 -0.003367 -0.000532] ...\n",
            "24\t0.162\t[ 2.74180e-02 -2.08028e-01 -5.25300e-03 -3.78200e-03  1.16000e-04] ...\n",
            "25\t0.169\t[ 0.03685  -0.211762  0.0009   -0.004308 -0.000377] ...\n",
            "26\t0.1761\t[ 0.046798 -0.217047  0.001858 -0.004333 -0.000556] ...\n",
            "27\t0.1831\t[ 0.05123  -0.21455   0.004077 -0.003768 -0.000704] ...\n",
            "28\t0.1901\t[ 0.059904 -0.209249 -0.00281  -0.004164 -0.001191] ...\n",
            "29\t0.1972\t[ 0.067622 -0.205336 -0.022613 -0.003337 -0.001292] ...\n",
            "30\t0.2042\t[ 0.066699 -0.203892  0.01106  -0.00377  -0.000867] ...\n",
            "31\t0.2113\t[ 0.063365 -0.196526  0.00039  -0.004509 -0.000773] ...\n",
            "32\t0.2183\t[ 0.066177 -0.195473 -0.001817 -0.003015 -0.001043] ...\n",
            "33\t0.2254\t[ 0.06602  -0.196776 -0.007093 -0.002988 -0.000637] ...\n",
            "34\t0.2324\t[ 0.058694 -0.198473  0.001025 -0.00378  -0.000431] ...\n",
            "35\t0.2394\t[ 0.058783 -0.198957 -0.002665 -0.004039 -0.000511] ...\n",
            "36\t0.2465\t[ 0.051471 -0.19939  -0.006402 -0.003978 -0.000582] ...\n",
            "37\t0.2535\t[ 0.041743 -0.202173  0.000594 -0.004049 -0.00079 ] ...\n",
            "38\t0.2606\t[ 0.033728 -0.204465  0.003499 -0.003554 -0.000806] ...\n",
            "39\t0.2676\t[ 0.033111 -0.204165  0.001949 -0.002738 -0.000719] ...\n",
            "40\t0.2746\t[ 0.03447  -0.205241  0.001061 -0.002956 -0.000925] ...\n",
            "41\t0.2817\t[ 0.037082 -0.20423  -0.004987 -0.003424 -0.000694] ...\n",
            "42\t0.2887\t[ 0.034743 -0.207421 -0.003967 -0.003049 -0.001277] ...\n",
            "43\t0.2958\t[ 0.040145 -0.205535 -0.010439 -0.004184 -0.001013] ...\n",
            "44\t0.3028\t[ 0.035628 -0.199659 -0.050426 -0.00335  -0.00092 ] ...\n",
            "45\t0.3099\t[ 0.041588 -0.196751 -0.074294 -0.003632 -0.00062 ] ...\n",
            "46\t0.3169\t[ 0.0501   -0.19529  -0.078705 -0.004313 -0.000326] ...\n",
            "47\t0.3239\t[ 0.060412 -0.196712 -0.051033 -0.004844 -0.000581] ...\n",
            "48\t0.331\t[ 0.071448 -0.194393  0.003344 -0.003141 -0.000376] ...\n",
            "49\t0.338\t[ 0.070468 -0.193774  0.006364 -0.003778 -0.001004] ...\n",
            "50\t0.3451\t[ 0.073143 -0.194007  0.002088 -0.004673 -0.000854] ...\n",
            "51\t0.3521\t[ 0.06956  -0.196198  0.001338 -0.004259 -0.000997] ...\n",
            "52\t0.3592\t[ 6.81820e-02 -1.96035e-01  2.00000e-06 -4.55800e-03 -1.11900e-03] ...\n",
            "53\t0.3662\t[ 0.068863 -0.193166 -0.015163 -0.003209 -0.001125] ...\n",
            "54\t0.3732\t[ 6.75800e-02 -1.92161e-01 -4.26680e-02 -2.19900e-03  8.60000e-05] ...\n",
            "55\t0.3803\t[ 6.7830e-02 -1.9404e-01 -4.9108e-02 -2.7910e-03 -1.5700e-04] ...\n",
            "56\t0.3873\t[ 0.065961 -0.1916   -0.054702 -0.002571  0.000372] ...\n",
            "57\t0.3944\t[ 0.068509 -0.196308 -0.009205 -0.004027 -0.0002  ] ...\n",
            "58\t0.4014\t[ 0.066023 -0.192885 -0.004671 -0.003217 -0.00086 ] ...\n",
            "59\t0.4085\t[ 0.055585 -0.187691 -0.021354 -0.003426 -0.000817] ...\n",
            "60\t0.4155\t[ 0.061068 -0.190464 -0.018701 -0.004029 -0.000366] ...\n",
            "61\t0.4225\t[ 0.059862 -0.190037 -0.038963 -0.003417  0.000518] ...\n",
            "62\t0.4296\t[ 0.055618 -0.189737 -0.050487 -0.002984  0.000571] ...\n",
            "63\t0.4366\t[ 0.050479 -0.189728 -0.051226 -0.003621 -0.000513] ...\n",
            "64\t0.4437\t[ 0.048696 -0.195647 -0.019344 -0.00387  -0.000507] ...\n",
            "65\t0.4507\t[ 0.039396 -0.198965 -0.020258 -0.003191 -0.000478] ...\n",
            "66\t0.4577\t[ 0.040785 -0.201376 -0.010131 -0.003205 -0.000661] ...\n",
            "67\t0.4648\t[ 3.16890e-02 -2.12235e-01  3.40000e-05 -4.52500e-03 -8.91000e-04] ...\n",
            "68\t0.4718\t[ 0.016933 -0.215127 -0.003422 -0.003394 -0.000584] ...\n",
            "69\t0.4789\t[ 0.008538 -0.217136  0.000317 -0.004137 -0.000704] ...\n",
            "70\t0.4859\t[-0.002395 -0.215036 -0.007704 -0.003988 -0.000686] ...\n",
            "71\t0.493\t[-0.013337 -0.211648 -0.008691 -0.00347  -0.00024 ] ...\n",
            "72\t0.5\t[-0.020298 -0.206818 -0.012169 -0.003159 -0.000564] ...\n",
            "73\t0.507\t[-0.023536 -0.20246  -0.002392 -0.00363  -0.001003] ...\n",
            "74\t0.5141\t[-0.02289  -0.203198 -0.003293 -0.003278 -0.000616] ...\n",
            "75\t0.5211\t[-0.035583 -0.201385 -0.005321 -0.004229 -0.001   ] ...\n",
            "76\t0.5282\t[-0.044523 -0.199059 -0.006703 -0.004235 -0.000949] ...\n",
            "77\t0.5352\t[-0.048919 -0.196252 -0.007909 -0.004224 -0.001032] ...\n",
            "78\t0.5423\t[-0.056481 -0.192858 -0.005079 -0.004289 -0.001097] ...\n",
            "79\t0.5493\t[-0.061051 -0.191907  0.001847 -0.004672 -0.000822] ...\n",
            "80\t0.5563\t[-0.063918 -0.19206  -0.001916 -0.004719 -0.000792] ...\n",
            "81\t0.5634\t[-0.063006 -0.193023 -0.008451 -0.004146 -0.000755] ...\n",
            "82\t0.5704\t[-0.06373  -0.188676 -0.027892 -0.004093 -0.001157] ...\n",
            "83\t0.5775\t[-0.065002 -0.187728 -0.030975 -0.00387  -0.001231] ...\n",
            "84\t0.5845\t[-0.066195 -0.192131 -0.007211 -0.003999 -0.000751] ...\n",
            "85\t0.5915\t[-0.061573 -0.192317 -0.003493 -0.00382  -0.00057 ] ...\n",
            "86\t0.5986\t[-0.065317 -0.195069  0.007198 -0.003692 -0.00068 ] ...\n",
            "87\t0.6056\t[-0.060736 -0.187257 -0.023932 -0.003589 -0.000954] ...\n",
            "88\t0.6127\t[-0.063973 -0.190218 -0.017833 -0.003156 -0.000994] ...\n",
            "89\t0.6197\t[-0.068013 -0.19176  -0.008654 -0.003274 -0.001034] ...\n",
            "90\t0.6268\t[-0.070278 -0.191655 -0.007653 -0.002572 -0.000795] ...\n",
            "91\t0.6338\t[-0.065349 -0.190966 -0.004312 -0.002511 -0.000595] ...\n",
            "92\t0.6408\t[-0.065599 -0.191842 -0.009177 -0.002633 -0.000607] ...\n",
            "93\t0.6479\t[-0.065673 -0.189756 -0.022641 -0.002928 -0.000646] ...\n",
            "94\t0.6549\t[-0.064158 -0.184314 -0.05462  -0.003408 -0.000835] ...\n",
            "95\t0.662\t[-0.065467 -0.182597 -0.061582 -0.003718 -0.00096 ] ...\n",
            "96\t0.669\t[-0.065325 -0.178689 -0.081102 -0.003457 -0.001105] ...\n",
            "97\t0.6761\t[-0.067087 -0.173931 -0.090762 -0.003499 -0.001318] ...\n",
            "98\t0.6831\t[-0.068009 -0.169897 -0.095433 -0.003481 -0.000765] ...\n",
            "99\t0.6901\t[-0.07433  -0.168223 -0.097611 -0.003441 -0.000943] ...\n",
            "100\t0.6972\t[-0.074943 -0.167228 -0.101816 -0.003614 -0.001032] ...\n",
            "101\t0.7042\t[-0.075496 -0.166968 -0.097963 -0.003415 -0.000847] ...\n",
            "102\t0.7113\t[-0.072536 -0.167356 -0.095603 -0.003413 -0.001048] ...\n",
            "103\t0.7183\t[-0.07459  -0.16936  -0.094184 -0.004632 -0.001359] ...\n",
            "104\t0.7254\t[-0.083996 -0.173602 -0.081096 -0.004896 -0.000716] ...\n",
            "105\t0.7324\t[-0.087768 -0.170049 -0.086215 -0.003552 -0.000439] ...\n",
            "106\t0.7394\t[-0.087518 -0.169009 -0.089978 -0.003647 -0.000256] ...\n",
            "107\t0.7465\t[-0.093022 -0.168619 -0.084808 -0.00411  -0.000831] ...\n",
            "108\t0.7535\t[-0.092054 -0.170048 -0.081454 -0.004136 -0.000838] ...\n",
            "109\t0.7606\t[-0.088444 -0.167398 -0.092426 -0.003923 -0.000749] ...\n",
            "110\t0.7676\t[-0.082777 -0.163617 -0.103125 -0.003814 -0.000812] ...\n",
            "111\t0.7746\t[-0.078449 -0.160586 -0.106155 -0.003388 -0.000551] ...\n",
            "112\t0.7817\t[-0.074579 -0.160249 -0.106714 -0.003323 -0.00023 ] ...\n",
            "113\t0.7887\t[-0.074851 -0.161604 -0.108305 -0.003481  0.000276] ...\n",
            "114\t0.7958\t[-7.31360e-02 -1.65127e-01 -1.02936e-01 -3.82900e-03  4.00000e-06] ...\n",
            "115\t0.8028\t[-0.074281 -0.162969 -0.102707 -0.003115 -0.000225] ...\n",
            "116\t0.8099\t[-0.072808 -0.160618 -0.108682 -0.002838 -0.000534] ...\n",
            "117\t0.8169\t[-0.068225 -0.155852 -0.116696 -0.003402 -0.000575] ...\n",
            "118\t0.8239\t[-0.070948 -0.148494 -0.132668 -0.003632 -0.000359] ...\n",
            "119\t0.831\t[-0.059639 -0.152038 -0.13391  -0.003744 -0.000466] ...\n",
            "120\t0.838\t[-0.056471 -0.1616   -0.117115 -0.004117 -0.00077 ] ...\n",
            "121\t0.8451\t[-0.049425 -0.171538 -0.102264 -0.004356 -0.00076 ] ...\n",
            "122\t0.8521\t[-0.044106 -0.182527 -0.085055 -0.004475 -0.000488] ...\n",
            "123\t0.8592\t[-0.036385 -0.189755 -0.061957 -0.004384 -0.00049 ] ...\n",
            "124\t0.8662\t[-0.022376 -0.194777 -0.040982 -0.00412  -0.000766] ...\n",
            "125\t0.8732\t[-0.000197 -0.190161 -0.043996 -0.004066 -0.001074] ...\n",
            "126\t0.8803\t[ 0.002423 -0.192001 -0.04039  -0.003955 -0.000997] ...\n",
            "127\t0.8873\t[ 0.00796  -0.185516 -0.05183  -0.003708 -0.001002] ...\n",
            "128\t0.8944\t[ 0.008319 -0.184961 -0.053571 -0.00383  -0.001069] ...\n",
            "129\t0.9014\t[ 0.007817 -0.185924 -0.049968 -0.003747 -0.001035] ...\n",
            "130\t0.9085\t[-0.009892 -0.188624 -0.048563 -0.003942 -0.000993] ...\n",
            "131\t0.9155\t[-0.004419 -0.18775  -0.050382 -0.003903 -0.000973] ...\n",
            "132\t0.9225\t[-0.005304 -0.197982 -0.02274  -0.003817 -0.000358] ...\n",
            "133\t0.9296\t[-3.67100e-03 -2.04622e-01 -2.35100e-03 -3.58200e-03 -4.00000e-05] ...\n",
            "134\t0.9366\t[ 0.000391 -0.206147  0.006524 -0.002978 -0.000282] ...\n",
            "135\t0.9437\t[ 0.002572 -0.192338 -0.026386 -0.003922 -0.000246] ...\n",
            "136\t0.9507\t[ 0.006612 -0.19235  -0.025701 -0.004475 -0.000621] ...\n",
            "137\t0.9577\t[ 0.005209 -0.205773 -0.009169 -0.004523 -0.00042 ] ...\n",
            "138\t0.9648\t[ 2.41720e-02 -2.04244e-01  1.34200e-03 -4.80900e-03  2.20000e-05] ...\n",
            "139\t0.9718\t[ 0.018863 -0.201283 -0.010036 -0.003737 -0.000314] ...\n",
            "140\t0.9789\t[ 2.03610e-02 -2.09076e-01  7.25600e-03 -4.65600e-03  1.95000e-04] ...\n",
            "141\t0.9859\t[ 0.00679  -0.206248 -0.012933 -0.004745 -0.000263] ...\n",
            "142\t0.993\t[-0.001626 -0.201949 -0.003187 -0.004538 -0.001011] ...\n",
            "143\t0.0\t[-0.00564  -0.198023 -0.016628 -0.004695 -0.001212] ...\n",
            "144\t0.0\t[-0.018509 -0.199951 -0.006609 -0.004819 -0.001337] ...\n",
            "145\t0.0\t[-0.020913 -0.201393  0.006216 -0.005015 -0.000718] ...\n",
            "146\t0.0\t[-0.024073 -0.195148 -0.012931 -0.004937 -0.00089 ] ...\n",
            "147\t0.0\t[-0.017992 -0.194596 -0.016204 -0.004365 -0.000786] ...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x200 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Line 2 - 147 frames\n",
            "Frame\tCounter\tSample Features\n",
            "1\t0.0\t[-0.032609 -0.188946 -0.054769 -0.004272 -0.001088] ...\n",
            "2\t0.009\t[-0.035367 -0.187685 -0.055947 -0.004053 -0.00112 ] ...\n",
            "3\t0.018\t[-0.03827  -0.195151 -0.034729 -0.004775 -0.00099 ] ...\n",
            "4\t0.027\t[-0.020617 -0.188635 -0.036708 -0.00484  -0.00033 ] ...\n",
            "5\t0.036\t[ 1.66300e-03 -1.94607e-01 -1.97960e-02 -3.55500e-03  5.60000e-05] ...\n",
            "6\t0.045\t[-4.52740e-02 -1.96264e-01 -2.75250e-02 -3.70800e-03 -1.42000e-04] ...\n",
            "7\t0.0541\t[-0.035867 -0.202266  0.003346 -0.00456  -0.000334] ...\n",
            "8\t0.0631\t[-0.031988 -0.204386 -0.024812 -0.004357 -0.001039] ...\n",
            "9\t0.0721\t[-0.044259 -0.201177 -0.056624 -0.004246 -0.001572] ...\n",
            "10\t0.0811\t[-0.038514 -0.201272 -0.057069 -0.00469  -0.001092] ...\n",
            "11\t0.0901\t[-3.71110e-02 -2.00686e-01 -5.54070e-02 -4.84900e-03 -1.26000e-04] ...\n",
            "12\t0.0991\t[-4.04650e-02 -2.02243e-01 -6.05030e-02 -5.72700e-03 -1.69000e-04] ...\n",
            "13\t0.1081\t[-0.025464 -0.197467 -0.061675 -0.004616 -0.000537] ...\n",
            "14\t0.1171\t[-0.020797 -0.183116 -0.106056 -0.00469  -0.000422] ...\n",
            "15\t0.1261\t[-0.007331 -0.190798 -0.070065 -0.005101 -0.000901] ...\n",
            "16\t0.1351\t[-0.01121  -0.188192 -0.068978 -0.004826 -0.000688] ...\n",
            "17\t0.1441\t[-1.46750e-02 -1.95569e-01 -5.83550e-02 -4.52400e-03 -1.10000e-04] ...\n",
            "18\t0.1532\t[-1.46070e-02 -2.00753e-01 -4.93450e-02 -4.47500e-03 -1.60000e-04] ...\n",
            "19\t0.1622\t[-7.74500e-03 -2.05961e-01 -4.49960e-02 -4.07700e-03 -2.00000e-05] ...\n",
            "20\t0.1712\t[-2.98700e-03 -2.14051e-01 -1.21260e-02 -4.18800e-03  6.50000e-05] ...\n",
            "21\t0.1802\t[ 5.99800e-03 -2.17736e-01  1.36130e-02 -4.20300e-03 -1.84000e-04] ...\n",
            "22\t0.1892\t[ 0.020414 -0.208159 -0.016985 -0.003099 -0.000336] ...\n",
            "23\t0.1982\t[ 0.02067  -0.210628 -0.003529 -0.003436 -0.000501] ...\n",
            "24\t0.2072\t[ 2.49620e-02 -2.07337e-01 -3.43800e-03 -4.17700e-03 -6.10000e-05] ...\n",
            "25\t0.2162\t[ 0.026615 -0.209116 -0.000244 -0.004678 -0.00055 ] ...\n",
            "26\t0.2252\t[ 0.033584 -0.213944  0.004997 -0.004926 -0.000603] ...\n",
            "27\t0.2342\t[ 0.022916 -0.212136  0.00654  -0.004396 -0.000751] ...\n",
            "28\t0.2432\t[ 0.025045 -0.209695  0.003353 -0.004678 -0.000985] ...\n",
            "29\t0.2523\t[ 0.025672 -0.209466 -0.002117 -0.004487 -0.001006] ...\n",
            "30\t0.2613\t[ 0.03361  -0.20913   0.011143 -0.004205 -0.000772] ...\n",
            "31\t0.2703\t[ 0.068503 -0.203054  0.005152 -0.004501 -0.00064 ] ...\n",
            "32\t0.2793\t[ 0.04844  -0.205279  0.00775  -0.00368  -0.000785] ...\n",
            "33\t0.2883\t[ 0.04128  -0.2062    0.003717 -0.003741 -0.000659] ...\n",
            "34\t0.2973\t[ 0.037706 -0.204126  0.005646 -0.004066 -0.000593] ...\n",
            "35\t0.3063\t[ 0.039746 -0.205071  0.005276 -0.004321 -0.000623] ...\n",
            "36\t0.3153\t[ 0.045824 -0.203134 -0.000694 -0.004115 -0.00062 ] ...\n",
            "37\t0.3243\t[ 0.034469 -0.204997  0.004129 -0.00412  -0.000809] ...\n",
            "38\t0.3333\t[ 0.022966 -0.207554  0.005883 -0.003988 -0.000964] ...\n",
            "39\t0.3423\t[ 0.023996 -0.20742   0.00348  -0.003045 -0.000923] ...\n",
            "40\t0.3514\t[ 0.024053 -0.211018  0.004832 -0.003601 -0.001041] ...\n",
            "41\t0.3604\t[ 0.025817 -0.211275 -0.000312 -0.004086 -0.000964] ...\n",
            "42\t0.3694\t[ 0.023094 -0.214799  0.002696 -0.003992 -0.00127 ] ...\n",
            "43\t0.3784\t[ 0.024224 -0.215141 -0.000676 -0.004661 -0.00115 ] ...\n",
            "44\t0.3874\t[ 0.023232 -0.209508 -0.024687 -0.004074 -0.000913] ...\n",
            "45\t0.3964\t[ 0.032734 -0.200764 -0.059101 -0.003758 -0.000627] ...\n",
            "46\t0.4054\t[ 0.037256 -0.201146 -0.05524  -0.004302 -0.000348] ...\n",
            "47\t0.4144\t[ 0.051093 -0.199668 -0.043943 -0.004867 -0.00053 ] ...\n",
            "48\t0.4234\t[ 6.66990e-02 -1.97772e-01  8.35000e-04 -3.29600e-03 -9.80000e-05] ...\n",
            "49\t0.4324\t[ 0.062457 -0.197835  0.006714 -0.003702 -0.000573] ...\n",
            "50\t0.4414\t[ 0.068104 -0.197756  0.003461 -0.004211 -0.000541] ...\n",
            "51\t0.4505\t[ 0.063763 -0.200778  0.006682 -0.003721 -0.000637] ...\n",
            "52\t0.4595\t[ 0.05791  -0.20298   0.006449 -0.003939 -0.000757] ...\n",
            "53\t0.4685\t[ 0.061399 -0.198253 -0.010743 -0.003007 -0.000905] ...\n",
            "54\t0.4775\t[ 0.055594 -0.199183 -0.024253 -0.002128 -0.000203] ...\n",
            "55\t0.4865\t[ 0.053627 -0.198808 -0.040899 -0.003336 -0.000247] ...\n",
            "56\t0.4955\t[ 0.057603 -0.193053 -0.06131  -0.003117  0.000456] ...\n",
            "57\t0.5045\t[ 0.069076 -0.195744 -0.025318 -0.004404 -0.000291] ...\n",
            "58\t0.5135\t[ 0.063615 -0.190513 -0.030703 -0.003494 -0.000505] ...\n",
            "59\t0.5225\t[ 0.054057 -0.185882 -0.050674 -0.003642 -0.000543] ...\n",
            "60\t0.5315\t[ 6.03250e-02 -1.88563e-01 -5.05130e-02 -4.04500e-03 -1.32000e-04] ...\n",
            "61\t0.5405\t[ 0.051528 -0.18587  -0.056796 -0.003478  0.000403] ...\n",
            "62\t0.5495\t[ 0.047218 -0.185207 -0.075533 -0.003277  0.000434] ...\n",
            "63\t0.5586\t[ 0.038533 -0.184695 -0.073933 -0.003576 -0.000633] ...\n",
            "64\t0.5676\t[ 0.040577 -0.188488 -0.044328 -0.004137 -0.000562] ...\n",
            "65\t0.5766\t[ 0.032784 -0.1927   -0.041511 -0.003141 -0.000407] ...\n",
            "66\t0.5856\t[ 0.0427   -0.197839 -0.021197 -0.002992 -0.000639] ...\n",
            "67\t0.5946\t[ 0.033361 -0.207221 -0.014209 -0.003951 -0.000623] ...\n",
            "68\t0.6036\t[ 0.015734 -0.207843 -0.030154 -0.003328 -0.000232] ...\n",
            "69\t0.6126\t[ 0.013878 -0.211227 -0.020979 -0.003753 -0.000441] ...\n",
            "70\t0.6216\t[ 0.009096 -0.204522 -0.04175  -0.003316 -0.000539] ...\n",
            "71\t0.6306\t[-0.009735 -0.201375 -0.036535 -0.004401 -0.000351] ...\n",
            "72\t0.6396\t[-0.015052 -0.19946  -0.035317 -0.003262 -0.000563] ...\n",
            "73\t0.6486\t[-0.018214 -0.1979   -0.017797 -0.003472 -0.000846] ...\n",
            "74\t0.6577\t[-0.021287 -0.19859  -0.015648 -0.003021 -0.000414] ...\n",
            "75\t0.6667\t[-0.034486 -0.198247 -0.013944 -0.00399  -0.00076 ] ...\n",
            "76\t0.6757\t[-0.03759  -0.195842 -0.015776 -0.004054 -0.000768] ...\n",
            "77\t0.6847\t[-0.037702 -0.193563 -0.023435 -0.00421  -0.000777] ...\n",
            "78\t0.6937\t[-0.045048 -0.192252 -0.017634 -0.004311 -0.001076] ...\n",
            "79\t0.7027\t[-0.049296 -0.188837 -0.009762 -0.004682 -0.000748] ...\n",
            "80\t0.7117\t[-0.054637 -0.188199 -0.007677 -0.004721 -0.000661] ...\n",
            "81\t0.7207\t[-0.051738 -0.189837 -0.012125 -0.004238 -0.00055 ] ...\n",
            "82\t0.7297\t[-0.050214 -0.183623 -0.04306  -0.004074 -0.001066] ...\n",
            "83\t0.7387\t[-0.046458 -0.182715 -0.048829 -0.003939 -0.001194] ...\n",
            "84\t0.7477\t[-0.050446 -0.187927 -0.020624 -0.003838 -0.000656] ...\n",
            "85\t0.7568\t[-0.043648 -0.188365 -0.017951 -0.003566 -0.000412] ...\n",
            "86\t0.7658\t[-0.042384 -0.189149 -0.022681 -0.003403 -0.000599] ...\n",
            "87\t0.7748\t[-0.029112 -0.180077 -0.061168 -0.003547 -0.000746] ...\n",
            "88\t0.7838\t[-0.047858 -0.186741 -0.040419 -0.003175 -0.000952] ...\n",
            "89\t0.7928\t[-0.056595 -0.189626 -0.029561 -0.00326  -0.001026] ...\n",
            "90\t0.8018\t[-0.05547  -0.189633 -0.031741 -0.002685 -0.000814] ...\n",
            "91\t0.8108\t[-0.050416 -0.188175 -0.025009 -0.002697 -0.000352] ...\n",
            "92\t0.8198\t[-0.050723 -0.187924 -0.029341 -0.002907 -0.000412] ...\n",
            "93\t0.8288\t[-0.055251 -0.18415  -0.047264 -0.003038 -0.000814] ...\n",
            "94\t0.8378\t[-0.056581 -0.17647  -0.082543 -0.003612 -0.001066] ...\n",
            "95\t0.8468\t[-0.058953 -0.172353 -0.093617 -0.004181 -0.000975] ...\n",
            "96\t0.8559\t[-0.056378 -0.170228 -0.100848 -0.003703 -0.000939] ...\n",
            "97\t0.8649\t[-0.055532 -0.167942 -0.105668 -0.003733 -0.001366] ...\n",
            "98\t0.8739\t[-0.056429 -0.165506 -0.105563 -0.003788 -0.000653] ...\n",
            "99\t0.8829\t[-0.060882 -0.164544 -0.108543 -0.003637 -0.000739] ...\n",
            "100\t0.8919\t[-0.062631 -0.164991 -0.110396 -0.003845 -0.000937] ...\n",
            "101\t0.9009\t[-0.05995  -0.165654 -0.108618 -0.003689 -0.000721] ...\n",
            "102\t0.9099\t[-0.050472 -0.167145 -0.1068   -0.003847 -0.000995] ...\n",
            "103\t0.9189\t[-0.058716 -0.169125 -0.103778 -0.004697 -0.001197] ...\n",
            "104\t0.9279\t[-0.054626 -0.173661 -0.08305  -0.004759 -0.000615] ...\n",
            "105\t0.9369\t[-0.062795 -0.167615 -0.093789 -0.003807 -0.000437] ...\n",
            "106\t0.9459\t[-6.83220e-02 -1.67299e-01 -9.16590e-02 -4.17800e-03 -1.38000e-04] ...\n",
            "107\t0.955\t[-0.062691 -0.169318 -0.088109 -0.004494 -0.000627] ...\n",
            "108\t0.964\t[-0.067026 -0.170556 -0.084475 -0.004849 -0.000571] ...\n",
            "109\t0.973\t[-0.058425 -0.17083  -0.093521 -0.004763 -0.000507] ...\n",
            "110\t0.982\t[-0.06591  -0.162733 -0.108073 -0.004805 -0.000518] ...\n",
            "111\t0.991\t[-0.054111 -0.163319 -0.109361 -0.004268 -0.000543] ...\n",
            "112\t0.0\t[-0.052808 -0.163453 -0.110477 -0.004177 -0.000276] ...\n",
            "113\t0.0\t[-5.48800e-02 -1.65097e-01 -1.11875e-01 -3.90100e-03 -1.70000e-05] ...\n",
            "114\t0.0\t[-0.05072  -0.169637 -0.107198 -0.00432  -0.000279] ...\n",
            "115\t0.0\t[-0.054188 -0.167867 -0.107774 -0.003789 -0.000452] ...\n",
            "116\t0.0\t[-0.055744 -0.163512 -0.114103 -0.003482 -0.000805] ...\n",
            "117\t0.0\t[-0.049269 -0.162105 -0.112835 -0.004175 -0.000855] ...\n",
            "118\t0.0\t[-0.054967 -0.155685 -0.127158 -0.003712 -0.000598] ...\n",
            "119\t0.0\t[-0.042388 -0.158347 -0.126352 -0.004381 -0.000611] ...\n",
            "120\t0.0\t[-0.041489 -0.166547 -0.104773 -0.004788 -0.00085 ] ...\n",
            "121\t0.0\t[-0.031587 -0.177304 -0.080238 -0.004747 -0.000716] ...\n",
            "122\t0.0\t[-0.026291 -0.184367 -0.074417 -0.005102 -0.000465] ...\n",
            "123\t0.0\t[-0.023971 -0.186527 -0.068842 -0.005012 -0.000443] ...\n",
            "124\t0.0\t[-0.012975 -0.192958 -0.043044 -0.004559 -0.000736] ...\n",
            "125\t0.0\t[-0.002202 -0.192716 -0.035973 -0.004337 -0.001056] ...\n",
            "126\t0.0\t[ 0.006482 -0.184369 -0.057133 -0.004137 -0.00122 ] ...\n",
            "127\t0.0\t[ 0.006721 -0.184722 -0.055832 -0.003979 -0.001211] ...\n",
            "128\t0.0\t[ 0.009034 -0.185668 -0.052956 -0.004129 -0.0012  ] ...\n",
            "129\t0.0\t[ 0.010511 -0.18822  -0.046131 -0.004115 -0.001115] ...\n",
            "130\t0.0\t[ 1.69000e-04 -1.94197e-01 -3.08820e-02 -4.23200e-03 -9.99000e-04] ...\n",
            "131\t0.0\t[ 0.001226 -0.18965  -0.046654 -0.004153 -0.001046] ...\n",
            "132\t0.0\t[ 0.003164 -0.200304 -0.018794 -0.004023 -0.000614] ...\n",
            "133\t0.0\t[ 0.012469 -0.21026   0.015632 -0.00352  -0.000383] ...\n",
            "134\t0.0\t[ 0.015026 -0.206378  0.010181 -0.003165 -0.000715] ...\n",
            "135\t0.0\t[ 0.01715  -0.198368 -0.012364 -0.003997 -0.000363] ...\n",
            "136\t0.0\t[ 0.021412 -0.196429 -0.01885  -0.004873 -0.000704] ...\n",
            "137\t0.0\t[ 0.023213 -0.202552 -0.006138 -0.004564 -0.000793] ...\n",
            "138\t0.0\t[ 3.33900e-02 -2.07054e-01  6.47400e-03 -4.66600e-03 -1.44000e-04] ...\n",
            "139\t0.0\t[ 2.46670e-02 -2.07806e-01  4.92000e-03 -4.20800e-03 -1.50000e-04] ...\n",
            "140\t0.0\t[ 2.58210e-02 -2.08703e-01  3.97400e-03 -4.53600e-03 -1.15000e-04] ...\n",
            "141\t0.0\t[ 0.01626  -0.203727 -0.018947 -0.004283 -0.000419] ...\n",
            "142\t0.0\t[ 0.015612 -0.199624 -0.010569 -0.004377 -0.001043] ...\n",
            "143\t0.0\t[ 0.004926 -0.192517 -0.027482 -0.004588 -0.001239] ...\n",
            "144\t0.0\t[-0.005759 -0.19706  -0.017941 -0.00497  -0.001329] ...\n",
            "145\t0.0\t[-0.013199 -0.199333  0.000995 -0.004841 -0.000887] ...\n",
            "146\t0.0\t[-0.016476 -0.194207 -0.016366 -0.004781 -0.001051] ...\n",
            "147\t0.0\t[-0.019439 -0.193954 -0.018017 -0.004176 -0.00119 ] ...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x200 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Line 3 - 147 frames\n",
            "Frame\tCounter\tSample Features\n",
            "1\t0.0\t[ 0.007941 -0.247535  0.001251 -0.004447 -0.000832] ...\n",
            "2\t0.0238\t[ 0.008222 -0.247318  0.001453 -0.004468 -0.000802] ...\n",
            "3\t0.0476\t[ 0.002614 -0.247587  0.00319  -0.00435  -0.000849] ...\n",
            "4\t0.0714\t[-0.008322 -0.249077 -0.003375 -0.004568 -0.001048] ...\n",
            "5\t0.0952\t[-0.036879 -0.243428 -0.019184 -0.004079 -0.000463] ...\n",
            "6\t0.119\t[-1.41940e-02 -2.42873e-01 -1.18220e-02 -3.79500e-03  2.18000e-04] ...\n",
            "7\t0.1429\t[-2.11740e-02 -2.47461e-01 -2.13180e-02 -3.41200e-03 -7.60000e-05] ...\n",
            "8\t0.1667\t[-0.0237   -0.258071  0.002807 -0.003551 -0.000266] ...\n",
            "9\t0.1905\t[-0.034829 -0.257775 -0.009732 -0.003921 -0.000382] ...\n",
            "10\t0.2143\t[-0.044841 -0.254883 -0.017537 -0.004212 -0.000498] ...\n",
            "11\t0.2381\t[-0.050554 -0.252442 -0.008706 -0.003822 -0.000598] ...\n",
            "12\t0.2619\t[-0.050665 -0.24888  -0.001187 -0.003738 -0.000601] ...\n",
            "13\t0.2857\t[-0.052344 -0.243671 -0.002786 -0.003911 -0.000626] ...\n",
            "14\t0.3095\t[-0.060356 -0.241728 -0.00066  -0.004008 -0.000776] ...\n",
            "15\t0.3333\t[-0.065579 -0.241405  0.000533 -0.003827 -0.00082 ] ...\n",
            "16\t0.3571\t[-0.06904  -0.239945 -0.001489 -0.003618 -0.00075 ] ...\n",
            "17\t0.381\t[-0.074288 -0.240232 -0.006177 -0.003403 -0.000648] ...\n",
            "18\t0.4048\t[-0.080318 -0.24234  -0.01641  -0.004115 -0.000563] ...\n",
            "19\t0.4286\t[-0.088352 -0.237233 -0.052573 -0.004293 -0.000483] ...\n",
            "20\t0.4524\t[-0.095019 -0.231107 -0.079463 -0.004296 -0.000501] ...\n",
            "21\t0.4762\t[-0.097936 -0.227705 -0.091376 -0.004311 -0.000571] ...\n",
            "22\t0.5\t[-0.095148 -0.228006 -0.080616 -0.004268 -0.000591] ...\n",
            "23\t0.5238\t[-0.095194 -0.229644 -0.048849 -0.00404  -0.000574] ...\n",
            "24\t0.5476\t[-0.099058 -0.232229 -0.015923 -0.003827 -0.000282] ...\n",
            "25\t0.5714\t[-0.098846 -0.23059  -0.011519 -0.003879 -0.000317] ...\n",
            "26\t0.5952\t[-0.100456 -0.231497 -0.009262 -0.003747 -0.000335] ...\n",
            "27\t0.619\t[-1.01491e-01 -2.34336e-01 -9.23700e-03 -3.80600e-03 -1.50000e-04] ...\n",
            "28\t0.6429\t[-1.02783e-01 -2.34119e-01 -2.16180e-02 -3.95800e-03 -2.10000e-04] ...\n",
            "29\t0.6667\t[-0.100245 -0.233485 -0.031682 -0.004386 -0.00031 ] ...\n",
            "30\t0.6905\t[-0.098877 -0.229319 -0.029939 -0.004325 -0.000528] ...\n",
            "31\t0.7143\t[-0.096792 -0.227593 -0.029584 -0.004701 -0.000469] ...\n",
            "32\t0.7381\t[-0.095097 -0.228404 -0.004074 -0.004783 -0.000503] ...\n",
            "33\t0.7619\t[-0.090994 -0.22898   0.003868 -0.004673 -0.000554] ...\n",
            "34\t0.7857\t[-0.087702 -0.228053 -0.008481 -0.004716 -0.000599] ...\n",
            "35\t0.8095\t[-0.084831 -0.229641  0.001584 -0.004701 -0.000748] ...\n",
            "36\t0.8333\t[-0.077055 -0.230047 -0.002044 -0.004463 -0.000827] ...\n",
            "37\t0.8571\t[-0.06652  -0.230858 -0.012569 -0.003179 -0.001065] ...\n",
            "38\t0.881\t[-0.065103 -0.231414 -0.006298 -0.003657 -0.001569] ...\n",
            "39\t0.9048\t[-0.064671 -0.232965 -0.012197 -0.002895 -0.000631] ...\n",
            "40\t0.9286\t[-0.067299 -0.232441 -0.017576 -0.004314 -0.000664] ...\n",
            "41\t0.9524\t[-0.068178 -0.230041 -0.016659 -0.003696 -0.000953] ...\n",
            "42\t0.9762\t[-0.061101 -0.234221  0.004462 -0.003154 -0.000728] ...\n",
            "43\t0.0\t[-0.059788 -0.232241  0.002246 -0.003345 -0.00118 ] ...\n",
            "44\t0.0\t[-0.059344 -0.233507  0.003016 -0.003132 -0.000968] ...\n",
            "45\t0.0\t[-0.060108 -0.232637  0.003817 -0.003384 -0.000827] ...\n",
            "46\t0.0\t[-0.066727 -0.233001 -0.004282 -0.003661 -0.00071 ] ...\n",
            "47\t0.0\t[-0.072913 -0.227276 -0.015103 -0.004039 -0.000651] ...\n",
            "48\t0.0\t[-0.079778 -0.21937  -0.05359  -0.004477 -0.000578] ...\n",
            "49\t0.0\t[-0.089497 -0.212873 -0.07322  -0.004664 -0.000595] ...\n",
            "50\t0.0\t[-0.093208 -0.206783 -0.087208 -0.003735 -0.000721] ...\n",
            "51\t0.0\t[-0.099985 -0.203351 -0.088728 -0.003753 -0.000646] ...\n",
            "52\t0.0\t[-0.108359 -0.203078 -0.089377 -0.003524 -0.000617] ...\n",
            "53\t0.0\t[-0.10815  -0.206942 -0.089183 -0.003285 -0.000587] ...\n",
            "54\t0.0\t[-0.113647 -0.199991 -0.098916 -0.003721 -0.000246] ...\n",
            "55\t0.0\t[-0.102766 -0.191636 -0.105658 -0.003314 -0.000753] ...\n",
            "56\t0.0\t[-0.102811 -0.184803 -0.115324 -0.00348  -0.000546] ...\n",
            "57\t0.0\t[-0.105184 -0.177995 -0.124844 -0.003412 -0.000655] ...\n",
            "58\t0.0\t[-0.107046 -0.173572 -0.128072 -0.003442 -0.000611] ...\n",
            "59\t0.0\t[-0.107999 -0.172947 -0.129713 -0.003678 -0.000553] ...\n",
            "60\t0.0\t[-0.107679 -0.174066 -0.130048 -0.00371  -0.000484] ...\n",
            "61\t0.0\t[-0.101529 -0.17478  -0.132193 -0.003819 -0.000386] ...\n",
            "62\t0.0\t[-0.099326 -0.174138 -0.134048 -0.00382  -0.000586] ...\n",
            "63\t0.0\t[-0.101169 -0.173302 -0.132348 -0.00351  -0.000991] ...\n",
            "64\t0.0\t[-0.1048   -0.172398 -0.130897 -0.003419 -0.000803] ...\n",
            "65\t0.0\t[-0.101577 -0.172353 -0.133992 -0.003562 -0.000718] ...\n",
            "66\t0.0\t[-0.099801 -0.174985 -0.133965 -0.00362  -0.0007  ] ...\n",
            "67\t0.0\t[-0.091621 -0.186808 -0.12378  -0.003547 -0.000722] ...\n",
            "68\t0.0\t[-0.082524 -0.201205 -0.105054 -0.003359 -0.000631] ...\n",
            "69\t0.0\t[-7.13430e-02 -2.14761e-01 -8.27570e-02 -4.13600e-03  2.20000e-05] ...\n",
            "70\t0.0\t[-6.25910e-02 -2.24029e-01 -6.45870e-02 -4.25200e-03  3.20000e-05] ...\n",
            "71\t0.0\t[-5.61830e-02 -2.30356e-01 -4.87930e-02 -4.44400e-03 -4.00000e-06] ...\n",
            "72\t0.0\t[-6.79370e-02 -2.29869e-01 -4.51820e-02 -4.87500e-03 -1.61000e-04] ...\n",
            "73\t0.0\t[-0.087493 -0.225591 -0.044406 -0.005487 -0.00053 ] ...\n",
            "74\t0.0\t[-0.117045 -0.208661 -0.091329 -0.004756 -0.000445] ...\n",
            "75\t0.0\t[-0.118243 -0.221399 -0.074108 -0.004889 -0.000644] ...\n",
            "76\t0.0\t[-0.118091 -0.224396 -0.080551 -0.004767 -0.000416] ...\n",
            "77\t0.0\t[-0.102617 -0.216846 -0.090138 -0.004828 -0.000451] ...\n",
            "78\t0.0\t[-0.11265  -0.219955 -0.08332  -0.005481 -0.000645] ...\n",
            "79\t0.0\t[-0.113875 -0.219946 -0.093015 -0.004777 -0.000534] ...\n",
            "80\t0.0\t[-0.105365 -0.211373 -0.106786 -0.005151 -0.00058 ] ...\n",
            "81\t0.0\t[-0.128875 -0.213622 -0.117803 -0.00433  -0.000879] ...\n",
            "82\t0.0\t[-0.097876 -0.212688 -0.101124 -0.004485 -0.000704] ...\n",
            "83\t0.0\t[-0.099154 -0.202943 -0.120846 -0.004751 -0.000824] ...\n",
            "84\t0.0\t[-0.099332 -0.21007  -0.100927 -0.004614 -0.000632] ...\n",
            "85\t0.0\t[-0.087728 -0.210266 -0.096197 -0.00455  -0.000411] ...\n",
            "86\t0.0\t[-0.104375 -0.203179 -0.116624 -0.00445  -0.000747] ...\n",
            "87\t0.0\t[-0.094297 -0.202423 -0.123967 -0.004941 -0.000699] ...\n",
            "88\t0.0\t[-0.093225 -0.20535  -0.113746 -0.004446 -0.0007  ] ...\n",
            "89\t0.0\t[-0.100095 -0.206026 -0.110935 -0.004208 -0.000815] ...\n",
            "90\t0.0\t[-0.09849  -0.197656 -0.133227 -0.005111 -0.000445] ...\n",
            "91\t0.0\t[-0.107654 -0.21045  -0.130812 -0.004614 -0.000547] ...\n",
            "92\t0.0\t[-1.12674e-01 -2.08272e-01 -1.33871e-01 -4.97500e-03 -1.02000e-04] ...\n",
            "93\t0.0\t[-0.113483 -0.196638 -0.136063 -0.00478  -0.000414] ...\n",
            "94\t0.0\t[-0.138027 -0.202728 -0.151165 -0.005167 -0.001494] ...\n",
            "95\t0.0\t[-0.129293 -0.19532  -0.172373 -0.005237 -0.001013] ...\n",
            "96\t0.0\t[-0.135875 -0.191794 -0.179092 -0.005094 -0.000585] ...\n",
            "97\t0.0\t[-0.138362 -0.194376 -0.185103 -0.004922 -0.001561] ...\n",
            "98\t0.0\t[-0.112587 -0.188855 -0.162981 -0.00588  -0.000702] ...\n",
            "99\t0.0\t[-0.113571 -0.196909 -0.157441 -0.00518  -0.000597] ...\n",
            "100\t0.0\t[-0.111657 -0.188161 -0.165007 -0.005248 -0.000911] ...\n",
            "101\t0.0\t[-0.107983 -0.192039 -0.158635 -0.005455 -0.000781] ...\n",
            "102\t0.0\t[-0.10571  -0.20392  -0.155188 -0.005025 -0.001064] ...\n",
            "103\t0.0\t[-0.107472 -0.191854 -0.165921 -0.005368 -0.000613] ...\n",
            "104\t0.0\t[-0.101589 -0.190575 -0.159753 -0.005524 -0.000334] ...\n",
            "105\t0.0\t[-0.096516 -0.188546 -0.163988 -0.004887 -0.000347] ...\n",
            "106\t0.0\t[-0.099855 -0.198349 -0.157408 -0.006173 -0.000439] ...\n",
            "107\t0.0\t[-0.086931 -0.199064 -0.148926 -0.005998 -0.00085 ] ...\n",
            "108\t0.0\t[-0.085312 -0.205933 -0.131552 -0.00567  -0.000994] ...\n",
            "109\t0.0\t[-0.098395 -0.205066 -0.149038 -0.005266 -0.001096] ...\n",
            "110\t0.0\t[-0.117033 -0.203967 -0.164563 -0.005427 -0.001245] ...\n",
            "111\t0.0\t[-0.107993 -0.223065 -0.125599 -0.004238 -0.000866] ...\n",
            "112\t0.0\t[-0.107863 -0.228486 -0.112494 -0.004469 -0.000936] ...\n",
            "113\t0.0\t[-0.099908 -0.228697 -0.106363 -0.004411 -0.001024] ...\n",
            "114\t0.0\t[-0.09066  -0.224582 -0.110503 -0.004692 -0.001047] ...\n",
            "115\t0.0\t[-0.089943 -0.223542 -0.117056 -0.004538 -0.000952] ...\n",
            "116\t0.0\t[-0.091693 -0.226378 -0.118305 -0.004352 -0.000935] ...\n",
            "117\t0.0\t[-0.09109  -0.22616  -0.117083 -0.004372 -0.000877] ...\n",
            "118\t0.0\t[-0.094387 -0.225518 -0.116619 -0.004195 -0.000801] ...\n",
            "119\t0.0\t[-0.09821  -0.22817  -0.101979 -0.004401 -0.000923] ...\n",
            "120\t0.0\t[-0.09877  -0.228528 -0.078775 -0.004455 -0.001202] ...\n",
            "121\t0.0\t[-0.104131 -0.230532 -0.066069 -0.003865 -0.001121] ...\n",
            "122\t0.0\t[-0.082172 -0.231184 -0.068214 -0.004498 -0.0013  ] ...\n",
            "123\t0.0\t[-0.073069 -0.229761 -0.081243 -0.004845 -0.001187] ...\n",
            "124\t0.0\t[-0.095474 -0.223449 -0.107154 -0.004381 -0.001243] ...\n",
            "125\t0.0\t[-0.090546 -0.222669 -0.117288 -0.00438  -0.001164] ...\n",
            "126\t0.0\t[-0.087479 -0.222388 -0.122098 -0.004267 -0.001134] ...\n",
            "127\t0.0\t[-0.088746 -0.225251 -0.114856 -0.004146 -0.001208] ...\n",
            "128\t0.0\t[-0.088231 -0.225214 -0.107255 -0.004292 -0.001275] ...\n",
            "129\t0.0\t[-0.087806 -0.228793 -0.100098 -0.004198 -0.001218] ...\n",
            "130\t0.0\t[-0.086449 -0.233227 -0.093659 -0.00409  -0.001106] ...\n",
            "131\t0.0\t[-0.084528 -0.234043 -0.08675  -0.004183 -0.001111] ...\n",
            "132\t0.0\t[-0.067985 -0.240586 -0.073185 -0.003973 -0.000458] ...\n",
            "133\t0.0\t[-0.065766 -0.242597 -0.074926 -0.003908 -0.000303] ...\n",
            "134\t0.0\t[-0.070775 -0.247231 -0.073174 -0.003621 -0.000639] ...\n",
            "135\t0.0\t[-0.07078  -0.236088 -0.07579  -0.004327 -0.000588] ...\n",
            "136\t0.0\t[-0.067708 -0.236741 -0.092646 -0.005352 -0.000901] ...\n",
            "137\t0.0\t[-0.066713 -0.239434 -0.083967 -0.00508  -0.00051 ] ...\n",
            "138\t0.0\t[-0.067207 -0.239831 -0.06793  -0.005572 -0.000618] ...\n",
            "139\t0.0\t[-0.065793 -0.236781 -0.065696 -0.004192 -0.000381] ...\n",
            "140\t0.0\t[-0.065465 -0.242839 -0.048678 -0.004646 -0.000326] ...\n",
            "141\t0.0\t[-0.067447 -0.237819 -0.056354 -0.004692 -0.000412] ...\n",
            "142\t0.0\t[-0.069396 -0.23351  -0.049777 -0.004429 -0.001057] ...\n",
            "143\t0.0\t[-0.079032 -0.229188 -0.072177 -0.004563 -0.001293] ...\n",
            "144\t0.0\t[-0.088192 -0.234308 -0.054092 -0.004843 -0.001339] ...\n",
            "145\t0.0\t[-0.095683 -0.235744 -0.048351 -0.005272 -0.000758] ...\n",
            "146\t0.0\t[-0.090729 -0.230401 -0.060568 -0.004781 -0.000725] ...\n",
            "147\t0.0\t[-0.090032 -0.229371 -0.072692 -0.003925 -0.000408] ...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x200 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Line 4 - 147 frames\n",
            "Frame\tCounter\tSample Features\n",
            "1\t0.0\t[-0.034068 -0.189199 -0.053895 -0.004341 -0.001086] ...\n",
            "2\t0.0068\t[-0.03481  -0.188383 -0.054765 -0.004093 -0.001061] ...\n",
            "3\t0.0135\t[-0.035768 -0.198415 -0.027675 -0.004828 -0.001033] ...\n",
            "4\t0.0203\t[-0.016157 -0.191428 -0.030598 -0.004852 -0.000211] ...\n",
            "5\t0.027\t[ 4.26700e-03 -1.97931e-01 -1.26810e-02 -3.45800e-03  1.60000e-05] ...\n",
            "6\t0.0338\t[-4.37160e-02 -1.96696e-01 -2.75920e-02 -3.61100e-03 -1.15000e-04] ...\n",
            "7\t0.0405\t[-0.034422 -0.203199  0.006056 -0.004583 -0.000345] ...\n",
            "8\t0.0473\t[-0.033217 -0.204347 -0.026817 -0.004356 -0.001072] ...\n",
            "9\t0.0541\t[-0.043026 -0.201708 -0.055191 -0.004226 -0.001548] ...\n",
            "10\t0.0608\t[-0.035722 -0.202301 -0.053042 -0.004695 -0.000967] ...\n",
            "11\t0.0676\t[-3.6356e-02 -2.0101e-01 -5.7671e-02 -4.8810e-03 -7.6000e-05] ...\n",
            "12\t0.0743\t[-3.85790e-02 -2.02754e-01 -5.73880e-02 -5.64600e-03 -1.08000e-04] ...\n",
            "13\t0.0811\t[-0.026569 -0.197637 -0.064267 -0.004712 -0.000522] ...\n",
            "14\t0.0878\t[-0.021436 -0.182657 -0.107303 -0.004712 -0.000364] ...\n",
            "15\t0.0946\t[-0.010124 -0.190143 -0.071193 -0.005018 -0.000888] ...\n",
            "16\t0.1014\t[-0.012546 -0.186097 -0.075271 -0.004727 -0.000725] ...\n",
            "17\t0.1081\t[-1.44320e-02 -1.96535e-01 -5.56020e-02 -4.56500e-03 -1.06000e-04] ...\n",
            "18\t0.1149\t[-1.44980e-02 -2.01305e-01 -4.74350e-02 -4.52400e-03 -1.54000e-04] ...\n",
            "19\t0.1216\t[-8.20200e-03 -2.06104e-01 -4.49360e-02 -4.12900e-03  1.30000e-05] ...\n",
            "20\t0.1284\t[-1.37600e-03 -2.14128e-01 -1.23570e-02 -4.18500e-03  9.90000e-05] ...\n",
            "21\t0.1351\t[ 7.16900e-03 -2.17758e-01  1.48520e-02 -4.07300e-03 -1.42000e-04] ...\n",
            "22\t0.1419\t[ 0.020406 -0.209196 -0.012981 -0.003014 -0.000338] ...\n",
            "23\t0.1486\t[ 0.019091 -0.210152 -0.003622 -0.003665 -0.000534] ...\n",
            "24\t0.1554\t[ 2.51710e-02 -2.07941e-01 -3.98800e-03 -4.14600e-03 -5.00000e-06] ...\n",
            "25\t0.1622\t[ 2.63030e-02 -2.09404e-01 -8.40000e-05 -4.67800e-03 -5.48000e-04] ...\n",
            "26\t0.1689\t[ 0.032932 -0.21459   0.005676 -0.004975 -0.000586] ...\n",
            "27\t0.1757\t[ 0.023467 -0.212723  0.006795 -0.004398 -0.000716] ...\n",
            "28\t0.1824\t[ 0.026259 -0.20988   0.003267 -0.004671 -0.000997] ...\n",
            "29\t0.1892\t[ 0.025993 -0.20975  -0.002646 -0.00447  -0.000995] ...\n",
            "30\t0.1959\t[ 0.039456 -0.209119  0.01173  -0.004067 -0.00076 ] ...\n",
            "31\t0.2027\t[ 0.065661 -0.20537   0.007998 -0.004287 -0.000686] ...\n",
            "32\t0.2095\t[ 0.049208 -0.205555  0.007883 -0.003736 -0.000806] ...\n",
            "33\t0.2162\t[ 0.045533 -0.205907  0.002838 -0.003657 -0.000643] ...\n",
            "34\t0.223\t[ 0.036623 -0.204612  0.005694 -0.004093 -0.000599] ...\n",
            "35\t0.2297\t[ 0.043487 -0.205432  0.006075 -0.004292 -0.000616] ...\n",
            "36\t0.2365\t[ 0.046612 -0.203074 -0.001406 -0.004094 -0.000616] ...\n",
            "37\t0.2432\t[ 0.0342   -0.204912  0.003568 -0.00411  -0.000811] ...\n",
            "38\t0.25\t[ 0.025792 -0.207797  0.006577 -0.003934 -0.000928] ...\n",
            "39\t0.2568\t[ 0.025867 -0.206995  0.002795 -0.002954 -0.000894] ...\n",
            "40\t0.2635\t[ 0.023745 -0.211204  0.003911 -0.003549 -0.001054] ...\n",
            "41\t0.2703\t[ 0.026847 -0.211836 -0.000233 -0.004128 -0.000924] ...\n",
            "42\t0.277\t[ 0.024254 -0.214528  0.002936 -0.003826 -0.001273] ...\n",
            "43\t0.2838\t[ 0.025657 -0.215691 -0.000433 -0.004717 -0.001119] ...\n",
            "44\t0.2905\t[ 0.023566 -0.210195 -0.025699 -0.00406  -0.000933] ...\n",
            "45\t0.2973\t[ 0.032315 -0.201244 -0.060017 -0.003716 -0.000636] ...\n",
            "46\t0.3041\t[ 0.041771 -0.199719 -0.061746 -0.004232 -0.000325] ...\n",
            "47\t0.3108\t[ 0.052016 -0.20008  -0.043296 -0.004847 -0.000515] ...\n",
            "48\t0.3176\t[ 6.67840e-02 -1.97479e-01 -5.93000e-04 -3.21100e-03 -7.80000e-05] ...\n",
            "49\t0.3243\t[ 0.065328 -0.196781  0.005254 -0.003632 -0.000616] ...\n",
            "50\t0.3311\t[ 0.067959 -0.198138  0.004366 -0.004183 -0.000528] ...\n",
            "51\t0.3378\t[ 0.063969 -0.201104  0.005488 -0.003618 -0.000644] ...\n",
            "52\t0.3446\t[ 0.05716  -0.203823  0.007668 -0.003898 -0.000778] ...\n",
            "53\t0.3514\t[ 0.06187  -0.1982   -0.009708 -0.002899 -0.000965] ...\n",
            "54\t0.3581\t[ 0.057873 -0.198991 -0.021389 -0.00193  -0.00024 ] ...\n",
            "55\t0.3649\t[ 0.055227 -0.198666 -0.042206 -0.003205 -0.000213] ...\n",
            "56\t0.3716\t[ 0.059739 -0.19281  -0.060372 -0.00294   0.000455] ...\n",
            "57\t0.3784\t[ 0.069021 -0.195491 -0.023252 -0.004412 -0.000254] ...\n",
            "58\t0.3851\t[ 0.063032 -0.191343 -0.031185 -0.003537 -0.000489] ...\n",
            "59\t0.3919\t[ 0.051499 -0.185658 -0.050089 -0.003627 -0.00054 ] ...\n",
            "60\t0.3986\t[ 5.6847e-02 -1.8659e-01 -5.4875e-02 -3.8800e-03 -1.4800e-04] ...\n",
            "61\t0.4054\t[ 0.050467 -0.185998 -0.059185 -0.003514  0.000442] ...\n",
            "62\t0.4122\t[ 0.048061 -0.185142 -0.074381 -0.00313   0.000437] ...\n",
            "63\t0.4189\t[ 0.03972  -0.185377 -0.072692 -0.003675 -0.000634] ...\n",
            "64\t0.4257\t[ 0.038903 -0.18811  -0.045474 -0.004127 -0.000538] ...\n",
            "65\t0.4324\t[ 0.032244 -0.192808 -0.041637 -0.003146 -0.000411] ...\n",
            "66\t0.4392\t[ 0.036262 -0.195458 -0.027261 -0.003065 -0.000611] ...\n",
            "67\t0.4459\t[ 0.036211 -0.208688 -0.011359 -0.003985 -0.000626] ...\n",
            "68\t0.4527\t[ 0.017121 -0.209285 -0.026001 -0.003354 -0.00022 ] ...\n",
            "69\t0.4595\t[ 0.013361 -0.211537 -0.019165 -0.00375  -0.000417] ...\n",
            "70\t0.4662\t[ 0.008765 -0.203645 -0.043102 -0.003264 -0.000556] ...\n",
            "71\t0.473\t[-0.012248 -0.201613 -0.035186 -0.004291 -0.000333] ...\n",
            "72\t0.4797\t[-0.015857 -0.199553 -0.035495 -0.003233 -0.000578] ...\n",
            "73\t0.4865\t[-0.019719 -0.198205 -0.015906 -0.003447 -0.000853] ...\n",
            "74\t0.4932\t[-0.022072 -0.198816 -0.014127 -0.002976 -0.000439] ...\n",
            "75\t0.5\t[-0.034547 -0.198599 -0.012853 -0.003949 -0.000762] ...\n",
            "76\t0.5068\t[-0.038993 -0.196118 -0.014308 -0.004005 -0.000742] ...\n",
            "77\t0.5135\t[-0.037419 -0.193919 -0.024035 -0.004191 -0.000761] ...\n",
            "78\t0.5203\t[-0.047346 -0.192058 -0.015447 -0.004322 -0.001035] ...\n",
            "79\t0.527\t[-0.050346 -0.189192 -0.006788 -0.004659 -0.00074 ] ...\n",
            "80\t0.5338\t[-0.055467 -0.188487 -0.006898 -0.004694 -0.000625] ...\n",
            "81\t0.5405\t[-0.054342 -0.189521 -0.011257 -0.004277 -0.000602] ...\n",
            "82\t0.5473\t[-0.051454 -0.183809 -0.042718 -0.004043 -0.001006] ...\n",
            "83\t0.5541\t[-0.04965  -0.183019 -0.045875 -0.003922 -0.001207] ...\n",
            "84\t0.5608\t[-0.050271 -0.187944 -0.019474 -0.003731 -0.000648] ...\n",
            "85\t0.5676\t[-0.04328  -0.188499 -0.016846 -0.003608 -0.000366] ...\n",
            "86\t0.5743\t[-0.04458  -0.188536 -0.02121  -0.003349 -0.000597] ...\n",
            "87\t0.5811\t[-0.030905 -0.179734 -0.061348 -0.003486 -0.000705] ...\n",
            "88\t0.5878\t[-0.047058 -0.1856   -0.044755 -0.003162 -0.000917] ...\n",
            "89\t0.5946\t[-0.057619 -0.189563 -0.028947 -0.003164 -0.001022] ...\n",
            "90\t0.6014\t[-0.054527 -0.189783 -0.031797 -0.002604 -0.000813] ...\n",
            "91\t0.6081\t[-0.051942 -0.188507 -0.022642 -0.002618 -0.000356] ...\n",
            "92\t0.6149\t[-0.053341 -0.188248 -0.027223 -0.002887 -0.000379] ...\n",
            "93\t0.6216\t[-0.056244 -0.18523  -0.041237 -0.002969 -0.000748] ...\n",
            "94\t0.6284\t[-0.056643 -0.17663  -0.081759 -0.003516 -0.000996] ...\n",
            "95\t0.6351\t[-0.059495 -0.172647 -0.091127 -0.00416  -0.000966] ...\n",
            "96\t0.6419\t[-0.057765 -0.170191 -0.101476 -0.003745 -0.00096 ] ...\n",
            "97\t0.6486\t[-0.053824 -0.168323 -0.104559 -0.003632 -0.001414] ...\n",
            "98\t0.6554\t[-0.05708  -0.165035 -0.106747 -0.003784 -0.000616] ...\n",
            "99\t0.6622\t[-0.063131 -0.165071 -0.106327 -0.003658 -0.000732] ...\n",
            "100\t0.6689\t[-0.062189 -0.164856 -0.111198 -0.003825 -0.000952] ...\n",
            "101\t0.6757\t[-0.061588 -0.16598  -0.107295 -0.003673 -0.00075 ] ...\n",
            "102\t0.6824\t[-0.052493 -0.167358 -0.1033   -0.003778 -0.000992] ...\n",
            "103\t0.6892\t[-0.059067 -0.168966 -0.102936 -0.004627 -0.001198] ...\n",
            "104\t0.6959\t[-0.057803 -0.173044 -0.08482  -0.004775 -0.00061 ] ...\n",
            "105\t0.7027\t[-0.064267 -0.16743  -0.093498 -0.003729 -0.000496] ...\n",
            "106\t0.7095\t[-6.95430e-02 -1.67325e-01 -9.25240e-02 -4.15700e-03 -1.61000e-04] ...\n",
            "107\t0.7162\t[-0.069449 -0.168212 -0.089272 -0.004482 -0.00063 ] ...\n",
            "108\t0.723\t[-0.066539 -0.170852 -0.084117 -0.004837 -0.000578] ...\n",
            "109\t0.7297\t[-0.059942 -0.170658 -0.093218 -0.004752 -0.000478] ...\n",
            "110\t0.7365\t[-0.066809 -0.162622 -0.108442 -0.004798 -0.000513] ...\n",
            "111\t0.7432\t[-0.05421  -0.164415 -0.107688 -0.004277 -0.000556] ...\n",
            "112\t0.75\t[-0.05345  -0.163344 -0.110229 -0.004162 -0.000248] ...\n",
            "113\t0.7568\t[-5.43990e-02 -1.65254e-01 -1.11493e-01 -3.84800e-03 -1.90000e-05] ...\n",
            "114\t0.7635\t[-0.050267 -0.170387 -0.105659 -0.004368 -0.000307] ...\n",
            "115\t0.7703\t[-0.054382 -0.168325 -0.107115 -0.003819 -0.000468] ...\n",
            "116\t0.777\t[-0.056215 -0.163868 -0.113138 -0.003529 -0.000784] ...\n",
            "117\t0.7838\t[-0.051303 -0.161641 -0.113658 -0.004225 -0.000809] ...\n",
            "118\t0.7905\t[-0.055181 -0.156588 -0.125005 -0.003767 -0.000613] ...\n",
            "119\t0.7973\t[-0.041513 -0.159081 -0.124431 -0.004405 -0.000622] ...\n",
            "120\t0.8041\t[-0.042073 -0.165981 -0.105979 -0.004815 -0.000831] ...\n",
            "121\t0.8108\t[-0.031191 -0.177324 -0.080596 -0.004783 -0.00073 ] ...\n",
            "122\t0.8176\t[-0.026491 -0.184652 -0.073617 -0.005133 -0.000482] ...\n",
            "123\t0.8243\t[-0.021985 -0.190611 -0.055071 -0.005122 -0.000342] ...\n",
            "124\t0.8311\t[-0.012965 -0.193638 -0.040796 -0.004622 -0.000761] ...\n",
            "125\t0.8378\t[ 0.000724 -0.18608  -0.052158 -0.004356 -0.00126 ] ...\n",
            "126\t0.8446\t[ 0.003764 -0.18407  -0.059413 -0.004176 -0.001285] ...\n",
            "127\t0.8514\t[ 0.002913 -0.18427  -0.057168 -0.004078 -0.001271] ...\n",
            "128\t0.8581\t[ 0.006108 -0.185236 -0.054675 -0.00417  -0.001283] ...\n",
            "129\t0.8649\t[ 0.005958 -0.186199 -0.052789 -0.004181 -0.001256] ...\n",
            "130\t0.8716\t[-0.002551 -0.192516 -0.035092 -0.004261 -0.001068] ...\n",
            "131\t0.8784\t[ 0.001729 -0.187431 -0.051965 -0.00417  -0.001172] ...\n",
            "132\t0.8851\t[ 0.001748 -0.19816  -0.0227   -0.00402  -0.00075 ] ...\n",
            "133\t0.8919\t[ 0.008345 -0.209671  0.013297 -0.003554 -0.00047 ] ...\n",
            "134\t0.8986\t[ 0.010894 -0.20537   0.006944 -0.003222 -0.000848] ...\n",
            "135\t0.9054\t[ 0.010049 -0.196217 -0.020428 -0.004244 -0.00048 ] ...\n",
            "136\t0.9122\t[ 0.014104 -0.194625 -0.020403 -0.004878 -0.000797] ...\n",
            "137\t0.9189\t[ 0.021598 -0.203203 -0.007974 -0.004526 -0.000839] ...\n",
            "138\t0.9257\t[ 0.031776 -0.206843  0.003937 -0.0047   -0.000243] ...\n",
            "139\t0.9324\t[ 0.023184 -0.20735   0.002108 -0.00418  -0.000248] ...\n",
            "140\t0.9392\t[ 2.56810e-02 -2.08666e-01  2.34700e-03 -4.43100e-03 -1.86000e-04] ...\n",
            "141\t0.9459\t[ 0.018841 -0.20457  -0.012106 -0.004153 -0.000401] ...\n",
            "142\t0.9527\t[ 0.012476 -0.198046 -0.016412 -0.004401 -0.001171] ...\n",
            "143\t0.9595\t[-0.002895 -0.196132 -0.01936  -0.004935 -0.001321] ...\n",
            "144\t0.9662\t[-0.011445 -0.197835 -0.014226 -0.004977 -0.001299] ...\n",
            "145\t0.973\t[-0.012066 -0.200698  0.004882 -0.004879 -0.00091 ] ...\n",
            "146\t0.9797\t[-0.019639 -0.194141 -0.014598 -0.004809 -0.00109 ] ...\n",
            "147\t0.9865\t[-0.019015 -0.194491 -0.015012 -0.004167 -0.001203] ...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x200 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Line 5 - 147 frames\n",
            "Frame\tCounter\tSample Features\n",
            "1\t0.0\t[-7.17100e-03 -1.92777e-01 -3.18290e-02 -4.57000e-03 -4.30000e-05] ...\n",
            "2\t0.0108\t[-1.2738e-02 -1.8978e-01 -4.0581e-02 -4.4070e-03 -1.8700e-04] ...\n",
            "3\t0.0215\t[-1.31360e-02 -1.94755e-01 -2.73880e-02 -4.61500e-03 -2.20000e-05] ...\n",
            "4\t0.0323\t[-0.023191 -0.182515 -0.059037 -0.003804  0.000207] ...\n",
            "5\t0.043\t[-0.017065 -0.187593 -0.045429 -0.003288  0.000282] ...\n",
            "6\t0.0538\t[-0.022219 -0.201185 -0.013263 -0.003876  0.000456] ...\n",
            "7\t0.0645\t[-0.025628 -0.204162 -0.00892  -0.003592  0.001106] ...\n",
            "8\t0.0753\t[-0.023944 -0.206528  0.002079 -0.002843  0.000965] ...\n",
            "9\t0.086\t[-0.027912 -0.207011  0.001026 -0.002079  0.000382] ...\n",
            "10\t0.0968\t[-0.02777  -0.206867  0.005003 -0.002945  0.000639] ...\n",
            "11\t0.1075\t[-0.027005 -0.206368  0.00262  -0.002993  0.000971] ...\n",
            "12\t0.1183\t[-0.026654 -0.207304  0.003291 -0.002962  0.000926] ...\n",
            "13\t0.129\t[-0.027985 -0.206916  0.002785 -0.00265   0.000541] ...\n",
            "14\t0.1398\t[-0.032665 -0.20484  -0.001604 -0.00254   0.000474] ...\n",
            "15\t0.1505\t[-0.030439 -0.205693 -0.001938 -0.002868  0.00033 ] ...\n",
            "16\t0.1613\t[-0.034776 -0.202988 -0.002575 -0.002874  0.000408] ...\n",
            "17\t0.172\t[-3.3076e-02 -2.0494e-01 -1.1400e-04 -3.1590e-03  4.7100e-04] ...\n",
            "18\t0.1828\t[-0.0312   -0.207089  0.003767 -0.003341  0.000453] ...\n",
            "19\t0.1935\t[-0.033844 -0.20629   0.00234  -0.002878  0.000537] ...\n",
            "20\t0.2043\t[-0.032247 -0.207338  0.001853 -0.002933  0.000465] ...\n",
            "21\t0.2151\t[-0.031406 -0.208175  0.003172 -0.002821  0.000493] ...\n",
            "22\t0.2258\t[-3.17590e-02 -2.06826e-01 -1.96000e-04 -2.45700e-03  5.84000e-04] ...\n",
            "23\t0.2366\t[-0.030583 -0.203795 -0.004868 -0.002256  0.000489] ...\n",
            "24\t0.2473\t[-1.42000e-02 -2.06449e-01 -1.31300e-03 -3.46400e-03  6.40000e-05] ...\n",
            "25\t0.2581\t[-0.005222 -0.20785   0.001189 -0.004305 -0.00044 ] ...\n",
            "26\t0.2688\t[-0.004043 -0.205624 -0.000951 -0.004553 -0.000681] ...\n",
            "27\t0.2796\t[-0.000949 -0.207586  0.003856 -0.004768 -0.000754] ...\n",
            "28\t0.2903\t[ 0.000898 -0.207923  0.005262 -0.00496  -0.000774] ...\n",
            "29\t0.3011\t[ 0.001767 -0.207716  0.003037 -0.004666 -0.000798] ...\n",
            "30\t0.3118\t[ 0.001922 -0.208533  0.004179 -0.004579 -0.000813] ...\n",
            "31\t0.3226\t[ 0.001088 -0.208095  0.001681 -0.004594 -0.000849] ...\n",
            "32\t0.3333\t[ 0.003629 -0.208828  0.007171 -0.004638 -0.000918] ...\n",
            "33\t0.3441\t[ 0.006821 -0.207711  0.007185 -0.004622 -0.000976] ...\n",
            "34\t0.3548\t[ 0.008304 -0.205117  0.003128 -0.004577 -0.001029] ...\n",
            "35\t0.3656\t[ 0.006974 -0.205613  0.002166 -0.004576 -0.001031] ...\n",
            "36\t0.3763\t[ 0.005566 -0.209921  0.006228 -0.00488  -0.001107] ...\n",
            "37\t0.3871\t[ 0.001518 -0.207169 -0.013918 -0.004884 -0.000982] ...\n",
            "38\t0.3978\t[-0.0004   -0.205033 -0.027192 -0.004671 -0.00095 ] ...\n",
            "39\t0.4086\t[-0.001012 -0.197271 -0.054039 -0.004369 -0.000957] ...\n",
            "40\t0.4194\t[ 0.000518 -0.201306 -0.050841 -0.004583 -0.000797] ...\n",
            "41\t0.4301\t[ 0.002011 -0.203705 -0.051552 -0.005456 -0.000651] ...\n",
            "42\t0.4409\t[ 0.00648  -0.208285 -0.036569 -0.005269 -0.00076 ] ...\n",
            "43\t0.4516\t[ 0.010475 -0.200294 -0.052233 -0.005154 -0.000589] ...\n",
            "44\t0.4624\t[ 0.010631 -0.193525 -0.059509 -0.004608 -0.000422] ...\n",
            "45\t0.4731\t[ 0.009434 -0.190335 -0.06259  -0.004229 -0.000261] ...\n",
            "46\t0.4839\t[ 1.01830e-02 -1.90678e-01 -5.62590e-02 -4.39700e-03 -1.04000e-04] ...\n",
            "47\t0.4946\t[ 0.011351 -0.189394 -0.05829  -0.004528 -0.000226] ...\n",
            "48\t0.5054\t[ 1.84890e-02 -1.90068e-01 -5.66440e-02 -4.66000e-03 -1.09000e-04] ...\n",
            "49\t0.5161\t[ 1.90590e-02 -1.90995e-01 -4.87240e-02 -4.36900e-03  1.20000e-05] ...\n",
            "50\t0.5269\t[ 0.017317 -0.187673 -0.060605 -0.004147 -0.000351] ...\n",
            "51\t0.5376\t[ 0.012943 -0.186351 -0.061825 -0.004042 -0.000552] ...\n",
            "52\t0.5484\t[ 0.013149 -0.188654 -0.051905 -0.004117 -0.000625] ...\n",
            "53\t0.5591\t[ 0.008912 -0.187309 -0.061443 -0.004103 -0.00076 ] ...\n",
            "54\t0.5699\t[ 0.003322 -0.192655 -0.046103 -0.004049 -0.001025] ...\n",
            "55\t0.5806\t[-0.001471 -0.194437 -0.053915 -0.003762 -0.000994] ...\n",
            "56\t0.5914\t[-0.01158  -0.19239  -0.063544 -0.003644 -0.000284] ...\n",
            "57\t0.6022\t[-0.015983 -0.187004 -0.079721 -0.004826 -0.000735] ...\n",
            "58\t0.6129\t[-0.02901  -0.187539 -0.07445  -0.004913 -0.000628] ...\n",
            "59\t0.6237\t[-0.034207 -0.184172 -0.076525 -0.004865 -0.000551] ...\n",
            "60\t0.6344\t[-4.0499e-02 -1.9227e-01 -4.8507e-02 -4.5160e-03  5.9000e-05] ...\n",
            "61\t0.6452\t[-0.03899  -0.193689 -0.036609 -0.004822  0.000206] ...\n",
            "62\t0.6559\t[-3.60820e-02 -1.95353e-01 -2.57140e-02 -5.17900e-03 -1.26000e-04] ...\n",
            "63\t0.6667\t[-0.034    -0.196732 -0.019468 -0.005122 -0.00027 ] ...\n",
            "64\t0.6774\t[-0.025133 -0.194494 -0.022026 -0.00476  -0.000363] ...\n",
            "65\t0.6882\t[-0.022642 -0.192512 -0.038491 -0.004119 -0.000594] ...\n",
            "66\t0.6989\t[-0.019226 -0.194854 -0.024964 -0.004088 -0.000465] ...\n",
            "67\t0.7097\t[-0.016466 -0.199647 -0.01439  -0.004019 -0.000369] ...\n",
            "68\t0.7204\t[-0.0217   -0.196563 -0.032341 -0.004555 -0.000683] ...\n",
            "69\t0.7312\t[-0.027006 -0.203839 -0.016465 -0.004709 -0.000879] ...\n",
            "70\t0.7419\t[-0.023655 -0.196016 -0.061301 -0.004689 -0.001073] ...\n",
            "71\t0.7527\t[-0.035601 -0.189314 -0.08539  -0.005808 -0.000686] ...\n",
            "72\t0.7634\t[-0.027175 -0.185613 -0.08225  -0.004871 -0.000517] ...\n",
            "73\t0.7742\t[-0.017494 -0.181249 -0.086993 -0.004634 -0.00053 ] ...\n",
            "74\t0.7849\t[-0.013358 -0.180528 -0.086731 -0.004338 -0.00039 ] ...\n",
            "75\t0.7957\t[-0.016085 -0.181945 -0.084726 -0.004782 -0.000469] ...\n",
            "76\t0.8065\t[-0.015037 -0.179411 -0.089642 -0.004926 -0.000478] ...\n",
            "77\t0.8172\t[-0.018128 -0.178804 -0.093496 -0.004753 -0.00038 ] ...\n",
            "78\t0.828\t[-0.019245 -0.181255 -0.089672 -0.004821 -0.000582] ...\n",
            "79\t0.8387\t[-0.025385 -0.170816 -0.117532 -0.005068 -0.000314] ...\n",
            "80\t0.8495\t[-0.024344 -0.168696 -0.12107  -0.004819 -0.000322] ...\n",
            "81\t0.8602\t[-2.21220e-02 -1.71788e-01 -1.12340e-01 -4.56600e-03 -1.53000e-04] ...\n",
            "82\t0.871\t[-0.021303 -0.172677 -0.11088  -0.004864 -0.00021 ] ...\n",
            "83\t0.8817\t[-0.01177  -0.175959 -0.100391 -0.004587 -0.000186] ...\n",
            "84\t0.8925\t[-0.004756 -0.185001 -0.075566 -0.004138 -0.0003  ] ...\n",
            "85\t0.9032\t[-0.002903 -0.188883 -0.066723 -0.003986 -0.000266] ...\n",
            "86\t0.914\t[-0.003169 -0.187823 -0.072157 -0.004308 -0.000329] ...\n",
            "87\t0.9247\t[-0.003643 -0.176903 -0.106615 -0.004457 -0.001412] ...\n",
            "88\t0.9355\t[-0.00505  -0.178542 -0.105319 -0.004223 -0.001539] ...\n",
            "89\t0.9462\t[-0.001292 -0.179972 -0.107636 -0.004157 -0.001664] ...\n",
            "90\t0.957\t[-0.004169 -0.17937  -0.106584 -0.004031 -0.001766] ...\n",
            "91\t0.9677\t[-0.011747 -0.17932  -0.099414 -0.004349 -0.001667] ...\n",
            "92\t0.9785\t[-0.019373 -0.175915 -0.109274 -0.004689 -0.001527] ...\n",
            "93\t0.9892\t[-0.027899 -0.165357 -0.133489 -0.004223 -0.002142] ...\n",
            "94\t0.0\t[-0.033192 -0.160901 -0.148491 -0.00417  -0.002368] ...\n",
            "95\t0.0\t[-0.047796 -0.159477 -0.147582 -0.004973 -0.002644] ...\n",
            "96\t0.0\t[-0.046426 -0.15342  -0.156589 -0.004581 -0.00212 ] ...\n",
            "97\t0.0\t[-0.035736 -0.158072 -0.142368 -0.004236 -0.002261] ...\n",
            "98\t0.0\t[-0.024981 -0.156678 -0.147182 -0.004932 -0.001733] ...\n",
            "99\t0.0\t[-0.029055 -0.166578 -0.129127 -0.004339 -0.00143 ] ...\n",
            "100\t0.0\t[-0.029387 -0.167258 -0.134378 -0.004659 -0.001508] ...\n",
            "101\t0.0\t[-0.021545 -0.169981 -0.130749 -0.004337 -0.0014  ] ...\n",
            "102\t0.0\t[-0.018457 -0.170569 -0.129732 -0.004691 -0.001487] ...\n",
            "103\t0.0\t[-0.019847 -0.168413 -0.139603 -0.00507  -0.001446] ...\n",
            "104\t0.0\t[-0.016343 -0.170849 -0.126733 -0.004413 -0.001779] ...\n",
            "105\t0.0\t[-0.003817 -0.172993 -0.120161 -0.00408  -0.001524] ...\n",
            "106\t0.0\t[-0.004047 -0.176066 -0.115353 -0.00514  -0.000928] ...\n",
            "107\t0.0\t[ 0.003315 -0.177061 -0.114037 -0.005228 -0.001679] ...\n",
            "108\t0.0\t[ 0.009414 -0.180247 -0.109961 -0.005099 -0.001376] ...\n",
            "109\t0.0\t[ 0.003746 -0.180069 -0.109278 -0.005136 -0.001793] ...\n",
            "110\t0.0\t[-0.018748 -0.170559 -0.133769 -0.006214 -0.001881] ...\n",
            "111\t0.0\t[ 0.003055 -0.17903  -0.117302 -0.005074 -0.001467] ...\n",
            "112\t0.0\t[-0.012517 -0.173821 -0.114382 -0.00538  -0.001093] ...\n",
            "113\t0.0\t[-0.010149 -0.175155 -0.11541  -0.005171 -0.00108 ] ...\n",
            "114\t0.0\t[-0.00829  -0.175427 -0.117417 -0.005501 -0.001071] ...\n",
            "115\t0.0\t[-0.006663 -0.175532 -0.118271 -0.005498 -0.001124] ...\n",
            "116\t0.0\t[-0.006031 -0.175456 -0.119262 -0.005512 -0.001168] ...\n",
            "117\t0.0\t[-0.005692 -0.176058 -0.116544 -0.005497 -0.001172] ...\n",
            "118\t0.0\t[-0.008283 -0.178045 -0.111151 -0.004824 -0.001037] ...\n",
            "119\t0.0\t[-0.007443 -0.183472 -0.091275 -0.005517 -0.001047] ...\n",
            "120\t0.0\t[-0.00243  -0.197532 -0.04965  -0.005353 -0.001463] ...\n",
            "121\t0.0\t[ 0.003892 -0.198228 -0.032896 -0.004196 -0.000841] ...\n",
            "122\t0.0\t[ 0.010403 -0.19915  -0.031993 -0.004544 -0.000936] ...\n",
            "123\t0.0\t[ 0.010614 -0.197039 -0.040174 -0.004632 -0.000913] ...\n",
            "124\t0.0\t[ 0.00854  -0.188518 -0.067895 -0.004769 -0.001172] ...\n",
            "125\t0.0\t[ 0.004108 -0.178231 -0.100183 -0.004988 -0.001356] ...\n",
            "126\t0.0\t[ 0.006831 -0.179452 -0.101591 -0.005053 -0.001376] ...\n",
            "127\t0.0\t[ 0.007958 -0.182292 -0.093598 -0.00493  -0.001399] ...\n",
            "128\t0.0\t[ 0.010016 -0.184805 -0.087328 -0.00499  -0.001457] ...\n",
            "129\t0.0\t[ 0.012747 -0.187628 -0.079713 -0.004977 -0.001428] ...\n",
            "130\t0.0\t[ 0.012677 -0.190563 -0.070759 -0.004895 -0.001312] ...\n",
            "131\t0.0\t[ 0.012462 -0.199803 -0.046002 -0.004841 -0.001033] ...\n",
            "132\t0.0\t[ 0.015295 -0.19772  -0.056243 -0.004742 -0.000989] ...\n",
            "133\t0.0\t[ 0.013534 -0.205072 -0.036413 -0.004622 -0.000771] ...\n",
            "134\t0.0\t[ 0.003987 -0.203204 -0.035421 -0.004535 -0.001493] ...\n",
            "135\t0.0\t[ 0.005266 -0.203044 -0.036564 -0.004832 -0.001434] ...\n",
            "136\t0.0\t[ 0.003589 -0.206011 -0.029049 -0.00521  -0.001876] ...\n",
            "137\t0.0\t[-8.50000e-05 -2.11668e-01 -1.74070e-02 -5.08000e-03 -2.49800e-03] ...\n",
            "138\t0.0\t[ 0.006714 -0.214131 -0.002727 -0.005009 -0.002385] ...\n",
            "139\t0.0\t[ 0.009347 -0.21066  -0.012507 -0.004308 -0.002627] ...\n",
            "140\t0.0\t[ 0.010248 -0.212389 -0.011009 -0.004821 -0.002048] ...\n",
            "141\t0.0\t[ 0.009858 -0.210176 -0.020431 -0.004379 -0.001692] ...\n",
            "142\t0.0\t[ 0.014227 -0.198784 -0.040743 -0.004622 -0.001857] ...\n",
            "143\t0.0\t[ 0.008378 -0.192554 -0.068546 -0.005317 -0.001808] ...\n",
            "144\t0.0\t[ 0.003896 -0.195904 -0.057269 -0.005428 -0.001733] ...\n",
            "145\t0.0\t[ 0.002052 -0.19846  -0.042668 -0.005478 -0.001256] ...\n",
            "146\t0.0\t[-0.00477  -0.189266 -0.067141 -0.005179 -0.001393] ...\n",
            "147\t0.0\t[-0.003134 -0.189916 -0.063286 -0.004557 -0.001229] ...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x200 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAArwAAADvCAYAAAAQPwczAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAQadJREFUeJzt3XtcVGX+B/DPzACDQCCIXEVBJe+iwkJ4+VWKoLWVWXkpE800NVaNLVcrRDQjNZU0V7p5WzXNVq3UUESxTEJFzSvkBbVUQFEEQWGYOb8/XCYHhssMcz183q8Xu845z5znOd9G+PrwPc8jEQRBABERERGRSEnNPQAiIiIiImNiwktEREREosaEl4iIiIhEjQkvEREREYkaE14iIiIiEjUmvEREREQkakx4iYiIiEjUmPASERERkagx4SUiIiIiUWPCS0QW4dKlS5BIJFi9erW5h0JERCLDhJeIjG716tWQSCQ4cuSIuYeiVdX4tH3l5eUZtK8xY8bU2ldKSopB+zK39PR0DB06FF5eXrCzs4OHhweeeeYZbNmyxdxDAwCUlZVh9uzZSE9PN/dQiMjIbMw9ACIiAGjTpg3u3bsHW1tbs41hzpw5CAgI0DjWvHlzg/cjl8vx5Zdf1jgeFBRk8L7MJT4+HnPmzEFgYCDeeOMNtGnTBoWFhdi5cydeeOEFrF+/Hi+//LJZx1hWVoaEhAQAwBNPPGHWsRCRcTHhJSKLIJFIYG9vb9YxDB48GCEhIUbvx8bGBqNGjWpw+9LSUjg6OhpxRIb17bffYs6cOXjxxRexYcMGjX/EvPPOO9i1axcUCoUZR2hc1vbfi6gpYEkDEVkEbTW8Y8aMgZOTE65evYohQ4bAyckJLVu2xNtvvw2lUqnxfpVKhaSkJHTp0gX29vbw9PTEG2+8gdu3b+s0jpKSkhrXNqXZs2dDIpHgzJkzePnll+Hq6oq+ffsCAE6cOIExY8agbdu2sLe3h5eXF1577TUUFhZqvcbvv/+OUaNGwcXFBS1btkRcXBwEQcAff/yB5557Ds7OzvDy8sKiRYtqjKO8vBzx8fFo37495HI5/Pz8MH36dJSXl9d7D3FxcXBzc8PKlSu1zthHRUXh73//u/p1QUEBxo0bB09PT9jb2yMoKAhr1qzReE96ejokEkmN8gN9PzeXLl1Cy5YtAQAJCQnqspLZs2err5OdnY0XX3wRbm5usLe3R0hICL7//nuN/qvKYfbv34/JkyfDw8MDrVq1AvDgszRt2jT4+/tDLpfDw8MDAwcOxNGjR+uNIREZFmd4iciiKZVKREVFISwsDB9//DH27NmDRYsWoV27dpg0aZK63RtvvIHVq1dj7NixmDJlCnJzc/Hpp5/i2LFj+OWXXxpUKvHkk0/i7t27sLOzQ1RUFBYtWoTAwECj3NfNmzc1Xtva2sLFxUX9+qWXXkJgYCA+/PBDCIIAAEhNTcXFixcxduxYeHl54fTp0/j8889x+vRp/Prrr5BIJBrXHD58ODp16oSPPvoIO3bswAcffAA3Nzd89tln6N+/P+bPn4/169fj7bffxt/+9jf83//9H4AH/3h49tlnceDAAUyYMAGdOnXCyZMnsWTJEvz+++/Ytm1brfd17tw5ZGdn47XXXsMjjzxSbxzu3buHJ554AufPn0dMTAwCAgKwefNmjBkzBkVFRZg6dWpDQ6qhvs9Ny5YtsWLFCkyaNAnPP/88hg4dCgDo3r07AOD06dPo06cPfH19MWPGDDg6OuKbb77BkCFD8N///hfPP/+8Rn+TJ09Gy5YtMWvWLJSWlgIAJk6ciG+//RYxMTHo3LkzCgsLceDAAZw9exa9evXS676ISE8CEZGRrVq1SgAgHD58uNY2ubm5AgBh1apV6mPR0dECAGHOnDkabXv27CkEBwerX//8888CAGH9+vUa7VJSUrQer27Tpk3CmDFjhDVr1ghbt24V3n//fcHBwUFwd3cXrly5osOd1q/qnqp/Pf7444IgCEJ8fLwAQBg5cmSN95aVldU49vXXXwsAhJ9++kl9rOoaEyZMUB+rrKwUWrVqJUgkEuGjjz5SH799+7bQrFkzITo6Wn3sP//5jyCVSoWff/5Zo6/k5GQBgPDLL7/Uen/fffedAEBYsmRJfaEQBEEQkpKSBADCunXr1McqKiqE8PBwwcnJSSguLhYEQRD27dsnABD27dun8f7GfG5u3LghABDi4+NrjGvAgAFCt27dhPv376uPqVQqoXfv3kJgYKD6WNVnu2/fvkJlZaXGNVxcXIQ333yzQXEgIuNiSQMRWbyJEydqvO7Xrx8uXryofr1582a4uLhg4MCBuHnzpvorODgYTk5O2LdvX53XHzZsGFatWoXRo0djyJAhmDt3Lnbt2oXCwkLMmzfP4Pdjb2+P1NRUja/qZQXV7xkAmjVrpv7z/fv3cfPmTTz22GMAoPXX5K+//rr6zzKZDCEhIRAEAePGjVMfb968OTp06FAjnp06dULHjh014tm/f38AqDOexcXFANCg2V0A2LlzJ7y8vDBy5Ej1MVtbW0yZMgV3797F/v37G3Qdber73NTm1q1b2Lt3L4YNG4aSkhL1/RcWFiIqKgrnzp3D1atXNd4zfvx4yGQyjWPNmzdHZmYmrl27pvc9EJFhsKSBiCyavb29utayiqurq0Zt7rlz53Dnzh14eHhovUZBQYHO/fbt2xdhYWHYs2dPne3u3LmDe/fuqV/b2dnBzc2tzvfIZDJERETU2ab6ahHAg0QsISEBGzdurHFPd+7cqdG+devWGq9dXFxgb28Pd3f3GscfrgM+d+4czp49WyPuVeqKp7OzM4AH9asNcfnyZQQGBkIq1Zx/6dSpk/q8PhryuanN+fPnIQgC4uLiEBcXp7VNQUEBfH191a+1/fdasGABoqOj4efnh+DgYDz11FMYPXo02rZtq+PdEFFjMeElIotWfdZMG5VKBQ8PD6xfv17r+doSt/r4+fkhJyenzjZTp07VeMDq8ccfN8i6rg/P5lYZNmwYDh48iHfeeQc9evSAk5MTVCoVBg0aBJVKVaO9ttjVFk/hf3XCwIN4duvWDYsXL9ba1s/Pr9Zxd+zYEQBw8uTJWtvoo3p9cpXaHjBsyOemNlWxfPvttxEVFaW1Tfv27TVe1/bfq1+/fti6dSt2796NhQsXYv78+diyZQsGDx6s9/iISHdMeInI6rVr1w579uxBnz59tCYe+rp48WK9yfL06dM1lhhzdXU1WP8Pu337NtLS0pCQkIBZs2apj587d87gfbVr1w6//fYbBgwYUGuiWZtHH30UHTp0wHfffYdPPvkETk5OdbZv06YNTpw4AZVKpTHLm52drT4P/BXXoqIijffrOwMM1J5EV83A2tra1jsTXx9vb29MnjwZkydPRkFBAXr16oV58+Yx4SUyMdbwEpHVGzZsGJRKJebOnVvjXGVlZY0kqbobN27UOLZz505kZWVh0KBBdb63c+fOiIiIUH8FBwfrNPaGqpqxfHgmFgCSkpIM3tewYcNw9epVfPHFFzXO3bt3T70KQW0SEhJQWFiI119/HZWVlTXO7969G9u3bwcAPPXUU8jLy8OmTZvU5ysrK7Fs2TI4OTnh8ccfB/Ag8ZXJZPjpp580rvXvf/9b5/ur4uDgAKBmEu3h4YEnnngCn332Ga5fv17jfdo+L9UplcoaZSYeHh7w8fFp0NJuRGRYnOElIpNZuXKl1u1z9V16qsrjjz+ON954A4mJiTh+/DgiIyNha2uLc+fOYfPmzfjkk0/w4osv1vr+3r17o2fPnggJCYGLiwuOHj2KlStXws/PD++++26jxmYozs7O+L//+z8sWLAACoUCvr6+2L17N3Jzcw3e16uvvopvvvkGEydOxL59+9CnTx8olUpkZ2fjm2++wa5du+rcoGP48OE4efIk5s2bh2PHjmHkyJHqndZSUlKQlpaGDRs2AAAmTJiAzz77DGPGjEFWVhb8/f3x7bff4pdffkFSUpL64TcXFxe89NJLWLZsGSQSCdq1a4ft27frVZ9dpVmzZujcuTM2bdqERx99FG5ubujatSu6du2K5cuXo2/fvujWrRvGjx+Ptm3bIj8/HxkZGfjzzz/x22+/1XntkpIStGrVCi+++CKCgoLg5OSEPXv24PDhw1rXPSYi42LCS0Qms2LFCq3Hx4wZ0+hrJycnIzg4GJ999hneffdd2NjYwN/fH6NGjUKfPn3qfO/w4cOxY8cO7N69G2VlZfD29sb48eMRHx8PT0/PRo/NUDZs2IB//OMfWL58OQRBQGRkJH788Uf4+PgYtB+pVIpt27ZhyZIlWLt2LbZu3QoHBwe0bdsWU6dOxaOPPlrvNT744AP0798fS5cuxYoVK3Dr1i24urrisccew3fffYdnn30WwIOkMz09HTNmzMCaNWtQXFyMDh06YNWqVTU+F8uWLYNCoUBycjLkcjmGDRuGhQsXomvXrnrf65dffol//OMfeOutt1BRUYH4+Hh07doVnTt3xpEjR5CQkIDVq1ejsLAQHh4e6Nmzp0ZJSW0cHBwwefJk7N69G1u2bIFKpUL79u3x73//W2P9aCIyDYlQ/fdjREREREQiwhpeIiIiIhI1JrxEREREJGpMeImIiIhI1JjwEhEREZGoMeElIiIiIlFjwktEREREosZ1eLVQqVS4du0aHnnkEZ231SQiIiIi4xMEASUlJfDx8dHYmlwbJrxaXLt2DX5+fuYeBhERERHV448//kCrVq3qbGPWhPenn37CwoULkZWVhevXr2Pr1q0YMmRIne9JT09HbGwsTp8+DT8/P7z//vs1duNZvnw5Fi5ciLy8PAQFBWHZsmUIDQ1t8LiqtrL8448/4OzsrOtt6UyhUGD37t3q7VCpfoyZfhg33TFmumPMdMeY6Y4x04+Y4lZcXAw/Pz913lYXsya8paWlCAoKwmuvvYahQ4fW2z43NxdPP/00Jk6ciPXr1yMtLQ2vv/46vL29ERUVBQDYtGkTYmNjkZycjLCwMCQlJSEqKgo5OTnw8PBo0LiqyhicnZ1NlvA6ODjA2dnZ6j98psKY6Ydx0x1jpjtrjJlSJeBQ7i0UlNyHxyP2CA1wg0xqupI2a4yZuTFm+hFj3BpSfmrWhHfw4MEYPHhwg9snJycjICAAixYtAgB06tQJBw4cwJIlS9QJ7+LFizF+/HiMHTtW/Z4dO3Zg5cqVmDFjhuFvgoiIrFJVkpt6Jg/bjl/DrdIK9TlvF3vEP9MZg7p6m3GERGQoVlXDm5GRgYiICI1jUVFRmDZtGgCgoqICWVlZmDlzpvq8VCpFREQEMjIyar1ueXk5ysvL1a+Li4sBPPhXkEKhMOAdaFfVhyn6EgvGTD+Mm+4YM91ZQ8x2nc7HBzuzkVdcrvV83p37mLTuKJaNCEJUF0+jj8caYmZpGDP9iCluutyDVSW8eXl58PTU/Mbj6emJ4uJi3Lt3D7dv34ZSqdTaJjs7u9brJiYmIiEhocbx3bt3w8HBwTCDb4DU1FST9SUWjJl+GDfdMWa6s7SYqQTgQrEEJ28D+69XPdGt/Vehwv/+9/0tx6G4pISpqhssLWbWgDHTjxjiVlZW1uC2VpXwGsvMmTMRGxurfl1VBB0ZGWmyGt7U1FQMHDhQNPU0xsaY6Ydx0x1jpjtLiZlSJeDI5dsoKCnH5cIybDryZ60zutpJUFQBtOz8GMIC3Iw2TsByYmZNGDP9iCluVb+RbwirSni9vLyQn5+vcSw/Px/Ozs5o1qwZZDIZZDKZ1jZeXl61Xlcul0Mul9c4bmtra9IPg6n7EwPGTD+Mm+4YM92ZM2Ypp64j4YczuH7nfqOvVVhWabL74OdMd4yZfsQQN13Gb1U7rYWHhyMtLU3jWGpqKsLDwwEAdnZ2CA4O1mijUqmQlpambkNEROKkVAnIuFCIOT+cxsR1Rw2S7AKAxyP2BrkOEZmPWWd47969i/Pnz6tf5+bm4vjx43Bzc0Pr1q0xc+ZMXL16FWvXrgUATJw4EZ9++immT5+O1157DXv37sU333yDHTt2qK8RGxuL6OhohISEIDQ0FElJSSgtLVWv2kBEROLw8FJil26W4etDV5BXbJgkF3hQ3evl8mCJMiKybmZNeI8cOYInn3xS/bqqjjY6OhqrV6/G9evXceXKFfX5gIAA7NixA2+99RY++eQTtGrVCl9++aV6STIAGD58OG7cuIFZs2YhLy8PPXr0QEpKSo0H2YiIyHoZsmRBm6pn1OKf6WzS9XiJyDjMmvA+8cQTEASh1vOrV6/W+p5jx47Ved2YmBjExMQ0dnhERGRBHl43d+Uvl4zalxfX4SUSFat6aI2IiJoOY5csVGdvK8V9hQqRnT2xYlQwZ3aJRIQJLxERWRxjlyw8bFwff0R09sLhS4VYnHoObo52THaJRIYJLxERWQRTliwANbcPPnm1CABQUakyet9EZFpMeImIyGweTnK3Hb+GW6UVRuvLy1mOkaGt4e/uCI9HHqy+8PBMrp3swUqd5UomvERiw4SXiIjMwlRlC1UlC9UT3OrsbGQAOMNLJEZMeImIyGRMWbZQvWShPnY2D2Z4mfASiQ8TXiIiMhpTrrRQX8lCfZjwEokXE14iIjKKXafzMe/HHIspWahPVQ1vBWt4iUSHCS8RERmMUiUgM/cWtlySYH/Gb0btS9eShfrIbTnDSyRWTHiJiEhvtZcsyAzeV2NLFuojlzHhJRIrJrxERKQXS1tlobGqanjLK5VG64OIzIMJLxERNZglr7LQWHxojUi8mPASEVGDmGJG183RFs/38DXJjG516oSXD60RiQ4TXiIiqpWpZnRNVbZQF/VOa5zhJRIdJrxERKTBlNv9mrpsoS4saSASLya8RESkZuyyBWOvtNAYD5c0CIIAicQyxkVEjceEl4ioiTNF2YIllCzURy57sJSaIACVKgG2MsscJxHpjgkvEVETY8rtfi2pZKE+VTO8wIOyBluZtI7WRGRNmPASETUhpihZGBbcCkV//o7IfmEIb+9hsTO61VVPeB3lZhwMERkUE14iIpEzdcmCSlmJnTtzEGbB5QvayKQSyKQSKFUClyYjEhkmvEREImPukgWVFW9UZieT4p5KyZUaiESGCS8RkYiIbbtfU7OzkeKeQsm1eIlEhgkvEZGVE/N2v6bGtXiJxIkJLxGRlTFlyQJg3u1+Ta1qtzXW8BKJCxNeIiIrYqqSBUC8ZQt1kXOGl0iULGKRweXLl8Pf3x/29vYICwvDoUOHam37xBNPQCKR1Ph6+umn1W3GjBlT4/ygQYNMcStEREahVAn4ZM85TFx31OjJrreLPZJH9ULcM10Q3q5Fk0l2AZY0EImV2Wd4N23ahNjYWCQnJyMsLAxJSUmIiopCTk4OPDw8arTfsmULKir+2te9sLAQQUFBeOmllzTaDRo0CKtWrVK/lsu5oCIRWY/qZQsbMi8jv6TcKH1Z8na/pqae4VVa8VITRFSD2RPexYsXY/z48Rg7diwAIDk5GTt27MDKlSsxY8aMGu3d3Nw0Xm/cuBEODg41El65XA4vLy/jDZyIyEi40oL5VM3wlis4w0skJmZNeCsqKpCVlYWZM2eqj0mlUkRERCAjI6NB1/jqq68wYsQIODo6ahxPT0+Hh4cHXF1d0b9/f3zwwQdo0aKF1muUl5ejvPyvmZPi4mIAgEKhgEKh0PW2dFbVhyn6EgvGTD+Mm+5MFTOlSsCRy7ex52wBVmdcMWpf3i5yvDe4I6K6eAIAVMpKg66da82fM9v/Jf5l5ab5/l/FmmNmLoyZfsQUN13uQSIIgmDEsdTp2rVr8PX1xcGDBxEeHq4+Pn36dOzfvx+ZmZl1vv/QoUMICwtDZmYmQkND1cerZn0DAgJw4cIFvPvuu3ByckJGRgZkMlmN68yePRsJCQk1jm/YsAEODg6NuEMiIu1UAnChWIJiBXDjHnCwQIo7FcaZZXWxFdDbU4WWzQBnW6CdswBO6Gr32VkpzhRJMbKdEo95mO3HIxE1QFlZGV5++WXcuXMHzs7OdbY1e0lDY3z11Vfo1q2bRrILACNGjFD/uVu3bujevTvatWuH9PR0DBgwoMZ1Zs6cidjYWPXr4uJi+Pn5ITIyst4AGoJCoUBqaioGDhwIW1tbo/cnBoyZfhg33RkjZrtO5yNxZzbyio1Tk1tlTHhrRHTyQEgbV5OWLFjz52x70XGcKSpAx85d8VSon8n6teaYmQtjph8xxa3qN/INYdaE193dHTKZDPn5+RrH8/Pz662/LS0txcaNGzFnzpx6+2nbti3c3d1x/vx5rQmvXC7X+lCbra2tST8Mpu5PDBgz/TBuumtszJri5hDW+Dmzt3vwY1EpSMwydmuMmbkxZvoRQ9x0Gb9ZE147OzsEBwcjLS0NQ4YMAQCoVCqkpaUhJiamzvdu3rwZ5eXlGDVqVL39/PnnnygsLIS3tzh3BiIiy2PKzSG4yoLhcOMJInEye0lDbGwsoqOjERISgtDQUCQlJaG0tFS9asPo0aPh6+uLxMREjfd99dVXGDJkSI0H0e7evYuEhAS88MIL8PLywoULFzB9+nS0b98eUVFRJrsvImq6TLk5xFsRgYjpH8gE10C4Di+ROJk94R0+fDhu3LiBWbNmIS8vDz169EBKSgo8PR88PXzlyhVIpZr7Y+Tk5ODAgQPYvXt3jevJZDKcOHECa9asQVFREXx8fBAZGYm5c+dyLV4iMhpTliwAllO2IDbcaY1InMye8AJATExMrSUM6enpNY516NABtS0u0axZM+zatcuQwyMi0urhJHfb8Wu4VVpR/5v0xLIF01DP8LKkgUhULCLhJSKyNtwcQpzUNbyc4SUSFSa8REQN1BRXWmhq1DutMeElEhUmvEREtVCqBGTm3kLWTQku7ruATUeucqUFkeNDa0TipHPCW1lZiQ8//BCvvfYaWrVqZYwxERGZnWbJggw4d8Eo/bBkwbJwWTIicdI54bWxscHChQsxevRoY4yHiMhsWLJAf83wKs08EiIyJL1KGvr374/9+/fD39/fwMMhIjIdbg5B1bGkgUic9Ep4Bw8ejBkzZuDkyZMIDg6Go6Ojxvlnn33WIIMjIjIWrrJA2si5LBmRKOmV8E6ePBkAsHjx4hrnJBIJlEr+KoiILA9LFqg+XJaMSJz0SnhVKn4jICLLZ8qSBQBwc7TF8z18OaNrxbgsGZE4NXpZsvv378Pe3t4QYyEiMhhTlSwALFsQE7mNDABneInERqrPm5RKJebOnQtfX184OTnh4sWLAIC4uDh89dVXBh0gEZEulCoBn+w5h4nrjho92fV2sUfyqF6Ie6YLwtu1YLIrAnxojUic9JrhnTdvHtasWYMFCxZg/Pjx6uNdu3ZFUlISxo0bZ7ABEhE1VMqp65j9/WnkFZcb5fpcaUH8WNJAJE56Jbxr167F559/jgEDBmDixInq40FBQcjOzjbY4IiI6mOKB9Ee91Jh/FOhCG/vwQRX5LjxBJE46ZXwXr16Fe3bt69xXKVSQaFQNHpQRES1MeWDaN4u9nhvcAcoL2chjLO5TQJLGojESa+Et3Pnzvj555/Rpk0bjePffvstevbsaZCBERFVZ+wH0bSVLKiUldh52SjdkQWSM+ElEiW9Et5Zs2YhOjoaV69ehUqlwpYtW5CTk4O1a9di+/bthh4jETVhpihZqGuVBRWXFW9S7LjxBJEo6ZXwPvfcc/jhhx8wZ84cODo6YtasWejVqxd++OEHDBw40NBjJKImxNQlC9wcgh5WVcOrVAlQqgSWsRCJhN7r8Pbr1w+pqamGHAsRNXHmKFlgQkMPq5rhBR6UNTSzk5lxNERkKHolvG3btsXhw4fRokULjeNFRUXo1auXel1eIqL6mGq737ciAhHTP5AJLtWJCS+ROOmV8F66dAlKZc3CtvLycly9erXRgyIi8TL1dr8sWyBd2EglkEgAQQDKlUoAtuYeEhEZgE4J7/fff6/+865du+Di4qJ+rVQqkZaWBn9/f4MNjojEhdv9kqWTSCSwk0lRXqniSg1EIqJTwjtkyBAAD74hREdHa5yztbWFv78/Fi1aZLDBEZE4KFUCPt17Hkv2/G70vjijS41lZ8OEl0hsdEp4VaoHf/kDAgJw+PBhuLu7G2VQRGTdqpctbMi8jPwSbvdL1kFuI0UJuDQZkZjoVcObm5tr6HEQkUiYqmyBJQtkLOrthTnDSyQaei9LlpaWhrS0NBQUFKhnfqusXLlSp2stX74cCxcuRF5eHoKCgrBs2TKEhoZqbbt69WqMHTtW45hcLsf9+3/9cBUEAfHx8fjiiy9QVFSEPn36YMWKFQgMDNRpXETUMKZaaQFgyQIZH7cXJhIfvRLehIQEzJkzByEhIfD29oZEov/syqZNmxAbG4vk5GSEhYUhKSkJUVFRyMnJgYeHh9b3ODs7IycnR/26ev8LFizA0qVLsWbNGgQEBCAuLg5RUVE4c+YM7O3t9R4rET1gypUWWLJAplaV8JYz4SUSDb0S3uTkZKxevRqvvvpqowewePFijB8/Xj1rm5ycjB07dmDlypWYMWOG1vdIJBJ4eXlpPScIApKSkvD+++/jueeeAwCsXbsWnp6e2LZtG0aMGNHoMRM1ZSxZILHjDC+R+OiV8FZUVKB3796N7ryiogJZWVmYOXOm+phUKkVERAQyMjJqfd/du3fRpk0bqFQq9OrVCx9++CG6dOkC4EF9cV5eHiIiItTtXVxcEBYWhoyMDK0Jb3l5OcrL/3qgpri4GACgUCigUCgafZ/1qerDFH2JBWOmH33jplQJOHL5NvacLcDqjCvGGJqat4sc7w3uiKgungAAlbISqprLfpsMP2u6s/aY2f7vH1hl5RUmuwdrj5k5MGb6EVPcdLkHvRLe119/HRs2bEBcXJw+b1e7efMmlEolPD09NY57enoiOztb63s6dOiAlStXonv37rhz5w4+/vhj9O7dG6dPn0arVq2Ql5envkb1a1adqy4xMREJCQk1ju/evRsODg763JpeuFWz7hgz/dQXN5UAXCiWoFgB3LgHHCyQ4k6FcWZZXWwF9PZUoWUzwNkWaOdcCuXlLOy8bJTu9MbPmu6sNWZ370gBSHHoyFFUXhJM2re1xsycGDP9iCFuZWVlDW6rV8J7//59fP7559izZw+6d+8OW1vNnWgWL16sz2UbJDw8HOHh4erXvXv3RqdOnfDZZ59h7ty5el1z5syZiI2NVb8uLi6Gn58fIiMj4ezs3Ogx10ehUCA1NRUDBw6sEUvSjjHTT0Pitut0PhJ3ZiOv2DjLiD1sypPtMPmJthZdssDPmu6sPWb/vZmFc8WF6NytO57q6WuSPq09ZubAmOlHTHGr+o18Q+iV8J44cQI9evQAAJw6dUrjnC4PsLm7u0MmkyE/P1/jeH5+fq01utXZ2tqiZ8+eOH/+PACo35efnw9v77+e4s7Pz1ePuTq5XA65XK712qb8MJi6PzFgzPRTPW6mXGUBsM6VFvhZ0521xkxu++BHo1KQmnz81hozc2LM9COGuOkyfr0S3n379unzthrs7OwQHByMtLQ09S5uKpUKaWlpiImJadA1lEolTp48iaeeegrAg00xvLy8kJaWpk5wi4uLkZmZiUmTJhlk3ETWTqkScORCoUlWWQC40gJZl78eWjNj8TgRGZTe6/AaSmxsLKKjoxESEoLQ0FAkJSWhtLRUvWrD6NGj4evri8TERADAnDlz8Nhjj6F9+/YoKirCwoULcfnyZbz++usAHswwT5s2DR988AECAwPVy5L5+Piok2qipuy3QgkSF/1kkpIFrrRA1khetfEEd1ojEg29Et4nn3yyztKFvXv3Nvhaw4cPx40bNzBr1izk5eWhR48eSElJUT90duXKFUilUnX727dvY/z48cjLy4OrqyuCg4Nx8OBBdO7cWd1m+vTpKC0txYQJE1BUVIS+ffsiJSWFa/BSk6ZUCfh03wWs/F0KwLjJrjWWLBBV4bJkROKjV8JbvRZWoVDg+PHjOHXqFKKjo3W+XkxMTK0lDOnp6RqvlyxZgiVLltR5PYlEgjlz5mDOnDk6j4VILKpvDrEh8zLyS8oBGH6mlSULJCZMeInER6+Et7aEc/bs2bh7926jBkREjcfNIYj0Z/e/koZyljQQiYZBa3hHjRqF0NBQfPzxx4a8LBE1gClXWmDJAokZZ3iJxMegCW9GRgbrZIlMpHrJgjFXWmDJAjUlTHiJxEevhHfo0KEarwVBwPXr13HkyJFG775GRPVjyQKR8TDhJRIfvRJeFxcXjddSqRQdOnTAnDlzEBkZaZCBEZEmliwQmYYdlyUjEh29Et5Vq1YZehxEVA1LFojMQ84ZXiLRaVQNb1ZWFs6ePQsA6NKlC3r27GmQQRE1daYqWQCAtyICEdM/kAku0f+wpIFIfPRKeAsKCjBixAikp6ejefPmAICioiI8+eST2LhxI1q2bGnIMRI1CaYsWQBYtkBUG3XCy5IGItHQK+H9xz/+gZKSEpw+fRqdOnUCAJw5cwbR0dGYMmUKvv76a4MOkkjsTDGj6+Usx7DgVij683dE9gtDeHsPzuoSaWEnkwEAyhVMeInEQq+ENyUlBXv27FEnuwDQuXNnLF++nA+tEelAqRLw6d7zWLLnd6P18fBKCyplJXbuzEEYa3SJalU1w8uNJ4jEQ6+EV6VSwdbWtsZxW1tbqFT8BkFUm9q3+zU8bSULKqVRuiISFdbwEomPXglv//79MXXqVHz99dfw8fEBAFy9ehVvvfUWBgwYYNABEomFscsWuNICkWGolyWr5L8QicRCr4T3008/xbPPPgt/f3/4+fkBAP744w907doV69atM+gAiayZKR5E4+YQRIYlt+VDa0Rio1fC6+fnh6NHj2LPnj3Izs4GAHTq1AkREREGHRyRtTHl2rlcZYHIOP6a4WXCSyQWOiW8e/fuRUxMDH799Vc4Oztj4MCBGDhwIADgzp076NKlC5KTk9GvXz+jDJbIkrFkgUgcuPEEkfjolPAmJSVh/PjxcHZ2rnHOxcUFb7zxBhYvXsyEl5oMliwQiQ8fWiMSH50S3t9++w3z58+v9XxkZCQ+/vjjRg+KyFKxZIFI/LjxBJH46JTw5ufna12OTH0xGxvcuHGj0YMiskTc7peoaaiq4VUoBahUAqT8e0hk9XRKeH19fXHq1Cm0b99e6/kTJ07A25uzUSQe3O6XqOmpmuEFHszy2ktlZhwNERmCTgnvU089hbi4OAwaNAj29vYa5+7du4f4+Hj8/e9/N+gAiUzt4SR32/FruFVaYbS++CAakeWpkfDaMuElsnY6Jbzvv/8+tmzZgkcffRQxMTHo0KEDACA7OxvLly+HUqnEe++9Z5SBEpmCqcoW+CAakeWqKmkA+OAakVjolPB6enri4MGDmDRpEmbOnAlBEAAAEokEUVFRWL58OTw9PY0yUCJjMWXZAksWiCyfRCKBnUyKCqWKCS+RSOi88USbNm2wc+dO3L59G+fPn4cgCAgMDISrq6sxxkdkcKZcaYElC0TWyc6GCS+RmOi10xoAuLq64m9/+5shx0JkdCxZIKKGsLORAuVcmoxILKT1NzG+5cuXw9/fH/b29ggLC8OhQ4dqbfvFF1+gX79+cHV1haurKyIiImq0HzNmDCQSicbXoEGDjH0bZKGUKgEZFwox54fTmLjuqFGTXW8XeySP6oW4Z7ogvF0LJrtEVorbCxOJi94zvIayadMmxMbGIjk5GWFhYUhKSkJUVBRycnLg4eFRo316ejpGjhyJ3r17w97eHvPnz0dkZCROnz4NX19fdbtBgwZh1apV6tdyudwk90Pmx5IFImqsqpUayiuVZh4JERmC2RPexYsXY/z48Rg7diwAIDk5GTt27MDKlSsxY8aMGu3Xr1+v8frLL7/Ef//7X6SlpWH06NHq43K5HF5eXsYdPFkcliwQkSH8lfByhpdIDMya8FZUVCArKwszZ85UH5NKpYiIiEBGRkaDrlFWVgaFQgE3NzeN4+np6fDw8ICrqyv69++PDz74AC1atNB6jfLycpSXl6tfFxcXAwAUCgUUCoWut6Wzqj5M0ZdYPBwzpUrAkcu3sedsAVZnXDFqv94ucrw3uCOiujxYjUSlrITKiiaA+FnTHWOmOzHEzPZ//5C9V86fA5aKMdOPmOKmyz1IhKq1xczg2rVr8PX1xcGDBxEeHq4+Pn36dOzfvx+ZmZn1XmPy5MnYtWsXTp8+rd4MY+PGjXBwcEBAQAAuXLiAd999F05OTsjIyIBMVnMB8dmzZyMhIaHG8Q0bNsDBwaERd0jGoBKAC8USFCuAG/eAgwVS3Kkw3iyro42AEHcB3dwEtHMWwAldIvFbfFKGy3cleL2DEt3czPZjkojqUFZWhpdffhl37tyBs7NznW3NXtLQGB999BE2btyI9PR0jZ3fRowYof5zt27d0L17d7Rr1w7p6ekYMGBAjevMnDkTsbGx6tfFxcXw8/NDZGRkvQE0BIVCgdTUVAwcOBC2trZG78+a7Tqdj8Sd2cgrLq+/cSONCW+NiE4eCGnjKpqyBX7WdMeY6U4MMVt3/TAu372NbkE98VQ345fHiSFmpsaY6UdMcav6jXxDmDXhdXd3h0wmQ35+vsbx/Pz8eutvP/74Y3z00UfYs2cPunfvXmfbtm3bwt3dHefPn9ea8Mrlcq0Ptdna2pr0w2Dq/qyJUiXg073nsWTP70bvqylsDsHPmu4YM91Zc8yqthNWQsKfAxaOMdOPGOKmy/jNmvDa2dkhODgYaWlpGDJkCABApVIhLS0NMTExtb5vwYIFmDdvHnbt2oWQkJB6+/nzzz9RWFgIb2/xJjBiU32lhQ2Zl5FfYpxZXa60QETVyW24LBmRmJi9pCE2NhbR0dEICQlBaGgokpKSUFpaql61YfTo0fD19UViYiIAYP78+Zg1axY2bNgAf39/5OXlAQCcnJzg5OSEu3fvIiEhAS+88AK8vLxw4cIFTJ8+He3bt0dUVJTZ7pMajistEJG5Va3SwI0niMTB7Anv8OHDcePGDcyaNQt5eXno0aMHUlJS4On54Cn4K1euQCr9a3+MFStWoKKiAi+++KLGdeLj4zF79mzIZDKcOHECa9asQVFREXx8fBAZGYm5c+dyLV4LVjWjm3omDyt/uWTUvppCyQIRNQ43niASF7MnvAAQExNTawlDenq6xutLly7Vea1mzZph165dBhoZGQs3hyAiS8Z1eInExSISXmpaWLJARJbOjjW8RKLChJdMgiULRGRN7P63ZjtreInEgQkvGQVLFojImnGGl0hcmPCSwZmqZAEQMOXJ9pg6sAMTXCIyKCa8ROLChJcMwpQlCwDg7SLHYM8y/KN/Oya7RGRwXIeXSFyY8JLeHk5ytx2/hlulFUbrq3rZQs9Wj2BXyo9G64+Imjb1smSs4SUSBSa8pBdzr7SgUCiM2i8RNW0saSASFya81GBcaYGImoq/1uFVmnkkRGQITHipQYw9o8uVFojIklSVNHDjCSJxYMJLtTLFjC43hyAiS8SSBiJxYcJLaqZcO5clC0RkydQJLx9aIxIFJrwEgCULREQP4wwvkbgw4W3CWLJARKSdXMaEl0hMmPA2ISxZICJqGJY0EIkLE94mwhTr5ro52uL5Hr6c0SUiqye3kQHgDC+RWDDhFTFTrZvLsgUiEhvW8BKJCxNekTHldr8sWyAisWLCSyQuTHhFxNzb/RIRiYV6pzXW8BKJAhNeK8ftfomIDM/uoVUaBEGARMJ/3BNZMya8VsaUKy1w7VwiaqqqZngBQKEUYGfD731E1owJrxVhyQIRkWnIH0p4K5QqjQSYiKwPE14Lx5IFIiLTqyppAP734JrcjIMhokZjwmthWLJARGR+UqkENlIJKlUCV2ogEgEmvBZk1+l8zPsxhyULREQWwM5GisoKJRNeIhFgwmtmSpWAzNxb2HJJgv0Zvxm1L5YsEBE1jFIloGphhkO5hfBysUfW5dsoKLkPj0fsEdzGVe/X7o5yQALcvFuuPnco9xaybkrQIvcWQtu2NGpfhrq2ufuqVFYaJWbmvC9T9F3bZ83QfVnapJpFJLzLly/HwoULkZeXh6CgICxbtgyhoaG1tt+8eTPi4uJw6dIlBAYGYv78+XjqqafU5wVBQHx8PL744gsUFRWhT58+WLFiBQIDA01xOw2m+RCazCh9cLtfIiLdVH1vLi1XAgDe/vYEpv/3BFTCX22kEjTq9cP+OifD2nNHGn3thvVlnNem78s4MTPnfZmmb+1xM2RfljbJZvbHTjdt2oTY2FjEx8fj6NGjCAoKQlRUFAoKCrS2P3jwIEaOHIlx48bh2LFjGDJkCIYMGYJTp06p2yxYsABLly5FcnIyMjMz4ejoiKioKNy/b9xSAV2knLqOSeuOGq18YVwff3w9/jEcfm8g4p7pgvB2LZjsEhHVo7bvzdWTgsa+Nua12Zd19yWW+8y7cx+T1h1FyqnrtXdiQmZPeBcvXozx48dj7Nix6Ny5M5KTk+Hg4ICVK1dqbf/JJ59g0KBBeOedd9CpUyfMnTsXvXr1wqeffgrgwexuUlIS3n//fTz33HPo3r071q5di2vXrmHbtm0mvLPaKVUCEn44gzo+Z3rzdrFH8qheTHKJiHRkzO/NRE1N1d+jhB/OQFlXZm0iZi1pqKioQFZWFmbOnKk+JpVKERERgYyMDK3vycjIQGxsrMaxqKgodTKbm5uLvLw8REREqM+7uLggLCwMGRkZGDFiRI1rlpeXo7y8XP26uLgYAKBQKKBQKPS+v9pk5t4y2Myul7MdhgX7wd/dAR6PyBHSxhUyqcQo47YkVfcn9vs0NMZNd4yZ7qw1Zob83kxED5Le63fuI+N8AcIC3Ax+fV2+x5g14b158yaUSiU8PT01jnt6eiI7O1vre/Ly8rS2z8vLU5+vOlZbm+oSExORkJBQ4/ju3bvh4ODQsJvRQdZNCfSv2X3wr6THvQR0cxPQzrkM0vs5wJ9AIYBdZw01SuuQmppq7iFYJcZNd4yZ7qwtZo373kxEtdn9cyYKzxp+lresrKzBbS3ioTVzmzlzpsascXFxMfz8/BAZGQlnZ2eD99ci9xbWnjui13u9Xezx3uCOiOriWX9jEVMoFEhNTcXAgQNha2tr7uFYDcZNd4yZ7qw1Zo353kxEtYvsF2aUGd6q38g3hFkTXnd3d8hkMuTn52scz8/Ph5eXl9b3eHl51dm+6v/z8/Ph7e2t0aZHjx5arymXyyGX19xGx9bW1ijfrMPbe8DbxR55d+7XWyvGzSHqZqz/RmLHuOmOMdOdtcVMl+/NRFQ/CQAvF3uEt/cwSu6iy/cXsz60Zmdnh+DgYKSlpamPqVQqpKWlITw8XOt7wsPDNdoDD35tVtU+ICAAXl5eGm2Ki4uRmZlZ6zVNTSaVIP6ZzgAefBi0qVpl4ZcZAzA14lE818OXD6ERERlRQ743E1HDVP0din+ms0XkLmZfpSE2NhZffPEF1qxZg7Nnz2LSpEkoLS3F2LFjAQCjR4/WeKht6tSpSElJwaJFi5CdnY3Zs2fjyJEjiImJAQBIJBJMmzYNH3zwAb7//nucPHkSo0ePho+PD4YMGWKOW9RqUFdvrBjVC14u9hrHucoCEZH51Pa9ufq34sa+Nua12Zd19yWW+/RysceKUb0sZh1es9fwDh8+HDdu3MCsWbOQl5eHHj16ICUlRf3Q2ZUrVyCV/pWX9+7dGxs2bMD777+Pd999F4GBgdi2bRu6du2qbjN9+nSUlpZiwoQJKCoqQt++fZGSkgJ7e/sa/ZvToK7eGNjZCxnnC7D750xE9gsz2rQ/ERE1TNX35kO5t0y2e9ahizfUPwe401rDd1pL++WQwWMm+p3WavmsiX2nNYkgCCxVqubOnTto3rw5/vjjD6M8tFadQqHA7t27ERkZaVX1bubEmOmHcdMdY6Y7xkx3jJnuGDP9iCluVYsMFBUVwcXFpc62Zp/htUQlJSUAAD8/PzOPhIiIiIjqUlJSUm/CyxleLVQqFa5du4ZHHnkEEonxp+Or/oViqhllMWDM9MO46Y4x0x1jpjvGTHeMmX7EFDdBEFBSUgIfHx+N8ldtOMOrhVQqRatWrUzer7Ozs9V/+EyNMdMP46Y7xkx3jJnuGDPdMWb6EUvc6pvZrWL2VRqIiIiIiIyJCS8RERERiRoTXgsgl8sRHx+vdbc30o4x0w/jpjvGTHeMme4YM90xZvppqnHjQ2tEREREJGqc4SUiIiIiUWPCS0RERESixoSXiIiIiESNCS8RERERiRoTXguwfPly+Pv7w97eHmFhYTh06JC5h2QxEhMT8be//Q2PPPIIPDw8MGTIEOTk5Gi0uX//Pt588020aNECTk5OeOGFF5Cfn2+mEVuejz76CBKJBNOmTVMfY8xqunr1KkaNGoUWLVqgWbNm6NatG44cOaI+LwgCZs2aBW9vbzRr1gwRERE4d+6cGUdsXkqlEnFxcQgICECzZs3Qrl07zJ07Fw8/B82YAT/99BOeeeYZ+Pj4QCKRYNu2bRrnGxKjW7du4ZVXXoGzszOaN2+OcePG4e7duya8C9OqK2YKhQL/+te/0K1bNzg6OsLHxwejR4/GtWvXNK7BmG2rte3EiRMhkUiQlJSkcVzsMWPCa2abNm1CbGws4uPjcfToUQQFBSEqKgoFBQXmHppF2L9/P9588038+uuvSE1NhUKhQGRkJEpLS9Vt3nrrLfzwww/YvHkz9u/fj2vXrmHo0KFmHLXlOHz4MD777DN0795d4zhjpun27dvo06cPbG1t8eOPP+LMmTNYtGgRXF1d1W0WLFiApUuXIjk5GZmZmXB0dERUVBTu379vxpGbz/z587FixQp8+umnOHv2LObPn48FCxZg2bJl6jaMGVBaWoqgoCAsX75c6/mGxOiVV17B6dOnkZqaiu3bt+Onn37ChAkTTHULJldXzMrKynD06FHExcXh6NGj2LJlC3JycvDss89qtGPMtNu6dSt+/fVX+Pj41Dgn+pgJZFahoaHCm2++qX6tVCoFHx8fITEx0YyjslwFBQUCAGH//v2CIAhCUVGRYGtrK2zevFnd5uzZswIAISMjw1zDtAglJSVCYGCgkJqaKjz++OPC1KlTBUFgzLT517/+JfTt27fW8yqVSvDy8hIWLlyoPlZUVCTI5XLh66+/NsUQLc7TTz8tvPbaaxrHhg4dKrzyyiuCIDBm2gAQtm7dqn7dkBidOXNGACAcPnxY3ebHH38UJBKJcPXqVZON3Vyqx0ybQ4cOCQCEy5cvC4LAmNUWsz///FPw9fUVTp06JbRp00ZYsmSJ+lxTiBlneM2ooqICWVlZiIiIUB+TSqWIiIhARkaGGUdmue7cuQMAcHNzAwBkZWVBoVBoxLBjx45o3bp1k4/hm2++iaefflojNgBjps3333+PkJAQvPTSS/Dw8EDPnj3xxRdfqM/n5uYiLy9PI2YuLi4ICwtrsjHr3bs30tLS8PvvvwMAfvvtNxw4cACDBw8GwJg1RENilJGRgebNmyMkJETdJiIiAlKpFJmZmSYfsyW6c+cOJBIJmjdvDoAx00alUuHVV1/FO++8gy5dutQ43xRiZmPuATRlN2/ehFKphKenp8ZxT09PZGdnm2lUlkulUmHatGno06cPunbtCgDIy8uDnZ2d+htdFU9PT+Tl5ZlhlJZh48aNOHr0KA4fPlzjHGNW08WLF7FixQrExsbi3XffxeHDhzFlyhTY2dkhOjpaHRdtf1ebasxmzJiB4uJidOzYETKZDEqlEvPmzcMrr7wCAIxZAzQkRnl5efDw8NA4b2NjAzc3N8YRD55H+Ne//oWRI0fC2dkZAGOmzfz582FjY4MpU6ZoPd8UYsaEl6zGm2++iVOnTuHAgQPmHopF++OPPzB16lSkpqbC3t7e3MOxCiqVCiEhIfjwww8BAD179sSpU6eQnJyM6OhoM4/OMn3zzTdYv349NmzYgC5duuD48eOYNm0afHx8GDMyCYVCgWHDhkEQBKxYscLcw7FYWVlZ+OSTT3D06FFIJBJzD8dsWNJgRu7u7pDJZDWejs/Pz4eXl5eZRmWZYmJisH37duzbtw+tWrVSH/fy8kJFRQWKioo02jflGGZlZaGgoAC9evWCjY0NbGxssH//fixduhQ2Njbw9PRkzKrx9vZG586dNY516tQJV65cAQB1XPh39S/vvPMOZsyYgREjRqBbt2549dVX8dZbbyExMREAY9YQDYmRl5dXjYeYKysrcevWrSYdx6pk9/Lly0hNTVXP7gKMWXU///wzCgoK0Lp1a/XPhMuXL+Of//wn/P39ATSNmDHhNSM7OzsEBwcjLS1NfUylUiEtLQ3h4eFmHJnlEAQBMTEx2Lp1K/bu3YuAgACN88HBwbC1tdWIYU5ODq5cudJkYzhgwACcPHkSx48fV3+FhITglVdeUf+ZMdPUp0+fGsvd/f7772jTpg0AICAgAF5eXhoxKy4uRmZmZpONWVlZGaRSzR8hMpkMKpUKAGPWEA2JUXh4OIqKipCVlaVus3fvXqhUKoSFhZl8zJagKtk9d+4c9uzZgxYtWmicZ8w0vfrqqzhx4oTGzwQfHx+888472LVrF4AmEjNzPzXX1G3cuFGQy+XC6tWrhTNnzggTJkwQmjdvLuTl5Zl7aBZh0qRJgouLi5Ceni5cv35d/VVWVqZuM3HiRKF169bC3r17hSNHjgjh4eFCeHi4GUdteR5epUEQGLPqDh06JNjY2Ajz5s0Tzp07J6xfv15wcHAQ1q1bp27z0UcfCc2bNxe+++474cSJE8Jzzz0nBAQECPfu3TPjyM0nOjpa8PX1FbZv3y7k5uYKW7ZsEdzd3YXp06er2zBmD1ZLOXbsmHDs2DEBgLB48WLh2LFj6hUFGhKjQYMGCT179hQyMzOFAwcOCIGBgcLIkSPNdUtGV1fMKioqhGeffVZo1aqVcPz4cY2fC+Xl5eprMGaan7Pqqq/SIAjijxkTXguwbNkyoXXr1oKdnZ0QGhoq/Prrr+YeksUAoPVr1apV6jb37t0TJk+eLLi6ugoODg7C888/L1y/ft18g7ZA1RNexqymH374Qejatasgl8uFjh07Cp9//rnGeZVKJcTFxQmenp6CXC4XBgwYIOTk5JhptOZXXFwsTJ06VWjdurVgb28vtG3bVnjvvfc0kg7GTBD27dun9XtYdHS0IAgNi1FhYaEwcuRIwcnJSXB2dhbGjh0rlJSUmOFuTKOumOXm5tb6c2Hfvn3qazBmmp+z6rQlvGKPmUQQHtoWh4iIiIhIZFjDS0RERESixoSXiIiIiESNCS8RERERiRoTXiIiIiISNSa8RERERCRqTHiJiIiISNSY8BIRERGRqDHhJSIiIiJRY8JLRERERKLGhJeIyEKNGTMGEomkxtf58+fNPTQiIqtiY+4BEBFR7QYNGoRVq1ZpHGvZsqXG64qKCtjZ2ZlyWEREVoUzvEREFkwul8PLy0vja8CAAYiJicG0adPg7u6OqKgoAMDixYvRrVs3ODo6ws/PD5MnT8bdu3fV11q9ejWaN2+O7du3o0OHDnBwcMCLL76IsrIyrFmzBv7+/nB1dcWUKVOgVCrV7ysvL8fbb78NX19fODo6IiwsDOnp6aYOBRGR3jjDS0RkhdasWYNJkybhl19+UR+TSqVYunQpAgICcPHiRUyePBnTp0/Hv//9b3WbsrIyLF26FBs3bkRJSQmGDh2K559/Hs2bN8fOnTtx8eJFvPDCC+jTpw+GDx8OAIiJicGZM2ewceNG+Pj4YOvWrRg0aBBOnjyJwMBAk987EZGuJIIgCOYeBBER1TRmzBisW7cO9vb26mODBw/GjRs3UFxcjKNHj9b5/m+//RYTJ07EzZs3ATyY4R07dizOnz+Pdu3aAQAmTpyI//znP8jPz4eTkxOAB2UU/v7+SE5OxpUrV9C2bVtcuXIFPj4+6mtHREQgNDQUH374oaFvm4jI4DjDS0RkwZ588kmsWLFC/drR0REjR45EcHBwjbZ79uxBYmIisrOzUVxcjMrKSty/fx9lZWVwcHAAADg4OKiTXQDw9PSEv7+/OtmtOlZQUAAAOHnyJJRKJR599FGNvsrLy9GiRQuD3isRkbEw4SUismCOjo5o37691uMPu3TpEv7+979j0qRJmDdvHtzc3HDgwAGMGzcOFRUV6oTX1tZW430SiUTrMZVKBQC4e/cuZDIZsrKyIJPJNNo9nCQTEVkyJrxERCKQlZUFlUqFRYsWQSp98DzyN9980+jr9uzZE0qlEgUFBejXr1+jr0dEZA5cpYGISATat28PhUKBZcuW4eLFi/jPf/6D5OTkRl/30UcfxSuvvILRo0djy5YtyM3NxaFDh5CYmIgdO3YYYORERMbHhJeISASCgoKwePFizJ8/H127dsX69euRmJhokGuvWrUKo0ePxj//+U906NABQ4YMweHDh9G6dWuDXJ+IyNi4SgMRERERiRpneImIiIhI1JjwEhEREZGoMeElIiIiIlFjwktEREREosaEl4iIiIhEjQkvEREREYkaE14iIiIiEjUmvEREREQkakx4iYiIiEjUmPASERERkagx4SUiIiIiUft/KtWiUZlCdo0AAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Path to the input .skels file\n",
        "file_path = \"/content/drive/MyDrive/Sign-IDD SLT/data/Inference/dev.pred.skels\"\n",
        "\n",
        "# Read all lines\n",
        "with open(file_path, 'r') as f:\n",
        "    lines = f.readlines()\n",
        "\n",
        "fixed_lines = []\n",
        "\n",
        "for i, line in enumerate(lines):\n",
        "    values = np.fromstring(line.strip(), sep=' ')\n",
        "    num_frames = len(values) // 151\n",
        "\n",
        "    # Sanity check\n",
        "    if len(values) % 151 != 0:\n",
        "        raise ValueError(f\"Line {i+1}: Unexpected length {len(values)} for 151-D features.\")\n",
        "\n",
        "    # Reshape into [frames × 151]\n",
        "    data = values.reshape(num_frames, 151)\n",
        "\n",
        "    # Extract features (first 150 columns)\n",
        "    features = data[:, :150]\n",
        "\n",
        "    # FIX: Replace last column with evenly spaced values from 0.0 to 1.0\n",
        "    correct_counters = np.linspace(0.0, 1.0, num_frames).reshape(num_frames, 1)\n",
        "\n",
        "    # Concatenate fixed features and counter column\n",
        "    fixed_data = np.concatenate([features, correct_counters], axis=1)\n",
        "\n",
        "    # Flatten and stringify\n",
        "    flat_line = ' '.join(map(str, fixed_data.flatten()))\n",
        "    fixed_lines.append(flat_line)\n",
        "\n",
        "# Save to new file\n",
        "fixed_path = file_path.replace(\".skels\", \".fixed.skels\")\n",
        "with open(fixed_path, 'w') as f:\n",
        "    for line in fixed_lines:\n",
        "        f.write(line + '\\n')\n",
        "\n",
        "print(f\"✅ Fixed skeletons saved to:\\n{fixed_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fy1QGifUVSbu",
        "outputId": "36599d19-c478-4e0c-ee21-ba37638bad58"
      },
      "id": "Fy1QGifUVSbu",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Fixed skeletons saved to:\n",
            "/content/drive/MyDrive/Sign-IDD SLT/data/Inference/dev.pred.fixed.skels\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Path to your file\n",
        "file_path = \"/content/drive/MyDrive/Sign-IDD SLT/data/Inference/dev.pred.fixed.skels\"\n",
        "\n",
        "# Read and parse the file\n",
        "with open(file_path, 'r') as f:\n",
        "    lines = f.readlines()\n",
        "\n",
        "# Assume 151 values per frame\n",
        "frame_dim = 151\n",
        "feature_dim = 150\n",
        "\n",
        "# Parse and visualize each line\n",
        "for i, line in enumerate(lines):\n",
        "    values = np.fromstring(line.strip(), sep=' ')\n",
        "    num_frames = len(values) // frame_dim\n",
        "    data = values.reshape(num_frames, frame_dim)\n",
        "\n",
        "    features = data[:, :feature_dim]\n",
        "    counters = data[:, -1]\n",
        "\n",
        "\n",
        "    print(f\"\\nLine {i+1} - {num_frames} frames\")\n",
        "    print(\"Frame\\tCounter\\tSample Features\")\n",
        "    j=0;\n",
        "    for f in range(num_frames):\n",
        "        print(f\"{f+1}\\t{counters[f]}\\t{features[f, :5]} ...\")\n",
        "\n",
        "    # Optional: Plot counter values across frames\n",
        "    plt.figure(figsize=(8, 2))\n",
        "    plt.plot(counters, marker='o')\n",
        "    plt.title(f\"Line {i+1} - Frame Counters\")\n",
        "    plt.xlabel(\"Frame\")\n",
        "    plt.ylabel(\"Counter\")\n",
        "    plt.grid(True)\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "wElbokzuVaWu",
        "outputId": "712da8bb-1f53-4666-b7ec-bbb5f5903972"
      },
      "id": "wElbokzuVaWu",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Line 1 - 147 frames\n",
            "Frame\tCounter\tSample Features\n",
            "1\t0.0\t[-0.031581 -0.199573 -0.029702 -0.004307 -0.00124 ] ...\n",
            "2\t0.00684931506849315\t[-0.032786 -0.200893 -0.026416 -0.00424  -0.001233] ...\n",
            "3\t0.0136986301369863\t[-0.034456 -0.202972 -0.017222 -0.004627 -0.001256] ...\n",
            "4\t0.02054794520547945\t[-0.034795 -0.201999 -0.011731 -0.005019 -0.000769] ...\n",
            "5\t0.0273972602739726\t[-2.53270e-02 -2.02674e-01 -1.40000e-03 -4.80400e-03 -6.60000e-05] ...\n",
            "6\t0.03424657534246575\t[-0.050608 -0.203272 -0.009485 -0.004561 -0.000373] ...\n",
            "7\t0.0410958904109589\t[-0.047572 -0.206092 -0.006808 -0.005187 -0.000955] ...\n",
            "8\t0.04794520547945205\t[-0.05118  -0.205186 -0.041507 -0.004965 -0.001523] ...\n",
            "9\t0.0547945205479452\t[-0.057365 -0.20251  -0.0678   -0.004891 -0.001797] ...\n",
            "10\t0.06164383561643835\t[-0.05453  -0.199635 -0.078505 -0.005192 -0.001448] ...\n",
            "11\t0.0684931506849315\t[-0.05455  -0.194831 -0.081472 -0.004991 -0.000677] ...\n",
            "12\t0.07534246575342465\t[-0.049295 -0.189491 -0.091973 -0.005768 -0.000655] ...\n",
            "13\t0.0821917808219178\t[-0.041835 -0.181679 -0.105262 -0.004915 -0.000772] ...\n",
            "14\t0.08904109589041095\t[-0.03011  -0.17338  -0.120211 -0.004553 -0.000588] ...\n",
            "15\t0.0958904109589041\t[-0.02771  -0.167802 -0.127675 -0.004125 -0.000896] ...\n",
            "16\t0.10273972602739725\t[-0.023559 -0.165914 -0.124401 -0.003848 -0.000575] ...\n",
            "17\t0.1095890410958904\t[-0.018937 -0.176483 -0.109341 -0.003104  0.00022 ] ...\n",
            "18\t0.11643835616438356\t[-1.58090e-02 -1.86394e-01 -9.38420e-02 -3.02800e-03  1.70000e-04] ...\n",
            "19\t0.1232876712328767\t[-0.003976 -0.198267 -0.068373 -0.003278  0.000402] ...\n",
            "20\t0.13013698630136986\t[-0.000342 -0.208168 -0.027544 -0.003531  0.00037 ] ...\n",
            "21\t0.136986301369863\t[ 8.64700e-03 -2.10316e-01  9.77000e-04 -2.85200e-03 -5.70000e-05] ...\n",
            "22\t0.14383561643835616\t[ 0.019077 -0.205468 -0.012057 -0.002691 -0.000327] ...\n",
            "23\t0.1506849315068493\t[ 0.020204 -0.208123 -0.003772 -0.003367 -0.000532] ...\n",
            "24\t0.15753424657534246\t[ 2.74180e-02 -2.08028e-01 -5.25300e-03 -3.78200e-03  1.16000e-04] ...\n",
            "25\t0.1643835616438356\t[ 0.03685  -0.211762  0.0009   -0.004308 -0.000377] ...\n",
            "26\t0.17123287671232876\t[ 0.046798 -0.217047  0.001858 -0.004333 -0.000556] ...\n",
            "27\t0.1780821917808219\t[ 0.05123  -0.21455   0.004077 -0.003768 -0.000704] ...\n",
            "28\t0.18493150684931506\t[ 0.059904 -0.209249 -0.00281  -0.004164 -0.001191] ...\n",
            "29\t0.1917808219178082\t[ 0.067622 -0.205336 -0.022613 -0.003337 -0.001292] ...\n",
            "30\t0.19863013698630136\t[ 0.066699 -0.203892  0.01106  -0.00377  -0.000867] ...\n",
            "31\t0.2054794520547945\t[ 0.063365 -0.196526  0.00039  -0.004509 -0.000773] ...\n",
            "32\t0.21232876712328766\t[ 0.066177 -0.195473 -0.001817 -0.003015 -0.001043] ...\n",
            "33\t0.2191780821917808\t[ 0.06602  -0.196776 -0.007093 -0.002988 -0.000637] ...\n",
            "34\t0.22602739726027396\t[ 0.058694 -0.198473  0.001025 -0.00378  -0.000431] ...\n",
            "35\t0.2328767123287671\t[ 0.058783 -0.198957 -0.002665 -0.004039 -0.000511] ...\n",
            "36\t0.23972602739726026\t[ 0.051471 -0.19939  -0.006402 -0.003978 -0.000582] ...\n",
            "37\t0.2465753424657534\t[ 0.041743 -0.202173  0.000594 -0.004049 -0.00079 ] ...\n",
            "38\t0.2534246575342466\t[ 0.033728 -0.204465  0.003499 -0.003554 -0.000806] ...\n",
            "39\t0.2602739726027397\t[ 0.033111 -0.204165  0.001949 -0.002738 -0.000719] ...\n",
            "40\t0.26712328767123283\t[ 0.03447  -0.205241  0.001061 -0.002956 -0.000925] ...\n",
            "41\t0.273972602739726\t[ 0.037082 -0.20423  -0.004987 -0.003424 -0.000694] ...\n",
            "42\t0.2808219178082192\t[ 0.034743 -0.207421 -0.003967 -0.003049 -0.001277] ...\n",
            "43\t0.2876712328767123\t[ 0.040145 -0.205535 -0.010439 -0.004184 -0.001013] ...\n",
            "44\t0.29452054794520544\t[ 0.035628 -0.199659 -0.050426 -0.00335  -0.00092 ] ...\n",
            "45\t0.3013698630136986\t[ 0.041588 -0.196751 -0.074294 -0.003632 -0.00062 ] ...\n",
            "46\t0.3082191780821918\t[ 0.0501   -0.19529  -0.078705 -0.004313 -0.000326] ...\n",
            "47\t0.3150684931506849\t[ 0.060412 -0.196712 -0.051033 -0.004844 -0.000581] ...\n",
            "48\t0.32191780821917804\t[ 0.071448 -0.194393  0.003344 -0.003141 -0.000376] ...\n",
            "49\t0.3287671232876712\t[ 0.070468 -0.193774  0.006364 -0.003778 -0.001004] ...\n",
            "50\t0.3356164383561644\t[ 0.073143 -0.194007  0.002088 -0.004673 -0.000854] ...\n",
            "51\t0.3424657534246575\t[ 0.06956  -0.196198  0.001338 -0.004259 -0.000997] ...\n",
            "52\t0.34931506849315064\t[ 6.81820e-02 -1.96035e-01  2.00000e-06 -4.55800e-03 -1.11900e-03] ...\n",
            "53\t0.3561643835616438\t[ 0.068863 -0.193166 -0.015163 -0.003209 -0.001125] ...\n",
            "54\t0.363013698630137\t[ 6.75800e-02 -1.92161e-01 -4.26680e-02 -2.19900e-03  8.60000e-05] ...\n",
            "55\t0.3698630136986301\t[ 6.7830e-02 -1.9404e-01 -4.9108e-02 -2.7910e-03 -1.5700e-04] ...\n",
            "56\t0.37671232876712324\t[ 0.065961 -0.1916   -0.054702 -0.002571  0.000372] ...\n",
            "57\t0.3835616438356164\t[ 0.068509 -0.196308 -0.009205 -0.004027 -0.0002  ] ...\n",
            "58\t0.3904109589041096\t[ 0.066023 -0.192885 -0.004671 -0.003217 -0.00086 ] ...\n",
            "59\t0.3972602739726027\t[ 0.055585 -0.187691 -0.021354 -0.003426 -0.000817] ...\n",
            "60\t0.40410958904109584\t[ 0.061068 -0.190464 -0.018701 -0.004029 -0.000366] ...\n",
            "61\t0.410958904109589\t[ 0.059862 -0.190037 -0.038963 -0.003417  0.000518] ...\n",
            "62\t0.4178082191780822\t[ 0.055618 -0.189737 -0.050487 -0.002984  0.000571] ...\n",
            "63\t0.4246575342465753\t[ 0.050479 -0.189728 -0.051226 -0.003621 -0.000513] ...\n",
            "64\t0.43150684931506844\t[ 0.048696 -0.195647 -0.019344 -0.00387  -0.000507] ...\n",
            "65\t0.4383561643835616\t[ 0.039396 -0.198965 -0.020258 -0.003191 -0.000478] ...\n",
            "66\t0.4452054794520548\t[ 0.040785 -0.201376 -0.010131 -0.003205 -0.000661] ...\n",
            "67\t0.4520547945205479\t[ 3.16890e-02 -2.12235e-01  3.40000e-05 -4.52500e-03 -8.91000e-04] ...\n",
            "68\t0.45890410958904104\t[ 0.016933 -0.215127 -0.003422 -0.003394 -0.000584] ...\n",
            "69\t0.4657534246575342\t[ 0.008538 -0.217136  0.000317 -0.004137 -0.000704] ...\n",
            "70\t0.4726027397260274\t[-0.002395 -0.215036 -0.007704 -0.003988 -0.000686] ...\n",
            "71\t0.4794520547945205\t[-0.013337 -0.211648 -0.008691 -0.00347  -0.00024 ] ...\n",
            "72\t0.48630136986301364\t[-0.020298 -0.206818 -0.012169 -0.003159 -0.000564] ...\n",
            "73\t0.4931506849315068\t[-0.023536 -0.20246  -0.002392 -0.00363  -0.001003] ...\n",
            "74\t0.5\t[-0.02289  -0.203198 -0.003293 -0.003278 -0.000616] ...\n",
            "75\t0.5068493150684932\t[-0.035583 -0.201385 -0.005321 -0.004229 -0.001   ] ...\n",
            "76\t0.5136986301369862\t[-0.044523 -0.199059 -0.006703 -0.004235 -0.000949] ...\n",
            "77\t0.5205479452054794\t[-0.048919 -0.196252 -0.007909 -0.004224 -0.001032] ...\n",
            "78\t0.5273972602739726\t[-0.056481 -0.192858 -0.005079 -0.004289 -0.001097] ...\n",
            "79\t0.5342465753424657\t[-0.061051 -0.191907  0.001847 -0.004672 -0.000822] ...\n",
            "80\t0.5410958904109588\t[-0.063918 -0.19206  -0.001916 -0.004719 -0.000792] ...\n",
            "81\t0.547945205479452\t[-0.063006 -0.193023 -0.008451 -0.004146 -0.000755] ...\n",
            "82\t0.5547945205479452\t[-0.06373  -0.188676 -0.027892 -0.004093 -0.001157] ...\n",
            "83\t0.5616438356164384\t[-0.065002 -0.187728 -0.030975 -0.00387  -0.001231] ...\n",
            "84\t0.5684931506849314\t[-0.066195 -0.192131 -0.007211 -0.003999 -0.000751] ...\n",
            "85\t0.5753424657534246\t[-0.061573 -0.192317 -0.003493 -0.00382  -0.00057 ] ...\n",
            "86\t0.5821917808219178\t[-0.065317 -0.195069  0.007198 -0.003692 -0.00068 ] ...\n",
            "87\t0.5890410958904109\t[-0.060736 -0.187257 -0.023932 -0.003589 -0.000954] ...\n",
            "88\t0.595890410958904\t[-0.063973 -0.190218 -0.017833 -0.003156 -0.000994] ...\n",
            "89\t0.6027397260273972\t[-0.068013 -0.19176  -0.008654 -0.003274 -0.001034] ...\n",
            "90\t0.6095890410958904\t[-0.070278 -0.191655 -0.007653 -0.002572 -0.000795] ...\n",
            "91\t0.6164383561643836\t[-0.065349 -0.190966 -0.004312 -0.002511 -0.000595] ...\n",
            "92\t0.6232876712328766\t[-0.065599 -0.191842 -0.009177 -0.002633 -0.000607] ...\n",
            "93\t0.6301369863013698\t[-0.065673 -0.189756 -0.022641 -0.002928 -0.000646] ...\n",
            "94\t0.636986301369863\t[-0.064158 -0.184314 -0.05462  -0.003408 -0.000835] ...\n",
            "95\t0.6438356164383561\t[-0.065467 -0.182597 -0.061582 -0.003718 -0.00096 ] ...\n",
            "96\t0.6506849315068493\t[-0.065325 -0.178689 -0.081102 -0.003457 -0.001105] ...\n",
            "97\t0.6575342465753424\t[-0.067087 -0.173931 -0.090762 -0.003499 -0.001318] ...\n",
            "98\t0.6643835616438356\t[-0.068009 -0.169897 -0.095433 -0.003481 -0.000765] ...\n",
            "99\t0.6712328767123288\t[-0.07433  -0.168223 -0.097611 -0.003441 -0.000943] ...\n",
            "100\t0.6780821917808219\t[-0.074943 -0.167228 -0.101816 -0.003614 -0.001032] ...\n",
            "101\t0.684931506849315\t[-0.075496 -0.166968 -0.097963 -0.003415 -0.000847] ...\n",
            "102\t0.6917808219178082\t[-0.072536 -0.167356 -0.095603 -0.003413 -0.001048] ...\n",
            "103\t0.6986301369863013\t[-0.07459  -0.16936  -0.094184 -0.004632 -0.001359] ...\n",
            "104\t0.7054794520547945\t[-0.083996 -0.173602 -0.081096 -0.004896 -0.000716] ...\n",
            "105\t0.7123287671232876\t[-0.087768 -0.170049 -0.086215 -0.003552 -0.000439] ...\n",
            "106\t0.7191780821917808\t[-0.087518 -0.169009 -0.089978 -0.003647 -0.000256] ...\n",
            "107\t0.726027397260274\t[-0.093022 -0.168619 -0.084808 -0.00411  -0.000831] ...\n",
            "108\t0.732876712328767\t[-0.092054 -0.170048 -0.081454 -0.004136 -0.000838] ...\n",
            "109\t0.7397260273972602\t[-0.088444 -0.167398 -0.092426 -0.003923 -0.000749] ...\n",
            "110\t0.7465753424657534\t[-0.082777 -0.163617 -0.103125 -0.003814 -0.000812] ...\n",
            "111\t0.7534246575342465\t[-0.078449 -0.160586 -0.106155 -0.003388 -0.000551] ...\n",
            "112\t0.7602739726027397\t[-0.074579 -0.160249 -0.106714 -0.003323 -0.00023 ] ...\n",
            "113\t0.7671232876712328\t[-0.074851 -0.161604 -0.108305 -0.003481  0.000276] ...\n",
            "114\t0.773972602739726\t[-7.31360e-02 -1.65127e-01 -1.02936e-01 -3.82900e-03  4.00000e-06] ...\n",
            "115\t0.7808219178082192\t[-0.074281 -0.162969 -0.102707 -0.003115 -0.000225] ...\n",
            "116\t0.7876712328767123\t[-0.072808 -0.160618 -0.108682 -0.002838 -0.000534] ...\n",
            "117\t0.7945205479452054\t[-0.068225 -0.155852 -0.116696 -0.003402 -0.000575] ...\n",
            "118\t0.8013698630136986\t[-0.070948 -0.148494 -0.132668 -0.003632 -0.000359] ...\n",
            "119\t0.8082191780821917\t[-0.059639 -0.152038 -0.13391  -0.003744 -0.000466] ...\n",
            "120\t0.8150684931506849\t[-0.056471 -0.1616   -0.117115 -0.004117 -0.00077 ] ...\n",
            "121\t0.821917808219178\t[-0.049425 -0.171538 -0.102264 -0.004356 -0.00076 ] ...\n",
            "122\t0.8287671232876712\t[-0.044106 -0.182527 -0.085055 -0.004475 -0.000488] ...\n",
            "123\t0.8356164383561644\t[-0.036385 -0.189755 -0.061957 -0.004384 -0.00049 ] ...\n",
            "124\t0.8424657534246575\t[-0.022376 -0.194777 -0.040982 -0.00412  -0.000766] ...\n",
            "125\t0.8493150684931506\t[-0.000197 -0.190161 -0.043996 -0.004066 -0.001074] ...\n",
            "126\t0.8561643835616438\t[ 0.002423 -0.192001 -0.04039  -0.003955 -0.000997] ...\n",
            "127\t0.8630136986301369\t[ 0.00796  -0.185516 -0.05183  -0.003708 -0.001002] ...\n",
            "128\t0.8698630136986301\t[ 0.008319 -0.184961 -0.053571 -0.00383  -0.001069] ...\n",
            "129\t0.8767123287671232\t[ 0.007817 -0.185924 -0.049968 -0.003747 -0.001035] ...\n",
            "130\t0.8835616438356164\t[-0.009892 -0.188624 -0.048563 -0.003942 -0.000993] ...\n",
            "131\t0.8904109589041096\t[-0.004419 -0.18775  -0.050382 -0.003903 -0.000973] ...\n",
            "132\t0.8972602739726027\t[-0.005304 -0.197982 -0.02274  -0.003817 -0.000358] ...\n",
            "133\t0.9041095890410958\t[-3.67100e-03 -2.04622e-01 -2.35100e-03 -3.58200e-03 -4.00000e-05] ...\n",
            "134\t0.910958904109589\t[ 0.000391 -0.206147  0.006524 -0.002978 -0.000282] ...\n",
            "135\t0.9178082191780821\t[ 0.002572 -0.192338 -0.026386 -0.003922 -0.000246] ...\n",
            "136\t0.9246575342465753\t[ 0.006612 -0.19235  -0.025701 -0.004475 -0.000621] ...\n",
            "137\t0.9315068493150684\t[ 0.005209 -0.205773 -0.009169 -0.004523 -0.00042 ] ...\n",
            "138\t0.9383561643835616\t[ 2.41720e-02 -2.04244e-01  1.34200e-03 -4.80900e-03  2.20000e-05] ...\n",
            "139\t0.9452054794520548\t[ 0.018863 -0.201283 -0.010036 -0.003737 -0.000314] ...\n",
            "140\t0.9520547945205479\t[ 2.03610e-02 -2.09076e-01  7.25600e-03 -4.65600e-03  1.95000e-04] ...\n",
            "141\t0.958904109589041\t[ 0.00679  -0.206248 -0.012933 -0.004745 -0.000263] ...\n",
            "142\t0.9657534246575342\t[-0.001626 -0.201949 -0.003187 -0.004538 -0.001011] ...\n",
            "143\t0.9726027397260273\t[-0.00564  -0.198023 -0.016628 -0.004695 -0.001212] ...\n",
            "144\t0.9794520547945205\t[-0.018509 -0.199951 -0.006609 -0.004819 -0.001337] ...\n",
            "145\t0.9863013698630136\t[-0.020913 -0.201393  0.006216 -0.005015 -0.000718] ...\n",
            "146\t0.9931506849315068\t[-0.024073 -0.195148 -0.012931 -0.004937 -0.00089 ] ...\n",
            "147\t1.0\t[-0.017992 -0.194596 -0.016204 -0.004365 -0.000786] ...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x200 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Line 2 - 147 frames\n",
            "Frame\tCounter\tSample Features\n",
            "1\t0.0\t[-0.032609 -0.188946 -0.054769 -0.004272 -0.001088] ...\n",
            "2\t0.00684931506849315\t[-0.035367 -0.187685 -0.055947 -0.004053 -0.00112 ] ...\n",
            "3\t0.0136986301369863\t[-0.03827  -0.195151 -0.034729 -0.004775 -0.00099 ] ...\n",
            "4\t0.02054794520547945\t[-0.020617 -0.188635 -0.036708 -0.00484  -0.00033 ] ...\n",
            "5\t0.0273972602739726\t[ 1.66300e-03 -1.94607e-01 -1.97960e-02 -3.55500e-03  5.60000e-05] ...\n",
            "6\t0.03424657534246575\t[-4.52740e-02 -1.96264e-01 -2.75250e-02 -3.70800e-03 -1.42000e-04] ...\n",
            "7\t0.0410958904109589\t[-0.035867 -0.202266  0.003346 -0.00456  -0.000334] ...\n",
            "8\t0.04794520547945205\t[-0.031988 -0.204386 -0.024812 -0.004357 -0.001039] ...\n",
            "9\t0.0547945205479452\t[-0.044259 -0.201177 -0.056624 -0.004246 -0.001572] ...\n",
            "10\t0.06164383561643835\t[-0.038514 -0.201272 -0.057069 -0.00469  -0.001092] ...\n",
            "11\t0.0684931506849315\t[-3.71110e-02 -2.00686e-01 -5.54070e-02 -4.84900e-03 -1.26000e-04] ...\n",
            "12\t0.07534246575342465\t[-4.04650e-02 -2.02243e-01 -6.05030e-02 -5.72700e-03 -1.69000e-04] ...\n",
            "13\t0.0821917808219178\t[-0.025464 -0.197467 -0.061675 -0.004616 -0.000537] ...\n",
            "14\t0.08904109589041095\t[-0.020797 -0.183116 -0.106056 -0.00469  -0.000422] ...\n",
            "15\t0.0958904109589041\t[-0.007331 -0.190798 -0.070065 -0.005101 -0.000901] ...\n",
            "16\t0.10273972602739725\t[-0.01121  -0.188192 -0.068978 -0.004826 -0.000688] ...\n",
            "17\t0.1095890410958904\t[-1.46750e-02 -1.95569e-01 -5.83550e-02 -4.52400e-03 -1.10000e-04] ...\n",
            "18\t0.11643835616438356\t[-1.46070e-02 -2.00753e-01 -4.93450e-02 -4.47500e-03 -1.60000e-04] ...\n",
            "19\t0.1232876712328767\t[-7.74500e-03 -2.05961e-01 -4.49960e-02 -4.07700e-03 -2.00000e-05] ...\n",
            "20\t0.13013698630136986\t[-2.98700e-03 -2.14051e-01 -1.21260e-02 -4.18800e-03  6.50000e-05] ...\n",
            "21\t0.136986301369863\t[ 5.99800e-03 -2.17736e-01  1.36130e-02 -4.20300e-03 -1.84000e-04] ...\n",
            "22\t0.14383561643835616\t[ 0.020414 -0.208159 -0.016985 -0.003099 -0.000336] ...\n",
            "23\t0.1506849315068493\t[ 0.02067  -0.210628 -0.003529 -0.003436 -0.000501] ...\n",
            "24\t0.15753424657534246\t[ 2.49620e-02 -2.07337e-01 -3.43800e-03 -4.17700e-03 -6.10000e-05] ...\n",
            "25\t0.1643835616438356\t[ 0.026615 -0.209116 -0.000244 -0.004678 -0.00055 ] ...\n",
            "26\t0.17123287671232876\t[ 0.033584 -0.213944  0.004997 -0.004926 -0.000603] ...\n",
            "27\t0.1780821917808219\t[ 0.022916 -0.212136  0.00654  -0.004396 -0.000751] ...\n",
            "28\t0.18493150684931506\t[ 0.025045 -0.209695  0.003353 -0.004678 -0.000985] ...\n",
            "29\t0.1917808219178082\t[ 0.025672 -0.209466 -0.002117 -0.004487 -0.001006] ...\n",
            "30\t0.19863013698630136\t[ 0.03361  -0.20913   0.011143 -0.004205 -0.000772] ...\n",
            "31\t0.2054794520547945\t[ 0.068503 -0.203054  0.005152 -0.004501 -0.00064 ] ...\n",
            "32\t0.21232876712328766\t[ 0.04844  -0.205279  0.00775  -0.00368  -0.000785] ...\n",
            "33\t0.2191780821917808\t[ 0.04128  -0.2062    0.003717 -0.003741 -0.000659] ...\n",
            "34\t0.22602739726027396\t[ 0.037706 -0.204126  0.005646 -0.004066 -0.000593] ...\n",
            "35\t0.2328767123287671\t[ 0.039746 -0.205071  0.005276 -0.004321 -0.000623] ...\n",
            "36\t0.23972602739726026\t[ 0.045824 -0.203134 -0.000694 -0.004115 -0.00062 ] ...\n",
            "37\t0.2465753424657534\t[ 0.034469 -0.204997  0.004129 -0.00412  -0.000809] ...\n",
            "38\t0.2534246575342466\t[ 0.022966 -0.207554  0.005883 -0.003988 -0.000964] ...\n",
            "39\t0.2602739726027397\t[ 0.023996 -0.20742   0.00348  -0.003045 -0.000923] ...\n",
            "40\t0.26712328767123283\t[ 0.024053 -0.211018  0.004832 -0.003601 -0.001041] ...\n",
            "41\t0.273972602739726\t[ 0.025817 -0.211275 -0.000312 -0.004086 -0.000964] ...\n",
            "42\t0.2808219178082192\t[ 0.023094 -0.214799  0.002696 -0.003992 -0.00127 ] ...\n",
            "43\t0.2876712328767123\t[ 0.024224 -0.215141 -0.000676 -0.004661 -0.00115 ] ...\n",
            "44\t0.29452054794520544\t[ 0.023232 -0.209508 -0.024687 -0.004074 -0.000913] ...\n",
            "45\t0.3013698630136986\t[ 0.032734 -0.200764 -0.059101 -0.003758 -0.000627] ...\n",
            "46\t0.3082191780821918\t[ 0.037256 -0.201146 -0.05524  -0.004302 -0.000348] ...\n",
            "47\t0.3150684931506849\t[ 0.051093 -0.199668 -0.043943 -0.004867 -0.00053 ] ...\n",
            "48\t0.32191780821917804\t[ 6.66990e-02 -1.97772e-01  8.35000e-04 -3.29600e-03 -9.80000e-05] ...\n",
            "49\t0.3287671232876712\t[ 0.062457 -0.197835  0.006714 -0.003702 -0.000573] ...\n",
            "50\t0.3356164383561644\t[ 0.068104 -0.197756  0.003461 -0.004211 -0.000541] ...\n",
            "51\t0.3424657534246575\t[ 0.063763 -0.200778  0.006682 -0.003721 -0.000637] ...\n",
            "52\t0.34931506849315064\t[ 0.05791  -0.20298   0.006449 -0.003939 -0.000757] ...\n",
            "53\t0.3561643835616438\t[ 0.061399 -0.198253 -0.010743 -0.003007 -0.000905] ...\n",
            "54\t0.363013698630137\t[ 0.055594 -0.199183 -0.024253 -0.002128 -0.000203] ...\n",
            "55\t0.3698630136986301\t[ 0.053627 -0.198808 -0.040899 -0.003336 -0.000247] ...\n",
            "56\t0.37671232876712324\t[ 0.057603 -0.193053 -0.06131  -0.003117  0.000456] ...\n",
            "57\t0.3835616438356164\t[ 0.069076 -0.195744 -0.025318 -0.004404 -0.000291] ...\n",
            "58\t0.3904109589041096\t[ 0.063615 -0.190513 -0.030703 -0.003494 -0.000505] ...\n",
            "59\t0.3972602739726027\t[ 0.054057 -0.185882 -0.050674 -0.003642 -0.000543] ...\n",
            "60\t0.40410958904109584\t[ 6.03250e-02 -1.88563e-01 -5.05130e-02 -4.04500e-03 -1.32000e-04] ...\n",
            "61\t0.410958904109589\t[ 0.051528 -0.18587  -0.056796 -0.003478  0.000403] ...\n",
            "62\t0.4178082191780822\t[ 0.047218 -0.185207 -0.075533 -0.003277  0.000434] ...\n",
            "63\t0.4246575342465753\t[ 0.038533 -0.184695 -0.073933 -0.003576 -0.000633] ...\n",
            "64\t0.43150684931506844\t[ 0.040577 -0.188488 -0.044328 -0.004137 -0.000562] ...\n",
            "65\t0.4383561643835616\t[ 0.032784 -0.1927   -0.041511 -0.003141 -0.000407] ...\n",
            "66\t0.4452054794520548\t[ 0.0427   -0.197839 -0.021197 -0.002992 -0.000639] ...\n",
            "67\t0.4520547945205479\t[ 0.033361 -0.207221 -0.014209 -0.003951 -0.000623] ...\n",
            "68\t0.45890410958904104\t[ 0.015734 -0.207843 -0.030154 -0.003328 -0.000232] ...\n",
            "69\t0.4657534246575342\t[ 0.013878 -0.211227 -0.020979 -0.003753 -0.000441] ...\n",
            "70\t0.4726027397260274\t[ 0.009096 -0.204522 -0.04175  -0.003316 -0.000539] ...\n",
            "71\t0.4794520547945205\t[-0.009735 -0.201375 -0.036535 -0.004401 -0.000351] ...\n",
            "72\t0.48630136986301364\t[-0.015052 -0.19946  -0.035317 -0.003262 -0.000563] ...\n",
            "73\t0.4931506849315068\t[-0.018214 -0.1979   -0.017797 -0.003472 -0.000846] ...\n",
            "74\t0.5\t[-0.021287 -0.19859  -0.015648 -0.003021 -0.000414] ...\n",
            "75\t0.5068493150684932\t[-0.034486 -0.198247 -0.013944 -0.00399  -0.00076 ] ...\n",
            "76\t0.5136986301369862\t[-0.03759  -0.195842 -0.015776 -0.004054 -0.000768] ...\n",
            "77\t0.5205479452054794\t[-0.037702 -0.193563 -0.023435 -0.00421  -0.000777] ...\n",
            "78\t0.5273972602739726\t[-0.045048 -0.192252 -0.017634 -0.004311 -0.001076] ...\n",
            "79\t0.5342465753424657\t[-0.049296 -0.188837 -0.009762 -0.004682 -0.000748] ...\n",
            "80\t0.5410958904109588\t[-0.054637 -0.188199 -0.007677 -0.004721 -0.000661] ...\n",
            "81\t0.547945205479452\t[-0.051738 -0.189837 -0.012125 -0.004238 -0.00055 ] ...\n",
            "82\t0.5547945205479452\t[-0.050214 -0.183623 -0.04306  -0.004074 -0.001066] ...\n",
            "83\t0.5616438356164384\t[-0.046458 -0.182715 -0.048829 -0.003939 -0.001194] ...\n",
            "84\t0.5684931506849314\t[-0.050446 -0.187927 -0.020624 -0.003838 -0.000656] ...\n",
            "85\t0.5753424657534246\t[-0.043648 -0.188365 -0.017951 -0.003566 -0.000412] ...\n",
            "86\t0.5821917808219178\t[-0.042384 -0.189149 -0.022681 -0.003403 -0.000599] ...\n",
            "87\t0.5890410958904109\t[-0.029112 -0.180077 -0.061168 -0.003547 -0.000746] ...\n",
            "88\t0.595890410958904\t[-0.047858 -0.186741 -0.040419 -0.003175 -0.000952] ...\n",
            "89\t0.6027397260273972\t[-0.056595 -0.189626 -0.029561 -0.00326  -0.001026] ...\n",
            "90\t0.6095890410958904\t[-0.05547  -0.189633 -0.031741 -0.002685 -0.000814] ...\n",
            "91\t0.6164383561643836\t[-0.050416 -0.188175 -0.025009 -0.002697 -0.000352] ...\n",
            "92\t0.6232876712328766\t[-0.050723 -0.187924 -0.029341 -0.002907 -0.000412] ...\n",
            "93\t0.6301369863013698\t[-0.055251 -0.18415  -0.047264 -0.003038 -0.000814] ...\n",
            "94\t0.636986301369863\t[-0.056581 -0.17647  -0.082543 -0.003612 -0.001066] ...\n",
            "95\t0.6438356164383561\t[-0.058953 -0.172353 -0.093617 -0.004181 -0.000975] ...\n",
            "96\t0.6506849315068493\t[-0.056378 -0.170228 -0.100848 -0.003703 -0.000939] ...\n",
            "97\t0.6575342465753424\t[-0.055532 -0.167942 -0.105668 -0.003733 -0.001366] ...\n",
            "98\t0.6643835616438356\t[-0.056429 -0.165506 -0.105563 -0.003788 -0.000653] ...\n",
            "99\t0.6712328767123288\t[-0.060882 -0.164544 -0.108543 -0.003637 -0.000739] ...\n",
            "100\t0.6780821917808219\t[-0.062631 -0.164991 -0.110396 -0.003845 -0.000937] ...\n",
            "101\t0.684931506849315\t[-0.05995  -0.165654 -0.108618 -0.003689 -0.000721] ...\n",
            "102\t0.6917808219178082\t[-0.050472 -0.167145 -0.1068   -0.003847 -0.000995] ...\n",
            "103\t0.6986301369863013\t[-0.058716 -0.169125 -0.103778 -0.004697 -0.001197] ...\n",
            "104\t0.7054794520547945\t[-0.054626 -0.173661 -0.08305  -0.004759 -0.000615] ...\n",
            "105\t0.7123287671232876\t[-0.062795 -0.167615 -0.093789 -0.003807 -0.000437] ...\n",
            "106\t0.7191780821917808\t[-6.83220e-02 -1.67299e-01 -9.16590e-02 -4.17800e-03 -1.38000e-04] ...\n",
            "107\t0.726027397260274\t[-0.062691 -0.169318 -0.088109 -0.004494 -0.000627] ...\n",
            "108\t0.732876712328767\t[-0.067026 -0.170556 -0.084475 -0.004849 -0.000571] ...\n",
            "109\t0.7397260273972602\t[-0.058425 -0.17083  -0.093521 -0.004763 -0.000507] ...\n",
            "110\t0.7465753424657534\t[-0.06591  -0.162733 -0.108073 -0.004805 -0.000518] ...\n",
            "111\t0.7534246575342465\t[-0.054111 -0.163319 -0.109361 -0.004268 -0.000543] ...\n",
            "112\t0.7602739726027397\t[-0.052808 -0.163453 -0.110477 -0.004177 -0.000276] ...\n",
            "113\t0.7671232876712328\t[-5.48800e-02 -1.65097e-01 -1.11875e-01 -3.90100e-03 -1.70000e-05] ...\n",
            "114\t0.773972602739726\t[-0.05072  -0.169637 -0.107198 -0.00432  -0.000279] ...\n",
            "115\t0.7808219178082192\t[-0.054188 -0.167867 -0.107774 -0.003789 -0.000452] ...\n",
            "116\t0.7876712328767123\t[-0.055744 -0.163512 -0.114103 -0.003482 -0.000805] ...\n",
            "117\t0.7945205479452054\t[-0.049269 -0.162105 -0.112835 -0.004175 -0.000855] ...\n",
            "118\t0.8013698630136986\t[-0.054967 -0.155685 -0.127158 -0.003712 -0.000598] ...\n",
            "119\t0.8082191780821917\t[-0.042388 -0.158347 -0.126352 -0.004381 -0.000611] ...\n",
            "120\t0.8150684931506849\t[-0.041489 -0.166547 -0.104773 -0.004788 -0.00085 ] ...\n",
            "121\t0.821917808219178\t[-0.031587 -0.177304 -0.080238 -0.004747 -0.000716] ...\n",
            "122\t0.8287671232876712\t[-0.026291 -0.184367 -0.074417 -0.005102 -0.000465] ...\n",
            "123\t0.8356164383561644\t[-0.023971 -0.186527 -0.068842 -0.005012 -0.000443] ...\n",
            "124\t0.8424657534246575\t[-0.012975 -0.192958 -0.043044 -0.004559 -0.000736] ...\n",
            "125\t0.8493150684931506\t[-0.002202 -0.192716 -0.035973 -0.004337 -0.001056] ...\n",
            "126\t0.8561643835616438\t[ 0.006482 -0.184369 -0.057133 -0.004137 -0.00122 ] ...\n",
            "127\t0.8630136986301369\t[ 0.006721 -0.184722 -0.055832 -0.003979 -0.001211] ...\n",
            "128\t0.8698630136986301\t[ 0.009034 -0.185668 -0.052956 -0.004129 -0.0012  ] ...\n",
            "129\t0.8767123287671232\t[ 0.010511 -0.18822  -0.046131 -0.004115 -0.001115] ...\n",
            "130\t0.8835616438356164\t[ 1.69000e-04 -1.94197e-01 -3.08820e-02 -4.23200e-03 -9.99000e-04] ...\n",
            "131\t0.8904109589041096\t[ 0.001226 -0.18965  -0.046654 -0.004153 -0.001046] ...\n",
            "132\t0.8972602739726027\t[ 0.003164 -0.200304 -0.018794 -0.004023 -0.000614] ...\n",
            "133\t0.9041095890410958\t[ 0.012469 -0.21026   0.015632 -0.00352  -0.000383] ...\n",
            "134\t0.910958904109589\t[ 0.015026 -0.206378  0.010181 -0.003165 -0.000715] ...\n",
            "135\t0.9178082191780821\t[ 0.01715  -0.198368 -0.012364 -0.003997 -0.000363] ...\n",
            "136\t0.9246575342465753\t[ 0.021412 -0.196429 -0.01885  -0.004873 -0.000704] ...\n",
            "137\t0.9315068493150684\t[ 0.023213 -0.202552 -0.006138 -0.004564 -0.000793] ...\n",
            "138\t0.9383561643835616\t[ 3.33900e-02 -2.07054e-01  6.47400e-03 -4.66600e-03 -1.44000e-04] ...\n",
            "139\t0.9452054794520548\t[ 2.46670e-02 -2.07806e-01  4.92000e-03 -4.20800e-03 -1.50000e-04] ...\n",
            "140\t0.9520547945205479\t[ 2.58210e-02 -2.08703e-01  3.97400e-03 -4.53600e-03 -1.15000e-04] ...\n",
            "141\t0.958904109589041\t[ 0.01626  -0.203727 -0.018947 -0.004283 -0.000419] ...\n",
            "142\t0.9657534246575342\t[ 0.015612 -0.199624 -0.010569 -0.004377 -0.001043] ...\n",
            "143\t0.9726027397260273\t[ 0.004926 -0.192517 -0.027482 -0.004588 -0.001239] ...\n",
            "144\t0.9794520547945205\t[-0.005759 -0.19706  -0.017941 -0.00497  -0.001329] ...\n",
            "145\t0.9863013698630136\t[-0.013199 -0.199333  0.000995 -0.004841 -0.000887] ...\n",
            "146\t0.9931506849315068\t[-0.016476 -0.194207 -0.016366 -0.004781 -0.001051] ...\n",
            "147\t1.0\t[-0.019439 -0.193954 -0.018017 -0.004176 -0.00119 ] ...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x200 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Line 3 - 147 frames\n",
            "Frame\tCounter\tSample Features\n",
            "1\t0.0\t[ 0.007941 -0.247535  0.001251 -0.004447 -0.000832] ...\n",
            "2\t0.00684931506849315\t[ 0.008222 -0.247318  0.001453 -0.004468 -0.000802] ...\n",
            "3\t0.0136986301369863\t[ 0.002614 -0.247587  0.00319  -0.00435  -0.000849] ...\n",
            "4\t0.02054794520547945\t[-0.008322 -0.249077 -0.003375 -0.004568 -0.001048] ...\n",
            "5\t0.0273972602739726\t[-0.036879 -0.243428 -0.019184 -0.004079 -0.000463] ...\n",
            "6\t0.03424657534246575\t[-1.41940e-02 -2.42873e-01 -1.18220e-02 -3.79500e-03  2.18000e-04] ...\n",
            "7\t0.0410958904109589\t[-2.11740e-02 -2.47461e-01 -2.13180e-02 -3.41200e-03 -7.60000e-05] ...\n",
            "8\t0.04794520547945205\t[-0.0237   -0.258071  0.002807 -0.003551 -0.000266] ...\n",
            "9\t0.0547945205479452\t[-0.034829 -0.257775 -0.009732 -0.003921 -0.000382] ...\n",
            "10\t0.06164383561643835\t[-0.044841 -0.254883 -0.017537 -0.004212 -0.000498] ...\n",
            "11\t0.0684931506849315\t[-0.050554 -0.252442 -0.008706 -0.003822 -0.000598] ...\n",
            "12\t0.07534246575342465\t[-0.050665 -0.24888  -0.001187 -0.003738 -0.000601] ...\n",
            "13\t0.0821917808219178\t[-0.052344 -0.243671 -0.002786 -0.003911 -0.000626] ...\n",
            "14\t0.08904109589041095\t[-0.060356 -0.241728 -0.00066  -0.004008 -0.000776] ...\n",
            "15\t0.0958904109589041\t[-0.065579 -0.241405  0.000533 -0.003827 -0.00082 ] ...\n",
            "16\t0.10273972602739725\t[-0.06904  -0.239945 -0.001489 -0.003618 -0.00075 ] ...\n",
            "17\t0.1095890410958904\t[-0.074288 -0.240232 -0.006177 -0.003403 -0.000648] ...\n",
            "18\t0.11643835616438356\t[-0.080318 -0.24234  -0.01641  -0.004115 -0.000563] ...\n",
            "19\t0.1232876712328767\t[-0.088352 -0.237233 -0.052573 -0.004293 -0.000483] ...\n",
            "20\t0.13013698630136986\t[-0.095019 -0.231107 -0.079463 -0.004296 -0.000501] ...\n",
            "21\t0.136986301369863\t[-0.097936 -0.227705 -0.091376 -0.004311 -0.000571] ...\n",
            "22\t0.14383561643835616\t[-0.095148 -0.228006 -0.080616 -0.004268 -0.000591] ...\n",
            "23\t0.1506849315068493\t[-0.095194 -0.229644 -0.048849 -0.00404  -0.000574] ...\n",
            "24\t0.15753424657534246\t[-0.099058 -0.232229 -0.015923 -0.003827 -0.000282] ...\n",
            "25\t0.1643835616438356\t[-0.098846 -0.23059  -0.011519 -0.003879 -0.000317] ...\n",
            "26\t0.17123287671232876\t[-0.100456 -0.231497 -0.009262 -0.003747 -0.000335] ...\n",
            "27\t0.1780821917808219\t[-1.01491e-01 -2.34336e-01 -9.23700e-03 -3.80600e-03 -1.50000e-04] ...\n",
            "28\t0.18493150684931506\t[-1.02783e-01 -2.34119e-01 -2.16180e-02 -3.95800e-03 -2.10000e-04] ...\n",
            "29\t0.1917808219178082\t[-0.100245 -0.233485 -0.031682 -0.004386 -0.00031 ] ...\n",
            "30\t0.19863013698630136\t[-0.098877 -0.229319 -0.029939 -0.004325 -0.000528] ...\n",
            "31\t0.2054794520547945\t[-0.096792 -0.227593 -0.029584 -0.004701 -0.000469] ...\n",
            "32\t0.21232876712328766\t[-0.095097 -0.228404 -0.004074 -0.004783 -0.000503] ...\n",
            "33\t0.2191780821917808\t[-0.090994 -0.22898   0.003868 -0.004673 -0.000554] ...\n",
            "34\t0.22602739726027396\t[-0.087702 -0.228053 -0.008481 -0.004716 -0.000599] ...\n",
            "35\t0.2328767123287671\t[-0.084831 -0.229641  0.001584 -0.004701 -0.000748] ...\n",
            "36\t0.23972602739726026\t[-0.077055 -0.230047 -0.002044 -0.004463 -0.000827] ...\n",
            "37\t0.2465753424657534\t[-0.06652  -0.230858 -0.012569 -0.003179 -0.001065] ...\n",
            "38\t0.2534246575342466\t[-0.065103 -0.231414 -0.006298 -0.003657 -0.001569] ...\n",
            "39\t0.2602739726027397\t[-0.064671 -0.232965 -0.012197 -0.002895 -0.000631] ...\n",
            "40\t0.26712328767123283\t[-0.067299 -0.232441 -0.017576 -0.004314 -0.000664] ...\n",
            "41\t0.273972602739726\t[-0.068178 -0.230041 -0.016659 -0.003696 -0.000953] ...\n",
            "42\t0.2808219178082192\t[-0.061101 -0.234221  0.004462 -0.003154 -0.000728] ...\n",
            "43\t0.2876712328767123\t[-0.059788 -0.232241  0.002246 -0.003345 -0.00118 ] ...\n",
            "44\t0.29452054794520544\t[-0.059344 -0.233507  0.003016 -0.003132 -0.000968] ...\n",
            "45\t0.3013698630136986\t[-0.060108 -0.232637  0.003817 -0.003384 -0.000827] ...\n",
            "46\t0.3082191780821918\t[-0.066727 -0.233001 -0.004282 -0.003661 -0.00071 ] ...\n",
            "47\t0.3150684931506849\t[-0.072913 -0.227276 -0.015103 -0.004039 -0.000651] ...\n",
            "48\t0.32191780821917804\t[-0.079778 -0.21937  -0.05359  -0.004477 -0.000578] ...\n",
            "49\t0.3287671232876712\t[-0.089497 -0.212873 -0.07322  -0.004664 -0.000595] ...\n",
            "50\t0.3356164383561644\t[-0.093208 -0.206783 -0.087208 -0.003735 -0.000721] ...\n",
            "51\t0.3424657534246575\t[-0.099985 -0.203351 -0.088728 -0.003753 -0.000646] ...\n",
            "52\t0.34931506849315064\t[-0.108359 -0.203078 -0.089377 -0.003524 -0.000617] ...\n",
            "53\t0.3561643835616438\t[-0.10815  -0.206942 -0.089183 -0.003285 -0.000587] ...\n",
            "54\t0.363013698630137\t[-0.113647 -0.199991 -0.098916 -0.003721 -0.000246] ...\n",
            "55\t0.3698630136986301\t[-0.102766 -0.191636 -0.105658 -0.003314 -0.000753] ...\n",
            "56\t0.37671232876712324\t[-0.102811 -0.184803 -0.115324 -0.00348  -0.000546] ...\n",
            "57\t0.3835616438356164\t[-0.105184 -0.177995 -0.124844 -0.003412 -0.000655] ...\n",
            "58\t0.3904109589041096\t[-0.107046 -0.173572 -0.128072 -0.003442 -0.000611] ...\n",
            "59\t0.3972602739726027\t[-0.107999 -0.172947 -0.129713 -0.003678 -0.000553] ...\n",
            "60\t0.40410958904109584\t[-0.107679 -0.174066 -0.130048 -0.00371  -0.000484] ...\n",
            "61\t0.410958904109589\t[-0.101529 -0.17478  -0.132193 -0.003819 -0.000386] ...\n",
            "62\t0.4178082191780822\t[-0.099326 -0.174138 -0.134048 -0.00382  -0.000586] ...\n",
            "63\t0.4246575342465753\t[-0.101169 -0.173302 -0.132348 -0.00351  -0.000991] ...\n",
            "64\t0.43150684931506844\t[-0.1048   -0.172398 -0.130897 -0.003419 -0.000803] ...\n",
            "65\t0.4383561643835616\t[-0.101577 -0.172353 -0.133992 -0.003562 -0.000718] ...\n",
            "66\t0.4452054794520548\t[-0.099801 -0.174985 -0.133965 -0.00362  -0.0007  ] ...\n",
            "67\t0.4520547945205479\t[-0.091621 -0.186808 -0.12378  -0.003547 -0.000722] ...\n",
            "68\t0.45890410958904104\t[-0.082524 -0.201205 -0.105054 -0.003359 -0.000631] ...\n",
            "69\t0.4657534246575342\t[-7.13430e-02 -2.14761e-01 -8.27570e-02 -4.13600e-03  2.20000e-05] ...\n",
            "70\t0.4726027397260274\t[-6.25910e-02 -2.24029e-01 -6.45870e-02 -4.25200e-03  3.20000e-05] ...\n",
            "71\t0.4794520547945205\t[-5.61830e-02 -2.30356e-01 -4.87930e-02 -4.44400e-03 -4.00000e-06] ...\n",
            "72\t0.48630136986301364\t[-6.79370e-02 -2.29869e-01 -4.51820e-02 -4.87500e-03 -1.61000e-04] ...\n",
            "73\t0.4931506849315068\t[-0.087493 -0.225591 -0.044406 -0.005487 -0.00053 ] ...\n",
            "74\t0.5\t[-0.117045 -0.208661 -0.091329 -0.004756 -0.000445] ...\n",
            "75\t0.5068493150684932\t[-0.118243 -0.221399 -0.074108 -0.004889 -0.000644] ...\n",
            "76\t0.5136986301369862\t[-0.118091 -0.224396 -0.080551 -0.004767 -0.000416] ...\n",
            "77\t0.5205479452054794\t[-0.102617 -0.216846 -0.090138 -0.004828 -0.000451] ...\n",
            "78\t0.5273972602739726\t[-0.11265  -0.219955 -0.08332  -0.005481 -0.000645] ...\n",
            "79\t0.5342465753424657\t[-0.113875 -0.219946 -0.093015 -0.004777 -0.000534] ...\n",
            "80\t0.5410958904109588\t[-0.105365 -0.211373 -0.106786 -0.005151 -0.00058 ] ...\n",
            "81\t0.547945205479452\t[-0.128875 -0.213622 -0.117803 -0.00433  -0.000879] ...\n",
            "82\t0.5547945205479452\t[-0.097876 -0.212688 -0.101124 -0.004485 -0.000704] ...\n",
            "83\t0.5616438356164384\t[-0.099154 -0.202943 -0.120846 -0.004751 -0.000824] ...\n",
            "84\t0.5684931506849314\t[-0.099332 -0.21007  -0.100927 -0.004614 -0.000632] ...\n",
            "85\t0.5753424657534246\t[-0.087728 -0.210266 -0.096197 -0.00455  -0.000411] ...\n",
            "86\t0.5821917808219178\t[-0.104375 -0.203179 -0.116624 -0.00445  -0.000747] ...\n",
            "87\t0.5890410958904109\t[-0.094297 -0.202423 -0.123967 -0.004941 -0.000699] ...\n",
            "88\t0.595890410958904\t[-0.093225 -0.20535  -0.113746 -0.004446 -0.0007  ] ...\n",
            "89\t0.6027397260273972\t[-0.100095 -0.206026 -0.110935 -0.004208 -0.000815] ...\n",
            "90\t0.6095890410958904\t[-0.09849  -0.197656 -0.133227 -0.005111 -0.000445] ...\n",
            "91\t0.6164383561643836\t[-0.107654 -0.21045  -0.130812 -0.004614 -0.000547] ...\n",
            "92\t0.6232876712328766\t[-1.12674e-01 -2.08272e-01 -1.33871e-01 -4.97500e-03 -1.02000e-04] ...\n",
            "93\t0.6301369863013698\t[-0.113483 -0.196638 -0.136063 -0.00478  -0.000414] ...\n",
            "94\t0.636986301369863\t[-0.138027 -0.202728 -0.151165 -0.005167 -0.001494] ...\n",
            "95\t0.6438356164383561\t[-0.129293 -0.19532  -0.172373 -0.005237 -0.001013] ...\n",
            "96\t0.6506849315068493\t[-0.135875 -0.191794 -0.179092 -0.005094 -0.000585] ...\n",
            "97\t0.6575342465753424\t[-0.138362 -0.194376 -0.185103 -0.004922 -0.001561] ...\n",
            "98\t0.6643835616438356\t[-0.112587 -0.188855 -0.162981 -0.00588  -0.000702] ...\n",
            "99\t0.6712328767123288\t[-0.113571 -0.196909 -0.157441 -0.00518  -0.000597] ...\n",
            "100\t0.6780821917808219\t[-0.111657 -0.188161 -0.165007 -0.005248 -0.000911] ...\n",
            "101\t0.684931506849315\t[-0.107983 -0.192039 -0.158635 -0.005455 -0.000781] ...\n",
            "102\t0.6917808219178082\t[-0.10571  -0.20392  -0.155188 -0.005025 -0.001064] ...\n",
            "103\t0.6986301369863013\t[-0.107472 -0.191854 -0.165921 -0.005368 -0.000613] ...\n",
            "104\t0.7054794520547945\t[-0.101589 -0.190575 -0.159753 -0.005524 -0.000334] ...\n",
            "105\t0.7123287671232876\t[-0.096516 -0.188546 -0.163988 -0.004887 -0.000347] ...\n",
            "106\t0.7191780821917808\t[-0.099855 -0.198349 -0.157408 -0.006173 -0.000439] ...\n",
            "107\t0.726027397260274\t[-0.086931 -0.199064 -0.148926 -0.005998 -0.00085 ] ...\n",
            "108\t0.732876712328767\t[-0.085312 -0.205933 -0.131552 -0.00567  -0.000994] ...\n",
            "109\t0.7397260273972602\t[-0.098395 -0.205066 -0.149038 -0.005266 -0.001096] ...\n",
            "110\t0.7465753424657534\t[-0.117033 -0.203967 -0.164563 -0.005427 -0.001245] ...\n",
            "111\t0.7534246575342465\t[-0.107993 -0.223065 -0.125599 -0.004238 -0.000866] ...\n",
            "112\t0.7602739726027397\t[-0.107863 -0.228486 -0.112494 -0.004469 -0.000936] ...\n",
            "113\t0.7671232876712328\t[-0.099908 -0.228697 -0.106363 -0.004411 -0.001024] ...\n",
            "114\t0.773972602739726\t[-0.09066  -0.224582 -0.110503 -0.004692 -0.001047] ...\n",
            "115\t0.7808219178082192\t[-0.089943 -0.223542 -0.117056 -0.004538 -0.000952] ...\n",
            "116\t0.7876712328767123\t[-0.091693 -0.226378 -0.118305 -0.004352 -0.000935] ...\n",
            "117\t0.7945205479452054\t[-0.09109  -0.22616  -0.117083 -0.004372 -0.000877] ...\n",
            "118\t0.8013698630136986\t[-0.094387 -0.225518 -0.116619 -0.004195 -0.000801] ...\n",
            "119\t0.8082191780821917\t[-0.09821  -0.22817  -0.101979 -0.004401 -0.000923] ...\n",
            "120\t0.8150684931506849\t[-0.09877  -0.228528 -0.078775 -0.004455 -0.001202] ...\n",
            "121\t0.821917808219178\t[-0.104131 -0.230532 -0.066069 -0.003865 -0.001121] ...\n",
            "122\t0.8287671232876712\t[-0.082172 -0.231184 -0.068214 -0.004498 -0.0013  ] ...\n",
            "123\t0.8356164383561644\t[-0.073069 -0.229761 -0.081243 -0.004845 -0.001187] ...\n",
            "124\t0.8424657534246575\t[-0.095474 -0.223449 -0.107154 -0.004381 -0.001243] ...\n",
            "125\t0.8493150684931506\t[-0.090546 -0.222669 -0.117288 -0.00438  -0.001164] ...\n",
            "126\t0.8561643835616438\t[-0.087479 -0.222388 -0.122098 -0.004267 -0.001134] ...\n",
            "127\t0.8630136986301369\t[-0.088746 -0.225251 -0.114856 -0.004146 -0.001208] ...\n",
            "128\t0.8698630136986301\t[-0.088231 -0.225214 -0.107255 -0.004292 -0.001275] ...\n",
            "129\t0.8767123287671232\t[-0.087806 -0.228793 -0.100098 -0.004198 -0.001218] ...\n",
            "130\t0.8835616438356164\t[-0.086449 -0.233227 -0.093659 -0.00409  -0.001106] ...\n",
            "131\t0.8904109589041096\t[-0.084528 -0.234043 -0.08675  -0.004183 -0.001111] ...\n",
            "132\t0.8972602739726027\t[-0.067985 -0.240586 -0.073185 -0.003973 -0.000458] ...\n",
            "133\t0.9041095890410958\t[-0.065766 -0.242597 -0.074926 -0.003908 -0.000303] ...\n",
            "134\t0.910958904109589\t[-0.070775 -0.247231 -0.073174 -0.003621 -0.000639] ...\n",
            "135\t0.9178082191780821\t[-0.07078  -0.236088 -0.07579  -0.004327 -0.000588] ...\n",
            "136\t0.9246575342465753\t[-0.067708 -0.236741 -0.092646 -0.005352 -0.000901] ...\n",
            "137\t0.9315068493150684\t[-0.066713 -0.239434 -0.083967 -0.00508  -0.00051 ] ...\n",
            "138\t0.9383561643835616\t[-0.067207 -0.239831 -0.06793  -0.005572 -0.000618] ...\n",
            "139\t0.9452054794520548\t[-0.065793 -0.236781 -0.065696 -0.004192 -0.000381] ...\n",
            "140\t0.9520547945205479\t[-0.065465 -0.242839 -0.048678 -0.004646 -0.000326] ...\n",
            "141\t0.958904109589041\t[-0.067447 -0.237819 -0.056354 -0.004692 -0.000412] ...\n",
            "142\t0.9657534246575342\t[-0.069396 -0.23351  -0.049777 -0.004429 -0.001057] ...\n",
            "143\t0.9726027397260273\t[-0.079032 -0.229188 -0.072177 -0.004563 -0.001293] ...\n",
            "144\t0.9794520547945205\t[-0.088192 -0.234308 -0.054092 -0.004843 -0.001339] ...\n",
            "145\t0.9863013698630136\t[-0.095683 -0.235744 -0.048351 -0.005272 -0.000758] ...\n",
            "146\t0.9931506849315068\t[-0.090729 -0.230401 -0.060568 -0.004781 -0.000725] ...\n",
            "147\t1.0\t[-0.090032 -0.229371 -0.072692 -0.003925 -0.000408] ...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x200 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Line 4 - 147 frames\n",
            "Frame\tCounter\tSample Features\n",
            "1\t0.0\t[-0.034068 -0.189199 -0.053895 -0.004341 -0.001086] ...\n",
            "2\t0.00684931506849315\t[-0.03481  -0.188383 -0.054765 -0.004093 -0.001061] ...\n",
            "3\t0.0136986301369863\t[-0.035768 -0.198415 -0.027675 -0.004828 -0.001033] ...\n",
            "4\t0.02054794520547945\t[-0.016157 -0.191428 -0.030598 -0.004852 -0.000211] ...\n",
            "5\t0.0273972602739726\t[ 4.26700e-03 -1.97931e-01 -1.26810e-02 -3.45800e-03  1.60000e-05] ...\n",
            "6\t0.03424657534246575\t[-4.37160e-02 -1.96696e-01 -2.75920e-02 -3.61100e-03 -1.15000e-04] ...\n",
            "7\t0.0410958904109589\t[-0.034422 -0.203199  0.006056 -0.004583 -0.000345] ...\n",
            "8\t0.04794520547945205\t[-0.033217 -0.204347 -0.026817 -0.004356 -0.001072] ...\n",
            "9\t0.0547945205479452\t[-0.043026 -0.201708 -0.055191 -0.004226 -0.001548] ...\n",
            "10\t0.06164383561643835\t[-0.035722 -0.202301 -0.053042 -0.004695 -0.000967] ...\n",
            "11\t0.0684931506849315\t[-3.6356e-02 -2.0101e-01 -5.7671e-02 -4.8810e-03 -7.6000e-05] ...\n",
            "12\t0.07534246575342465\t[-3.85790e-02 -2.02754e-01 -5.73880e-02 -5.64600e-03 -1.08000e-04] ...\n",
            "13\t0.0821917808219178\t[-0.026569 -0.197637 -0.064267 -0.004712 -0.000522] ...\n",
            "14\t0.08904109589041095\t[-0.021436 -0.182657 -0.107303 -0.004712 -0.000364] ...\n",
            "15\t0.0958904109589041\t[-0.010124 -0.190143 -0.071193 -0.005018 -0.000888] ...\n",
            "16\t0.10273972602739725\t[-0.012546 -0.186097 -0.075271 -0.004727 -0.000725] ...\n",
            "17\t0.1095890410958904\t[-1.44320e-02 -1.96535e-01 -5.56020e-02 -4.56500e-03 -1.06000e-04] ...\n",
            "18\t0.11643835616438356\t[-1.44980e-02 -2.01305e-01 -4.74350e-02 -4.52400e-03 -1.54000e-04] ...\n",
            "19\t0.1232876712328767\t[-8.20200e-03 -2.06104e-01 -4.49360e-02 -4.12900e-03  1.30000e-05] ...\n",
            "20\t0.13013698630136986\t[-1.37600e-03 -2.14128e-01 -1.23570e-02 -4.18500e-03  9.90000e-05] ...\n",
            "21\t0.136986301369863\t[ 7.16900e-03 -2.17758e-01  1.48520e-02 -4.07300e-03 -1.42000e-04] ...\n",
            "22\t0.14383561643835616\t[ 0.020406 -0.209196 -0.012981 -0.003014 -0.000338] ...\n",
            "23\t0.1506849315068493\t[ 0.019091 -0.210152 -0.003622 -0.003665 -0.000534] ...\n",
            "24\t0.15753424657534246\t[ 2.51710e-02 -2.07941e-01 -3.98800e-03 -4.14600e-03 -5.00000e-06] ...\n",
            "25\t0.1643835616438356\t[ 2.63030e-02 -2.09404e-01 -8.40000e-05 -4.67800e-03 -5.48000e-04] ...\n",
            "26\t0.17123287671232876\t[ 0.032932 -0.21459   0.005676 -0.004975 -0.000586] ...\n",
            "27\t0.1780821917808219\t[ 0.023467 -0.212723  0.006795 -0.004398 -0.000716] ...\n",
            "28\t0.18493150684931506\t[ 0.026259 -0.20988   0.003267 -0.004671 -0.000997] ...\n",
            "29\t0.1917808219178082\t[ 0.025993 -0.20975  -0.002646 -0.00447  -0.000995] ...\n",
            "30\t0.19863013698630136\t[ 0.039456 -0.209119  0.01173  -0.004067 -0.00076 ] ...\n",
            "31\t0.2054794520547945\t[ 0.065661 -0.20537   0.007998 -0.004287 -0.000686] ...\n",
            "32\t0.21232876712328766\t[ 0.049208 -0.205555  0.007883 -0.003736 -0.000806] ...\n",
            "33\t0.2191780821917808\t[ 0.045533 -0.205907  0.002838 -0.003657 -0.000643] ...\n",
            "34\t0.22602739726027396\t[ 0.036623 -0.204612  0.005694 -0.004093 -0.000599] ...\n",
            "35\t0.2328767123287671\t[ 0.043487 -0.205432  0.006075 -0.004292 -0.000616] ...\n",
            "36\t0.23972602739726026\t[ 0.046612 -0.203074 -0.001406 -0.004094 -0.000616] ...\n",
            "37\t0.2465753424657534\t[ 0.0342   -0.204912  0.003568 -0.00411  -0.000811] ...\n",
            "38\t0.2534246575342466\t[ 0.025792 -0.207797  0.006577 -0.003934 -0.000928] ...\n",
            "39\t0.2602739726027397\t[ 0.025867 -0.206995  0.002795 -0.002954 -0.000894] ...\n",
            "40\t0.26712328767123283\t[ 0.023745 -0.211204  0.003911 -0.003549 -0.001054] ...\n",
            "41\t0.273972602739726\t[ 0.026847 -0.211836 -0.000233 -0.004128 -0.000924] ...\n",
            "42\t0.2808219178082192\t[ 0.024254 -0.214528  0.002936 -0.003826 -0.001273] ...\n",
            "43\t0.2876712328767123\t[ 0.025657 -0.215691 -0.000433 -0.004717 -0.001119] ...\n",
            "44\t0.29452054794520544\t[ 0.023566 -0.210195 -0.025699 -0.00406  -0.000933] ...\n",
            "45\t0.3013698630136986\t[ 0.032315 -0.201244 -0.060017 -0.003716 -0.000636] ...\n",
            "46\t0.3082191780821918\t[ 0.041771 -0.199719 -0.061746 -0.004232 -0.000325] ...\n",
            "47\t0.3150684931506849\t[ 0.052016 -0.20008  -0.043296 -0.004847 -0.000515] ...\n",
            "48\t0.32191780821917804\t[ 6.67840e-02 -1.97479e-01 -5.93000e-04 -3.21100e-03 -7.80000e-05] ...\n",
            "49\t0.3287671232876712\t[ 0.065328 -0.196781  0.005254 -0.003632 -0.000616] ...\n",
            "50\t0.3356164383561644\t[ 0.067959 -0.198138  0.004366 -0.004183 -0.000528] ...\n",
            "51\t0.3424657534246575\t[ 0.063969 -0.201104  0.005488 -0.003618 -0.000644] ...\n",
            "52\t0.34931506849315064\t[ 0.05716  -0.203823  0.007668 -0.003898 -0.000778] ...\n",
            "53\t0.3561643835616438\t[ 0.06187  -0.1982   -0.009708 -0.002899 -0.000965] ...\n",
            "54\t0.363013698630137\t[ 0.057873 -0.198991 -0.021389 -0.00193  -0.00024 ] ...\n",
            "55\t0.3698630136986301\t[ 0.055227 -0.198666 -0.042206 -0.003205 -0.000213] ...\n",
            "56\t0.37671232876712324\t[ 0.059739 -0.19281  -0.060372 -0.00294   0.000455] ...\n",
            "57\t0.3835616438356164\t[ 0.069021 -0.195491 -0.023252 -0.004412 -0.000254] ...\n",
            "58\t0.3904109589041096\t[ 0.063032 -0.191343 -0.031185 -0.003537 -0.000489] ...\n",
            "59\t0.3972602739726027\t[ 0.051499 -0.185658 -0.050089 -0.003627 -0.00054 ] ...\n",
            "60\t0.40410958904109584\t[ 5.6847e-02 -1.8659e-01 -5.4875e-02 -3.8800e-03 -1.4800e-04] ...\n",
            "61\t0.410958904109589\t[ 0.050467 -0.185998 -0.059185 -0.003514  0.000442] ...\n",
            "62\t0.4178082191780822\t[ 0.048061 -0.185142 -0.074381 -0.00313   0.000437] ...\n",
            "63\t0.4246575342465753\t[ 0.03972  -0.185377 -0.072692 -0.003675 -0.000634] ...\n",
            "64\t0.43150684931506844\t[ 0.038903 -0.18811  -0.045474 -0.004127 -0.000538] ...\n",
            "65\t0.4383561643835616\t[ 0.032244 -0.192808 -0.041637 -0.003146 -0.000411] ...\n",
            "66\t0.4452054794520548\t[ 0.036262 -0.195458 -0.027261 -0.003065 -0.000611] ...\n",
            "67\t0.4520547945205479\t[ 0.036211 -0.208688 -0.011359 -0.003985 -0.000626] ...\n",
            "68\t0.45890410958904104\t[ 0.017121 -0.209285 -0.026001 -0.003354 -0.00022 ] ...\n",
            "69\t0.4657534246575342\t[ 0.013361 -0.211537 -0.019165 -0.00375  -0.000417] ...\n",
            "70\t0.4726027397260274\t[ 0.008765 -0.203645 -0.043102 -0.003264 -0.000556] ...\n",
            "71\t0.4794520547945205\t[-0.012248 -0.201613 -0.035186 -0.004291 -0.000333] ...\n",
            "72\t0.48630136986301364\t[-0.015857 -0.199553 -0.035495 -0.003233 -0.000578] ...\n",
            "73\t0.4931506849315068\t[-0.019719 -0.198205 -0.015906 -0.003447 -0.000853] ...\n",
            "74\t0.5\t[-0.022072 -0.198816 -0.014127 -0.002976 -0.000439] ...\n",
            "75\t0.5068493150684932\t[-0.034547 -0.198599 -0.012853 -0.003949 -0.000762] ...\n",
            "76\t0.5136986301369862\t[-0.038993 -0.196118 -0.014308 -0.004005 -0.000742] ...\n",
            "77\t0.5205479452054794\t[-0.037419 -0.193919 -0.024035 -0.004191 -0.000761] ...\n",
            "78\t0.5273972602739726\t[-0.047346 -0.192058 -0.015447 -0.004322 -0.001035] ...\n",
            "79\t0.5342465753424657\t[-0.050346 -0.189192 -0.006788 -0.004659 -0.00074 ] ...\n",
            "80\t0.5410958904109588\t[-0.055467 -0.188487 -0.006898 -0.004694 -0.000625] ...\n",
            "81\t0.547945205479452\t[-0.054342 -0.189521 -0.011257 -0.004277 -0.000602] ...\n",
            "82\t0.5547945205479452\t[-0.051454 -0.183809 -0.042718 -0.004043 -0.001006] ...\n",
            "83\t0.5616438356164384\t[-0.04965  -0.183019 -0.045875 -0.003922 -0.001207] ...\n",
            "84\t0.5684931506849314\t[-0.050271 -0.187944 -0.019474 -0.003731 -0.000648] ...\n",
            "85\t0.5753424657534246\t[-0.04328  -0.188499 -0.016846 -0.003608 -0.000366] ...\n",
            "86\t0.5821917808219178\t[-0.04458  -0.188536 -0.02121  -0.003349 -0.000597] ...\n",
            "87\t0.5890410958904109\t[-0.030905 -0.179734 -0.061348 -0.003486 -0.000705] ...\n",
            "88\t0.595890410958904\t[-0.047058 -0.1856   -0.044755 -0.003162 -0.000917] ...\n",
            "89\t0.6027397260273972\t[-0.057619 -0.189563 -0.028947 -0.003164 -0.001022] ...\n",
            "90\t0.6095890410958904\t[-0.054527 -0.189783 -0.031797 -0.002604 -0.000813] ...\n",
            "91\t0.6164383561643836\t[-0.051942 -0.188507 -0.022642 -0.002618 -0.000356] ...\n",
            "92\t0.6232876712328766\t[-0.053341 -0.188248 -0.027223 -0.002887 -0.000379] ...\n",
            "93\t0.6301369863013698\t[-0.056244 -0.18523  -0.041237 -0.002969 -0.000748] ...\n",
            "94\t0.636986301369863\t[-0.056643 -0.17663  -0.081759 -0.003516 -0.000996] ...\n",
            "95\t0.6438356164383561\t[-0.059495 -0.172647 -0.091127 -0.00416  -0.000966] ...\n",
            "96\t0.6506849315068493\t[-0.057765 -0.170191 -0.101476 -0.003745 -0.00096 ] ...\n",
            "97\t0.6575342465753424\t[-0.053824 -0.168323 -0.104559 -0.003632 -0.001414] ...\n",
            "98\t0.6643835616438356\t[-0.05708  -0.165035 -0.106747 -0.003784 -0.000616] ...\n",
            "99\t0.6712328767123288\t[-0.063131 -0.165071 -0.106327 -0.003658 -0.000732] ...\n",
            "100\t0.6780821917808219\t[-0.062189 -0.164856 -0.111198 -0.003825 -0.000952] ...\n",
            "101\t0.684931506849315\t[-0.061588 -0.16598  -0.107295 -0.003673 -0.00075 ] ...\n",
            "102\t0.6917808219178082\t[-0.052493 -0.167358 -0.1033   -0.003778 -0.000992] ...\n",
            "103\t0.6986301369863013\t[-0.059067 -0.168966 -0.102936 -0.004627 -0.001198] ...\n",
            "104\t0.7054794520547945\t[-0.057803 -0.173044 -0.08482  -0.004775 -0.00061 ] ...\n",
            "105\t0.7123287671232876\t[-0.064267 -0.16743  -0.093498 -0.003729 -0.000496] ...\n",
            "106\t0.7191780821917808\t[-6.95430e-02 -1.67325e-01 -9.25240e-02 -4.15700e-03 -1.61000e-04] ...\n",
            "107\t0.726027397260274\t[-0.069449 -0.168212 -0.089272 -0.004482 -0.00063 ] ...\n",
            "108\t0.732876712328767\t[-0.066539 -0.170852 -0.084117 -0.004837 -0.000578] ...\n",
            "109\t0.7397260273972602\t[-0.059942 -0.170658 -0.093218 -0.004752 -0.000478] ...\n",
            "110\t0.7465753424657534\t[-0.066809 -0.162622 -0.108442 -0.004798 -0.000513] ...\n",
            "111\t0.7534246575342465\t[-0.05421  -0.164415 -0.107688 -0.004277 -0.000556] ...\n",
            "112\t0.7602739726027397\t[-0.05345  -0.163344 -0.110229 -0.004162 -0.000248] ...\n",
            "113\t0.7671232876712328\t[-5.43990e-02 -1.65254e-01 -1.11493e-01 -3.84800e-03 -1.90000e-05] ...\n",
            "114\t0.773972602739726\t[-0.050267 -0.170387 -0.105659 -0.004368 -0.000307] ...\n",
            "115\t0.7808219178082192\t[-0.054382 -0.168325 -0.107115 -0.003819 -0.000468] ...\n",
            "116\t0.7876712328767123\t[-0.056215 -0.163868 -0.113138 -0.003529 -0.000784] ...\n",
            "117\t0.7945205479452054\t[-0.051303 -0.161641 -0.113658 -0.004225 -0.000809] ...\n",
            "118\t0.8013698630136986\t[-0.055181 -0.156588 -0.125005 -0.003767 -0.000613] ...\n",
            "119\t0.8082191780821917\t[-0.041513 -0.159081 -0.124431 -0.004405 -0.000622] ...\n",
            "120\t0.8150684931506849\t[-0.042073 -0.165981 -0.105979 -0.004815 -0.000831] ...\n",
            "121\t0.821917808219178\t[-0.031191 -0.177324 -0.080596 -0.004783 -0.00073 ] ...\n",
            "122\t0.8287671232876712\t[-0.026491 -0.184652 -0.073617 -0.005133 -0.000482] ...\n",
            "123\t0.8356164383561644\t[-0.021985 -0.190611 -0.055071 -0.005122 -0.000342] ...\n",
            "124\t0.8424657534246575\t[-0.012965 -0.193638 -0.040796 -0.004622 -0.000761] ...\n",
            "125\t0.8493150684931506\t[ 0.000724 -0.18608  -0.052158 -0.004356 -0.00126 ] ...\n",
            "126\t0.8561643835616438\t[ 0.003764 -0.18407  -0.059413 -0.004176 -0.001285] ...\n",
            "127\t0.8630136986301369\t[ 0.002913 -0.18427  -0.057168 -0.004078 -0.001271] ...\n",
            "128\t0.8698630136986301\t[ 0.006108 -0.185236 -0.054675 -0.00417  -0.001283] ...\n",
            "129\t0.8767123287671232\t[ 0.005958 -0.186199 -0.052789 -0.004181 -0.001256] ...\n",
            "130\t0.8835616438356164\t[-0.002551 -0.192516 -0.035092 -0.004261 -0.001068] ...\n",
            "131\t0.8904109589041096\t[ 0.001729 -0.187431 -0.051965 -0.00417  -0.001172] ...\n",
            "132\t0.8972602739726027\t[ 0.001748 -0.19816  -0.0227   -0.00402  -0.00075 ] ...\n",
            "133\t0.9041095890410958\t[ 0.008345 -0.209671  0.013297 -0.003554 -0.00047 ] ...\n",
            "134\t0.910958904109589\t[ 0.010894 -0.20537   0.006944 -0.003222 -0.000848] ...\n",
            "135\t0.9178082191780821\t[ 0.010049 -0.196217 -0.020428 -0.004244 -0.00048 ] ...\n",
            "136\t0.9246575342465753\t[ 0.014104 -0.194625 -0.020403 -0.004878 -0.000797] ...\n",
            "137\t0.9315068493150684\t[ 0.021598 -0.203203 -0.007974 -0.004526 -0.000839] ...\n",
            "138\t0.9383561643835616\t[ 0.031776 -0.206843  0.003937 -0.0047   -0.000243] ...\n",
            "139\t0.9452054794520548\t[ 0.023184 -0.20735   0.002108 -0.00418  -0.000248] ...\n",
            "140\t0.9520547945205479\t[ 2.56810e-02 -2.08666e-01  2.34700e-03 -4.43100e-03 -1.86000e-04] ...\n",
            "141\t0.958904109589041\t[ 0.018841 -0.20457  -0.012106 -0.004153 -0.000401] ...\n",
            "142\t0.9657534246575342\t[ 0.012476 -0.198046 -0.016412 -0.004401 -0.001171] ...\n",
            "143\t0.9726027397260273\t[-0.002895 -0.196132 -0.01936  -0.004935 -0.001321] ...\n",
            "144\t0.9794520547945205\t[-0.011445 -0.197835 -0.014226 -0.004977 -0.001299] ...\n",
            "145\t0.9863013698630136\t[-0.012066 -0.200698  0.004882 -0.004879 -0.00091 ] ...\n",
            "146\t0.9931506849315068\t[-0.019639 -0.194141 -0.014598 -0.004809 -0.00109 ] ...\n",
            "147\t1.0\t[-0.019015 -0.194491 -0.015012 -0.004167 -0.001203] ...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x200 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Line 5 - 147 frames\n",
            "Frame\tCounter\tSample Features\n",
            "1\t0.0\t[-7.17100e-03 -1.92777e-01 -3.18290e-02 -4.57000e-03 -4.30000e-05] ...\n",
            "2\t0.00684931506849315\t[-1.2738e-02 -1.8978e-01 -4.0581e-02 -4.4070e-03 -1.8700e-04] ...\n",
            "3\t0.0136986301369863\t[-1.31360e-02 -1.94755e-01 -2.73880e-02 -4.61500e-03 -2.20000e-05] ...\n",
            "4\t0.02054794520547945\t[-0.023191 -0.182515 -0.059037 -0.003804  0.000207] ...\n",
            "5\t0.0273972602739726\t[-0.017065 -0.187593 -0.045429 -0.003288  0.000282] ...\n",
            "6\t0.03424657534246575\t[-0.022219 -0.201185 -0.013263 -0.003876  0.000456] ...\n",
            "7\t0.0410958904109589\t[-0.025628 -0.204162 -0.00892  -0.003592  0.001106] ...\n",
            "8\t0.04794520547945205\t[-0.023944 -0.206528  0.002079 -0.002843  0.000965] ...\n",
            "9\t0.0547945205479452\t[-0.027912 -0.207011  0.001026 -0.002079  0.000382] ...\n",
            "10\t0.06164383561643835\t[-0.02777  -0.206867  0.005003 -0.002945  0.000639] ...\n",
            "11\t0.0684931506849315\t[-0.027005 -0.206368  0.00262  -0.002993  0.000971] ...\n",
            "12\t0.07534246575342465\t[-0.026654 -0.207304  0.003291 -0.002962  0.000926] ...\n",
            "13\t0.0821917808219178\t[-0.027985 -0.206916  0.002785 -0.00265   0.000541] ...\n",
            "14\t0.08904109589041095\t[-0.032665 -0.20484  -0.001604 -0.00254   0.000474] ...\n",
            "15\t0.0958904109589041\t[-0.030439 -0.205693 -0.001938 -0.002868  0.00033 ] ...\n",
            "16\t0.10273972602739725\t[-0.034776 -0.202988 -0.002575 -0.002874  0.000408] ...\n",
            "17\t0.1095890410958904\t[-3.3076e-02 -2.0494e-01 -1.1400e-04 -3.1590e-03  4.7100e-04] ...\n",
            "18\t0.11643835616438356\t[-0.0312   -0.207089  0.003767 -0.003341  0.000453] ...\n",
            "19\t0.1232876712328767\t[-0.033844 -0.20629   0.00234  -0.002878  0.000537] ...\n",
            "20\t0.13013698630136986\t[-0.032247 -0.207338  0.001853 -0.002933  0.000465] ...\n",
            "21\t0.136986301369863\t[-0.031406 -0.208175  0.003172 -0.002821  0.000493] ...\n",
            "22\t0.14383561643835616\t[-3.17590e-02 -2.06826e-01 -1.96000e-04 -2.45700e-03  5.84000e-04] ...\n",
            "23\t0.1506849315068493\t[-0.030583 -0.203795 -0.004868 -0.002256  0.000489] ...\n",
            "24\t0.15753424657534246\t[-1.42000e-02 -2.06449e-01 -1.31300e-03 -3.46400e-03  6.40000e-05] ...\n",
            "25\t0.1643835616438356\t[-0.005222 -0.20785   0.001189 -0.004305 -0.00044 ] ...\n",
            "26\t0.17123287671232876\t[-0.004043 -0.205624 -0.000951 -0.004553 -0.000681] ...\n",
            "27\t0.1780821917808219\t[-0.000949 -0.207586  0.003856 -0.004768 -0.000754] ...\n",
            "28\t0.18493150684931506\t[ 0.000898 -0.207923  0.005262 -0.00496  -0.000774] ...\n",
            "29\t0.1917808219178082\t[ 0.001767 -0.207716  0.003037 -0.004666 -0.000798] ...\n",
            "30\t0.19863013698630136\t[ 0.001922 -0.208533  0.004179 -0.004579 -0.000813] ...\n",
            "31\t0.2054794520547945\t[ 0.001088 -0.208095  0.001681 -0.004594 -0.000849] ...\n",
            "32\t0.21232876712328766\t[ 0.003629 -0.208828  0.007171 -0.004638 -0.000918] ...\n",
            "33\t0.2191780821917808\t[ 0.006821 -0.207711  0.007185 -0.004622 -0.000976] ...\n",
            "34\t0.22602739726027396\t[ 0.008304 -0.205117  0.003128 -0.004577 -0.001029] ...\n",
            "35\t0.2328767123287671\t[ 0.006974 -0.205613  0.002166 -0.004576 -0.001031] ...\n",
            "36\t0.23972602739726026\t[ 0.005566 -0.209921  0.006228 -0.00488  -0.001107] ...\n",
            "37\t0.2465753424657534\t[ 0.001518 -0.207169 -0.013918 -0.004884 -0.000982] ...\n",
            "38\t0.2534246575342466\t[-0.0004   -0.205033 -0.027192 -0.004671 -0.00095 ] ...\n",
            "39\t0.2602739726027397\t[-0.001012 -0.197271 -0.054039 -0.004369 -0.000957] ...\n",
            "40\t0.26712328767123283\t[ 0.000518 -0.201306 -0.050841 -0.004583 -0.000797] ...\n",
            "41\t0.273972602739726\t[ 0.002011 -0.203705 -0.051552 -0.005456 -0.000651] ...\n",
            "42\t0.2808219178082192\t[ 0.00648  -0.208285 -0.036569 -0.005269 -0.00076 ] ...\n",
            "43\t0.2876712328767123\t[ 0.010475 -0.200294 -0.052233 -0.005154 -0.000589] ...\n",
            "44\t0.29452054794520544\t[ 0.010631 -0.193525 -0.059509 -0.004608 -0.000422] ...\n",
            "45\t0.3013698630136986\t[ 0.009434 -0.190335 -0.06259  -0.004229 -0.000261] ...\n",
            "46\t0.3082191780821918\t[ 1.01830e-02 -1.90678e-01 -5.62590e-02 -4.39700e-03 -1.04000e-04] ...\n",
            "47\t0.3150684931506849\t[ 0.011351 -0.189394 -0.05829  -0.004528 -0.000226] ...\n",
            "48\t0.32191780821917804\t[ 1.84890e-02 -1.90068e-01 -5.66440e-02 -4.66000e-03 -1.09000e-04] ...\n",
            "49\t0.3287671232876712\t[ 1.90590e-02 -1.90995e-01 -4.87240e-02 -4.36900e-03  1.20000e-05] ...\n",
            "50\t0.3356164383561644\t[ 0.017317 -0.187673 -0.060605 -0.004147 -0.000351] ...\n",
            "51\t0.3424657534246575\t[ 0.012943 -0.186351 -0.061825 -0.004042 -0.000552] ...\n",
            "52\t0.34931506849315064\t[ 0.013149 -0.188654 -0.051905 -0.004117 -0.000625] ...\n",
            "53\t0.3561643835616438\t[ 0.008912 -0.187309 -0.061443 -0.004103 -0.00076 ] ...\n",
            "54\t0.363013698630137\t[ 0.003322 -0.192655 -0.046103 -0.004049 -0.001025] ...\n",
            "55\t0.3698630136986301\t[-0.001471 -0.194437 -0.053915 -0.003762 -0.000994] ...\n",
            "56\t0.37671232876712324\t[-0.01158  -0.19239  -0.063544 -0.003644 -0.000284] ...\n",
            "57\t0.3835616438356164\t[-0.015983 -0.187004 -0.079721 -0.004826 -0.000735] ...\n",
            "58\t0.3904109589041096\t[-0.02901  -0.187539 -0.07445  -0.004913 -0.000628] ...\n",
            "59\t0.3972602739726027\t[-0.034207 -0.184172 -0.076525 -0.004865 -0.000551] ...\n",
            "60\t0.40410958904109584\t[-4.0499e-02 -1.9227e-01 -4.8507e-02 -4.5160e-03  5.9000e-05] ...\n",
            "61\t0.410958904109589\t[-0.03899  -0.193689 -0.036609 -0.004822  0.000206] ...\n",
            "62\t0.4178082191780822\t[-3.60820e-02 -1.95353e-01 -2.57140e-02 -5.17900e-03 -1.26000e-04] ...\n",
            "63\t0.4246575342465753\t[-0.034    -0.196732 -0.019468 -0.005122 -0.00027 ] ...\n",
            "64\t0.43150684931506844\t[-0.025133 -0.194494 -0.022026 -0.00476  -0.000363] ...\n",
            "65\t0.4383561643835616\t[-0.022642 -0.192512 -0.038491 -0.004119 -0.000594] ...\n",
            "66\t0.4452054794520548\t[-0.019226 -0.194854 -0.024964 -0.004088 -0.000465] ...\n",
            "67\t0.4520547945205479\t[-0.016466 -0.199647 -0.01439  -0.004019 -0.000369] ...\n",
            "68\t0.45890410958904104\t[-0.0217   -0.196563 -0.032341 -0.004555 -0.000683] ...\n",
            "69\t0.4657534246575342\t[-0.027006 -0.203839 -0.016465 -0.004709 -0.000879] ...\n",
            "70\t0.4726027397260274\t[-0.023655 -0.196016 -0.061301 -0.004689 -0.001073] ...\n",
            "71\t0.4794520547945205\t[-0.035601 -0.189314 -0.08539  -0.005808 -0.000686] ...\n",
            "72\t0.48630136986301364\t[-0.027175 -0.185613 -0.08225  -0.004871 -0.000517] ...\n",
            "73\t0.4931506849315068\t[-0.017494 -0.181249 -0.086993 -0.004634 -0.00053 ] ...\n",
            "74\t0.5\t[-0.013358 -0.180528 -0.086731 -0.004338 -0.00039 ] ...\n",
            "75\t0.5068493150684932\t[-0.016085 -0.181945 -0.084726 -0.004782 -0.000469] ...\n",
            "76\t0.5136986301369862\t[-0.015037 -0.179411 -0.089642 -0.004926 -0.000478] ...\n",
            "77\t0.5205479452054794\t[-0.018128 -0.178804 -0.093496 -0.004753 -0.00038 ] ...\n",
            "78\t0.5273972602739726\t[-0.019245 -0.181255 -0.089672 -0.004821 -0.000582] ...\n",
            "79\t0.5342465753424657\t[-0.025385 -0.170816 -0.117532 -0.005068 -0.000314] ...\n",
            "80\t0.5410958904109588\t[-0.024344 -0.168696 -0.12107  -0.004819 -0.000322] ...\n",
            "81\t0.547945205479452\t[-2.21220e-02 -1.71788e-01 -1.12340e-01 -4.56600e-03 -1.53000e-04] ...\n",
            "82\t0.5547945205479452\t[-0.021303 -0.172677 -0.11088  -0.004864 -0.00021 ] ...\n",
            "83\t0.5616438356164384\t[-0.01177  -0.175959 -0.100391 -0.004587 -0.000186] ...\n",
            "84\t0.5684931506849314\t[-0.004756 -0.185001 -0.075566 -0.004138 -0.0003  ] ...\n",
            "85\t0.5753424657534246\t[-0.002903 -0.188883 -0.066723 -0.003986 -0.000266] ...\n",
            "86\t0.5821917808219178\t[-0.003169 -0.187823 -0.072157 -0.004308 -0.000329] ...\n",
            "87\t0.5890410958904109\t[-0.003643 -0.176903 -0.106615 -0.004457 -0.001412] ...\n",
            "88\t0.595890410958904\t[-0.00505  -0.178542 -0.105319 -0.004223 -0.001539] ...\n",
            "89\t0.6027397260273972\t[-0.001292 -0.179972 -0.107636 -0.004157 -0.001664] ...\n",
            "90\t0.6095890410958904\t[-0.004169 -0.17937  -0.106584 -0.004031 -0.001766] ...\n",
            "91\t0.6164383561643836\t[-0.011747 -0.17932  -0.099414 -0.004349 -0.001667] ...\n",
            "92\t0.6232876712328766\t[-0.019373 -0.175915 -0.109274 -0.004689 -0.001527] ...\n",
            "93\t0.6301369863013698\t[-0.027899 -0.165357 -0.133489 -0.004223 -0.002142] ...\n",
            "94\t0.636986301369863\t[-0.033192 -0.160901 -0.148491 -0.00417  -0.002368] ...\n",
            "95\t0.6438356164383561\t[-0.047796 -0.159477 -0.147582 -0.004973 -0.002644] ...\n",
            "96\t0.6506849315068493\t[-0.046426 -0.15342  -0.156589 -0.004581 -0.00212 ] ...\n",
            "97\t0.6575342465753424\t[-0.035736 -0.158072 -0.142368 -0.004236 -0.002261] ...\n",
            "98\t0.6643835616438356\t[-0.024981 -0.156678 -0.147182 -0.004932 -0.001733] ...\n",
            "99\t0.6712328767123288\t[-0.029055 -0.166578 -0.129127 -0.004339 -0.00143 ] ...\n",
            "100\t0.6780821917808219\t[-0.029387 -0.167258 -0.134378 -0.004659 -0.001508] ...\n",
            "101\t0.684931506849315\t[-0.021545 -0.169981 -0.130749 -0.004337 -0.0014  ] ...\n",
            "102\t0.6917808219178082\t[-0.018457 -0.170569 -0.129732 -0.004691 -0.001487] ...\n",
            "103\t0.6986301369863013\t[-0.019847 -0.168413 -0.139603 -0.00507  -0.001446] ...\n",
            "104\t0.7054794520547945\t[-0.016343 -0.170849 -0.126733 -0.004413 -0.001779] ...\n",
            "105\t0.7123287671232876\t[-0.003817 -0.172993 -0.120161 -0.00408  -0.001524] ...\n",
            "106\t0.7191780821917808\t[-0.004047 -0.176066 -0.115353 -0.00514  -0.000928] ...\n",
            "107\t0.726027397260274\t[ 0.003315 -0.177061 -0.114037 -0.005228 -0.001679] ...\n",
            "108\t0.732876712328767\t[ 0.009414 -0.180247 -0.109961 -0.005099 -0.001376] ...\n",
            "109\t0.7397260273972602\t[ 0.003746 -0.180069 -0.109278 -0.005136 -0.001793] ...\n",
            "110\t0.7465753424657534\t[-0.018748 -0.170559 -0.133769 -0.006214 -0.001881] ...\n",
            "111\t0.7534246575342465\t[ 0.003055 -0.17903  -0.117302 -0.005074 -0.001467] ...\n",
            "112\t0.7602739726027397\t[-0.012517 -0.173821 -0.114382 -0.00538  -0.001093] ...\n",
            "113\t0.7671232876712328\t[-0.010149 -0.175155 -0.11541  -0.005171 -0.00108 ] ...\n",
            "114\t0.773972602739726\t[-0.00829  -0.175427 -0.117417 -0.005501 -0.001071] ...\n",
            "115\t0.7808219178082192\t[-0.006663 -0.175532 -0.118271 -0.005498 -0.001124] ...\n",
            "116\t0.7876712328767123\t[-0.006031 -0.175456 -0.119262 -0.005512 -0.001168] ...\n",
            "117\t0.7945205479452054\t[-0.005692 -0.176058 -0.116544 -0.005497 -0.001172] ...\n",
            "118\t0.8013698630136986\t[-0.008283 -0.178045 -0.111151 -0.004824 -0.001037] ...\n",
            "119\t0.8082191780821917\t[-0.007443 -0.183472 -0.091275 -0.005517 -0.001047] ...\n",
            "120\t0.8150684931506849\t[-0.00243  -0.197532 -0.04965  -0.005353 -0.001463] ...\n",
            "121\t0.821917808219178\t[ 0.003892 -0.198228 -0.032896 -0.004196 -0.000841] ...\n",
            "122\t0.8287671232876712\t[ 0.010403 -0.19915  -0.031993 -0.004544 -0.000936] ...\n",
            "123\t0.8356164383561644\t[ 0.010614 -0.197039 -0.040174 -0.004632 -0.000913] ...\n",
            "124\t0.8424657534246575\t[ 0.00854  -0.188518 -0.067895 -0.004769 -0.001172] ...\n",
            "125\t0.8493150684931506\t[ 0.004108 -0.178231 -0.100183 -0.004988 -0.001356] ...\n",
            "126\t0.8561643835616438\t[ 0.006831 -0.179452 -0.101591 -0.005053 -0.001376] ...\n",
            "127\t0.8630136986301369\t[ 0.007958 -0.182292 -0.093598 -0.00493  -0.001399] ...\n",
            "128\t0.8698630136986301\t[ 0.010016 -0.184805 -0.087328 -0.00499  -0.001457] ...\n",
            "129\t0.8767123287671232\t[ 0.012747 -0.187628 -0.079713 -0.004977 -0.001428] ...\n",
            "130\t0.8835616438356164\t[ 0.012677 -0.190563 -0.070759 -0.004895 -0.001312] ...\n",
            "131\t0.8904109589041096\t[ 0.012462 -0.199803 -0.046002 -0.004841 -0.001033] ...\n",
            "132\t0.8972602739726027\t[ 0.015295 -0.19772  -0.056243 -0.004742 -0.000989] ...\n",
            "133\t0.9041095890410958\t[ 0.013534 -0.205072 -0.036413 -0.004622 -0.000771] ...\n",
            "134\t0.910958904109589\t[ 0.003987 -0.203204 -0.035421 -0.004535 -0.001493] ...\n",
            "135\t0.9178082191780821\t[ 0.005266 -0.203044 -0.036564 -0.004832 -0.001434] ...\n",
            "136\t0.9246575342465753\t[ 0.003589 -0.206011 -0.029049 -0.00521  -0.001876] ...\n",
            "137\t0.9315068493150684\t[-8.50000e-05 -2.11668e-01 -1.74070e-02 -5.08000e-03 -2.49800e-03] ...\n",
            "138\t0.9383561643835616\t[ 0.006714 -0.214131 -0.002727 -0.005009 -0.002385] ...\n",
            "139\t0.9452054794520548\t[ 0.009347 -0.21066  -0.012507 -0.004308 -0.002627] ...\n",
            "140\t0.9520547945205479\t[ 0.010248 -0.212389 -0.011009 -0.004821 -0.002048] ...\n",
            "141\t0.958904109589041\t[ 0.009858 -0.210176 -0.020431 -0.004379 -0.001692] ...\n",
            "142\t0.9657534246575342\t[ 0.014227 -0.198784 -0.040743 -0.004622 -0.001857] ...\n",
            "143\t0.9726027397260273\t[ 0.008378 -0.192554 -0.068546 -0.005317 -0.001808] ...\n",
            "144\t0.9794520547945205\t[ 0.003896 -0.195904 -0.057269 -0.005428 -0.001733] ...\n",
            "145\t0.9863013698630136\t[ 0.002052 -0.19846  -0.042668 -0.005478 -0.001256] ...\n",
            "146\t0.9931506849315068\t[-0.00477  -0.189266 -0.067141 -0.005179 -0.001393] ...\n",
            "147\t1.0\t[-0.003134 -0.189916 -0.063286 -0.004557 -0.001229] ...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x200 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Path to the input .skels file\n",
        "file_path = \"/content/drive/MyDrive/Sign-IDD SLT/data/Inference/test.pred.skels\"\n",
        "\n",
        "# Read all lines\n",
        "with open(file_path, 'r') as f:\n",
        "    lines = f.readlines()\n",
        "\n",
        "fixed_lines = []\n",
        "\n",
        "for i, line in enumerate(lines):\n",
        "    values = np.fromstring(line.strip(), sep=' ')\n",
        "    num_frames = len(values) // 151\n",
        "\n",
        "    # Sanity check\n",
        "    if len(values) % 151 != 0:\n",
        "        raise ValueError(f\"Line {i+1}: Unexpected length {len(values)} for 151-D features.\")\n",
        "\n",
        "    # Reshape into [frames × 151]\n",
        "    data = values.reshape(num_frames, 151)\n",
        "\n",
        "    # Extract features (first 150 columns)\n",
        "    features = data[:, :150]\n",
        "\n",
        "    # FIX: Replace last column with evenly spaced values from 0.0 to 1.0\n",
        "    correct_counters = np.linspace(0.0, 1.0, num_frames).reshape(num_frames, 1)\n",
        "\n",
        "    # Concatenate fixed features and counter column\n",
        "    fixed_data = np.concatenate([features, correct_counters], axis=1)\n",
        "\n",
        "    # Flatten and stringify\n",
        "    flat_line = ' '.join(map(str, fixed_data.flatten()))\n",
        "    fixed_lines.append(flat_line)\n",
        "\n",
        "# Save to new file\n",
        "fixed_path = file_path.replace(\".skels\", \".fixed.skels\")\n",
        "with open(fixed_path, 'w') as f:\n",
        "    for line in fixed_lines:\n",
        "        f.write(line + '\\n')\n",
        "\n",
        "print(f\"✅ Fixed skeletons saved to:\\n{fixed_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LgG7OUAWVuOD",
        "outputId": "5dcacf7c-43b4-4b79-ad7f-a780ae99516d"
      },
      "id": "LgG7OUAWVuOD",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Fixed skeletons saved to:\n",
            "/content/drive/MyDrive/Sign-IDD SLT/data/Inference/test.pred.fixed.skels\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fixed pred skels"
      ],
      "metadata": {
        "id": "ZwYNTYvKWwwN"
      },
      "id": "ZwYNTYvKWwwN"
    },
    {
      "cell_type": "code",
      "source": [
        "import gzip\n",
        "import joblib\n",
        "import pandas as pd\n",
        "\n",
        "# Config\n",
        "feature_dim = 150\n",
        "input_csv = \"/content/drive/MyDrive/Sign-IDD SLT/data/PHOENIX2014T/Dev/dev.csv\"\n",
        "input_skels = \"/content/drive/MyDrive/Sign-IDD SLT/data/Inference/dev.pred.fixed.skels\"\n",
        "output_dir = \"/content/drive/MyDrive/Sign-IDD SLT/data/Inference\"\n",
        "output_file = os.path.join(output_dir, \"phoenix14t.infer.fixed.skels.dev\")\n",
        "\n",
        "# Ensure output directory exists\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Load metadata CSV\n",
        "df = pd.read_csv(input_csv, sep=\"|\")\n",
        "assert len(df) == 5, \"Expected 5 samples in dev.csv\"\n",
        "\n",
        "# Read skeleton lines\n",
        "with open(input_skels, \"r\") as f:\n",
        "    skel_lines = [line.strip() for line in f.readlines()]\n",
        "\n",
        "assert len(skel_lines) == len(df), \"Mismatch between CSV and skels line count\"\n",
        "\n",
        "samples = []\n",
        "\n",
        "for idx, (line, meta) in enumerate(zip(skel_lines, df.itertuples())):\n",
        "    raw = list(map(float, line.split()))\n",
        "    assert len(raw) % (feature_dim + 1) == 0, f\"Line {idx} isn't multiple of {feature_dim+1}\"\n",
        "\n",
        "    num_frames = len(raw) // (feature_dim + 1)\n",
        "    frames = []\n",
        "    for i in range(num_frames):\n",
        "        frame_vec = raw[i * (feature_dim + 1) : i * (feature_dim + 1) + feature_dim]\n",
        "        frames.append(torch.tensor(frame_vec, dtype=torch.float32))\n",
        "\n",
        "    sign_tensor = torch.stack(frames, dim=0)  # [T, 150]\n",
        "\n",
        "    sample = {\n",
        "        \"name\": meta.name,\n",
        "        \"signer\": meta.speaker,\n",
        "        \"sign\": sign_tensor + 1e-8,  # small value added for numerical stability\n",
        "        \"gloss\": meta.orth.strip(),\n",
        "        \"text\": meta.translation.strip(),\n",
        "    }\n",
        "    samples.append(sample)\n",
        "\n",
        "# Save as joblib file in gzip\n",
        "with gzip.open(output_file, \"wb\") as f:\n",
        "    joblib.dump(samples, f)\n",
        "\n",
        "print(f\"Saved {len(samples)} samples to {output_file}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U0Lxs-C2W07I",
        "outputId": "b0ac9d0a-b6b6-4b44-c413-4105621e8b68"
      },
      "id": "U0Lxs-C2W07I",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved 5 samples to /content/drive/MyDrive/Sign-IDD SLT/data/Inference/phoenix14t.infer.fixed.skels.dev\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gzip\n",
        "import joblib\n",
        "import pandas as pd\n",
        "\n",
        "# Config\n",
        "feature_dim = 150\n",
        "input_csv = \"/content/drive/MyDrive/Sign-IDD SLT/data/PHOENIX2014T/Test/test.csv\"\n",
        "input_skels = \"/content/drive/MyDrive/Sign-IDD SLT/data/Inference/test.pred.fixed.skels\"\n",
        "output_dir = \"/content/drive/MyDrive/Sign-IDD SLT/data/Inference\"\n",
        "output_file = os.path.join(output_dir, \"phoenix14t.infer.fixed.skels.test\")\n",
        "\n",
        "# Ensure output directory exists\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Load metadata CSV\n",
        "df = pd.read_csv(input_csv, sep=\"|\")\n",
        "assert len(df) == 5, \"Expected 5 samples in dev.csv\"\n",
        "\n",
        "# Read skeleton lines\n",
        "with open(input_skels, \"r\") as f:\n",
        "    skel_lines = [line.strip() for line in f.readlines()]\n",
        "\n",
        "assert len(skel_lines) == len(df), \"Mismatch between CSV and skels line count\"\n",
        "\n",
        "samples = []\n",
        "\n",
        "for idx, (line, meta) in enumerate(zip(skel_lines, df.itertuples())):\n",
        "    raw = list(map(float, line.split()))\n",
        "    assert len(raw) % (feature_dim + 1) == 0, f\"Line {idx} isn't multiple of {feature_dim+1}\"\n",
        "\n",
        "    num_frames = len(raw) // (feature_dim + 1)\n",
        "    frames = []\n",
        "    for i in range(num_frames):\n",
        "        frame_vec = raw[i * (feature_dim + 1) : i * (feature_dim + 1) + feature_dim]\n",
        "        frames.append(torch.tensor(frame_vec, dtype=torch.float32))\n",
        "\n",
        "    sign_tensor = torch.stack(frames, dim=0)  # [T, 150]\n",
        "\n",
        "    sample = {\n",
        "        \"name\": meta.name,\n",
        "        \"signer\": meta.speaker,\n",
        "        \"sign\": sign_tensor + 1e-8,  # small value added for numerical stability\n",
        "        \"gloss\": meta.orth.strip(),\n",
        "        \"text\": meta.translation.strip(),\n",
        "    }\n",
        "    samples.append(sample)\n",
        "\n",
        "# Save as joblib file in gzip\n",
        "with gzip.open(output_file, \"wb\") as f:\n",
        "    joblib.dump(samples, f)\n",
        "\n",
        "print(f\"Saved {len(samples)} samples to {output_file}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZvKvtIWaW_dB",
        "outputId": "62cc9d1f-aae1-49eb-ef41-d2f14d788fec"
      },
      "id": "ZvKvtIWaW_dB",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved 5 samples to /content/drive/MyDrive/Sign-IDD SLT/data/Inference/phoenix14t.infer.fixed.skels.test\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inference(try again)"
      ],
      "metadata": {
        "id": "xGGk5o1fcygm"
      },
      "id": "xGGk5o1fcygm"
    },
    {
      "cell_type": "code",
      "source": [
        "test(cfg_file='/content/drive/MyDrive/Sign-IDD SLT/configs/fixed_inference.yaml',\n",
        "     ckpt='/content/drive/MyDrive/Sign-IDD SLT/signjoey/sign_skels_model/best.ckpt',\n",
        "     output_path='/content/drive/MyDrive/Sign-IDD SLT/signjoey/sign_skels_model/Inference_output')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "leiJz1zIc0_z",
        "outputId": "e1b8c5e8-9b2c-48df-bd21-010dc1a3d82f"
      },
      "id": "leiJz1zIc0_z",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:prediction:----------------------------------------------------------------------------------------\n",
            "INFO:prediction:[DEV] partition [RECOGNITION] experiment [BW]: 10\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "The size of tensor a (147) must match the size of tensor b (148) at non-singleton dimension 3",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-28-1908909731.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m test(cfg_file='/content/drive/MyDrive/Sign-IDD SLT/configs/fixed_inference.yaml',\n\u001b[0m\u001b[1;32m      2\u001b[0m      \u001b[0mckpt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'/content/drive/MyDrive/Sign-IDD SLT/signjoey/sign_skels_model/best.ckpt'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m      output_path='/content/drive/MyDrive/Sign-IDD SLT/signjoey/sign_skels_model/Inference_output')\n",
            "\u001b[0;32m/content/drive/MyDrive/Sign-IDD SLT/signjoey/prediction.py\u001b[0m in \u001b[0;36mtest\u001b[0;34m(cfg_file, ckpt, output_path, logger)\u001b[0m\n\u001b[1;32m    439\u001b[0m             \u001b[0mvalid_start_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"[DEV] partition [RECOGNITION] experiment [BW]: %d\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrbw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 441\u001b[0;31m             dev_recognition_results[rbw] = validate_on_data(\n\u001b[0m\u001b[1;32m    442\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m                 \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdev_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/MyDrive/Sign-IDD SLT/signjoey/prediction.py\u001b[0m in \u001b[0;36mvalidate_on_data\u001b[0;34m(model, data, ground_data, batch_size, use_cuda, sgn_dim, do_recognition, recognition_loss_function, recognition_loss_weight, do_translation, translation_loss_function, translation_loss_weight, translation_max_output_length, level, txt_pad_index, recognition_beam_size, translation_beam_size, translation_beam_alpha, batch_type, dataset_version, frame_subsampling_ratio, i)\u001b[0m\n\u001b[1;32m    186\u001b[0m                 \u001b[0mbatch_signs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0mbatch_sign_grounds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m             \u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m                 \u001b[0mbatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m                 \u001b[0mbatch_ground\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_ground\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/MyDrive/Sign-IDD SLT/signjoey/model.py\u001b[0m in \u001b[0;36mrun_batch\u001b[0;34m(self, batch, batch_ground, recognition_beam_size, translation_beam_size, translation_beam_alpha, translation_max_output_length)\u001b[0m\n\u001b[1;32m    279\u001b[0m         )\n\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 281\u001b[0;31m         encoder_output1, encoder_hidden = self.encode(\n\u001b[0m\u001b[1;32m    282\u001b[0m             \u001b[0msgn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msgn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msgn_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_ground\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msgn_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msgn_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_ground\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msgn_lengths\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m         )\n",
            "\u001b[0;32m/content/drive/MyDrive/Sign-IDD SLT/signjoey/model.py\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, sgn, sgn_mask, sgn_length)\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0;32mreturn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mencoder\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_concat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m         \"\"\"\n\u001b[0;32m--> 153\u001b[0;31m         return self.encoder(\n\u001b[0m\u001b[1;32m    154\u001b[0m             \u001b[0membed_src\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msgn_embed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msgn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msgn_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0msrc_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msgn_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/MyDrive/Sign-IDD SLT/signjoey/encoders.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, embed_src, src_length, mask)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    240\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/MyDrive/Sign-IDD SLT/signjoey/transformer_layers.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, mask)\u001b[0m\n\u001b[1;32m    204\u001b[0m         \"\"\"\n\u001b[1;32m    205\u001b[0m         \u001b[0mx_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc_src_att\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0mo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/MyDrive/Sign-IDD SLT/signjoey/transformer_layers.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, k, v, q, mask)\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;31m# we add a dimension for the heads to it below: [B, 1, 1, M]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m             \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmasked_fill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"-inf\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;31m# apply attention dropout and compute context vectors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (147) must match the size of tensor b (148) at non-singleton dimension 3"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Wn2eK80afNai"
      },
      "id": "Wn2eK80afNai"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fix again by padding frames"
      ],
      "metadata": {
        "id": "W9E7XC6lfPPF"
      },
      "id": "W9E7XC6lfPPF"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Original (ground truth) dev skels path\n",
        "original_path = \"/content/drive/MyDrive/Sign-IDD SLT/data/PHOENIX2014T/Dev/dev.skels\"\n",
        "# Predicted skels path\n",
        "pred_path = \"/content/drive/MyDrive/Sign-IDD SLT/data/Inference/dev.pred.skels\"\n",
        "# Output fixed prediction file\n",
        "fixed_path = pred_path.replace(\".skels\", \".frame_aligned.skels\")\n",
        "\n",
        "feature_dim = 150\n",
        "total_dim = 151\n",
        "\n",
        "# Read original and predicted\n",
        "with open(original_path, 'r') as f:\n",
        "    original_lines = f.readlines()\n",
        "\n",
        "with open(pred_path, 'r') as f:\n",
        "    pred_lines = f.readlines()\n",
        "\n",
        "assert len(original_lines) == len(pred_lines), \"Mismatch in number of samples.\"\n",
        "\n",
        "fixed_lines = []\n",
        "\n",
        "for i, (orig_line, pred_line) in enumerate(zip(original_lines, pred_lines)):\n",
        "    orig_vals = np.fromstring(orig_line.strip(), sep=' ')\n",
        "    pred_vals = np.fromstring(pred_line.strip(), sep=' ')\n",
        "\n",
        "    orig_frames = len(orig_vals) // total_dim\n",
        "    pred_frames = len(pred_vals) // total_dim\n",
        "\n",
        "    # Reshape prediction and fix counters\n",
        "    pred_data = pred_vals.reshape(pred_frames, -1)\n",
        "    pred_data[:, -1] = np.linspace(0.0, 1.0, pred_frames)  # fix counters\n",
        "\n",
        "    if pred_frames < orig_frames:\n",
        "        # Pad with zeros to match\n",
        "        pad_len = orig_frames - pred_frames\n",
        "        padding = np.zeros((pad_len, total_dim))\n",
        "        pred_data = np.vstack([pred_data, padding])\n",
        "    elif pred_frames > orig_frames:\n",
        "        # Truncate\n",
        "        print(f\"⚠️ Warning: Sample {i+1} has {pred_frames} > {orig_frames} frames, truncating.\")\n",
        "        pred_data = pred_data[:orig_frames]\n",
        "\n",
        "    flat_line = ' '.join(map(str, pred_data.flatten()))\n",
        "    fixed_lines.append(flat_line)\n",
        "\n",
        "# Save\n",
        "with open(fixed_path, 'w') as f:\n",
        "    for line in fixed_lines:\n",
        "        f.write(line + '\\n')\n",
        "\n",
        "print(f\"✅ Frame-aligned predictions saved to: {fixed_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UAaQZT69fS6D",
        "outputId": "889857c6-e818-4602-a9a5-939e23953254"
      },
      "id": "UAaQZT69fS6D",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⚠️ Warning: Sample 1 has 147 > 42 frames, truncating.\n",
            "⚠️ Warning: Sample 2 has 147 > 111 frames, truncating.\n",
            "⚠️ Warning: Sample 4 has 147 > 142 frames, truncating.\n",
            "⚠️ Warning: Sample 5 has 147 > 93 frames, truncating.\n",
            "✅ Frame-aligned predictions saved to: /content/drive/MyDrive/Sign-IDD SLT/data/Inference/dev.pred.frame_aligned.skels\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Original (ground truth) dev skels path\n",
        "original_path = \"/content/drive/MyDrive/Sign-IDD SLT/data/PHOENIX2014T/Test/test.skels\"\n",
        "# Predicted skels path\n",
        "pred_path = \"/content/drive/MyDrive/Sign-IDD SLT/data/Inference/test.pred.skels\"\n",
        "# Output fixed prediction file\n",
        "fixed_path = pred_path.replace(\".skels\", \".frame_aligned.skels\")\n",
        "\n",
        "feature_dim = 150\n",
        "total_dim = 151\n",
        "\n",
        "# Read original and predicted\n",
        "with open(original_path, 'r') as f:\n",
        "    original_lines = f.readlines()\n",
        "\n",
        "with open(pred_path, 'r') as f:\n",
        "    pred_lines = f.readlines()\n",
        "\n",
        "assert len(original_lines) == len(pred_lines), \"Mismatch in number of samples.\"\n",
        "\n",
        "fixed_lines = []\n",
        "\n",
        "for i, (orig_line, pred_line) in enumerate(zip(original_lines, pred_lines)):\n",
        "    orig_vals = np.fromstring(orig_line.strip(), sep=' ')\n",
        "    pred_vals = np.fromstring(pred_line.strip(), sep=' ')\n",
        "\n",
        "    orig_frames = len(orig_vals) // total_dim\n",
        "    pred_frames = len(pred_vals) // total_dim\n",
        "\n",
        "    # Reshape prediction and fix counters\n",
        "    pred_data = pred_vals.reshape(pred_frames, -1)\n",
        "    pred_data[:, -1] = np.linspace(0.0, 1.0, pred_frames)  # fix counters\n",
        "\n",
        "    if pred_frames < orig_frames:\n",
        "        # Pad with zeros to match\n",
        "        pad_len = orig_frames - pred_frames\n",
        "        padding = np.zeros((pad_len, total_dim))\n",
        "        pred_data = np.vstack([pred_data, padding])\n",
        "    elif pred_frames > orig_frames:\n",
        "        # Truncate\n",
        "        print(f\"⚠️ Warning: Sample {i+1} has {pred_frames} > {orig_frames} frames, truncating.\")\n",
        "        pred_data = pred_data[:orig_frames]\n",
        "\n",
        "    flat_line = ' '.join(map(str, pred_data.flatten()))\n",
        "    fixed_lines.append(flat_line)\n",
        "\n",
        "# Save\n",
        "with open(fixed_path, 'w') as f:\n",
        "    for line in fixed_lines:\n",
        "        f.write(line + '\\n')\n",
        "\n",
        "print(f\"✅ Frame-aligned predictions saved to: {fixed_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nsuzcfr4im8b",
        "outputId": "5d33954b-ebb0-480b-9056-ef302e22f7bf"
      },
      "id": "nsuzcfr4im8b",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⚠️ Warning: Sample 1 has 197 > 181 frames, truncating.\n",
            "⚠️ Warning: Sample 2 has 197 > 150 frames, truncating.\n",
            "⚠️ Warning: Sample 4 has 197 > 130 frames, truncating.\n",
            "⚠️ Warning: Sample 5 has 197 > 111 frames, truncating.\n",
            "✅ Frame-aligned predictions saved to: /content/drive/MyDrive/Sign-IDD SLT/data/Inference/test.pred.frame_aligned.skels\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## frame aligned dataset creation"
      ],
      "metadata": {
        "id": "ln-fTY1Ji5ss"
      },
      "id": "ln-fTY1Ji5ss"
    },
    {
      "cell_type": "code",
      "source": [
        "import gzip\n",
        "import joblib\n",
        "import pandas as pd\n",
        "\n",
        "# Config\n",
        "feature_dim = 150\n",
        "input_csv = \"/content/drive/MyDrive/Sign-IDD SLT/data/PHOENIX2014T/Dev/dev.csv\"\n",
        "input_skels = \"/content/drive/MyDrive/Sign-IDD SLT/data/Inference/dev.pred.frame_aligned.skels\"\n",
        "output_dir = \"/content/drive/MyDrive/Sign-IDD SLT/data/Inference\"\n",
        "output_file = os.path.join(output_dir, \"phoenix14t.infer.fixed2.skels.dev\")\n",
        "\n",
        "# Ensure output directory exists\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Load metadata CSV\n",
        "df = pd.read_csv(input_csv, sep=\"|\")\n",
        "assert len(df) == 5, \"Expected 5 samples in dev.csv\"\n",
        "\n",
        "# Read skeleton lines\n",
        "with open(input_skels, \"r\") as f:\n",
        "    skel_lines = [line.strip() for line in f.readlines()]\n",
        "\n",
        "assert len(skel_lines) == len(df), \"Mismatch between CSV and skels line count\"\n",
        "\n",
        "samples = []\n",
        "\n",
        "for idx, (line, meta) in enumerate(zip(skel_lines, df.itertuples())):\n",
        "    raw = list(map(float, line.split()))\n",
        "    assert len(raw) % (feature_dim + 1) == 0, f\"Line {idx} isn't multiple of {feature_dim+1}\"\n",
        "\n",
        "    num_frames = len(raw) // (feature_dim + 1)\n",
        "    frames = []\n",
        "    for i in range(num_frames):\n",
        "        frame_vec = raw[i * (feature_dim + 1) : i * (feature_dim + 1) + feature_dim]\n",
        "        frames.append(torch.tensor(frame_vec, dtype=torch.float32))\n",
        "\n",
        "    sign_tensor = torch.stack(frames, dim=0)  # [T, 150]\n",
        "\n",
        "    sample = {\n",
        "        \"name\": meta.name,\n",
        "        \"signer\": meta.speaker,\n",
        "        \"sign\": sign_tensor + 1e-8,  # small value added for numerical stability\n",
        "        \"gloss\": meta.orth.strip(),\n",
        "        \"text\": meta.translation.strip(),\n",
        "    }\n",
        "    samples.append(sample)\n",
        "\n",
        "# Save as joblib file in gzip\n",
        "with gzip.open(output_file, \"wb\") as f:\n",
        "    joblib.dump(samples, f)\n",
        "\n",
        "print(f\"Saved {len(samples)} samples to {output_file}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f1n-eYw1i_jV",
        "outputId": "dce6cf41-7610-4f9b-a13a-7a6735fccf2a"
      },
      "id": "f1n-eYw1i_jV",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved 5 samples to /content/drive/MyDrive/Sign-IDD SLT/data/Inference/phoenix14t.infer.fixed2.skels.dev\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gzip\n",
        "import joblib\n",
        "import pandas as pd\n",
        "\n",
        "# Config\n",
        "feature_dim = 150\n",
        "input_csv = \"/content/drive/MyDrive/Sign-IDD SLT/data/PHOENIX2014T/Test/test.csv\"\n",
        "input_skels = \"/content/drive/MyDrive/Sign-IDD SLT/data/Inference/test.pred.frame_aligned.skels\"\n",
        "output_dir = \"/content/drive/MyDrive/Sign-IDD SLT/data/Inference\"\n",
        "output_file = os.path.join(output_dir, \"phoenix14t.infer.fixed2.skels.test\")\n",
        "\n",
        "# Ensure output directory exists\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Load metadata CSV\n",
        "df = pd.read_csv(input_csv, sep=\"|\")\n",
        "assert len(df) == 5, \"Expected 5 samples in dev.csv\"\n",
        "\n",
        "# Read skeleton lines\n",
        "with open(input_skels, \"r\") as f:\n",
        "    skel_lines = [line.strip() for line in f.readlines()]\n",
        "\n",
        "assert len(skel_lines) == len(df), \"Mismatch between CSV and skels line count\"\n",
        "\n",
        "samples = []\n",
        "\n",
        "for idx, (line, meta) in enumerate(zip(skel_lines, df.itertuples())):\n",
        "    raw = list(map(float, line.split()))\n",
        "    assert len(raw) % (feature_dim + 1) == 0, f\"Line {idx} isn't multiple of {feature_dim+1}\"\n",
        "\n",
        "    num_frames = len(raw) // (feature_dim + 1)\n",
        "    frames = []\n",
        "    for i in range(num_frames):\n",
        "        frame_vec = raw[i * (feature_dim + 1) : i * (feature_dim + 1) + feature_dim]\n",
        "        frames.append(torch.tensor(frame_vec, dtype=torch.float32))\n",
        "\n",
        "    sign_tensor = torch.stack(frames, dim=0)  # [T, 150]\n",
        "\n",
        "    sample = {\n",
        "        \"name\": meta.name,\n",
        "        \"signer\": meta.speaker,\n",
        "        \"sign\": sign_tensor + 1e-8,  # small value added for numerical stability\n",
        "        \"gloss\": meta.orth.strip(),\n",
        "        \"text\": meta.translation.strip(),\n",
        "    }\n",
        "    samples.append(sample)\n",
        "\n",
        "# Save as joblib file in gzip\n",
        "with gzip.open(output_file, \"wb\") as f:\n",
        "    joblib.dump(samples, f)\n",
        "\n",
        "print(f\"Saved {len(samples)} samples to {output_file}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C4yR9c-njduX",
        "outputId": "17105267-8285-4e1e-bbfe-f9ebed5c4a28"
      },
      "id": "C4yR9c-njduX",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved 5 samples to /content/drive/MyDrive/Sign-IDD SLT/data/Inference/phoenix14t.infer.fixed2.skels.test\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference (AGAIN)"
      ],
      "metadata": {
        "id": "x0xb5nIwj_S6"
      },
      "id": "x0xb5nIwj_S6"
    },
    {
      "cell_type": "code",
      "source": [
        "test(cfg_file='/content/drive/MyDrive/Sign-IDD SLT/configs/frame_inference.yaml',\n",
        "     ckpt='/content/drive/MyDrive/Sign-IDD SLT/signjoey/sign_skels_model/best.ckpt',\n",
        "     output_path='/content/drive/MyDrive/Sign-IDD SLT/signjoey/sign_skels_model/Inference_output')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vUtdhQjxkB3Y",
        "outputId": "9e0b5d0a-39c2-4ece-bbea-ffbab2b6d8b2"
      },
      "id": "vUtdhQjxkB3Y",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:prediction:----------------------------------------------------------------------------------------\n",
            "INFO:prediction:[DEV] partition [RECOGNITION] experiment [BW]: 10\n",
            "INFO:prediction:finished in 5.7053s \n",
            "INFO:prediction:****************************************************************************************\n",
            "INFO:prediction:[DEV] partition [RECOGNITION] results:\n",
            "\tNew Best CTC Decode Beam Size: 10\n",
            "\tWER 88.57\t(DEL: 20.00,\tINS: 2.86,\tSUB: 65.71)\n",
            "INFO:prediction:****************************************************************************************\n",
            "INFO:prediction:========================================================================================\n",
            "INFO:prediction:[DEV] partition [Translation] results:\n",
            "\tNew Best Translation Beam Size: 10 and Alpha: -1\n",
            "\tBLEU-4 2.89\t(BLEU-1: 8.00,\tBLEU-2: 4.70,\tBLEU-3: 3.62,\tBLEU-4: 2.89)\n",
            "\tCHRF 24.19\tROUGE 12.39\tFID 0.73\tMPJPE 0.50\tMPVPE 0.00\tMPJAE 33.69\t\n",
            "INFO:prediction:----------------------------------------------------------------------------------------\n",
            "INFO:prediction:****************************************************************************************\n",
            "INFO:prediction:[DEV] partition [Recognition & Translation] results:\n",
            "\tBest CTC Decode Beam Size: 10\n",
            "\tBest Translation Beam Size: 10 and Alpha: -1\n",
            "\tWER 88.57\t(DEL: 20.00,\tINS: 2.86,\tSUB: 65.71)\n",
            "\tBLEU-4 2.89\t(BLEU-1: 8.00,\tBLEU-2: 4.70,\tBLEU-3: 3.62,\tBLEU-4: 2.89)\n",
            "\tCHRF 24.19\tROUGE 12.39\tFID 0.73\tMPJPE 0.50\tMPVPE 0.00\tMPJAE 33.69\n",
            "INFO:prediction:****************************************************************************************\n",
            "INFO:prediction:[TEST] partition [Recognition & Translation] results:\n",
            "\tBest CTC Decode Beam Size: 10\n",
            "\tBest Translation Beam Size: 10 and Alpha: -1\n",
            "\tWER 139.58\t(DEL: 14.58,\tINS: 45.83,\tSUB: 79.17)\n",
            "\tBLEU-4 0.00\t(BLEU-1: 4.00,\tBLEU-2: 0.00,\tBLEU-3: 0.00,\tBLEU-4: 0.00)\n",
            "\tCHRF 16.29\tROUGE 4.14\tFID 0.53\tMPJPE 0.40\tMPVPE 0.00\tMPJAE 30.11\n",
            "INFO:prediction:****************************************************************************************\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python (cv2env)",
      "language": "python",
      "name": "cv2env"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "-NaBxf1TxtkK",
        "-CLsu8vJzNtF",
        "ZwYNTYvKWwwN",
        "xGGk5o1fcygm",
        "W9E7XC6lfPPF",
        "ln-fTY1Ji5ss",
        "x0xb5nIwj_S6"
      ]
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}